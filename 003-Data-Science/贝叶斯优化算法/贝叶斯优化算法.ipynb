{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假设有一个 CSV 文件，包含配方参数和对应的发酵产量\n",
    "data = pd.read_table('fermentation_data.csv')\n",
    "\n",
    "\n",
    "# 将数据分为输入 (X) 和输出 (y)\n",
    "X = data[['carbon_source', 'nitrogen_source', 'pH', 'temperature', 'stirring_speed', 'aeration_rate']].values\n",
    "y = data['yield'].values  # 发酵产量\n",
    "\n",
    "# 标准化输入数据\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 拆分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 55901.7930\n",
      "Epoch [20/10000], Loss: 55783.1445\n",
      "Epoch [30/10000], Loss: 55606.1094\n",
      "Epoch [40/10000], Loss: 55316.9102\n",
      "Epoch [50/10000], Loss: 54889.9531\n",
      "Epoch [60/10000], Loss: 54283.6211\n",
      "Epoch [70/10000], Loss: 53450.5039\n",
      "Epoch [80/10000], Loss: 52340.4336\n",
      "Epoch [90/10000], Loss: 50905.5391\n",
      "Epoch [100/10000], Loss: 49107.5273\n",
      "Epoch [110/10000], Loss: 46920.3125\n",
      "Epoch [120/10000], Loss: 44336.3477\n",
      "Epoch [130/10000], Loss: 41370.9102\n",
      "Epoch [140/10000], Loss: 38065.9766\n",
      "Epoch [150/10000], Loss: 34492.8477\n",
      "Epoch [160/10000], Loss: 30752.5762\n",
      "Epoch [170/10000], Loss: 26972.6426\n",
      "Epoch [180/10000], Loss: 23299.1309\n",
      "Epoch [190/10000], Loss: 19884.6738\n",
      "Epoch [200/10000], Loss: 16869.2461\n",
      "Epoch [210/10000], Loss: 14358.5576\n",
      "Epoch [220/10000], Loss: 12403.9814\n",
      "Epoch [230/10000], Loss: 10991.4492\n",
      "Epoch [240/10000], Loss: 10046.3311\n",
      "Epoch [250/10000], Loss: 9456.2266\n",
      "Epoch [260/10000], Loss: 9102.1826\n",
      "Epoch [270/10000], Loss: 8884.4414\n",
      "Epoch [280/10000], Loss: 8734.4707\n",
      "Epoch [290/10000], Loss: 8612.9287\n",
      "Epoch [300/10000], Loss: 8501.0596\n",
      "Epoch [310/10000], Loss: 8390.6172\n",
      "Epoch [320/10000], Loss: 8278.5918\n",
      "Epoch [330/10000], Loss: 8163.7388\n",
      "Epoch [340/10000], Loss: 8045.9795\n",
      "Epoch [350/10000], Loss: 7924.7070\n",
      "Epoch [360/10000], Loss: 7799.3213\n",
      "Epoch [370/10000], Loss: 7669.5615\n",
      "Epoch [380/10000], Loss: 7535.3550\n",
      "Epoch [390/10000], Loss: 7396.3970\n",
      "Epoch [400/10000], Loss: 7252.6348\n",
      "Epoch [410/10000], Loss: 7103.5527\n",
      "Epoch [420/10000], Loss: 6948.4600\n",
      "Epoch [430/10000], Loss: 6787.0801\n",
      "Epoch [440/10000], Loss: 6618.4102\n",
      "Epoch [450/10000], Loss: 6441.7700\n",
      "Epoch [460/10000], Loss: 6256.6807\n",
      "Epoch [470/10000], Loss: 6063.6699\n",
      "Epoch [480/10000], Loss: 5861.9741\n",
      "Epoch [490/10000], Loss: 5654.1113\n",
      "Epoch [500/10000], Loss: 5441.5200\n",
      "Epoch [510/10000], Loss: 5223.6377\n",
      "Epoch [520/10000], Loss: 5000.4746\n",
      "Epoch [530/10000], Loss: 4770.2808\n",
      "Epoch [540/10000], Loss: 4531.6743\n",
      "Epoch [550/10000], Loss: 4286.4849\n",
      "Epoch [560/10000], Loss: 4036.3833\n",
      "Epoch [570/10000], Loss: 3781.6008\n",
      "Epoch [580/10000], Loss: 3524.3818\n",
      "Epoch [590/10000], Loss: 3268.8518\n",
      "Epoch [600/10000], Loss: 3020.4771\n",
      "Epoch [610/10000], Loss: 2783.1038\n",
      "Epoch [620/10000], Loss: 2561.2078\n",
      "Epoch [630/10000], Loss: 2357.9614\n",
      "Epoch [640/10000], Loss: 2173.2114\n",
      "Epoch [650/10000], Loss: 2003.8542\n",
      "Epoch [660/10000], Loss: 1847.4799\n",
      "Epoch [670/10000], Loss: 1699.2944\n",
      "Epoch [680/10000], Loss: 1556.6053\n",
      "Epoch [690/10000], Loss: 1419.8927\n",
      "Epoch [700/10000], Loss: 1289.7439\n",
      "Epoch [710/10000], Loss: 1166.6012\n",
      "Epoch [720/10000], Loss: 1051.5717\n",
      "Epoch [730/10000], Loss: 945.3256\n",
      "Epoch [740/10000], Loss: 847.1843\n",
      "Epoch [750/10000], Loss: 756.6447\n",
      "Epoch [760/10000], Loss: 672.8776\n",
      "Epoch [770/10000], Loss: 595.3402\n",
      "Epoch [780/10000], Loss: 524.0538\n",
      "Epoch [790/10000], Loss: 459.4496\n",
      "Epoch [800/10000], Loss: 401.5834\n",
      "Epoch [810/10000], Loss: 349.8188\n",
      "Epoch [820/10000], Loss: 303.4236\n",
      "Epoch [830/10000], Loss: 261.9626\n",
      "Epoch [840/10000], Loss: 225.1718\n",
      "Epoch [850/10000], Loss: 192.7364\n",
      "Epoch [860/10000], Loss: 164.2938\n",
      "Epoch [870/10000], Loss: 139.4851\n",
      "Epoch [880/10000], Loss: 117.9232\n",
      "Epoch [890/10000], Loss: 99.3596\n",
      "Epoch [900/10000], Loss: 83.4837\n",
      "Epoch [910/10000], Loss: 69.9973\n",
      "Epoch [920/10000], Loss: 58.6122\n",
      "Epoch [930/10000], Loss: 49.0250\n",
      "Epoch [940/10000], Loss: 40.9903\n",
      "Epoch [950/10000], Loss: 34.2888\n",
      "Epoch [960/10000], Loss: 28.7545\n",
      "Epoch [970/10000], Loss: 24.2040\n",
      "Epoch [980/10000], Loss: 20.4828\n",
      "Epoch [990/10000], Loss: 17.4493\n",
      "Epoch [1000/10000], Loss: 14.9817\n",
      "Epoch [1010/10000], Loss: 12.9827\n",
      "Epoch [1020/10000], Loss: 11.3553\n",
      "Epoch [1030/10000], Loss: 10.0222\n",
      "Epoch [1040/10000], Loss: 8.9161\n",
      "Epoch [1050/10000], Loss: 7.9868\n",
      "Epoch [1060/10000], Loss: 7.2003\n",
      "Epoch [1070/10000], Loss: 6.5239\n",
      "Epoch [1080/10000], Loss: 5.9330\n",
      "Epoch [1090/10000], Loss: 5.4149\n",
      "Epoch [1100/10000], Loss: 4.9578\n",
      "Epoch [1110/10000], Loss: 4.5494\n",
      "Epoch [1120/10000], Loss: 4.1824\n",
      "Epoch [1130/10000], Loss: 3.8535\n",
      "Epoch [1140/10000], Loss: 3.5560\n",
      "Epoch [1150/10000], Loss: 3.2843\n",
      "Epoch [1160/10000], Loss: 3.0362\n",
      "Epoch [1170/10000], Loss: 2.8100\n",
      "Epoch [1180/10000], Loss: 2.6035\n",
      "Epoch [1190/10000], Loss: 2.4142\n",
      "Epoch [1200/10000], Loss: 2.2406\n",
      "Epoch [1210/10000], Loss: 2.0818\n",
      "Epoch [1220/10000], Loss: 1.9370\n",
      "Epoch [1230/10000], Loss: 1.8049\n",
      "Epoch [1240/10000], Loss: 1.6838\n",
      "Epoch [1250/10000], Loss: 1.5715\n",
      "Epoch [1260/10000], Loss: 1.4685\n",
      "Epoch [1270/10000], Loss: 1.3732\n",
      "Epoch [1280/10000], Loss: 1.2844\n",
      "Epoch [1290/10000], Loss: 1.2025\n",
      "Epoch [1300/10000], Loss: 1.1265\n",
      "Epoch [1310/10000], Loss: 1.0550\n",
      "Epoch [1320/10000], Loss: 0.9887\n",
      "Epoch [1330/10000], Loss: 0.9276\n",
      "Epoch [1340/10000], Loss: 0.8712\n",
      "Epoch [1350/10000], Loss: 0.8192\n",
      "Epoch [1360/10000], Loss: 0.7712\n",
      "Epoch [1370/10000], Loss: 0.7262\n",
      "Epoch [1380/10000], Loss: 0.6838\n",
      "Epoch [1390/10000], Loss: 0.6443\n",
      "Epoch [1400/10000], Loss: 0.6078\n",
      "Epoch [1410/10000], Loss: 0.5729\n",
      "Epoch [1420/10000], Loss: 0.5401\n",
      "Epoch [1430/10000], Loss: 0.5095\n",
      "Epoch [1440/10000], Loss: 0.4812\n",
      "Epoch [1450/10000], Loss: 0.4549\n",
      "Epoch [1460/10000], Loss: 0.4304\n",
      "Epoch [1470/10000], Loss: 0.4076\n",
      "Epoch [1480/10000], Loss: 0.3863\n",
      "Epoch [1490/10000], Loss: 0.3666\n",
      "Epoch [1500/10000], Loss: 0.3482\n",
      "Epoch [1510/10000], Loss: 0.3311\n",
      "Epoch [1520/10000], Loss: 0.3151\n",
      "Epoch [1530/10000], Loss: 0.3002\n",
      "Epoch [1540/10000], Loss: 0.2863\n",
      "Epoch [1550/10000], Loss: 0.2732\n",
      "Epoch [1560/10000], Loss: 0.2604\n",
      "Epoch [1570/10000], Loss: 0.2484\n",
      "Epoch [1580/10000], Loss: 0.2370\n",
      "Epoch [1590/10000], Loss: 0.2264\n",
      "Epoch [1600/10000], Loss: 0.2166\n",
      "Epoch [1610/10000], Loss: 0.2073\n",
      "Epoch [1620/10000], Loss: 0.1985\n",
      "Epoch [1630/10000], Loss: 0.1899\n",
      "Epoch [1640/10000], Loss: 0.1818\n",
      "Epoch [1650/10000], Loss: 0.1741\n",
      "Epoch [1660/10000], Loss: 0.1670\n",
      "Epoch [1670/10000], Loss: 0.1604\n",
      "Epoch [1680/10000], Loss: 0.1542\n",
      "Epoch [1690/10000], Loss: 0.1485\n",
      "Epoch [1700/10000], Loss: 0.1429\n",
      "Epoch [1710/10000], Loss: 0.1374\n",
      "Epoch [1720/10000], Loss: 0.1322\n",
      "Epoch [1730/10000], Loss: 0.1274\n",
      "Epoch [1740/10000], Loss: 0.1230\n",
      "Epoch [1750/10000], Loss: 0.1188\n",
      "Epoch [1760/10000], Loss: 0.1149\n",
      "Epoch [1770/10000], Loss: 0.1112\n",
      "Epoch [1780/10000], Loss: 0.1078\n",
      "Epoch [1790/10000], Loss: 0.1046\n",
      "Epoch [1800/10000], Loss: 0.1015\n",
      "Epoch [1810/10000], Loss: 0.0984\n",
      "Epoch [1820/10000], Loss: 0.0954\n",
      "Epoch [1830/10000], Loss: 0.0926\n",
      "Epoch [1840/10000], Loss: 0.0900\n",
      "Epoch [1850/10000], Loss: 0.0876\n",
      "Epoch [1860/10000], Loss: 0.0853\n",
      "Epoch [1870/10000], Loss: 0.0832\n",
      "Epoch [1880/10000], Loss: 0.0812\n",
      "Epoch [1890/10000], Loss: 0.0794\n",
      "Epoch [1900/10000], Loss: 0.0776\n",
      "Epoch [1910/10000], Loss: 0.0760\n",
      "Epoch [1920/10000], Loss: 0.0745\n",
      "Epoch [1930/10000], Loss: 0.0730\n",
      "Epoch [1940/10000], Loss: 0.0717\n",
      "Epoch [1950/10000], Loss: 0.0702\n",
      "Epoch [1960/10000], Loss: 0.0688\n",
      "Epoch [1970/10000], Loss: 0.0674\n",
      "Epoch [1980/10000], Loss: 0.0661\n",
      "Epoch [1990/10000], Loss: 0.0649\n",
      "Epoch [2000/10000], Loss: 0.0637\n",
      "Epoch [2010/10000], Loss: 0.0626\n",
      "Epoch [2020/10000], Loss: 0.0616\n",
      "Epoch [2030/10000], Loss: 0.0606\n",
      "Epoch [2040/10000], Loss: 0.0597\n",
      "Epoch [2050/10000], Loss: 0.0588\n",
      "Epoch [2060/10000], Loss: 0.0580\n",
      "Epoch [2070/10000], Loss: 0.0572\n",
      "Epoch [2080/10000], Loss: 0.0565\n",
      "Epoch [2090/10000], Loss: 0.0558\n",
      "Epoch [2100/10000], Loss: 0.0552\n",
      "Epoch [2110/10000], Loss: 0.0545\n",
      "Epoch [2120/10000], Loss: 0.0539\n",
      "Epoch [2130/10000], Loss: 0.0534\n",
      "Epoch [2140/10000], Loss: 0.0528\n",
      "Epoch [2150/10000], Loss: 0.0523\n",
      "Epoch [2160/10000], Loss: 0.0518\n",
      "Epoch [2170/10000], Loss: 0.0513\n",
      "Epoch [2180/10000], Loss: 0.0508\n",
      "Epoch [2190/10000], Loss: 0.0504\n",
      "Epoch [2200/10000], Loss: 0.0500\n",
      "Epoch [2210/10000], Loss: 0.0495\n",
      "Epoch [2220/10000], Loss: 0.0491\n",
      "Epoch [2230/10000], Loss: 0.0487\n",
      "Epoch [2240/10000], Loss: 0.0484\n",
      "Epoch [2250/10000], Loss: 0.0480\n",
      "Epoch [2260/10000], Loss: 0.0476\n",
      "Epoch [2270/10000], Loss: 0.0472\n",
      "Epoch [2280/10000], Loss: 0.0469\n",
      "Epoch [2290/10000], Loss: 0.0465\n",
      "Epoch [2300/10000], Loss: 0.0462\n",
      "Epoch [2310/10000], Loss: 0.0458\n",
      "Epoch [2320/10000], Loss: 0.0455\n",
      "Epoch [2330/10000], Loss: 0.0452\n",
      "Epoch [2340/10000], Loss: 0.0449\n",
      "Epoch [2350/10000], Loss: 0.0446\n",
      "Epoch [2360/10000], Loss: 0.0443\n",
      "Epoch [2370/10000], Loss: 0.0440\n",
      "Epoch [2380/10000], Loss: 0.0437\n",
      "Epoch [2390/10000], Loss: 0.0434\n",
      "Epoch [2400/10000], Loss: 0.0431\n",
      "Epoch [2410/10000], Loss: 0.0428\n",
      "Epoch [2420/10000], Loss: 0.0426\n",
      "Epoch [2430/10000], Loss: 0.0423\n",
      "Epoch [2440/10000], Loss: 0.0420\n",
      "Epoch [2450/10000], Loss: 0.0418\n",
      "Epoch [2460/10000], Loss: 0.0415\n",
      "Epoch [2470/10000], Loss: 0.0413\n",
      "Epoch [2480/10000], Loss: 0.0410\n",
      "Epoch [2490/10000], Loss: 0.0408\n",
      "Epoch [2500/10000], Loss: 0.0405\n",
      "Epoch [2510/10000], Loss: 0.0403\n",
      "Epoch [2520/10000], Loss: 0.0400\n",
      "Epoch [2530/10000], Loss: 0.0398\n",
      "Epoch [2540/10000], Loss: 0.0396\n",
      "Epoch [2550/10000], Loss: 0.0393\n",
      "Epoch [2560/10000], Loss: 0.0391\n",
      "Epoch [2570/10000], Loss: 0.0389\n",
      "Epoch [2580/10000], Loss: 0.0387\n",
      "Epoch [2590/10000], Loss: 0.0384\n",
      "Epoch [2600/10000], Loss: 0.0382\n",
      "Epoch [2610/10000], Loss: 0.0380\n",
      "Epoch [2620/10000], Loss: 0.0378\n",
      "Epoch [2630/10000], Loss: 0.0376\n",
      "Epoch [2640/10000], Loss: 0.0374\n",
      "Epoch [2650/10000], Loss: 0.0372\n",
      "Epoch [2660/10000], Loss: 0.0370\n",
      "Epoch [2670/10000], Loss: 0.0367\n",
      "Epoch [2680/10000], Loss: 0.0365\n",
      "Epoch [2690/10000], Loss: 0.0363\n",
      "Epoch [2700/10000], Loss: 0.0361\n",
      "Epoch [2710/10000], Loss: 0.0360\n",
      "Epoch [2720/10000], Loss: 0.0358\n",
      "Epoch [2730/10000], Loss: 0.0356\n",
      "Epoch [2740/10000], Loss: 0.0354\n",
      "Epoch [2750/10000], Loss: 0.0352\n",
      "Epoch [2760/10000], Loss: 0.0350\n",
      "Epoch [2770/10000], Loss: 0.0348\n",
      "Epoch [2780/10000], Loss: 0.0346\n",
      "Epoch [2790/10000], Loss: 0.0345\n",
      "Epoch [2800/10000], Loss: 0.0343\n",
      "Epoch [2810/10000], Loss: 0.0341\n",
      "Epoch [2820/10000], Loss: 0.0339\n",
      "Epoch [2830/10000], Loss: 0.0337\n",
      "Epoch [2840/10000], Loss: 0.0336\n",
      "Epoch [2850/10000], Loss: 0.0334\n",
      "Epoch [2860/10000], Loss: 0.0332\n",
      "Epoch [2870/10000], Loss: 0.0331\n",
      "Epoch [2880/10000], Loss: 0.0329\n",
      "Epoch [2890/10000], Loss: 0.0327\n",
      "Epoch [2900/10000], Loss: 0.0325\n",
      "Epoch [2910/10000], Loss: 0.0324\n",
      "Epoch [2920/10000], Loss: 0.0322\n",
      "Epoch [2930/10000], Loss: 0.0321\n",
      "Epoch [2940/10000], Loss: 0.0319\n",
      "Epoch [2950/10000], Loss: 0.0317\n",
      "Epoch [2960/10000], Loss: 0.0316\n",
      "Epoch [2970/10000], Loss: 0.0314\n",
      "Epoch [2980/10000], Loss: 0.0312\n",
      "Epoch [2990/10000], Loss: 0.0311\n",
      "Epoch [3000/10000], Loss: 0.0309\n",
      "Epoch [3010/10000], Loss: 0.0308\n",
      "Epoch [3020/10000], Loss: 0.0306\n",
      "Epoch [3030/10000], Loss: 0.0305\n",
      "Epoch [3040/10000], Loss: 0.0303\n",
      "Epoch [3050/10000], Loss: 0.0302\n",
      "Epoch [3060/10000], Loss: 0.0300\n",
      "Epoch [3070/10000], Loss: 0.0299\n",
      "Epoch [3080/10000], Loss: 0.0297\n",
      "Epoch [3090/10000], Loss: 0.0296\n",
      "Epoch [3100/10000], Loss: 0.0294\n",
      "Epoch [3110/10000], Loss: 0.0293\n",
      "Epoch [3120/10000], Loss: 0.0291\n",
      "Epoch [3130/10000], Loss: 0.0290\n",
      "Epoch [3140/10000], Loss: 0.0288\n",
      "Epoch [3150/10000], Loss: 0.0287\n",
      "Epoch [3160/10000], Loss: 0.0285\n",
      "Epoch [3170/10000], Loss: 0.0284\n",
      "Epoch [3180/10000], Loss: 0.0283\n",
      "Epoch [3190/10000], Loss: 0.0281\n",
      "Epoch [3200/10000], Loss: 0.0280\n",
      "Epoch [3210/10000], Loss: 0.0278\n",
      "Epoch [3220/10000], Loss: 0.0277\n",
      "Epoch [3230/10000], Loss: 0.0276\n",
      "Epoch [3240/10000], Loss: 0.0274\n",
      "Epoch [3250/10000], Loss: 0.0273\n",
      "Epoch [3260/10000], Loss: 0.0271\n",
      "Epoch [3270/10000], Loss: 0.0270\n",
      "Epoch [3280/10000], Loss: 0.0269\n",
      "Epoch [3290/10000], Loss: 0.0267\n",
      "Epoch [3300/10000], Loss: 0.0266\n",
      "Epoch [3310/10000], Loss: 0.0265\n",
      "Epoch [3320/10000], Loss: 0.0263\n",
      "Epoch [3330/10000], Loss: 0.0262\n",
      "Epoch [3340/10000], Loss: 0.0261\n",
      "Epoch [3350/10000], Loss: 0.0259\n",
      "Epoch [3360/10000], Loss: 0.0258\n",
      "Epoch [3370/10000], Loss: 0.0257\n",
      "Epoch [3380/10000], Loss: 0.0255\n",
      "Epoch [3390/10000], Loss: 0.0254\n",
      "Epoch [3400/10000], Loss: 0.0253\n",
      "Epoch [3410/10000], Loss: 0.0251\n",
      "Epoch [3420/10000], Loss: 0.0250\n",
      "Epoch [3430/10000], Loss: 0.0249\n",
      "Epoch [3440/10000], Loss: 0.0247\n",
      "Epoch [3450/10000], Loss: 0.0246\n",
      "Epoch [3460/10000], Loss: 0.0245\n",
      "Epoch [3470/10000], Loss: 0.0244\n",
      "Epoch [3480/10000], Loss: 0.0242\n",
      "Epoch [3490/10000], Loss: 0.0241\n",
      "Epoch [3500/10000], Loss: 0.0240\n",
      "Epoch [3510/10000], Loss: 0.0239\n",
      "Epoch [3520/10000], Loss: 0.0237\n",
      "Epoch [3530/10000], Loss: 0.0236\n",
      "Epoch [3540/10000], Loss: 0.0235\n",
      "Epoch [3550/10000], Loss: 0.0234\n",
      "Epoch [3560/10000], Loss: 0.0233\n",
      "Epoch [3570/10000], Loss: 0.0231\n",
      "Epoch [3580/10000], Loss: 0.0230\n",
      "Epoch [3590/10000], Loss: 0.0229\n",
      "Epoch [3600/10000], Loss: 0.0228\n",
      "Epoch [3610/10000], Loss: 0.0227\n",
      "Epoch [3620/10000], Loss: 0.0225\n",
      "Epoch [3630/10000], Loss: 0.0224\n",
      "Epoch [3640/10000], Loss: 0.0223\n",
      "Epoch [3650/10000], Loss: 0.0222\n",
      "Epoch [3660/10000], Loss: 0.0221\n",
      "Epoch [3670/10000], Loss: 0.0220\n",
      "Epoch [3680/10000], Loss: 0.0218\n",
      "Epoch [3690/10000], Loss: 0.0217\n",
      "Epoch [3700/10000], Loss: 0.0216\n",
      "Epoch [3710/10000], Loss: 0.0215\n",
      "Epoch [3720/10000], Loss: 0.0214\n",
      "Epoch [3730/10000], Loss: 0.0213\n",
      "Epoch [3740/10000], Loss: 0.0212\n",
      "Epoch [3750/10000], Loss: 0.0211\n",
      "Epoch [3760/10000], Loss: 0.0209\n",
      "Epoch [3770/10000], Loss: 0.0208\n",
      "Epoch [3780/10000], Loss: 0.0207\n",
      "Epoch [3790/10000], Loss: 0.0206\n",
      "Epoch [3800/10000], Loss: 0.0205\n",
      "Epoch [3810/10000], Loss: 0.0204\n",
      "Epoch [3820/10000], Loss: 0.0203\n",
      "Epoch [3830/10000], Loss: 0.0202\n",
      "Epoch [3840/10000], Loss: 0.0201\n",
      "Epoch [3850/10000], Loss: 0.0200\n",
      "Epoch [3860/10000], Loss: 0.0199\n",
      "Epoch [3870/10000], Loss: 0.0198\n",
      "Epoch [3880/10000], Loss: 0.0196\n",
      "Epoch [3890/10000], Loss: 0.0195\n",
      "Epoch [3900/10000], Loss: 0.0194\n",
      "Epoch [3910/10000], Loss: 0.0193\n",
      "Epoch [3920/10000], Loss: 0.0192\n",
      "Epoch [3930/10000], Loss: 0.0191\n",
      "Epoch [3940/10000], Loss: 0.0190\n",
      "Epoch [3950/10000], Loss: 0.0189\n",
      "Epoch [3960/10000], Loss: 0.0188\n",
      "Epoch [3970/10000], Loss: 0.0187\n",
      "Epoch [3980/10000], Loss: 0.0186\n",
      "Epoch [3990/10000], Loss: 0.0185\n",
      "Epoch [4000/10000], Loss: 0.0184\n",
      "Epoch [4010/10000], Loss: 0.0183\n",
      "Epoch [4020/10000], Loss: 0.0182\n",
      "Epoch [4030/10000], Loss: 0.0181\n",
      "Epoch [4040/10000], Loss: 0.0180\n",
      "Epoch [4050/10000], Loss: 0.0179\n",
      "Epoch [4060/10000], Loss: 0.0178\n",
      "Epoch [4070/10000], Loss: 0.0177\n",
      "Epoch [4080/10000], Loss: 0.0176\n",
      "Epoch [4090/10000], Loss: 0.0175\n",
      "Epoch [4100/10000], Loss: 0.0174\n",
      "Epoch [4110/10000], Loss: 0.0173\n",
      "Epoch [4120/10000], Loss: 0.0172\n",
      "Epoch [4130/10000], Loss: 0.0171\n",
      "Epoch [4140/10000], Loss: 0.0170\n",
      "Epoch [4150/10000], Loss: 0.0169\n",
      "Epoch [4160/10000], Loss: 0.0168\n",
      "Epoch [4170/10000], Loss: 0.0167\n",
      "Epoch [4180/10000], Loss: 0.0166\n",
      "Epoch [4190/10000], Loss: 0.0165\n",
      "Epoch [4200/10000], Loss: 0.0164\n",
      "Epoch [4210/10000], Loss: 0.0163\n",
      "Epoch [4220/10000], Loss: 0.0162\n",
      "Epoch [4230/10000], Loss: 0.0161\n",
      "Epoch [4240/10000], Loss: 0.0160\n",
      "Epoch [4250/10000], Loss: 0.0159\n",
      "Epoch [4260/10000], Loss: 0.0159\n",
      "Epoch [4270/10000], Loss: 0.0158\n",
      "Epoch [4280/10000], Loss: 0.0157\n",
      "Epoch [4290/10000], Loss: 0.0156\n",
      "Epoch [4300/10000], Loss: 0.0155\n",
      "Epoch [4310/10000], Loss: 0.0154\n",
      "Epoch [4320/10000], Loss: 0.0153\n",
      "Epoch [4330/10000], Loss: 0.0152\n",
      "Epoch [4340/10000], Loss: 0.0151\n",
      "Epoch [4350/10000], Loss: 0.0151\n",
      "Epoch [4360/10000], Loss: 0.0150\n",
      "Epoch [4370/10000], Loss: 0.0149\n",
      "Epoch [4380/10000], Loss: 0.0148\n",
      "Epoch [4390/10000], Loss: 0.0147\n",
      "Epoch [4400/10000], Loss: 0.0146\n",
      "Epoch [4410/10000], Loss: 0.0145\n",
      "Epoch [4420/10000], Loss: 0.0145\n",
      "Epoch [4430/10000], Loss: 0.0144\n",
      "Epoch [4440/10000], Loss: 0.0143\n",
      "Epoch [4450/10000], Loss: 0.0142\n",
      "Epoch [4460/10000], Loss: 0.0141\n",
      "Epoch [4470/10000], Loss: 0.0140\n",
      "Epoch [4480/10000], Loss: 0.0140\n",
      "Epoch [4490/10000], Loss: 0.0139\n",
      "Epoch [4500/10000], Loss: 0.0138\n",
      "Epoch [4510/10000], Loss: 0.0137\n",
      "Epoch [4520/10000], Loss: 0.0136\n",
      "Epoch [4530/10000], Loss: 0.0135\n",
      "Epoch [4540/10000], Loss: 0.0135\n",
      "Epoch [4550/10000], Loss: 0.0134\n",
      "Epoch [4560/10000], Loss: 0.0133\n",
      "Epoch [4570/10000], Loss: 0.0132\n",
      "Epoch [4580/10000], Loss: 0.0131\n",
      "Epoch [4590/10000], Loss: 0.0130\n",
      "Epoch [4600/10000], Loss: 0.0129\n",
      "Epoch [4610/10000], Loss: 0.0129\n",
      "Epoch [4620/10000], Loss: 0.0128\n",
      "Epoch [4630/10000], Loss: 0.0127\n",
      "Epoch [4640/10000], Loss: 0.0126\n",
      "Epoch [4650/10000], Loss: 0.0126\n",
      "Epoch [4660/10000], Loss: 0.0125\n",
      "Epoch [4670/10000], Loss: 0.0124\n",
      "Epoch [4680/10000], Loss: 0.0123\n",
      "Epoch [4690/10000], Loss: 0.0122\n",
      "Epoch [4700/10000], Loss: 0.0122\n",
      "Epoch [4710/10000], Loss: 0.0121\n",
      "Epoch [4720/10000], Loss: 0.0120\n",
      "Epoch [4730/10000], Loss: 0.0119\n",
      "Epoch [4740/10000], Loss: 0.0119\n",
      "Epoch [4750/10000], Loss: 0.0118\n",
      "Epoch [4760/10000], Loss: 0.0117\n",
      "Epoch [4770/10000], Loss: 0.0116\n",
      "Epoch [4780/10000], Loss: 0.0116\n",
      "Epoch [4790/10000], Loss: 0.0115\n",
      "Epoch [4800/10000], Loss: 0.0114\n",
      "Epoch [4810/10000], Loss: 0.0114\n",
      "Epoch [4820/10000], Loss: 0.0113\n",
      "Epoch [4830/10000], Loss: 0.0112\n",
      "Epoch [4840/10000], Loss: 0.0112\n",
      "Epoch [4850/10000], Loss: 0.0111\n",
      "Epoch [4860/10000], Loss: 0.0110\n",
      "Epoch [4870/10000], Loss: 0.0110\n",
      "Epoch [4880/10000], Loss: 0.0109\n",
      "Epoch [4890/10000], Loss: 0.0108\n",
      "Epoch [4900/10000], Loss: 0.0108\n",
      "Epoch [4910/10000], Loss: 0.0107\n",
      "Epoch [4920/10000], Loss: 0.0106\n",
      "Epoch [4930/10000], Loss: 0.0106\n",
      "Epoch [4940/10000], Loss: 0.0105\n",
      "Epoch [4950/10000], Loss: 0.0104\n",
      "Epoch [4960/10000], Loss: 0.0104\n",
      "Epoch [4970/10000], Loss: 0.0103\n",
      "Epoch [4980/10000], Loss: 0.0102\n",
      "Epoch [4990/10000], Loss: 0.0101\n",
      "Epoch [5000/10000], Loss: 0.0101\n",
      "Epoch [5010/10000], Loss: 0.0100\n",
      "Epoch [5020/10000], Loss: 0.0099\n",
      "Epoch [5030/10000], Loss: 0.0099\n",
      "Epoch [5040/10000], Loss: 0.0098\n",
      "Epoch [5050/10000], Loss: 0.0097\n",
      "Epoch [5060/10000], Loss: 0.0097\n",
      "Epoch [5070/10000], Loss: 0.0096\n",
      "Epoch [5080/10000], Loss: 0.0095\n",
      "Epoch [5090/10000], Loss: 0.0095\n",
      "Epoch [5100/10000], Loss: 0.0094\n",
      "Epoch [5110/10000], Loss: 0.0094\n",
      "Epoch [5120/10000], Loss: 0.0093\n",
      "Epoch [5130/10000], Loss: 0.0092\n",
      "Epoch [5140/10000], Loss: 0.0092\n",
      "Epoch [5150/10000], Loss: 0.0091\n",
      "Epoch [5160/10000], Loss: 0.0091\n",
      "Epoch [5170/10000], Loss: 0.0090\n",
      "Epoch [5180/10000], Loss: 0.0089\n",
      "Epoch [5190/10000], Loss: 0.0089\n",
      "Epoch [5200/10000], Loss: 0.0088\n",
      "Epoch [5210/10000], Loss: 0.0088\n",
      "Epoch [5220/10000], Loss: 0.0087\n",
      "Epoch [5230/10000], Loss: 0.0087\n",
      "Epoch [5240/10000], Loss: 0.0086\n",
      "Epoch [5250/10000], Loss: 0.0085\n",
      "Epoch [5260/10000], Loss: 0.0085\n",
      "Epoch [5270/10000], Loss: 0.0084\n",
      "Epoch [5280/10000], Loss: 0.0084\n",
      "Epoch [5290/10000], Loss: 0.0083\n",
      "Epoch [5300/10000], Loss: 0.0083\n",
      "Epoch [5310/10000], Loss: 0.0082\n",
      "Epoch [5320/10000], Loss: 0.0082\n",
      "Epoch [5330/10000], Loss: 0.0081\n",
      "Epoch [5340/10000], Loss: 0.0081\n",
      "Epoch [5350/10000], Loss: 0.0080\n",
      "Epoch [5360/10000], Loss: 0.0080\n",
      "Epoch [5370/10000], Loss: 0.0079\n",
      "Epoch [5380/10000], Loss: 0.0079\n",
      "Epoch [5390/10000], Loss: 0.0078\n",
      "Epoch [5400/10000], Loss: 0.0078\n",
      "Epoch [5410/10000], Loss: 0.0077\n",
      "Epoch [5420/10000], Loss: 0.0077\n",
      "Epoch [5430/10000], Loss: 0.0076\n",
      "Epoch [5440/10000], Loss: 0.0076\n",
      "Epoch [5450/10000], Loss: 0.0075\n",
      "Epoch [5460/10000], Loss: 0.0075\n",
      "Epoch [5470/10000], Loss: 0.0074\n",
      "Epoch [5480/10000], Loss: 0.0074\n",
      "Epoch [5490/10000], Loss: 0.0073\n",
      "Epoch [5500/10000], Loss: 0.0073\n",
      "Epoch [5510/10000], Loss: 0.0072\n",
      "Epoch [5520/10000], Loss: 0.0072\n",
      "Epoch [5530/10000], Loss: 0.0072\n",
      "Epoch [5540/10000], Loss: 0.0071\n",
      "Epoch [5550/10000], Loss: 0.0071\n",
      "Epoch [5560/10000], Loss: 0.0070\n",
      "Epoch [5570/10000], Loss: 0.0070\n",
      "Epoch [5580/10000], Loss: 0.0069\n",
      "Epoch [5590/10000], Loss: 0.0069\n",
      "Epoch [5600/10000], Loss: 0.0069\n",
      "Epoch [5610/10000], Loss: 0.0068\n",
      "Epoch [5620/10000], Loss: 0.0068\n",
      "Epoch [5630/10000], Loss: 0.0067\n",
      "Epoch [5640/10000], Loss: 0.0067\n",
      "Epoch [5650/10000], Loss: 0.0067\n",
      "Epoch [5660/10000], Loss: 0.0066\n",
      "Epoch [5670/10000], Loss: 0.0066\n",
      "Epoch [5680/10000], Loss: 0.0066\n",
      "Epoch [5690/10000], Loss: 0.0065\n",
      "Epoch [5700/10000], Loss: 0.0065\n",
      "Epoch [5710/10000], Loss: 0.0064\n",
      "Epoch [5720/10000], Loss: 0.0064\n",
      "Epoch [5730/10000], Loss: 0.0064\n",
      "Epoch [5740/10000], Loss: 0.0063\n",
      "Epoch [5750/10000], Loss: 0.0063\n",
      "Epoch [5760/10000], Loss: 0.0063\n",
      "Epoch [5770/10000], Loss: 0.0062\n",
      "Epoch [5780/10000], Loss: 0.0062\n",
      "Epoch [5790/10000], Loss: 0.0062\n",
      "Epoch [5800/10000], Loss: 0.0061\n",
      "Epoch [5810/10000], Loss: 0.0061\n",
      "Epoch [5820/10000], Loss: 0.0061\n",
      "Epoch [5830/10000], Loss: 0.0060\n",
      "Epoch [5840/10000], Loss: 0.0060\n",
      "Epoch [5850/10000], Loss: 0.0060\n",
      "Epoch [5860/10000], Loss: 0.0060\n",
      "Epoch [5870/10000], Loss: 0.0059\n",
      "Epoch [5880/10000], Loss: 0.0059\n",
      "Epoch [5890/10000], Loss: 0.0059\n",
      "Epoch [5900/10000], Loss: 0.0058\n",
      "Epoch [5910/10000], Loss: 0.0058\n",
      "Epoch [5920/10000], Loss: 0.0058\n",
      "Epoch [5930/10000], Loss: 0.0058\n",
      "Epoch [5940/10000], Loss: 0.0057\n",
      "Epoch [5950/10000], Loss: 0.0057\n",
      "Epoch [5960/10000], Loss: 0.0057\n",
      "Epoch [5970/10000], Loss: 0.0056\n",
      "Epoch [5980/10000], Loss: 0.0056\n",
      "Epoch [5990/10000], Loss: 0.0056\n",
      "Epoch [6000/10000], Loss: 0.0056\n",
      "Epoch [6010/10000], Loss: 0.0055\n",
      "Epoch [6020/10000], Loss: 0.0055\n",
      "Epoch [6030/10000], Loss: 0.0055\n",
      "Epoch [6040/10000], Loss: 0.0055\n",
      "Epoch [6050/10000], Loss: 0.0054\n",
      "Epoch [6060/10000], Loss: 0.0054\n",
      "Epoch [6070/10000], Loss: 0.0054\n",
      "Epoch [6080/10000], Loss: 0.0054\n",
      "Epoch [6090/10000], Loss: 0.0053\n",
      "Epoch [6100/10000], Loss: 0.0053\n",
      "Epoch [6110/10000], Loss: 0.0053\n",
      "Epoch [6120/10000], Loss: 0.0053\n",
      "Epoch [6130/10000], Loss: 0.0052\n",
      "Epoch [6140/10000], Loss: 0.0052\n",
      "Epoch [6150/10000], Loss: 0.0052\n",
      "Epoch [6160/10000], Loss: 0.0052\n",
      "Epoch [6170/10000], Loss: 0.0051\n",
      "Epoch [6180/10000], Loss: 0.0051\n",
      "Epoch [6190/10000], Loss: 0.0051\n",
      "Epoch [6200/10000], Loss: 0.0051\n",
      "Epoch [6210/10000], Loss: 0.0050\n",
      "Epoch [6220/10000], Loss: 0.0050\n",
      "Epoch [6230/10000], Loss: 0.0050\n",
      "Epoch [6240/10000], Loss: 0.0050\n",
      "Epoch [6250/10000], Loss: 0.0050\n",
      "Epoch [6260/10000], Loss: 0.0049\n",
      "Epoch [6270/10000], Loss: 0.0049\n",
      "Epoch [6280/10000], Loss: 0.0049\n",
      "Epoch [6290/10000], Loss: 0.0049\n",
      "Epoch [6300/10000], Loss: 0.0048\n",
      "Epoch [6310/10000], Loss: 0.0048\n",
      "Epoch [6320/10000], Loss: 0.0048\n",
      "Epoch [6330/10000], Loss: 0.0048\n",
      "Epoch [6340/10000], Loss: 0.0048\n",
      "Epoch [6350/10000], Loss: 0.0047\n",
      "Epoch [6360/10000], Loss: 0.0047\n",
      "Epoch [6370/10000], Loss: 0.0047\n",
      "Epoch [6380/10000], Loss: 0.0047\n",
      "Epoch [6390/10000], Loss: 0.0047\n",
      "Epoch [6400/10000], Loss: 0.0046\n",
      "Epoch [6410/10000], Loss: 0.0046\n",
      "Epoch [6420/10000], Loss: 0.0046\n",
      "Epoch [6430/10000], Loss: 0.0046\n",
      "Epoch [6440/10000], Loss: 0.0046\n",
      "Epoch [6450/10000], Loss: 0.0045\n",
      "Epoch [6460/10000], Loss: 0.0045\n",
      "Epoch [6470/10000], Loss: 0.0045\n",
      "Epoch [6480/10000], Loss: 0.0045\n",
      "Epoch [6490/10000], Loss: 0.0045\n",
      "Epoch [6500/10000], Loss: 0.0044\n",
      "Epoch [6510/10000], Loss: 0.0045\n",
      "Epoch [6520/10000], Loss: 0.0044\n",
      "Epoch [6530/10000], Loss: 0.0044\n",
      "Epoch [6540/10000], Loss: 0.0044\n",
      "Epoch [6550/10000], Loss: 0.0044\n",
      "Epoch [6560/10000], Loss: 0.0043\n",
      "Epoch [6570/10000], Loss: 0.0043\n",
      "Epoch [6580/10000], Loss: 0.0043\n",
      "Epoch [6590/10000], Loss: 0.0043\n",
      "Epoch [6600/10000], Loss: 0.0043\n",
      "Epoch [6610/10000], Loss: 0.0043\n",
      "Epoch [6620/10000], Loss: 0.0042\n",
      "Epoch [6630/10000], Loss: 0.0042\n",
      "Epoch [6640/10000], Loss: 0.0042\n",
      "Epoch [6650/10000], Loss: 0.0042\n",
      "Epoch [6660/10000], Loss: 0.0042\n",
      "Epoch [6670/10000], Loss: 0.0042\n",
      "Epoch [6680/10000], Loss: 0.0041\n",
      "Epoch [6690/10000], Loss: 0.0041\n",
      "Epoch [6700/10000], Loss: 0.0041\n",
      "Epoch [6710/10000], Loss: 0.0041\n",
      "Epoch [6720/10000], Loss: 0.0041\n",
      "Epoch [6730/10000], Loss: 0.0041\n",
      "Epoch [6740/10000], Loss: 0.0041\n",
      "Epoch [6750/10000], Loss: 0.0040\n",
      "Epoch [6760/10000], Loss: 0.0040\n",
      "Epoch [6770/10000], Loss: 0.0040\n",
      "Epoch [6780/10000], Loss: 0.0040\n",
      "Epoch [6790/10000], Loss: 0.0040\n",
      "Epoch [6800/10000], Loss: 0.0040\n",
      "Epoch [6810/10000], Loss: 0.0039\n",
      "Epoch [6820/10000], Loss: 0.0039\n",
      "Epoch [6830/10000], Loss: 0.0039\n",
      "Epoch [6840/10000], Loss: 0.0039\n",
      "Epoch [6850/10000], Loss: 0.0039\n",
      "Epoch [6860/10000], Loss: 0.0039\n",
      "Epoch [6870/10000], Loss: 0.0039\n",
      "Epoch [6880/10000], Loss: 0.0038\n",
      "Epoch [6890/10000], Loss: 0.0038\n",
      "Epoch [6900/10000], Loss: 0.0038\n",
      "Epoch [6910/10000], Loss: 0.0038\n",
      "Epoch [6920/10000], Loss: 0.0038\n",
      "Epoch [6930/10000], Loss: 0.0038\n",
      "Epoch [6940/10000], Loss: 0.0038\n",
      "Epoch [6950/10000], Loss: 0.0037\n",
      "Epoch [6960/10000], Loss: 0.0037\n",
      "Epoch [6970/10000], Loss: 0.0037\n",
      "Epoch [6980/10000], Loss: 0.0037\n",
      "Epoch [6990/10000], Loss: 0.0037\n",
      "Epoch [7000/10000], Loss: 0.0037\n",
      "Epoch [7010/10000], Loss: 0.0037\n",
      "Epoch [7020/10000], Loss: 0.0036\n",
      "Epoch [7030/10000], Loss: 0.0036\n",
      "Epoch [7040/10000], Loss: 0.0036\n",
      "Epoch [7050/10000], Loss: 0.0036\n",
      "Epoch [7060/10000], Loss: 0.0036\n",
      "Epoch [7070/10000], Loss: 0.0036\n",
      "Epoch [7080/10000], Loss: 0.0036\n",
      "Epoch [7090/10000], Loss: 0.0036\n",
      "Epoch [7100/10000], Loss: 0.0035\n",
      "Epoch [7110/10000], Loss: 0.0035\n",
      "Epoch [7120/10000], Loss: 0.0035\n",
      "Epoch [7130/10000], Loss: 0.0035\n",
      "Epoch [7140/10000], Loss: 0.0035\n",
      "Epoch [7150/10000], Loss: 0.0036\n",
      "Epoch [7160/10000], Loss: 0.0035\n",
      "Epoch [7170/10000], Loss: 0.0035\n",
      "Epoch [7180/10000], Loss: 0.0034\n",
      "Epoch [7190/10000], Loss: 0.0034\n",
      "Epoch [7200/10000], Loss: 0.0034\n",
      "Epoch [7210/10000], Loss: 0.0034\n",
      "Epoch [7220/10000], Loss: 0.0034\n",
      "Epoch [7230/10000], Loss: 0.0034\n",
      "Epoch [7240/10000], Loss: 0.0034\n",
      "Epoch [7250/10000], Loss: 0.0033\n",
      "Epoch [7260/10000], Loss: 0.0033\n",
      "Epoch [7270/10000], Loss: 0.0033\n",
      "Epoch [7280/10000], Loss: 0.0033\n",
      "Epoch [7290/10000], Loss: 0.0033\n",
      "Epoch [7300/10000], Loss: 0.0033\n",
      "Epoch [7310/10000], Loss: 0.0033\n",
      "Epoch [7320/10000], Loss: 0.0033\n",
      "Epoch [7330/10000], Loss: 0.0032\n",
      "Epoch [7340/10000], Loss: 0.0032\n",
      "Epoch [7350/10000], Loss: 0.0032\n",
      "Epoch [7360/10000], Loss: 0.0032\n",
      "Epoch [7370/10000], Loss: 0.0032\n",
      "Epoch [7380/10000], Loss: 0.0032\n",
      "Epoch [7390/10000], Loss: 0.0032\n",
      "Epoch [7400/10000], Loss: 0.0033\n",
      "Epoch [7410/10000], Loss: 0.0032\n",
      "Epoch [7420/10000], Loss: 0.0031\n",
      "Epoch [7430/10000], Loss: 0.0031\n",
      "Epoch [7440/10000], Loss: 0.0031\n",
      "Epoch [7450/10000], Loss: 0.0031\n",
      "Epoch [7460/10000], Loss: 0.0031\n",
      "Epoch [7470/10000], Loss: 0.0031\n",
      "Epoch [7480/10000], Loss: 0.0031\n",
      "Epoch [7490/10000], Loss: 0.0030\n",
      "Epoch [7500/10000], Loss: 0.0030\n",
      "Epoch [7510/10000], Loss: 0.0030\n",
      "Epoch [7520/10000], Loss: 0.0030\n",
      "Epoch [7530/10000], Loss: 0.0030\n",
      "Epoch [7540/10000], Loss: 0.0030\n",
      "Epoch [7550/10000], Loss: 0.0030\n",
      "Epoch [7560/10000], Loss: 0.0030\n",
      "Epoch [7570/10000], Loss: 0.0030\n",
      "Epoch [7580/10000], Loss: 0.0031\n",
      "Epoch [7590/10000], Loss: 0.0030\n",
      "Epoch [7600/10000], Loss: 0.0029\n",
      "Epoch [7610/10000], Loss: 0.0029\n",
      "Epoch [7620/10000], Loss: 0.0029\n",
      "Epoch [7630/10000], Loss: 0.0029\n",
      "Epoch [7640/10000], Loss: 0.0029\n",
      "Epoch [7650/10000], Loss: 0.0029\n",
      "Epoch [7660/10000], Loss: 0.0029\n",
      "Epoch [7670/10000], Loss: 0.0028\n",
      "Epoch [7680/10000], Loss: 0.0028\n",
      "Epoch [7690/10000], Loss: 0.0028\n",
      "Epoch [7700/10000], Loss: 0.0028\n",
      "Epoch [7710/10000], Loss: 0.0028\n",
      "Epoch [7720/10000], Loss: 0.0028\n",
      "Epoch [7730/10000], Loss: 0.0028\n",
      "Epoch [7740/10000], Loss: 0.0028\n",
      "Epoch [7750/10000], Loss: 0.0028\n",
      "Epoch [7760/10000], Loss: 0.0027\n",
      "Epoch [7770/10000], Loss: 0.0027\n",
      "Epoch [7780/10000], Loss: 0.0029\n",
      "Epoch [7790/10000], Loss: 0.0028\n",
      "Epoch [7800/10000], Loss: 0.0027\n",
      "Epoch [7810/10000], Loss: 0.0027\n",
      "Epoch [7820/10000], Loss: 0.0027\n",
      "Epoch [7830/10000], Loss: 0.0027\n",
      "Epoch [7840/10000], Loss: 0.0027\n",
      "Epoch [7850/10000], Loss: 0.0027\n",
      "Epoch [7860/10000], Loss: 0.0026\n",
      "Epoch [7870/10000], Loss: 0.0026\n",
      "Epoch [7880/10000], Loss: 0.0026\n",
      "Epoch [7890/10000], Loss: 0.0026\n",
      "Epoch [7900/10000], Loss: 0.0026\n",
      "Epoch [7910/10000], Loss: 0.0026\n",
      "Epoch [7920/10000], Loss: 0.0026\n",
      "Epoch [7930/10000], Loss: 0.0026\n",
      "Epoch [7940/10000], Loss: 0.0026\n",
      "Epoch [7950/10000], Loss: 0.0026\n",
      "Epoch [7960/10000], Loss: 0.0025\n",
      "Epoch [7970/10000], Loss: 0.0025\n",
      "Epoch [7980/10000], Loss: 0.0025\n",
      "Epoch [7990/10000], Loss: 0.0025\n",
      "Epoch [8000/10000], Loss: 0.0030\n",
      "Epoch [8010/10000], Loss: 0.0026\n",
      "Epoch [8020/10000], Loss: 0.0025\n",
      "Epoch [8030/10000], Loss: 0.0025\n",
      "Epoch [8040/10000], Loss: 0.0025\n",
      "Epoch [8050/10000], Loss: 0.0025\n",
      "Epoch [8060/10000], Loss: 0.0025\n",
      "Epoch [8070/10000], Loss: 0.0024\n",
      "Epoch [8080/10000], Loss: 0.0024\n",
      "Epoch [8090/10000], Loss: 0.0024\n",
      "Epoch [8100/10000], Loss: 0.0024\n",
      "Epoch [8110/10000], Loss: 0.0024\n",
      "Epoch [8120/10000], Loss: 0.0024\n",
      "Epoch [8130/10000], Loss: 0.0024\n",
      "Epoch [8140/10000], Loss: 0.0024\n",
      "Epoch [8150/10000], Loss: 0.0024\n",
      "Epoch [8160/10000], Loss: 0.0024\n",
      "Epoch [8170/10000], Loss: 0.0024\n",
      "Epoch [8180/10000], Loss: 0.0023\n",
      "Epoch [8190/10000], Loss: 0.0023\n",
      "Epoch [8200/10000], Loss: 0.0023\n",
      "Epoch [8210/10000], Loss: 0.0023\n",
      "Epoch [8220/10000], Loss: 0.0023\n",
      "Epoch [8230/10000], Loss: 0.0023\n",
      "Epoch [8240/10000], Loss: 0.0023\n",
      "Epoch [8250/10000], Loss: 0.0024\n",
      "Epoch [8260/10000], Loss: 0.0023\n",
      "Epoch [8270/10000], Loss: 0.0024\n",
      "Epoch [8280/10000], Loss: 0.0023\n",
      "Epoch [8290/10000], Loss: 0.0023\n",
      "Epoch [8300/10000], Loss: 0.0022\n",
      "Epoch [8310/10000], Loss: 0.0022\n",
      "Epoch [8320/10000], Loss: 0.0022\n",
      "Epoch [8330/10000], Loss: 0.0022\n",
      "Epoch [8340/10000], Loss: 0.0022\n",
      "Epoch [8350/10000], Loss: 0.0022\n",
      "Epoch [8360/10000], Loss: 0.0022\n",
      "Epoch [8370/10000], Loss: 0.0022\n",
      "Epoch [8380/10000], Loss: 0.0022\n",
      "Epoch [8390/10000], Loss: 0.0022\n",
      "Epoch [8400/10000], Loss: 0.0022\n",
      "Epoch [8410/10000], Loss: 0.0022\n",
      "Epoch [8420/10000], Loss: 0.0021\n",
      "Epoch [8430/10000], Loss: 0.0021\n",
      "Epoch [8440/10000], Loss: 0.0021\n",
      "Epoch [8450/10000], Loss: 0.0021\n",
      "Epoch [8460/10000], Loss: 0.0021\n",
      "Epoch [8470/10000], Loss: 0.0021\n",
      "Epoch [8480/10000], Loss: 0.0034\n",
      "Epoch [8490/10000], Loss: 0.0022\n",
      "Epoch [8500/10000], Loss: 0.0022\n",
      "Epoch [8510/10000], Loss: 0.0021\n",
      "Epoch [8520/10000], Loss: 0.0021\n",
      "Epoch [8530/10000], Loss: 0.0021\n",
      "Epoch [8540/10000], Loss: 0.0021\n",
      "Epoch [8550/10000], Loss: 0.0020\n",
      "Epoch [8560/10000], Loss: 0.0020\n",
      "Epoch [8570/10000], Loss: 0.0020\n",
      "Epoch [8580/10000], Loss: 0.0020\n",
      "Epoch [8590/10000], Loss: 0.0020\n",
      "Epoch [8600/10000], Loss: 0.0020\n",
      "Epoch [8610/10000], Loss: 0.0020\n",
      "Epoch [8620/10000], Loss: 0.0020\n",
      "Epoch [8630/10000], Loss: 0.0020\n",
      "Epoch [8640/10000], Loss: 0.0020\n",
      "Epoch [8650/10000], Loss: 0.0020\n",
      "Epoch [8660/10000], Loss: 0.0020\n",
      "Epoch [8670/10000], Loss: 0.0020\n",
      "Epoch [8680/10000], Loss: 0.0019\n",
      "Epoch [8690/10000], Loss: 0.0019\n",
      "Epoch [8700/10000], Loss: 0.0019\n",
      "Epoch [8710/10000], Loss: 0.0019\n",
      "Epoch [8720/10000], Loss: 0.0019\n",
      "Epoch [8730/10000], Loss: 0.0019\n",
      "Epoch [8740/10000], Loss: 0.0019\n",
      "Epoch [8750/10000], Loss: 0.0019\n",
      "Epoch [8760/10000], Loss: 0.0020\n",
      "Epoch [8770/10000], Loss: 0.0028\n",
      "Epoch [8780/10000], Loss: 0.0026\n",
      "Epoch [8790/10000], Loss: 0.0021\n",
      "Epoch [8800/10000], Loss: 0.0019\n",
      "Epoch [8810/10000], Loss: 0.0019\n",
      "Epoch [8820/10000], Loss: 0.0018\n",
      "Epoch [8830/10000], Loss: 0.0018\n",
      "Epoch [8840/10000], Loss: 0.0018\n",
      "Epoch [8850/10000], Loss: 0.0018\n",
      "Epoch [8860/10000], Loss: 0.0018\n",
      "Epoch [8870/10000], Loss: 0.0018\n",
      "Epoch [8880/10000], Loss: 0.0018\n",
      "Epoch [8890/10000], Loss: 0.0018\n",
      "Epoch [8900/10000], Loss: 0.0018\n",
      "Epoch [8910/10000], Loss: 0.0018\n",
      "Epoch [8920/10000], Loss: 0.0018\n",
      "Epoch [8930/10000], Loss: 0.0018\n",
      "Epoch [8940/10000], Loss: 0.0018\n",
      "Epoch [8950/10000], Loss: 0.0018\n",
      "Epoch [8960/10000], Loss: 0.0018\n",
      "Epoch [8970/10000], Loss: 0.0017\n",
      "Epoch [8980/10000], Loss: 0.0017\n",
      "Epoch [8990/10000], Loss: 0.0017\n",
      "Epoch [9000/10000], Loss: 0.0017\n",
      "Epoch [9010/10000], Loss: 0.0017\n",
      "Epoch [9020/10000], Loss: 0.0018\n",
      "Epoch [9030/10000], Loss: 0.0043\n",
      "Epoch [9040/10000], Loss: 0.0027\n",
      "Epoch [9050/10000], Loss: 0.0017\n",
      "Epoch [9060/10000], Loss: 0.0018\n",
      "Epoch [9070/10000], Loss: 0.0017\n",
      "Epoch [9080/10000], Loss: 0.0017\n",
      "Epoch [9090/10000], Loss: 0.0017\n",
      "Epoch [9100/10000], Loss: 0.0017\n",
      "Epoch [9110/10000], Loss: 0.0017\n",
      "Epoch [9120/10000], Loss: 0.0017\n",
      "Epoch [9130/10000], Loss: 0.0016\n",
      "Epoch [9140/10000], Loss: 0.0016\n",
      "Epoch [9150/10000], Loss: 0.0016\n",
      "Epoch [9160/10000], Loss: 0.0016\n",
      "Epoch [9170/10000], Loss: 0.0016\n",
      "Epoch [9180/10000], Loss: 0.0016\n",
      "Epoch [9190/10000], Loss: 0.0016\n",
      "Epoch [9200/10000], Loss: 0.0016\n",
      "Epoch [9210/10000], Loss: 0.0017\n",
      "Epoch [9220/10000], Loss: 0.0055\n",
      "Epoch [9230/10000], Loss: 0.0034\n",
      "Epoch [9240/10000], Loss: 0.0016\n",
      "Epoch [9250/10000], Loss: 0.0018\n",
      "Epoch [9260/10000], Loss: 0.0016\n",
      "Epoch [9270/10000], Loss: 0.0016\n",
      "Epoch [9280/10000], Loss: 0.0016\n",
      "Epoch [9290/10000], Loss: 0.0015\n",
      "Epoch [9300/10000], Loss: 0.0015\n",
      "Epoch [9310/10000], Loss: 0.0015\n",
      "Epoch [9320/10000], Loss: 0.0015\n",
      "Epoch [9330/10000], Loss: 0.0015\n",
      "Epoch [9340/10000], Loss: 0.0015\n",
      "Epoch [9350/10000], Loss: 0.0015\n",
      "Epoch [9360/10000], Loss: 0.0015\n",
      "Epoch [9370/10000], Loss: 0.0015\n",
      "Epoch [9380/10000], Loss: 0.0015\n",
      "Epoch [9390/10000], Loss: 0.0015\n",
      "Epoch [9400/10000], Loss: 0.0036\n",
      "Epoch [9410/10000], Loss: 0.0015\n",
      "Epoch [9420/10000], Loss: 0.0019\n",
      "Epoch [9430/10000], Loss: 0.0018\n",
      "Epoch [9440/10000], Loss: 0.0016\n",
      "Epoch [9450/10000], Loss: 0.0015\n",
      "Epoch [9460/10000], Loss: 0.0014\n",
      "Epoch [9470/10000], Loss: 0.0014\n",
      "Epoch [9480/10000], Loss: 0.0014\n",
      "Epoch [9490/10000], Loss: 0.0014\n",
      "Epoch [9500/10000], Loss: 0.0014\n",
      "Epoch [9510/10000], Loss: 0.0014\n",
      "Epoch [9520/10000], Loss: 0.0014\n",
      "Epoch [9530/10000], Loss: 0.0014\n",
      "Epoch [9540/10000], Loss: 0.0024\n",
      "Epoch [9550/10000], Loss: 0.0053\n",
      "Epoch [9560/10000], Loss: 0.0019\n",
      "Epoch [9570/10000], Loss: 0.0014\n",
      "Epoch [9580/10000], Loss: 0.0014\n",
      "Epoch [9590/10000], Loss: 0.0014\n",
      "Epoch [9600/10000], Loss: 0.0014\n",
      "Epoch [9610/10000], Loss: 0.0014\n",
      "Epoch [9620/10000], Loss: 0.0014\n",
      "Epoch [9630/10000], Loss: 0.0013\n",
      "Epoch [9640/10000], Loss: 0.0013\n",
      "Epoch [9650/10000], Loss: 0.0013\n",
      "Epoch [9660/10000], Loss: 0.0013\n",
      "Epoch [9670/10000], Loss: 0.0014\n",
      "Epoch [9680/10000], Loss: 0.0027\n",
      "Epoch [9690/10000], Loss: 0.0076\n",
      "Epoch [9700/10000], Loss: 0.0016\n",
      "Epoch [9710/10000], Loss: 0.0016\n",
      "Epoch [9720/10000], Loss: 0.0017\n",
      "Epoch [9730/10000], Loss: 0.0014\n",
      "Epoch [9740/10000], Loss: 0.0013\n",
      "Epoch [9750/10000], Loss: 0.0013\n",
      "Epoch [9760/10000], Loss: 0.0013\n",
      "Epoch [9770/10000], Loss: 0.0013\n",
      "Epoch [9780/10000], Loss: 0.0013\n",
      "Epoch [9790/10000], Loss: 0.0013\n",
      "Epoch [9800/10000], Loss: 0.0013\n",
      "Epoch [9810/10000], Loss: 0.0012\n",
      "Epoch [9820/10000], Loss: 0.0012\n",
      "Epoch [9830/10000], Loss: 0.0013\n",
      "Epoch [9840/10000], Loss: 0.0052\n",
      "Epoch [9850/10000], Loss: 0.0015\n",
      "Epoch [9860/10000], Loss: 0.0042\n",
      "Epoch [9870/10000], Loss: 0.0016\n",
      "Epoch [9880/10000], Loss: 0.0015\n",
      "Epoch [9890/10000], Loss: 0.0013\n",
      "Epoch [9900/10000], Loss: 0.0012\n",
      "Epoch [9910/10000], Loss: 0.0012\n",
      "Epoch [9920/10000], Loss: 0.0012\n",
      "Epoch [9930/10000], Loss: 0.0012\n",
      "Epoch [9940/10000], Loss: 0.0012\n",
      "Epoch [9950/10000], Loss: 0.0012\n",
      "Epoch [9960/10000], Loss: 0.0012\n",
      "Epoch [9970/10000], Loss: 0.0012\n",
      "Epoch [9980/10000], Loss: 0.0012\n",
      "Epoch [9990/10000], Loss: 0.0012\n",
      "Epoch [10000/10000], Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义神经网络模型\n",
    "class FermentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FermentationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 64)  # 输入层有 6 个变量\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # 输出层，预测发酵产量\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "model = FermentationModel()\n",
    "criterion = nn.MSELoss()  # 使用均方误差作为损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 转换数据为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优配方: [200.0, 500.0, 7.5, 40.0, 500.0, 2.0]\n",
      "预测的最优产量: 591.3811\n"
     ]
    }
   ],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "\n",
    "# 定义目标函数，用深度学习模型预测产量\n",
    "def objective_function(params):\n",
    "    carbon_source, nitrogen_source, pH, temperature, stirring_speed, aeration_rate = params\n",
    "    \n",
    "    # 将参数标准化\n",
    "    input_data = scaler.transform([[carbon_source, nitrogen_source, pH, temperature, stirring_speed, aeration_rate]])\n",
    "    input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "    # 使用模型预测产量\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_yield = model(input_tensor).item()\n",
    "\n",
    "    # 我们希望最大化产量，但优化工具通常是最小化问题，所以返回负值\n",
    "    return -predicted_yield\n",
    "\n",
    "# 定义搜索空间\n",
    "search_space = [\n",
    "    Real(0, 200, name='carbon_source'),       # 碳源浓度范围\n",
    "    Real(0, 500, name='nitrogen_source'),      # 氮源浓度范围\n",
    "    Real(0, 7.5, name='pH'),                 # pH 范围\n",
    "    Real(0, 40, name='temperature'),          # 温度范围\n",
    "    Real(0, 500, name='stirring_speed'),     # 搅拌速度范围\n",
    "    Real(0, 2.0, name='aeration_rate')       # 通气量范围\n",
    "]\n",
    "\n",
    "# 使用贝叶斯优化进行最优配方搜索\n",
    "result = gp_minimize(objective_function, search_space, n_calls=50, random_state=42)\n",
    "\n",
    "# 输出最优配方\n",
    "best_params = result.x\n",
    "best_yield = -result.fun\n",
    "print(f\"最优配方: {best_params}\")\n",
    "print(f\"预测的最优产量: {best_yield:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "碳源浓度: 200.00 g/L\n",
      "氮源浓度: 500.00 g/L\n",
      "pH: 7.50\n",
      "温度: 40.00 °C\n",
      "搅拌速度: 500.00 rpm\n",
      "通气量: 2.00 vvm\n",
      "最优配方下的预测产量: 591.3811\n"
     ]
    }
   ],
   "source": [
    "# 打印最优配方\n",
    "print(f\"碳源浓度: {best_params[0]:.2f} g/L\")\n",
    "print(f\"氮源浓度: {best_params[1]:.2f} g/L\")\n",
    "print(f\"pH: {best_params[2]:.2f}\")\n",
    "print(f\"温度: {best_params[3]:.2f} °C\")\n",
    "print(f\"搅拌速度: {best_params[4]:.2f} rpm\")\n",
    "print(f\"通气量: {best_params[5]:.2f} vvm\")\n",
    "\n",
    "# 预测的最优发酵产量\n",
    "print(f\"最优配方下的预测产量: {best_yield:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遗传优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "d:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generation 1 --\n",
      "Best individual: [228.6140688655108, 222.8549703660248, 2.003791883648972, 24.55632268026442, 369.36552143615324, 0.4145133504225806], Yield: 571.1956\n",
      "-- Generation 2 --\n",
      "Best individual: [228.6140688655108, 222.8549703660248, 2.003791883648972, 24.55632268026442, 369.36552143615324, 0.4145133504225806], Yield: 571.1956\n",
      "-- Generation 3 --\n",
      "Best individual: [231.47490133650786, 158.01896035378402, 2.378600581475164, 25.646261728615766, 493.37452148614767, 0.20457744726756344], Yield: 598.4065\n",
      "-- Generation 4 --\n",
      "Best individual: [215.4929749261005, 213.4158363913748, 0.6261149181232968, 44.104828231267376, 631.0479668480585, 0.426092916472306], Yield: 616.9981\n",
      "-- Generation 5 --\n",
      "Best individual: [237.92726725380345, 203.82888795263477, 3.6390260947069906, 43.113641336755585, 556.4597657047425, 0.3753766172639556], Yield: 637.9508\n",
      "-- Generation 6 --\n",
      "Best individual: [228.3150506099568, 227.9446993871583, 4.4054741172901535, 44.743656013159296, 612.2660454895697, 0.35843638217395457], Yield: 638.3752\n",
      "-- Generation 7 --\n",
      "Best individual: [228.27817194795682, 254.7251199953969, 1.6392835548087477, 45.52744730641926, 669.2350489355003, 0.38056180622339764], Yield: 659.0322\n",
      "-- Generation 8 --\n",
      "Best individual: [232.1018471138699, 266.73259484224695, 1.584710944712719, 46.214645429814055, 778.3192626642668, 0.6547512943867945], Yield: 713.0212\n",
      "-- Generation 9 --\n",
      "Best individual: [233.11444409786404, 237.52296043023682, 6.567870151148042, 45.42191756220778, 817.2782942670118, 0.7055219150726587], Yield: 730.9110\n",
      "-- Generation 10 --\n",
      "Best individual: [233.11444409786404, 237.52296043023682, 6.567870151148042, 45.42191756220778, 817.2782942670118, 0.7055219150726587], Yield: 730.9110\n",
      "-- Generation 11 --\n",
      "Best individual: [228.74900664278871, 231.10816491405077, 1.8782146783343798, 54.596353603587765, 911.7225165271672, 0.37068155311069984], Yield: 758.0475\n",
      "-- Generation 12 --\n",
      "Best individual: [232.2367599195478, 246.76786570933734, 8.034946963045286, 48.55326923956489, 918.7969608244165, 0.47262381886674], Yield: 770.2777\n",
      "-- Generation 13 --\n",
      "Best individual: [230.57789313843068, 243.5805682822985, 6.569240654498337, 39.97552422677977, 951.0910887343517, 0.4218057074095616], Yield: 774.0143\n",
      "-- Generation 14 --\n",
      "Best individual: [232.26325128113376, 246.53855044792007, 8.327308083911868, 69.70018225303929, 916.1091221587371, 0.4490959776826854], Yield: 779.8510\n",
      "-- Generation 15 --\n",
      "Best individual: [229.89216216629484, 211.81038953931727, 3.942783592850956, 41.226512935853805, 1006.7854961370776, 0.5535865265749554], Yield: 792.5952\n",
      "-- Generation 16 --\n",
      "Best individual: [240.8352843235385, 371.9021939529835, 6.4704136307302225, 49.31333427130653, 924.0484387551746, 0.6850089409767448], Yield: 800.5785\n",
      "-- Generation 17 --\n",
      "Best individual: [240.8352843235385, 371.9021939529835, 6.4704136307302225, 49.31333427130653, 924.0484387551746, 0.6850089409767448], Yield: 800.5785\n",
      "-- Generation 18 --\n",
      "Best individual: [241.8063467083016, 318.30761581733594, 7.014211097404649, 75.7781169746179, 922.958455254447, 1.1397604645929709], Yield: 815.9210\n",
      "-- Generation 19 --\n",
      "Best individual: [245.07024449033753, 254.715438731844, 6.790995028377886, 86.6266501646767, 923.3482742854369, 1.1715162509460253], Yield: 822.2719\n",
      "-- Generation 20 --\n",
      "Best individual: [246.85395041396734, 251.05118516439535, 6.963972962082696, 95.97074874833383, 923.1273449076742, 1.2031005523651244], Yield: 830.2975\n",
      "-- Generation 21 --\n",
      "Best individual: [244.8324466823863, 309.2261617450652, 6.902624233172807, 103.64942285183123, 923.3300570863706, 1.39250181011172], Yield: 837.5686\n",
      "-- Generation 22 --\n",
      "Best individual: [245.14923694826132, 296.77580298287364, 6.780603891423105, 111.06833914573588, 923.3437817339429, 1.4965014662385054], Yield: 842.4530\n",
      "-- Generation 23 --\n",
      "Best individual: [242.69044218931828, 310.74164812494496, 7.8137723221320305, 118.47743194525611, 970.2822531504206, 1.0239156993485847], Yield: 857.0059\n",
      "-- Generation 24 --\n",
      "Best individual: [245.02786033815389, 308.58331277739194, 8.129775134356537, 113.97194697567275, 988.6759165668212, 1.0318361460151757], Yield: 865.8265\n",
      "-- Generation 25 --\n",
      "Best individual: [245.1638726842412, 309.36352385479006, 7.5654193046302645, 110.29491162513057, 1002.6341333137038, 1.2088244418652563], Yield: 871.3885\n",
      "-- Generation 26 --\n",
      "Best individual: [247.1682822584126, 242.81478095643595, 6.903901571830408, 109.69384453861987, 1049.9040855826834, 1.2707356951530302], Yield: 888.0387\n",
      "-- Generation 27 --\n",
      "Best individual: [249.21218821325954, 248.71275073361002, 6.208722394880229, 108.89861003907151, 1076.5115954168048, 1.5151551798028795], Yield: 904.6481\n",
      "-- Generation 28 --\n",
      "Best individual: [246.74635985348607, 285.40352770962835, 7.513900062499035, 102.6780592042232, 1101.6415767540268, 1.4831174903986508], Yield: 910.5380\n",
      "-- Generation 29 --\n",
      "Best individual: [247.22232171452686, 354.1283364709508, 8.11452873775353, 120.83843739925467, 1104.4289566364025, 1.2472536327465744], Yield: 926.7094\n",
      "-- Generation 30 --\n",
      "Best individual: [247.1219201578836, 351.1629885478949, 6.578843152538505, 143.58363177046556, 1078.956386027095, 1.280578167230078], Yield: 932.8910\n",
      "-- Generation 31 --\n",
      "Best individual: [247.1219201578836, 351.1629885478949, 6.578843152538505, 143.58363177046556, 1078.956386027095, 1.280578167230078], Yield: 932.8910\n",
      "-- Generation 32 --\n",
      "Best individual: [247.29271062109646, 355.0842793363774, 6.7428420388171935, 120.60869590176083, 1142.967172997059, 1.205087818310117], Yield: 941.9708\n",
      "-- Generation 33 --\n",
      "Best individual: [247.29271062109646, 355.0842793363774, 6.7428420388171935, 120.60869590176083, 1142.967172997059, 1.205087818310117], Yield: 941.9708\n",
      "-- Generation 34 --\n",
      "Best individual: [247.5014480027442, 324.46361495451436, 7.563505043831477, 131.63288371552363, 1162.7838715746614, 1.2687200129830478], Yield: 956.1108\n",
      "-- Generation 35 --\n",
      "Best individual: [247.5014480027442, 324.46361495451436, 7.563505043831477, 131.63288371552363, 1162.7838715746614, 1.2687200129830478], Yield: 956.1108\n",
      "-- Generation 36 --\n",
      "Best individual: [246.95583363004, 351.3267899394337, 7.0840916390573, 154.7741017976328, 1169.3256466428088, 1.269496507580087], Yield: 978.7528\n",
      "-- Generation 37 --\n",
      "Best individual: [247.0310986498343, 359.99578197306704, 8.180315497986703, 151.10521272910825, 1207.3836853186133, 1.6924654082975852], Yield: 998.5166\n",
      "-- Generation 38 --\n",
      "Best individual: [247.0310986498343, 359.99578197306704, 8.180315497986703, 151.10521272910825, 1207.3836853186133, 1.6924654082975852], Yield: 998.5166\n",
      "-- Generation 39 --\n",
      "Best individual: [247.09022219048185, 359.99578197306704, 8.180315497986703, 151.10521272910825, 1207.3836853186133, 1.6924654082975852], Yield: 998.6176\n",
      "-- Generation 40 --\n",
      "Best individual: [247.06706620250807, 365.903004279921, 8.366307005942886, 159.42795833844315, 1207.2165482997118, 1.4953926505321977], Yield: 1004.2150\n",
      "\n",
      "最优配方: [247.06706620250807, 365.903004279921, 8.366307005942886, 159.42795833844315, 1207.2165482997118, 1.4953926505321977]\n",
      "最优配方的预测产量: 1004.2150\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from deap import base, creator, tools\n",
    "\n",
    "# 假设深度学习模型和数据已经准备好（与之前相同）\n",
    "\n",
    "# 1. 定义适应度函数，使用深度学习模型预测配方的发酵产量\n",
    "def evaluate(individual):\n",
    "    # individual 是遗传算法中的一个个体（即一个配方），包含6个参数\n",
    "    carbon_source, nitrogen_source, pH, temperature, stirring_speed, aeration_rate = individual\n",
    "    \n",
    "    # 将配方参数进行标准化\n",
    "    input_data = scaler.transform([[carbon_source, nitrogen_source, pH, temperature, stirring_speed, aeration_rate]])\n",
    "    input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "    # 使用训练好的深度学习模型预测产量\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_yield = model(input_tensor).item()\n",
    "\n",
    "    return predicted_yield,  # DEAP 要求返回一个元组\n",
    "\n",
    "# 2. 初始化遗传算法的配置\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # 我们要最大化产量\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# 定义个体和种群的生成规则\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", random.uniform, 0, 1)  # 随机生成 [0, 1] 范围内的浮点数\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (lambda: random.uniform(0, 200),   # 碳源浓度\n",
    "                  lambda: random.uniform(0, 500),    # 氮源浓度\n",
    "                  lambda: random.uniform(0, 7.5),  # pH\n",
    "                  lambda: random.uniform(0, 40),    # 温度\n",
    "                  lambda: random.uniform(0, 500),  # 搅拌速度\n",
    "                  lambda: random.uniform(0, 2.0)), # 通气量\n",
    "                 n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# 注册遗传算法的操作\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)  # 使用均匀交叉（blend crossover）\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)  # 使用高斯变异\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # 使用锦标赛选择\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# 3. 设置遗传算法的参数和运行流程\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    \n",
    "    # 生成初始种群\n",
    "    population = toolbox.population(n=50)  # 50 个个体的种群\n",
    "    ngen = 40  # 进化的代数\n",
    "    cxpb = 0.5  # 交叉概率\n",
    "    mutpb = 0.2  # 变异概率\n",
    "    \n",
    "    # 评估初始种群\n",
    "    fitnesses = list(map(toolbox.evaluate, population))\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    # 进化过程\n",
    "    for gen in range(ngen):\n",
    "        print(f\"-- Generation {gen+1} --\")\n",
    "        \n",
    "        # 选择下一代个体\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # 交叉操作\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < cxpb:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        \n",
    "        # 变异操作\n",
    "        for mutant in offspring:\n",
    "            if random.random() < mutpb:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        \n",
    "        # 重新评估变异和交叉后的个体\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        # 替换种群\n",
    "        population[:] = offspring\n",
    "        \n",
    "        # 输出当前种群中最好的个体\n",
    "        top_ind = tools.selBest(population, 1)[0]\n",
    "        print(f\"Best individual: {top_ind}, Yield: {top_ind.fitness.values[0]:.4f}\")\n",
    "    \n",
    "    # 最终最优配方\n",
    "    best_individual = tools.selBest(population, 1)[0]\n",
    "    print(f\"\\n最优配方: {best_individual}\")\n",
    "    print(f\"最优配方的预测产量: {best_individual.fitness.values[0]:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遗传规划（Genetic Programming, GP）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "d:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The gp.generate function tried to add a primitive of type '<class 'tuple'>', but there is none available.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\gp.py:643\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(pset, min_, max_, condition, type_)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 643\u001b[0m     prim \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(pset\u001b[38;5;241m.\u001b[39mprimitives[type_])\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\random.py:347\u001b[0m, in \u001b[0;36mRandom.choice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seq):\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot choose from an empty sequence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seq[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow(\u001b[38;5;28mlen\u001b[39m(seq))]\n",
      "\u001b[1;31mIndexError\u001b[0m: Cannot choose from an empty sequence",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 129\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最优配方的预测产量: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_individual\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 129\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[57], line 79\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# 生成初始种群\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m population \u001b[38;5;241m=\u001b[39m toolbox\u001b[38;5;241m.\u001b[39mpopulation(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     80\u001b[0m ngen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# 进化的代数\u001b[39;00m\n\u001b[0;32m     81\u001b[0m cxpb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# 交叉概率\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\tools\\init.py:23\u001b[0m, in \u001b[0;36minitRepeat\u001b[1;34m(container, func, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitRepeat\u001b[39m(container, func, n):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the function *func* *n* times and return the results in a\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    container type `container`\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    See the :ref:`list-of-floats` and :ref:`population` tutorials for more examples.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m container(func() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n))\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\tools\\init.py:23\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitRepeat\u001b[39m(container, func, n):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the function *func* *n* times and return the results in a\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    container type `container`\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    See the :ref:`list-of-floats` and :ref:`population` tutorials for more examples.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m container(func() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n))\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\tools\\init.py:51\u001b[0m, in \u001b[0;36minitIterate\u001b[1;34m(container, generator)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitIterate\u001b[39m(container, generator):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the function *container* with an iterable as\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    its only argument. The iterable must be returned by\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    the method or the object *generator*.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    more examples.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m container(generator())\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\gp.py:550\u001b[0m, in \u001b[0;36mgenFull\u001b[1;34m(pset, min_, max_, type_)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Expression generation stops when the depth is equal to height.\"\"\"\u001b[39;00m\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m height\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generate(pset, min_, max_, condition, type_)\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\gp.py:646\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(pset, min_, max_, condition, type_)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m     _, _, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[1;32m--> 646\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe gp.generate function tried to add \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma primitive of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, but there is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    648\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone available.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (type_,))\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m    649\u001b[0m expr\u001b[38;5;241m.\u001b[39mappend(prim)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(prim\u001b[38;5;241m.\u001b[39margs):\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\deap\\gp.py:643\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(pset, min_, max_, condition, type_)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 643\u001b[0m         prim \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(pset\u001b[38;5;241m.\u001b[39mprimitives[type_])\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m         _, _, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\random.py:347\u001b[0m, in \u001b[0;36mRandom.choice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# As an accommodation for NumPy, we don't use \"if not seq\"\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# because bool(numpy.array()) raises a ValueError.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seq):\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot choose from an empty sequence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seq[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow(\u001b[38;5;28mlen\u001b[39m(seq))]\n",
      "\u001b[1;31mIndexError\u001b[0m: The gp.generate function tried to add a primitive of type '<class 'tuple'>', but there is none available."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import torch\n",
    "from deap import base, creator, tools, gp\n",
    "\n",
    "# 假设深度学习模型和数据已经准备好\n",
    "# model: 经过训练的发酵模型\n",
    "# scaler: 数据标准化的工具\n",
    "\n",
    "# 1. 定义适应度函数，使用深度学习模型来评估每个表达式的产量\n",
    "def evaluate(individual):\n",
    "    # 将遗传规划生成的表达式转化为可执行函数\n",
    "    func = toolbox.compile(expr=individual)\n",
    "    \n",
    "    # 假设 func 返回一个发酵配方的参数\n",
    "    # 将生成的参数应用到发酵模型进行评估\n",
    "    carbon_source, nitrogen_source, pH, temperature, stirring_speed, aeration_rate = func()\n",
    "    \n",
    "    # 将配方参数进行标准化\n",
    "    input_data = scaler.transform([[carbon_source, nitrogen_source, pH, temperature, stirring_speed, aeration_rate]])\n",
    "    input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "    # 使用深度学习模型预测产量\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_yield = model(input_tensor).item()\n",
    "\n",
    "    return predicted_yield,  # DEAP 需要返回元组\n",
    "\n",
    "# 2. 初始化遗传规划\n",
    "pset = gp.PrimitiveSetTyped(\"MAIN\", [], tuple, \"ARG\")\n",
    "\n",
    "# 添加运算符（加减乘除）\n",
    "pset.addPrimitive(operator.add, [float, float], float)\n",
    "pset.addPrimitive(operator.sub, [float, float], float)\n",
    "pset.addPrimitive(operator.mul, [float, float], float)\n",
    "pset.addPrimitive(operator.truediv, [float, float], float, name=\"div\")\n",
    "\n",
    "# 添加终端（常量），可以表示发酵配方的具体参数范围\n",
    "pset.addTerminal(50.0, float)   # 碳源浓度的最小值\n",
    "pset.addTerminal(200.0, float)  # 碳源浓度的最大值\n",
    "pset.addTerminal(10.0, float)   # 氮源浓度的最小值\n",
    "pset.addTerminal(50.0, float)   # 氮源浓度的最大值\n",
    "pset.addTerminal(4.5, float)    # pH 值的最小值\n",
    "pset.addTerminal(7.5, float)    # pH 值的最大值\n",
    "pset.addTerminal(20.0, float)   # 温度的最小值\n",
    "pset.addTerminal(40.0, float)   # 温度的最大值\n",
    "pset.addTerminal(100.0, float)  # 搅拌速度的最小值\n",
    "pset.addTerminal(500.0, float)  # 搅拌速度的最大值\n",
    "pset.addTerminal(0.5, float)    # 通气量的最小值\n",
    "pset.addTerminal(2.0, float)    # 通气量的最大值\n",
    "\n",
    "# 3. 设置适应度和个体\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMax)\n",
    "\n",
    "# 注册遗传规划的操作\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"expr\", gp.genFull, pset=pset, min_=1, max_=3)  # 生成深度为 1 到 3 的表达式树\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# 4. 注册评估函数和遗传操作\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"mate\", gp.cxOnePoint)  # 单点交叉\n",
    "toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr, pset=pset)  # 变异操作\n",
    "toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n",
    "\n",
    "# 5. 编译表达式\n",
    "toolbox.register(\"compile\", gp.compile, pset=pset)\n",
    "\n",
    "# 6. 遗传规划的主循环\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    \n",
    "    # 生成初始种群\n",
    "    population = toolbox.population(n=100)\n",
    "    ngen = 50  # 进化的代数\n",
    "    cxpb = 0.5  # 交叉概率\n",
    "    mutpb = 0.2  # 变异概率\n",
    "    \n",
    "    # 评估初始种群\n",
    "    fitnesses = list(map(toolbox.evaluate, population))\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    # 进化过程\n",
    "    for gen in range(ngen):\n",
    "        print(f\"-- Generation {gen+1} --\")\n",
    "        \n",
    "        # 选择下一代个体\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # 交叉操作\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < cxpb:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        \n",
    "        # 变异操作\n",
    "        for mutant in offspring:\n",
    "            if random.random() < mutpb:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        \n",
    "        # 重新评估变异和交叉后的个体\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        # 替换种群\n",
    "        population[:] = offspring\n",
    "        \n",
    "        # 输出当前种群中最好的个体\n",
    "        top_ind = tools.selBest(population, 1)[0]\n",
    "        print(f\"Best individual: {top_ind}, Yield: {top_ind.fitness.values[0]:.4f}\")\n",
    "    \n",
    "    # 最终最优配方\n",
    "    best_individual = tools.selBest(population, 1)[0]\n",
    "    print(f\"\\n最优配方表达式: {best_individual}\")\n",
    "    print(f\"最优配方的预测产量: {best_individual.fitness.values[0]:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "C:\\Users\\zhao\\AppData\\Local\\Temp\\ipykernel_11532\\316625911.py:117: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -5805512.91\n",
      "Episode 2, Total Reward: -4589139.64\n",
      "Episode 3, Total Reward: -4232036.81\n",
      "Episode 4, Total Reward: -2804409.10\n",
      "Episode 5, Total Reward: -4718730.90\n",
      "Episode 6, Total Reward: -3345145.17\n",
      "Episode 7, Total Reward: -3185697.91\n",
      "Episode 8, Total Reward: -4278255.15\n",
      "Episode 9, Total Reward: -5869251.99\n",
      "Episode 10, Total Reward: -934338.71\n",
      "Episode 11, Total Reward: -567004.81\n",
      "Episode 12, Total Reward: -1364695.51\n",
      "Episode 13, Total Reward: -735861.78\n",
      "Episode 14, Total Reward: -743409.04\n",
      "Episode 15, Total Reward: -1019669.32\n",
      "Episode 16, Total Reward: -2695855.09\n",
      "Episode 17, Total Reward: -671659.38\n",
      "Episode 18, Total Reward: -588143.35\n",
      "Episode 19, Total Reward: -1745808.15\n",
      "Episode 20, Total Reward: -1522140.50\n",
      "Episode 21, Total Reward: -318957.84\n",
      "Episode 22, Total Reward: -268103.03\n",
      "Episode 23, Total Reward: -688899.27\n",
      "Episode 24, Total Reward: -938979.51\n",
      "Episode 25, Total Reward: -1415318.73\n",
      "Episode 26, Total Reward: -421929.40\n",
      "Episode 27, Total Reward: -779062.64\n",
      "Episode 28, Total Reward: -2032704.12\n",
      "Episode 29, Total Reward: -1083285.14\n",
      "Episode 30, Total Reward: -2306812.77\n",
      "Episode 31, Total Reward: -1029267.45\n",
      "Episode 32, Total Reward: -1170870.37\n",
      "Episode 33, Total Reward: -431744.64\n",
      "Episode 34, Total Reward: -808370.56\n",
      "Episode 35, Total Reward: -1767323.68\n",
      "Episode 36, Total Reward: -708960.75\n",
      "Episode 37, Total Reward: -618405.30\n",
      "Episode 38, Total Reward: -3361833.57\n",
      "Episode 39, Total Reward: -936300.61\n",
      "Episode 40, Total Reward: -3817021.67\n",
      "Episode 41, Total Reward: -2907707.01\n",
      "Episode 42, Total Reward: -469794.37\n",
      "Episode 43, Total Reward: -5652569.76\n",
      "Episode 44, Total Reward: -1406354.51\n",
      "Episode 45, Total Reward: -1141901.80\n",
      "Episode 46, Total Reward: -1301405.27\n",
      "Episode 47, Total Reward: -2362515.91\n",
      "Episode 48, Total Reward: -2435165.79\n",
      "Episode 49, Total Reward: -6266877.20\n",
      "Episode 50, Total Reward: -4388171.46\n",
      "Episode 51, Total Reward: -2134423.86\n",
      "Episode 52, Total Reward: -916826.42\n",
      "Episode 53, Total Reward: -437070.29\n",
      "Episode 54, Total Reward: -2148479.49\n",
      "Episode 55, Total Reward: -5499006.43\n",
      "Episode 56, Total Reward: -3117731.70\n",
      "Episode 57, Total Reward: -2572942.54\n",
      "Episode 58, Total Reward: -2342025.76\n",
      "Episode 59, Total Reward: -775648.59\n",
      "Episode 60, Total Reward: -3419828.38\n",
      "Episode 61, Total Reward: -4713798.99\n",
      "Episode 62, Total Reward: -1753717.40\n",
      "Episode 63, Total Reward: -2006395.19\n",
      "Episode 64, Total Reward: -965042.65\n",
      "Episode 65, Total Reward: -2193093.09\n",
      "Episode 66, Total Reward: -2444419.65\n",
      "Episode 67, Total Reward: -1772803.11\n",
      "Episode 68, Total Reward: -1616481.96\n",
      "Episode 69, Total Reward: -3995844.50\n",
      "Episode 70, Total Reward: -1681741.37\n",
      "Episode 71, Total Reward: -4294865.06\n",
      "Episode 72, Total Reward: -2521499.35\n",
      "Episode 73, Total Reward: -3493540.48\n",
      "Episode 74, Total Reward: -1212690.74\n",
      "Episode 75, Total Reward: -1109570.42\n",
      "Episode 76, Total Reward: -3297594.51\n",
      "Episode 77, Total Reward: -2824525.94\n",
      "Episode 78, Total Reward: -2603192.01\n",
      "Episode 79, Total Reward: -2066141.45\n",
      "Episode 80, Total Reward: -2472774.87\n",
      "Episode 81, Total Reward: -1074710.23\n",
      "Episode 82, Total Reward: -1929839.29\n",
      "Episode 83, Total Reward: -804359.52\n",
      "Episode 84, Total Reward: -1405573.38\n",
      "Episode 85, Total Reward: -2476512.28\n",
      "Episode 86, Total Reward: -2371217.95\n",
      "Episode 87, Total Reward: -1664117.82\n",
      "Episode 88, Total Reward: -1195680.84\n",
      "Episode 89, Total Reward: -2973982.78\n",
      "Episode 90, Total Reward: -983526.68\n",
      "Episode 91, Total Reward: -705834.82\n",
      "Episode 92, Total Reward: -1278281.22\n",
      "Episode 93, Total Reward: -2615462.71\n",
      "Episode 94, Total Reward: -1677429.73\n",
      "Episode 95, Total Reward: -3245502.33\n",
      "Episode 96, Total Reward: -1329898.30\n",
      "Episode 97, Total Reward: -1039712.80\n",
      "Episode 98, Total Reward: -1675676.04\n",
      "Episode 99, Total Reward: -2343646.78\n",
      "Episode 100, Total Reward: -2007221.77\n",
      "Episode 101, Total Reward: -914534.68\n",
      "Episode 102, Total Reward: -1817003.43\n",
      "Episode 103, Total Reward: -3598281.28\n",
      "Episode 104, Total Reward: -1867507.15\n",
      "Episode 105, Total Reward: -1134619.11\n",
      "Episode 106, Total Reward: -1622347.49\n",
      "Episode 107, Total Reward: -2110392.41\n",
      "Episode 108, Total Reward: -1672599.22\n",
      "Episode 109, Total Reward: -3503024.56\n",
      "Episode 110, Total Reward: -3079301.20\n",
      "Episode 111, Total Reward: -1232595.73\n",
      "Episode 112, Total Reward: -2082342.07\n",
      "Episode 113, Total Reward: -1695284.90\n",
      "Episode 114, Total Reward: -3886293.04\n",
      "Episode 115, Total Reward: -3398899.50\n",
      "Episode 116, Total Reward: -693720.72\n",
      "Episode 117, Total Reward: -1327061.33\n",
      "Episode 118, Total Reward: -1495273.84\n",
      "Episode 119, Total Reward: -2455826.35\n",
      "Episode 120, Total Reward: -2923135.25\n",
      "Episode 121, Total Reward: -828613.93\n",
      "Episode 122, Total Reward: -1181021.60\n",
      "Episode 123, Total Reward: -4505582.30\n",
      "Episode 124, Total Reward: -3468346.53\n",
      "Episode 125, Total Reward: -1734606.74\n",
      "Episode 126, Total Reward: -4436625.34\n",
      "Episode 127, Total Reward: -319066.45\n",
      "Episode 128, Total Reward: -2282457.48\n",
      "Episode 129, Total Reward: -1325893.06\n",
      "Episode 130, Total Reward: -1287348.60\n",
      "Episode 131, Total Reward: -2866795.01\n",
      "Episode 132, Total Reward: -3153755.84\n",
      "Episode 133, Total Reward: -2572030.21\n",
      "Episode 134, Total Reward: -1493036.53\n",
      "Episode 135, Total Reward: -1332570.88\n",
      "Episode 136, Total Reward: -1498966.24\n",
      "Episode 137, Total Reward: -659748.94\n",
      "Episode 138, Total Reward: -4477737.67\n",
      "Episode 139, Total Reward: -1725309.75\n",
      "Episode 140, Total Reward: -2291346.98\n",
      "Episode 141, Total Reward: -754878.42\n",
      "Episode 142, Total Reward: -3826143.50\n",
      "Episode 143, Total Reward: -752913.30\n",
      "Episode 144, Total Reward: -2771150.21\n",
      "Episode 145, Total Reward: -2034423.24\n",
      "Episode 146, Total Reward: -3208600.38\n",
      "Episode 147, Total Reward: -3094292.88\n",
      "Episode 148, Total Reward: -748943.72\n",
      "Episode 149, Total Reward: -1384448.01\n",
      "Episode 150, Total Reward: -3440964.65\n",
      "Episode 151, Total Reward: -734238.57\n",
      "Episode 152, Total Reward: -2985253.35\n",
      "Episode 153, Total Reward: -3859678.68\n",
      "Episode 154, Total Reward: -2277904.95\n",
      "Episode 155, Total Reward: -986755.80\n",
      "Episode 156, Total Reward: -1730988.12\n",
      "Episode 157, Total Reward: -2102483.59\n",
      "Episode 158, Total Reward: -4752287.63\n",
      "Episode 159, Total Reward: -2231241.04\n",
      "Episode 160, Total Reward: -4083470.45\n",
      "Episode 161, Total Reward: -726735.33\n",
      "Episode 162, Total Reward: -685068.42\n",
      "Episode 163, Total Reward: -3480377.07\n",
      "Episode 164, Total Reward: -3687647.01\n",
      "Episode 165, Total Reward: -3336570.51\n",
      "Episode 166, Total Reward: -1578595.85\n",
      "Episode 167, Total Reward: -3313328.32\n",
      "Episode 168, Total Reward: -1576226.34\n",
      "Episode 169, Total Reward: -2175106.63\n",
      "Episode 170, Total Reward: -3177874.33\n",
      "Episode 171, Total Reward: -1614014.40\n",
      "Episode 172, Total Reward: -2419243.39\n",
      "Episode 173, Total Reward: -2165624.37\n",
      "Episode 174, Total Reward: -5406813.72\n",
      "Episode 175, Total Reward: -1532054.20\n",
      "Episode 176, Total Reward: -1208153.81\n",
      "Episode 177, Total Reward: -2563390.91\n",
      "Episode 178, Total Reward: -1906684.83\n",
      "Episode 179, Total Reward: -2441632.36\n",
      "Episode 180, Total Reward: -2202066.87\n",
      "Episode 181, Total Reward: -1784212.96\n",
      "Episode 182, Total Reward: -3437356.70\n",
      "Episode 183, Total Reward: -3530400.73\n",
      "Episode 184, Total Reward: -884687.77\n",
      "Episode 185, Total Reward: -2289682.76\n",
      "Episode 186, Total Reward: -2186198.58\n",
      "Episode 187, Total Reward: -1556871.01\n",
      "Episode 188, Total Reward: -2121860.89\n",
      "Episode 189, Total Reward: -1540905.56\n",
      "Episode 190, Total Reward: -2707994.87\n",
      "Episode 191, Total Reward: -3167055.85\n",
      "Episode 192, Total Reward: -3589212.97\n",
      "Episode 193, Total Reward: -1198029.60\n",
      "Episode 194, Total Reward: -1997958.13\n",
      "Episode 195, Total Reward: -2917210.38\n",
      "Episode 196, Total Reward: -845093.75\n",
      "Episode 197, Total Reward: -2126873.95\n",
      "Episode 198, Total Reward: -2043974.18\n",
      "Episode 199, Total Reward: -2435658.32\n",
      "Episode 200, Total Reward: -2455137.06\n",
      "Episode 201, Total Reward: -1509424.38\n",
      "Episode 202, Total Reward: -2471644.81\n",
      "Episode 203, Total Reward: -2675665.45\n",
      "Episode 204, Total Reward: -1802496.15\n",
      "Episode 205, Total Reward: -1473145.08\n",
      "Episode 206, Total Reward: -2974716.85\n",
      "Episode 207, Total Reward: -3989703.24\n",
      "Episode 208, Total Reward: -1142782.06\n",
      "Episode 209, Total Reward: -2492908.12\n",
      "Episode 210, Total Reward: -3554233.59\n",
      "Episode 211, Total Reward: -1857678.34\n",
      "Episode 212, Total Reward: -2377162.24\n",
      "Episode 213, Total Reward: -1069297.78\n",
      "Episode 214, Total Reward: -926208.24\n",
      "Episode 215, Total Reward: -1762442.58\n",
      "Episode 216, Total Reward: -2493065.02\n",
      "Episode 217, Total Reward: -2689141.62\n",
      "Episode 218, Total Reward: -2331216.41\n",
      "Episode 219, Total Reward: -2494985.18\n",
      "Episode 220, Total Reward: -1710368.70\n",
      "Episode 221, Total Reward: -1166346.57\n",
      "Episode 222, Total Reward: -1244076.86\n",
      "Episode 223, Total Reward: -3636030.25\n",
      "Episode 224, Total Reward: -2441857.91\n",
      "Episode 225, Total Reward: -1103103.08\n",
      "Episode 226, Total Reward: -2711005.18\n",
      "Episode 227, Total Reward: -2001161.61\n",
      "Episode 228, Total Reward: -1431456.10\n",
      "Episode 229, Total Reward: -2261739.83\n",
      "Episode 230, Total Reward: -2062862.35\n",
      "Episode 231, Total Reward: -2104367.39\n",
      "Episode 232, Total Reward: -4925741.58\n",
      "Episode 233, Total Reward: -743870.32\n",
      "Episode 234, Total Reward: -1039524.09\n",
      "Episode 235, Total Reward: -2019433.10\n",
      "Episode 236, Total Reward: -3730279.90\n",
      "Episode 237, Total Reward: -1892375.91\n",
      "Episode 238, Total Reward: -3698963.88\n",
      "Episode 239, Total Reward: -3160588.66\n",
      "Episode 240, Total Reward: -2609937.35\n",
      "Episode 241, Total Reward: -3528353.17\n",
      "Episode 242, Total Reward: -4658401.43\n",
      "Episode 243, Total Reward: -3356709.02\n",
      "Episode 244, Total Reward: -3342364.82\n",
      "Episode 245, Total Reward: -2426254.88\n",
      "Episode 246, Total Reward: -1922523.42\n",
      "Episode 247, Total Reward: -2680432.03\n",
      "Episode 248, Total Reward: -1269786.32\n",
      "Episode 249, Total Reward: -1783825.18\n",
      "Episode 250, Total Reward: -1567648.06\n",
      "Episode 251, Total Reward: -3062440.07\n",
      "Episode 252, Total Reward: -1630589.18\n",
      "Episode 253, Total Reward: -1899450.96\n",
      "Episode 254, Total Reward: -917462.46\n",
      "Episode 255, Total Reward: -2334293.62\n",
      "Episode 256, Total Reward: -1407752.90\n",
      "Episode 257, Total Reward: -994379.00\n",
      "Episode 258, Total Reward: -3775417.98\n",
      "Episode 259, Total Reward: -3800753.81\n",
      "Episode 260, Total Reward: -2230484.80\n",
      "Episode 261, Total Reward: -1506224.06\n",
      "Episode 262, Total Reward: -2471090.56\n",
      "Episode 263, Total Reward: -2875469.89\n",
      "Episode 264, Total Reward: -3328644.60\n",
      "Episode 265, Total Reward: -1897059.74\n",
      "Episode 266, Total Reward: -3145883.85\n",
      "Episode 267, Total Reward: -1390374.36\n",
      "Episode 268, Total Reward: -2874070.39\n",
      "Episode 269, Total Reward: -1641552.76\n",
      "Episode 270, Total Reward: -1548114.41\n",
      "Episode 271, Total Reward: -907962.17\n",
      "Episode 272, Total Reward: -2724950.26\n",
      "Episode 273, Total Reward: -1199198.32\n",
      "Episode 274, Total Reward: -1798429.91\n",
      "Episode 275, Total Reward: -4241368.79\n",
      "Episode 276, Total Reward: -1763294.68\n",
      "Episode 277, Total Reward: -1487585.82\n",
      "Episode 278, Total Reward: -3123924.49\n",
      "Episode 279, Total Reward: -4176919.71\n",
      "Episode 280, Total Reward: -3501471.55\n",
      "Episode 281, Total Reward: -1137778.62\n",
      "Episode 282, Total Reward: -1941933.58\n",
      "Episode 283, Total Reward: -2607539.77\n",
      "Episode 284, Total Reward: -4108769.56\n",
      "Episode 285, Total Reward: -2153310.34\n",
      "Episode 286, Total Reward: -3003329.16\n",
      "Episode 287, Total Reward: -1297574.99\n",
      "Episode 288, Total Reward: -1716110.31\n",
      "Episode 289, Total Reward: -1133542.09\n",
      "Episode 290, Total Reward: -2900994.05\n",
      "Episode 291, Total Reward: -870505.80\n",
      "Episode 292, Total Reward: -1886967.31\n",
      "Episode 293, Total Reward: -1889744.74\n",
      "Episode 294, Total Reward: -2550475.79\n",
      "Episode 295, Total Reward: -1395098.79\n",
      "Episode 296, Total Reward: -1126295.71\n",
      "Episode 297, Total Reward: -1552872.22\n",
      "Episode 298, Total Reward: -2640806.06\n",
      "Episode 299, Total Reward: -2186226.89\n",
      "Episode 300, Total Reward: -3974074.73\n",
      "Episode 301, Total Reward: -1535799.32\n",
      "Episode 302, Total Reward: -3251376.76\n",
      "Episode 303, Total Reward: -3727647.61\n",
      "Episode 304, Total Reward: -3517736.34\n",
      "Episode 305, Total Reward: -2628619.07\n",
      "Episode 306, Total Reward: -1671220.28\n",
      "Episode 307, Total Reward: -1670454.12\n",
      "Episode 308, Total Reward: -1718980.19\n",
      "Episode 309, Total Reward: -2694761.03\n",
      "Episode 310, Total Reward: -1541741.36\n",
      "Episode 311, Total Reward: -3313865.36\n",
      "Episode 312, Total Reward: -1525287.00\n",
      "Episode 313, Total Reward: -1028664.66\n",
      "Episode 314, Total Reward: -1734880.29\n",
      "Episode 315, Total Reward: -1284971.37\n",
      "Episode 316, Total Reward: -1216217.57\n",
      "Episode 317, Total Reward: -1842982.22\n",
      "Episode 318, Total Reward: -2127118.91\n",
      "Episode 319, Total Reward: -3164696.33\n",
      "Episode 320, Total Reward: -3022758.62\n",
      "Episode 321, Total Reward: -2501219.74\n",
      "Episode 322, Total Reward: -1672752.20\n",
      "Episode 323, Total Reward: -2280180.33\n",
      "Episode 324, Total Reward: -2715071.81\n",
      "Episode 325, Total Reward: -1808795.51\n",
      "Episode 326, Total Reward: -2654714.31\n",
      "Episode 327, Total Reward: -1679006.17\n",
      "Episode 328, Total Reward: -1752421.67\n",
      "Episode 329, Total Reward: -1884182.74\n",
      "Episode 330, Total Reward: -2895562.27\n",
      "Episode 331, Total Reward: -2708976.99\n",
      "Episode 332, Total Reward: -1807774.30\n",
      "Episode 333, Total Reward: -2851520.59\n",
      "Episode 334, Total Reward: -860742.29\n",
      "Episode 335, Total Reward: -3072811.69\n",
      "Episode 336, Total Reward: -2078160.64\n",
      "Episode 337, Total Reward: -1022482.83\n",
      "Episode 338, Total Reward: -1414164.82\n",
      "Episode 339, Total Reward: -1897311.63\n",
      "Episode 340, Total Reward: -3235357.07\n",
      "Episode 341, Total Reward: -1363287.84\n",
      "Episode 342, Total Reward: -1706928.34\n",
      "Episode 343, Total Reward: -2176624.39\n",
      "Episode 344, Total Reward: -3163716.13\n",
      "Episode 345, Total Reward: -3648708.88\n",
      "Episode 346, Total Reward: -1349707.22\n",
      "Episode 347, Total Reward: -1589939.35\n",
      "Episode 348, Total Reward: -1478071.98\n",
      "Episode 349, Total Reward: -1332057.03\n",
      "Episode 350, Total Reward: -1962230.73\n",
      "Episode 351, Total Reward: -1795254.42\n",
      "Episode 352, Total Reward: -1525170.37\n",
      "Episode 353, Total Reward: -2176707.03\n",
      "Episode 354, Total Reward: -2316252.96\n",
      "Episode 355, Total Reward: -2112834.04\n",
      "Episode 356, Total Reward: -2461166.79\n",
      "Episode 357, Total Reward: -2093338.54\n",
      "Episode 358, Total Reward: -2257974.87\n",
      "Episode 359, Total Reward: -2169105.13\n",
      "Episode 360, Total Reward: -2018947.00\n",
      "Episode 361, Total Reward: -2609784.67\n",
      "Episode 362, Total Reward: -2114771.06\n",
      "Episode 363, Total Reward: -3141883.72\n",
      "Episode 364, Total Reward: -746040.71\n",
      "Episode 365, Total Reward: -2641255.37\n",
      "Episode 366, Total Reward: -1912989.60\n",
      "Episode 367, Total Reward: -3055177.04\n",
      "Episode 368, Total Reward: -1378002.67\n",
      "Episode 369, Total Reward: -1805058.86\n",
      "Episode 370, Total Reward: -2518211.68\n",
      "Episode 371, Total Reward: -2995035.37\n",
      "Episode 372, Total Reward: -2208154.79\n",
      "Episode 373, Total Reward: -2436420.11\n",
      "Episode 374, Total Reward: -1937908.66\n",
      "Episode 375, Total Reward: -1453116.78\n",
      "Episode 376, Total Reward: -1589263.38\n",
      "Episode 377, Total Reward: -1922786.59\n",
      "Episode 378, Total Reward: -2059995.21\n",
      "Episode 379, Total Reward: -2438368.19\n",
      "Episode 380, Total Reward: -2871648.84\n",
      "Episode 381, Total Reward: -1121169.79\n",
      "Episode 382, Total Reward: -2203471.77\n",
      "Episode 383, Total Reward: -2033464.57\n",
      "Episode 384, Total Reward: -2033042.99\n",
      "Episode 385, Total Reward: -2313857.49\n",
      "Episode 386, Total Reward: -1153926.11\n",
      "Episode 387, Total Reward: -2205237.28\n",
      "Episode 388, Total Reward: -2073696.44\n",
      "Episode 389, Total Reward: -1990562.53\n",
      "Episode 390, Total Reward: -2731887.11\n",
      "Episode 391, Total Reward: -2076434.65\n",
      "Episode 392, Total Reward: -1521581.06\n",
      "Episode 393, Total Reward: -2128866.47\n",
      "Episode 394, Total Reward: -2439448.05\n",
      "Episode 395, Total Reward: -1524530.83\n",
      "Episode 396, Total Reward: -1299205.36\n",
      "Episode 397, Total Reward: -3543363.99\n",
      "Episode 398, Total Reward: -1730399.00\n",
      "Episode 399, Total Reward: -2024724.61\n",
      "Episode 400, Total Reward: -1718898.09\n",
      "Episode 401, Total Reward: -1742125.32\n",
      "Episode 402, Total Reward: -1672171.87\n",
      "Episode 403, Total Reward: -1363435.80\n",
      "Episode 404, Total Reward: -1456687.19\n",
      "Episode 405, Total Reward: -1818055.47\n",
      "Episode 406, Total Reward: -2964639.34\n",
      "Episode 407, Total Reward: -1471533.71\n",
      "Episode 408, Total Reward: -2712388.31\n",
      "Episode 409, Total Reward: -2974023.24\n",
      "Episode 410, Total Reward: -3598686.84\n",
      "Episode 411, Total Reward: -1276037.05\n",
      "Episode 412, Total Reward: -1419815.07\n",
      "Episode 413, Total Reward: -1346882.32\n",
      "Episode 414, Total Reward: -2430721.00\n",
      "Episode 415, Total Reward: -1648115.28\n",
      "Episode 416, Total Reward: -2456597.42\n",
      "Episode 417, Total Reward: -2965887.29\n",
      "Episode 418, Total Reward: -2097361.95\n",
      "Episode 419, Total Reward: -1442936.23\n",
      "Episode 420, Total Reward: -2204801.85\n",
      "Episode 421, Total Reward: -2791083.97\n",
      "Episode 422, Total Reward: -1403047.17\n",
      "Episode 423, Total Reward: -2398609.83\n",
      "Episode 424, Total Reward: -2476982.49\n",
      "Episode 425, Total Reward: -2216132.75\n",
      "Episode 426, Total Reward: -1686070.25\n",
      "Episode 427, Total Reward: -1665492.57\n",
      "Episode 428, Total Reward: -2585964.69\n",
      "Episode 429, Total Reward: -1896573.26\n",
      "Episode 430, Total Reward: -3010494.72\n",
      "Episode 431, Total Reward: -1293979.30\n",
      "Episode 432, Total Reward: -2956568.81\n",
      "Episode 433, Total Reward: -1834984.51\n",
      "Episode 434, Total Reward: -1387387.77\n",
      "Episode 435, Total Reward: -2019668.69\n",
      "Episode 436, Total Reward: -2097895.16\n",
      "Episode 437, Total Reward: -2309418.50\n",
      "Episode 438, Total Reward: -1509572.43\n",
      "Episode 439, Total Reward: -1979096.54\n",
      "Episode 440, Total Reward: -2629286.12\n",
      "Episode 441, Total Reward: -1903531.13\n",
      "Episode 442, Total Reward: -1440937.30\n",
      "Episode 443, Total Reward: -2289281.07\n",
      "Episode 444, Total Reward: -2500344.16\n",
      "Episode 445, Total Reward: -2107717.92\n",
      "Episode 446, Total Reward: -1588879.46\n",
      "Episode 447, Total Reward: -803878.34\n",
      "Episode 448, Total Reward: -3157870.93\n",
      "Episode 449, Total Reward: -2969263.01\n",
      "Episode 450, Total Reward: -2145489.88\n",
      "Episode 451, Total Reward: -2926663.76\n",
      "Episode 452, Total Reward: -1765462.12\n",
      "Episode 453, Total Reward: -2238062.99\n",
      "Episode 454, Total Reward: -2564501.70\n",
      "Episode 455, Total Reward: -1820547.79\n",
      "Episode 456, Total Reward: -2381538.10\n",
      "Episode 457, Total Reward: -1531143.50\n",
      "Episode 458, Total Reward: -1786668.83\n",
      "Episode 459, Total Reward: -1462300.36\n",
      "Episode 460, Total Reward: -1984996.52\n",
      "Episode 461, Total Reward: -2309638.61\n",
      "Episode 462, Total Reward: -2353854.23\n",
      "Episode 463, Total Reward: -3102099.93\n",
      "Episode 464, Total Reward: -1620930.34\n",
      "Episode 465, Total Reward: -2509282.02\n",
      "Episode 466, Total Reward: -2113168.36\n",
      "Episode 467, Total Reward: -1675831.03\n",
      "Episode 468, Total Reward: -1702754.47\n",
      "Episode 469, Total Reward: -1164284.82\n",
      "Episode 470, Total Reward: -2145545.43\n",
      "Episode 471, Total Reward: -2621976.57\n",
      "Episode 472, Total Reward: -2204345.30\n",
      "Episode 473, Total Reward: -2179133.12\n",
      "Episode 474, Total Reward: -3540544.58\n",
      "Episode 475, Total Reward: -1684550.08\n",
      "Episode 476, Total Reward: -1957382.00\n",
      "Episode 477, Total Reward: -1760148.49\n",
      "Episode 478, Total Reward: -2363397.89\n",
      "Episode 479, Total Reward: -1548773.95\n",
      "Episode 480, Total Reward: -1676137.03\n",
      "Episode 481, Total Reward: -1927344.28\n",
      "Episode 482, Total Reward: -1729390.08\n",
      "Episode 483, Total Reward: -1560998.76\n",
      "Episode 484, Total Reward: -1925093.71\n",
      "Episode 485, Total Reward: -1830051.77\n",
      "Episode 486, Total Reward: -2242517.24\n",
      "Episode 487, Total Reward: -2213210.31\n",
      "Episode 488, Total Reward: -1242480.62\n",
      "Episode 489, Total Reward: -1048266.15\n",
      "Episode 490, Total Reward: -2714868.21\n",
      "Episode 491, Total Reward: -2346931.48\n",
      "Episode 492, Total Reward: -1830110.20\n",
      "Episode 493, Total Reward: -2200191.88\n",
      "Episode 494, Total Reward: -1650314.95\n",
      "Episode 495, Total Reward: -2456487.81\n",
      "Episode 496, Total Reward: -1687653.21\n",
      "Episode 497, Total Reward: -2036374.61\n",
      "Episode 498, Total Reward: -2786017.09\n",
      "Episode 499, Total Reward: -2756257.46\n",
      "Episode 500, Total Reward: -2178848.98\n",
      "训练完成！\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# 1. 创建一个发酵模拟环境 (这里使用 Gym 库来构建)\n",
    "class FermentationEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(FermentationEnv, self).__init__()\n",
    "        \n",
    "        # 状态空间：假设有6个状态变量 (温度, pH, 搅拌速度等)\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([20, 4.5, 100, 0.5]), \n",
    "                                                high=np.array([40, 7.5, 500, 2.0]),\n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "        # 动作空间：假设每次可以调整4个参数（温度、pH、搅拌速度、通气量）\n",
    "        self.action_space = gym.spaces.Discrete(8)\n",
    "        \n",
    "        # 初始状态 (温度, pH, 搅拌速度, 通气量)\n",
    "        self.state = np.array([30.0, 6.0, 300.0, 1.0], dtype=np.float32)\n",
    "        \n",
    "        # 目标：最大化产量，初始化产量为0\n",
    "        self.yield_production = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        # 重置环境状态\n",
    "        self.state = np.array([30.0, 6.0, 300.0, 1.0], dtype=np.float32)\n",
    "        self.yield_production = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 根据动作调整状态，动作对应于不同的调整策略\n",
    "        if action == 0:\n",
    "            self.state[0] -= 1.0  # 降低温度\n",
    "        elif action == 1:\n",
    "            self.state[0] += 1.0  # 增加温度\n",
    "        elif action == 2:\n",
    "            self.state[1] -= 0.1  # 降低 pH\n",
    "        elif action == 3:\n",
    "            self.state[1] += 0.1  # 增加 pH\n",
    "        elif action == 4:\n",
    "            self.state[2] -= 10.0  # 降低搅拌速度\n",
    "        elif action == 5:\n",
    "            self.state[2] += 10.0  # 增加搅拌速度\n",
    "        elif action == 6:\n",
    "            self.state[3] -= 0.1  # 降低通气量\n",
    "        elif action == 7:\n",
    "            self.state[3] += 0.1  # 增加通气量\n",
    "        \n",
    "        # 限制状态变量在合理范围内\n",
    "        self.state = np.clip(self.state, self.observation_space.low, self.observation_space.high)\n",
    "        \n",
    "        # 产量计算：这里用简单函数模拟产量与状态的关系\n",
    "        self.yield_production = - (self.state[0] - 35) ** 2 - (self.state[1] - 5.5) ** 2 \\\n",
    "                                - (self.state[2] - 400) ** 2 - (self.state[3] - 1.5) ** 2\n",
    "        reward = self.yield_production\n",
    "        \n",
    "        # 判断是否完成\n",
    "        done = False\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "# 2. 构建 DQN 的神经网络模型\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 3. 训练 DQN 模型\n",
    "def train_dqn(env):\n",
    "    # 定义超参数\n",
    "    learning_rate = 0.001\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    epsilon_min = 0.01\n",
    "    memory_size = 2000\n",
    "    batch_size = 64\n",
    "    target_update = 10  # 目标网络更新频率\n",
    "    \n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    \n",
    "    # 创建 DQN 模型和目标网络\n",
    "    policy_net = DQN(input_dim, output_dim)\n",
    "    target_net = DQN(input_dim, output_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    def choose_action(state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return env.action_space.sample()  # 随机动作\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return torch.argmax(q_values).item()  # 选择最优动作\n",
    "    \n",
    "    def replay():\n",
    "        if len(memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(memory, batch_size)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        \n",
    "        q_values = policy_net(states).gather(1, actions)\n",
    "        next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + gamma * next_q_values\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 训练过程\n",
    "    episodes = 500\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for t in range(200):  # 限定每个 episode 的最大步数\n",
    "            action = choose_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            replay()  # 经验回放\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 衰减探索率\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        # 更新目标网络\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    print(\"训练完成！\")\n",
    "\n",
    "# 4. 运行强化学习\n",
    "if __name__ == \"__main__\":\n",
    "    env = FermentationEnv()\n",
    "    train_dqn(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_params, best_reward\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 评估强化学习智能体\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m best_params, best_reward \u001b[38;5;241m=\u001b[39m evaluate_agent(env, policy_net)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy_net' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(env, policy_net, episodes=5):\n",
    "    \"\"\"\n",
    "    评估智能体在强化学习完成后的表现，返回最优的参数和优化结果。\n",
    "    \n",
    "    :param env: 发酵模拟环境\n",
    "    :param policy_net: 训练完成的 DQN 模型\n",
    "    :param episodes: 评估的实验次数\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    optimal_params = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        params_per_episode = []\n",
    "        \n",
    "        for t in range(200):  # 限定每个 episode 的最大步数\n",
    "            # 选择动作 (不再使用随机探索，直接使用训练好的策略)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            params_per_episode.append(state)  # 保存每一步的状态（参数）\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        optimal_params.append(params_per_episode[-1])  # 保存每个 episode 的最终参数\n",
    "    \n",
    "    # 输出结果\n",
    "    best_episode_index = np.argmax(total_rewards)  # 找到最优的 episode\n",
    "    best_params = optimal_params[best_episode_index]\n",
    "    best_reward = total_rewards[best_episode_index]\n",
    "    \n",
    "    print(f\"最优参数: {best_params}\")\n",
    "    print(f\"最优产量: {best_reward:.2f}\")\n",
    "    \n",
    "    return best_params, best_reward\n",
    "\n",
    "# 评估强化学习智能体\n",
    "best_params, best_reward = evaluate_agent(env, policy_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -2499471.93\n",
      "Episode 2, Total Reward: -2457329.07\n",
      "Episode 3, Total Reward: -1795462.24\n",
      "Episode 4, Total Reward: -4159304.46\n",
      "Episode 5, Total Reward: -3635325.10\n",
      "Episode 6, Total Reward: -5920159.36\n",
      "Episode 7, Total Reward: -661494.13\n",
      "Episode 8, Total Reward: -657810.72\n",
      "Episode 9, Total Reward: -605850.16\n",
      "Episode 10, Total Reward: -916545.68\n",
      "Episode 11, Total Reward: -887898.37\n",
      "Episode 12, Total Reward: -665502.09\n",
      "Episode 13, Total Reward: -883909.48\n",
      "Episode 14, Total Reward: -1225507.66\n",
      "Episode 15, Total Reward: -469315.64\n",
      "Episode 16, Total Reward: -546885.54\n",
      "Episode 17, Total Reward: -715370.22\n",
      "Episode 18, Total Reward: -385686.06\n",
      "Episode 19, Total Reward: -1583913.79\n",
      "Episode 20, Total Reward: -1299900.77\n",
      "Episode 21, Total Reward: -1170742.90\n",
      "Episode 22, Total Reward: -498404.05\n",
      "Episode 23, Total Reward: -643611.39\n",
      "Episode 24, Total Reward: -3158226.83\n",
      "Episode 25, Total Reward: -2843086.57\n",
      "Episode 26, Total Reward: -2539061.20\n",
      "Episode 27, Total Reward: -3661685.52\n",
      "Episode 28, Total Reward: -2281055.90\n",
      "Episode 29, Total Reward: -3158113.47\n",
      "Episode 30, Total Reward: -2601357.78\n",
      "Episode 31, Total Reward: -5213175.52\n",
      "Episode 32, Total Reward: -967366.34\n",
      "Episode 33, Total Reward: -998808.47\n",
      "Episode 34, Total Reward: -2325824.60\n",
      "Episode 35, Total Reward: -877607.76\n",
      "Episode 36, Total Reward: -4885283.91\n",
      "Episode 37, Total Reward: -2305434.05\n",
      "Episode 38, Total Reward: -2119458.31\n",
      "Episode 39, Total Reward: -1380995.92\n",
      "Episode 40, Total Reward: -2267080.68\n",
      "Episode 41, Total Reward: -1192656.17\n",
      "Episode 42, Total Reward: -1440014.17\n",
      "Episode 43, Total Reward: -1132415.91\n",
      "Episode 44, Total Reward: -3910627.27\n",
      "Episode 45, Total Reward: -2303720.58\n",
      "Episode 46, Total Reward: -5929366.95\n",
      "Episode 47, Total Reward: -1363055.96\n",
      "Episode 48, Total Reward: -3331784.73\n",
      "Episode 49, Total Reward: -2381422.89\n",
      "Episode 50, Total Reward: -2056549.98\n",
      "Episode 51, Total Reward: -1443850.63\n",
      "Episode 52, Total Reward: -716286.26\n",
      "Episode 53, Total Reward: -1449415.15\n",
      "Episode 54, Total Reward: -2462440.29\n",
      "Episode 55, Total Reward: -1869436.35\n",
      "Episode 56, Total Reward: -2539439.22\n",
      "Episode 57, Total Reward: -4328123.14\n",
      "Episode 58, Total Reward: -3912889.11\n",
      "Episode 59, Total Reward: -1615732.59\n",
      "Episode 60, Total Reward: -2385082.67\n",
      "Episode 61, Total Reward: -1357244.37\n",
      "Episode 62, Total Reward: -1124161.36\n",
      "Episode 63, Total Reward: -1654379.32\n",
      "Episode 64, Total Reward: -3203411.94\n",
      "Episode 65, Total Reward: -1139303.40\n",
      "Episode 66, Total Reward: -520620.06\n",
      "Episode 67, Total Reward: -4191869.65\n",
      "Episode 68, Total Reward: -1213242.87\n",
      "Episode 69, Total Reward: -2205728.04\n",
      "Episode 70, Total Reward: -1916966.75\n",
      "Episode 71, Total Reward: -3214131.16\n",
      "Episode 72, Total Reward: -2056654.29\n",
      "Episode 73, Total Reward: -2598380.85\n",
      "Episode 74, Total Reward: -2705921.90\n",
      "Episode 75, Total Reward: -1931019.26\n",
      "Episode 76, Total Reward: -3499889.28\n",
      "Episode 77, Total Reward: -3454519.32\n",
      "Episode 78, Total Reward: -2854773.93\n",
      "Episode 79, Total Reward: -3028992.78\n",
      "Episode 80, Total Reward: -1755044.67\n",
      "Episode 81, Total Reward: -1006182.90\n",
      "Episode 82, Total Reward: -2394446.11\n",
      "Episode 83, Total Reward: -2154595.68\n",
      "Episode 84, Total Reward: -3988651.37\n",
      "Episode 85, Total Reward: -2556665.29\n",
      "Episode 86, Total Reward: -2452196.01\n",
      "Episode 87, Total Reward: -1289208.43\n",
      "Episode 88, Total Reward: -4273425.54\n",
      "Episode 89, Total Reward: -1945961.45\n",
      "Episode 90, Total Reward: -5670811.99\n",
      "Episode 91, Total Reward: -1859911.05\n",
      "Episode 92, Total Reward: -1772964.96\n",
      "Episode 93, Total Reward: -661203.97\n",
      "Episode 94, Total Reward: -1443491.12\n",
      "Episode 95, Total Reward: -2785721.17\n",
      "Episode 96, Total Reward: -835716.38\n",
      "Episode 97, Total Reward: -2007912.58\n",
      "Episode 98, Total Reward: -2919035.79\n",
      "Episode 99, Total Reward: -2276514.53\n",
      "Episode 100, Total Reward: -1298092.10\n",
      "Episode 101, Total Reward: -2996328.88\n",
      "Episode 102, Total Reward: -1972865.40\n",
      "Episode 103, Total Reward: -886247.55\n",
      "Episode 104, Total Reward: -1824593.51\n",
      "Episode 105, Total Reward: -2839110.67\n",
      "Episode 106, Total Reward: -2385372.62\n",
      "Episode 107, Total Reward: -2172355.14\n",
      "Episode 108, Total Reward: -544288.75\n",
      "Episode 109, Total Reward: -2592599.11\n",
      "Episode 110, Total Reward: -2817671.08\n",
      "Episode 111, Total Reward: -1914234.41\n",
      "Episode 112, Total Reward: -2020237.85\n",
      "Episode 113, Total Reward: -1910391.53\n",
      "Episode 114, Total Reward: -2097489.81\n",
      "Episode 115, Total Reward: -2343631.32\n",
      "Episode 116, Total Reward: -3601550.71\n",
      "Episode 117, Total Reward: -780370.00\n",
      "Episode 118, Total Reward: -1317594.17\n",
      "Episode 119, Total Reward: -2140675.42\n",
      "Episode 120, Total Reward: -2990122.24\n",
      "Episode 121, Total Reward: -2663790.23\n",
      "Episode 122, Total Reward: -1035755.85\n",
      "Episode 123, Total Reward: -956485.91\n",
      "Episode 124, Total Reward: -1655042.62\n",
      "Episode 125, Total Reward: -1161669.31\n",
      "Episode 126, Total Reward: -4010701.64\n",
      "Episode 127, Total Reward: -2256707.81\n",
      "Episode 128, Total Reward: -2872087.38\n",
      "Episode 129, Total Reward: -2427456.62\n",
      "Episode 130, Total Reward: -395476.55\n",
      "Episode 131, Total Reward: -1232433.52\n",
      "Episode 132, Total Reward: -1752128.77\n",
      "Episode 133, Total Reward: -1404479.76\n",
      "Episode 134, Total Reward: -2927367.21\n",
      "Episode 135, Total Reward: -5576986.45\n",
      "Episode 136, Total Reward: -5060289.85\n",
      "Episode 137, Total Reward: -1376373.93\n",
      "Episode 138, Total Reward: -1627411.78\n",
      "Episode 139, Total Reward: -1198706.81\n",
      "Episode 140, Total Reward: -3214479.88\n",
      "Episode 141, Total Reward: -1614570.28\n",
      "Episode 142, Total Reward: -4163184.48\n",
      "Episode 143, Total Reward: -941080.43\n",
      "Episode 144, Total Reward: -2617826.31\n",
      "Episode 145, Total Reward: -3572462.95\n",
      "Episode 146, Total Reward: -1492853.52\n",
      "Episode 147, Total Reward: -1645600.64\n",
      "Episode 148, Total Reward: -2233954.93\n",
      "Episode 149, Total Reward: -2000340.61\n",
      "Episode 150, Total Reward: -2381135.94\n",
      "Episode 151, Total Reward: -2136370.48\n",
      "Episode 152, Total Reward: -738396.55\n",
      "Episode 153, Total Reward: -4214623.43\n",
      "Episode 154, Total Reward: -2494775.50\n",
      "Episode 155, Total Reward: -1429203.86\n",
      "Episode 156, Total Reward: -2833375.80\n",
      "Episode 157, Total Reward: -2018378.81\n",
      "Episode 158, Total Reward: -448507.90\n",
      "Episode 159, Total Reward: -1381715.04\n",
      "Episode 160, Total Reward: -4271101.45\n",
      "Episode 161, Total Reward: -1646947.75\n",
      "Episode 162, Total Reward: -2193404.55\n",
      "Episode 163, Total Reward: -2408862.90\n",
      "Episode 164, Total Reward: -1190081.21\n",
      "Episode 165, Total Reward: -2598468.75\n",
      "Episode 166, Total Reward: -2778631.60\n",
      "Episode 167, Total Reward: -4847981.91\n",
      "Episode 168, Total Reward: -2305344.41\n",
      "Episode 169, Total Reward: -2765326.16\n",
      "Episode 170, Total Reward: -651810.57\n",
      "Episode 171, Total Reward: -860700.09\n",
      "Episode 172, Total Reward: -959668.69\n",
      "Episode 173, Total Reward: -1798361.17\n",
      "Episode 174, Total Reward: -2933097.52\n",
      "Episode 175, Total Reward: -612031.99\n",
      "Episode 176, Total Reward: -2394260.37\n",
      "Episode 177, Total Reward: -5521171.01\n",
      "Episode 178, Total Reward: -1319107.03\n",
      "Episode 179, Total Reward: -1990385.07\n",
      "Episode 180, Total Reward: -767992.92\n",
      "Episode 181, Total Reward: -1182538.14\n",
      "Episode 182, Total Reward: -4302129.55\n",
      "Episode 183, Total Reward: -651636.21\n",
      "Episode 184, Total Reward: -807342.57\n",
      "Episode 185, Total Reward: -3008472.18\n",
      "Episode 186, Total Reward: -2965031.73\n",
      "Episode 187, Total Reward: -3334586.37\n",
      "Episode 188, Total Reward: -971423.33\n",
      "Episode 189, Total Reward: -3731921.00\n",
      "Episode 190, Total Reward: -2152391.48\n",
      "Episode 191, Total Reward: -4182585.04\n",
      "Episode 192, Total Reward: -1943640.99\n",
      "Episode 193, Total Reward: -4205616.89\n",
      "Episode 194, Total Reward: -1504238.38\n",
      "Episode 195, Total Reward: -2224233.73\n",
      "Episode 196, Total Reward: -2348004.82\n",
      "Episode 197, Total Reward: -1721786.80\n",
      "Episode 198, Total Reward: -1770472.49\n",
      "Episode 199, Total Reward: -921891.31\n",
      "Episode 200, Total Reward: -2288136.37\n",
      "Episode 201, Total Reward: -3279996.54\n",
      "Episode 202, Total Reward: -1207580.18\n",
      "Episode 203, Total Reward: -2677552.68\n",
      "Episode 204, Total Reward: -2792556.05\n",
      "Episode 205, Total Reward: -5179185.97\n",
      "Episode 206, Total Reward: -2335141.52\n",
      "Episode 207, Total Reward: -2064334.51\n",
      "Episode 208, Total Reward: -1664634.36\n",
      "Episode 209, Total Reward: -1225632.89\n",
      "Episode 210, Total Reward: -2158139.40\n",
      "Episode 211, Total Reward: -1189151.77\n",
      "Episode 212, Total Reward: -2971230.55\n",
      "Episode 213, Total Reward: -1061522.32\n",
      "Episode 214, Total Reward: -2031258.42\n",
      "Episode 215, Total Reward: -2763990.92\n",
      "Episode 216, Total Reward: -2824187.32\n",
      "Episode 217, Total Reward: -1136459.93\n",
      "Episode 218, Total Reward: -3158739.51\n",
      "Episode 219, Total Reward: -2567418.34\n",
      "Episode 220, Total Reward: -1824600.90\n",
      "Episode 221, Total Reward: -645883.51\n",
      "Episode 222, Total Reward: -2054880.84\n",
      "Episode 223, Total Reward: -2716751.11\n",
      "Episode 224, Total Reward: -710686.35\n",
      "Episode 225, Total Reward: -3220644.30\n",
      "Episode 226, Total Reward: -1145805.07\n",
      "Episode 227, Total Reward: -1333437.07\n",
      "Episode 228, Total Reward: -1514941.37\n",
      "Episode 229, Total Reward: -1801485.13\n",
      "Episode 230, Total Reward: -2855533.11\n",
      "Episode 231, Total Reward: -2320877.09\n",
      "Episode 232, Total Reward: -1999355.67\n",
      "Episode 233, Total Reward: -3185674.42\n",
      "Episode 234, Total Reward: -704471.04\n",
      "Episode 235, Total Reward: -948399.83\n",
      "Episode 236, Total Reward: -2379259.66\n",
      "Episode 237, Total Reward: -1545167.21\n",
      "Episode 238, Total Reward: -2436237.43\n",
      "Episode 239, Total Reward: -1373234.08\n",
      "Episode 240, Total Reward: -1822830.42\n",
      "Episode 241, Total Reward: -1859590.64\n",
      "Episode 242, Total Reward: -3746998.40\n",
      "Episode 243, Total Reward: -1970126.06\n",
      "Episode 244, Total Reward: -744761.09\n",
      "Episode 245, Total Reward: -1694903.63\n",
      "Episode 246, Total Reward: -1995256.15\n",
      "Episode 247, Total Reward: -1556496.95\n",
      "Episode 248, Total Reward: -2452117.50\n",
      "Episode 249, Total Reward: -2475237.01\n",
      "Episode 250, Total Reward: -2914778.79\n",
      "Episode 251, Total Reward: -2070662.22\n",
      "Episode 252, Total Reward: -1569649.66\n",
      "Episode 253, Total Reward: -876372.28\n",
      "Episode 254, Total Reward: -2310943.50\n",
      "Episode 255, Total Reward: -1109389.91\n",
      "Episode 256, Total Reward: -1404881.95\n",
      "Episode 257, Total Reward: -1177384.76\n",
      "Episode 258, Total Reward: -1190281.16\n",
      "Episode 259, Total Reward: -2120654.25\n",
      "Episode 260, Total Reward: -3978095.68\n",
      "Episode 261, Total Reward: -2833691.66\n",
      "Episode 262, Total Reward: -1302461.80\n",
      "Episode 263, Total Reward: -2418169.04\n",
      "Episode 264, Total Reward: -1531331.00\n",
      "Episode 265, Total Reward: -2071133.37\n",
      "Episode 266, Total Reward: -3433017.68\n",
      "Episode 267, Total Reward: -2087890.03\n",
      "Episode 268, Total Reward: -2098204.84\n",
      "Episode 269, Total Reward: -2982442.22\n",
      "Episode 270, Total Reward: -1693287.03\n",
      "Episode 271, Total Reward: -1847848.29\n",
      "Episode 272, Total Reward: -1672406.59\n",
      "Episode 273, Total Reward: -2145115.50\n",
      "Episode 274, Total Reward: -1712212.67\n",
      "Episode 275, Total Reward: -2961367.94\n",
      "Episode 276, Total Reward: -1992459.03\n",
      "Episode 277, Total Reward: -1653216.62\n",
      "Episode 278, Total Reward: -1143977.16\n",
      "Episode 279, Total Reward: -2894460.61\n",
      "Episode 280, Total Reward: -1538723.85\n",
      "Episode 281, Total Reward: -2767039.74\n",
      "Episode 282, Total Reward: -1364313.59\n",
      "Episode 283, Total Reward: -1267369.85\n",
      "Episode 284, Total Reward: -3091393.36\n",
      "Episode 285, Total Reward: -2030265.75\n",
      "Episode 286, Total Reward: -2039191.29\n",
      "Episode 287, Total Reward: -2840937.68\n",
      "Episode 288, Total Reward: -1056268.00\n",
      "Episode 289, Total Reward: -2925120.38\n",
      "Episode 290, Total Reward: -3176743.58\n",
      "Episode 291, Total Reward: -2887839.27\n",
      "Episode 292, Total Reward: -2430704.06\n",
      "Episode 293, Total Reward: -1510238.35\n",
      "Episode 294, Total Reward: -2782365.94\n",
      "Episode 295, Total Reward: -1334059.40\n",
      "Episode 296, Total Reward: -962806.01\n",
      "Episode 297, Total Reward: -1500130.05\n",
      "Episode 298, Total Reward: -2050296.18\n",
      "Episode 299, Total Reward: -1802736.99\n",
      "Episode 300, Total Reward: -2621943.83\n",
      "Episode 301, Total Reward: -1561955.53\n",
      "Episode 302, Total Reward: -2503748.02\n",
      "Episode 303, Total Reward: -1254609.84\n",
      "Episode 304, Total Reward: -1779014.62\n",
      "Episode 305, Total Reward: -1753850.94\n",
      "Episode 306, Total Reward: -1505254.71\n",
      "Episode 307, Total Reward: -1880786.02\n",
      "Episode 308, Total Reward: -2495659.49\n",
      "Episode 309, Total Reward: -3060634.39\n",
      "Episode 310, Total Reward: -964649.44\n",
      "Episode 311, Total Reward: -920039.99\n",
      "Episode 312, Total Reward: -2110452.13\n",
      "Episode 313, Total Reward: -2481620.12\n",
      "Episode 314, Total Reward: -2571426.51\n",
      "Episode 315, Total Reward: -2028279.04\n",
      "Episode 316, Total Reward: -603094.11\n",
      "Episode 317, Total Reward: -2078334.52\n",
      "Episode 318, Total Reward: -2193268.63\n",
      "Episode 319, Total Reward: -2601404.29\n",
      "Episode 320, Total Reward: -1372114.03\n",
      "Episode 321, Total Reward: -2535007.12\n",
      "Episode 322, Total Reward: -1863335.93\n",
      "Episode 323, Total Reward: -3013294.20\n",
      "Episode 324, Total Reward: -2668971.62\n",
      "Episode 325, Total Reward: -1652667.97\n",
      "Episode 326, Total Reward: -2874346.03\n",
      "Episode 327, Total Reward: -1970933.36\n",
      "Episode 328, Total Reward: -1463881.71\n",
      "Episode 329, Total Reward: -1020723.26\n",
      "Episode 330, Total Reward: -1692470.92\n",
      "Episode 331, Total Reward: -973943.75\n",
      "Episode 332, Total Reward: -2463716.78\n",
      "Episode 333, Total Reward: -1662747.68\n",
      "Episode 334, Total Reward: -2652961.79\n",
      "Episode 335, Total Reward: -1127122.52\n",
      "Episode 336, Total Reward: -3263176.42\n",
      "Episode 337, Total Reward: -3450474.80\n",
      "Episode 338, Total Reward: -1711278.54\n",
      "Episode 339, Total Reward: -824108.12\n",
      "Episode 340, Total Reward: -2017682.73\n",
      "Episode 341, Total Reward: -3267008.82\n",
      "Episode 342, Total Reward: -2681853.40\n",
      "Episode 343, Total Reward: -1947961.54\n",
      "Episode 344, Total Reward: -1164672.12\n",
      "Episode 345, Total Reward: -1506511.17\n",
      "Episode 346, Total Reward: -1450973.09\n",
      "Episode 347, Total Reward: -1474662.81\n",
      "Episode 348, Total Reward: -2037551.15\n",
      "Episode 349, Total Reward: -2315038.06\n",
      "Episode 350, Total Reward: -2100408.86\n",
      "Episode 351, Total Reward: -1463161.79\n",
      "Episode 352, Total Reward: -1666497.26\n",
      "Episode 353, Total Reward: -2958725.29\n",
      "Episode 354, Total Reward: -2128595.99\n",
      "Episode 355, Total Reward: -2665150.53\n",
      "Episode 356, Total Reward: -2511372.54\n",
      "Episode 357, Total Reward: -1576203.09\n",
      "Episode 358, Total Reward: -2346489.07\n",
      "Episode 359, Total Reward: -1236671.71\n",
      "Episode 360, Total Reward: -3003403.78\n",
      "Episode 361, Total Reward: -1118114.02\n",
      "Episode 362, Total Reward: -2189846.13\n",
      "Episode 363, Total Reward: -1374888.35\n",
      "Episode 364, Total Reward: -2199768.84\n",
      "Episode 365, Total Reward: -2193820.03\n",
      "Episode 366, Total Reward: -1422587.92\n",
      "Episode 367, Total Reward: -3065025.13\n",
      "Episode 368, Total Reward: -2304633.04\n",
      "Episode 369, Total Reward: -2333174.44\n",
      "Episode 370, Total Reward: -1083361.09\n",
      "Episode 371, Total Reward: -2163243.08\n",
      "Episode 372, Total Reward: -3570900.11\n",
      "Episode 373, Total Reward: -3054592.65\n",
      "Episode 374, Total Reward: -1838303.09\n",
      "Episode 375, Total Reward: -1556845.55\n",
      "Episode 376, Total Reward: -2555605.62\n",
      "Episode 377, Total Reward: -2389313.64\n",
      "Episode 378, Total Reward: -3450180.99\n",
      "Episode 379, Total Reward: -1407703.76\n",
      "Episode 380, Total Reward: -2384236.51\n",
      "Episode 381, Total Reward: -2963424.24\n",
      "Episode 382, Total Reward: -1314757.59\n",
      "Episode 383, Total Reward: -2428647.66\n",
      "Episode 384, Total Reward: -2420512.99\n",
      "Episode 385, Total Reward: -2136462.15\n",
      "Episode 386, Total Reward: -1587963.64\n",
      "Episode 387, Total Reward: -3229973.32\n",
      "Episode 388, Total Reward: -2377824.88\n",
      "Episode 389, Total Reward: -3328771.92\n",
      "Episode 390, Total Reward: -3695265.07\n",
      "Episode 391, Total Reward: -1805622.98\n",
      "Episode 392, Total Reward: -1323145.23\n",
      "Episode 393, Total Reward: -2241985.00\n",
      "Episode 394, Total Reward: -2435562.90\n",
      "Episode 395, Total Reward: -2169481.75\n",
      "Episode 396, Total Reward: -2004839.51\n",
      "Episode 397, Total Reward: -1350741.19\n",
      "Episode 398, Total Reward: -3343318.07\n",
      "Episode 399, Total Reward: -2536917.80\n",
      "Episode 400, Total Reward: -1559984.60\n",
      "Episode 401, Total Reward: -1812897.05\n",
      "Episode 402, Total Reward: -2082318.08\n",
      "Episode 403, Total Reward: -3114661.40\n",
      "Episode 404, Total Reward: -1251526.75\n",
      "Episode 405, Total Reward: -2568949.32\n",
      "Episode 406, Total Reward: -2325672.85\n",
      "Episode 407, Total Reward: -1697728.82\n",
      "Episode 408, Total Reward: -2062097.17\n",
      "Episode 409, Total Reward: -1099309.84\n",
      "Episode 410, Total Reward: -1265609.25\n",
      "Episode 411, Total Reward: -2269478.72\n",
      "Episode 412, Total Reward: -2101519.98\n",
      "Episode 413, Total Reward: -1632478.19\n",
      "Episode 414, Total Reward: -2027984.83\n",
      "Episode 415, Total Reward: -1790203.89\n",
      "Episode 416, Total Reward: -1175911.68\n",
      "Episode 417, Total Reward: -3131503.44\n",
      "Episode 418, Total Reward: -1817393.45\n",
      "Episode 419, Total Reward: -1530661.69\n",
      "Episode 420, Total Reward: -2432784.58\n",
      "Episode 421, Total Reward: -2128832.50\n",
      "Episode 422, Total Reward: -2241499.88\n",
      "Episode 423, Total Reward: -2545388.14\n",
      "Episode 424, Total Reward: -1759090.83\n",
      "Episode 425, Total Reward: -2486374.60\n",
      "Episode 426, Total Reward: -1446482.29\n",
      "Episode 427, Total Reward: -3251602.81\n",
      "Episode 428, Total Reward: -1182726.11\n",
      "Episode 429, Total Reward: -3225856.69\n",
      "Episode 430, Total Reward: -2418075.20\n",
      "Episode 431, Total Reward: -1860460.00\n",
      "Episode 432, Total Reward: -957135.06\n",
      "Episode 433, Total Reward: -1228427.00\n",
      "Episode 434, Total Reward: -2313930.12\n",
      "Episode 435, Total Reward: -2194030.12\n",
      "Episode 436, Total Reward: -1088370.82\n",
      "Episode 437, Total Reward: -2733052.68\n",
      "Episode 438, Total Reward: -1249209.53\n",
      "Episode 439, Total Reward: -1793804.71\n",
      "Episode 440, Total Reward: -1438892.56\n",
      "Episode 441, Total Reward: -1519075.89\n",
      "Episode 442, Total Reward: -3442169.81\n",
      "Episode 443, Total Reward: -1828169.02\n",
      "Episode 444, Total Reward: -1694065.55\n",
      "Episode 445, Total Reward: -1584373.99\n",
      "Episode 446, Total Reward: -2400603.07\n",
      "Episode 447, Total Reward: -2237289.22\n",
      "Episode 448, Total Reward: -1534158.41\n",
      "Episode 449, Total Reward: -2025314.91\n",
      "Episode 450, Total Reward: -2763790.12\n",
      "Episode 451, Total Reward: -1569768.35\n",
      "Episode 452, Total Reward: -1841270.61\n",
      "Episode 453, Total Reward: -1678066.61\n",
      "Episode 454, Total Reward: -1633546.70\n",
      "Episode 455, Total Reward: -2231731.12\n",
      "Episode 456, Total Reward: -2721670.06\n",
      "Episode 457, Total Reward: -1869490.57\n",
      "Episode 458, Total Reward: -2572353.51\n",
      "Episode 459, Total Reward: -2483619.77\n",
      "Episode 460, Total Reward: -1933597.14\n",
      "Episode 461, Total Reward: -1528202.50\n",
      "Episode 462, Total Reward: -1613164.62\n",
      "Episode 463, Total Reward: -1805926.94\n",
      "Episode 464, Total Reward: -1945705.94\n",
      "Episode 465, Total Reward: -2198235.63\n",
      "Episode 466, Total Reward: -1673861.16\n",
      "Episode 467, Total Reward: -1977097.50\n",
      "Episode 468, Total Reward: -2317790.64\n",
      "Episode 469, Total Reward: -1158604.41\n",
      "Episode 470, Total Reward: -1901875.22\n",
      "Episode 471, Total Reward: -2528539.34\n",
      "Episode 472, Total Reward: -1648608.76\n",
      "Episode 473, Total Reward: -1196313.99\n",
      "Episode 474, Total Reward: -2959644.65\n",
      "Episode 475, Total Reward: -2578169.45\n",
      "Episode 476, Total Reward: -3369197.21\n",
      "Episode 477, Total Reward: -1626581.82\n",
      "Episode 478, Total Reward: -2128801.07\n",
      "Episode 479, Total Reward: -2642820.99\n",
      "Episode 480, Total Reward: -2065698.97\n",
      "Episode 481, Total Reward: -2122511.01\n",
      "Episode 482, Total Reward: -2217631.36\n",
      "Episode 483, Total Reward: -2600621.77\n",
      "Episode 484, Total Reward: -1875736.30\n",
      "Episode 485, Total Reward: -2674104.82\n",
      "Episode 486, Total Reward: -2375590.16\n",
      "Episode 487, Total Reward: -2443540.26\n",
      "Episode 488, Total Reward: -2517624.39\n",
      "Episode 489, Total Reward: -2617536.55\n",
      "Episode 490, Total Reward: -1784494.84\n",
      "Episode 491, Total Reward: -1272307.82\n",
      "Episode 492, Total Reward: -2387578.83\n",
      "Episode 493, Total Reward: -2020846.35\n",
      "Episode 494, Total Reward: -2704796.92\n",
      "Episode 495, Total Reward: -1778828.10\n",
      "Episode 496, Total Reward: -2498490.55\n",
      "Episode 497, Total Reward: -1704744.47\n",
      "Episode 498, Total Reward: -1756606.94\n",
      "Episode 499, Total Reward: -1847542.65\n",
      "Episode 500, Total Reward: -1664249.56\n",
      "训练完成！\n",
      "最优参数: [ 30.   6. 500.   1.]\n",
      "最优产量: -1872100.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# 1. 创建一个发酵模拟环境\n",
    "class FermentationEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(FermentationEnv, self).__init__()\n",
    "        \n",
    "        # 状态空间\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([20, 4.5, 100, 0.5]), \n",
    "                                                high=np.array([40, 7.5, 500, 2.0]),\n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "        # 动作空间\n",
    "        self.action_space = gym.spaces.Discrete(8)\n",
    "        \n",
    "        # 初始状态\n",
    "        self.state = np.array([30.0, 6.0, 300.0, 1.0], dtype=np.float32)\n",
    "        \n",
    "        # 目标：最大化产量，初始化产量为0\n",
    "        self.yield_production = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([30.0, 6.0, 300.0, 1.0], dtype=np.float32)\n",
    "        self.yield_production = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.state[0] -= 1.0  # 降低温度\n",
    "        elif action == 1:\n",
    "            self.state[0] += 1.0  # 增加温度\n",
    "        elif action == 2:\n",
    "            self.state[1] -= 0.1  # 降低 pH\n",
    "        elif action == 3:\n",
    "            self.state[1] += 0.1  # 增加 pH\n",
    "        elif action == 4:\n",
    "            self.state[2] -= 10.0  # 降低搅拌速度\n",
    "        elif action == 5:\n",
    "            self.state[2] += 10.0  # 增加搅拌速度\n",
    "        elif action == 6:\n",
    "            self.state[3] -= 0.1  # 降低通气量\n",
    "        elif action == 7:\n",
    "            self.state[3] += 0.1  # 增加通气量\n",
    "        \n",
    "        self.state = np.clip(self.state, self.observation_space.low, self.observation_space.high)\n",
    "        \n",
    "        # 产量计算\n",
    "        self.yield_production = - (self.state[0] - 35) ** 2 - (self.state[1] - 5.5) ** 2 \\\n",
    "                                - (self.state[2] - 400) ** 2 - (self.state[3] - 1.5) ** 2\n",
    "        reward = self.yield_production\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "# 2. 构建 DQN 模型\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 3. 训练 DQN 模型\n",
    "def train_dqn(env):\n",
    "    learning_rate = 0.001\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    epsilon_min = 0.01\n",
    "    memory_size = 2000\n",
    "    batch_size = 64\n",
    "    target_update = 10\n",
    "    \n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    \n",
    "    policy_net = DQN(input_dim, output_dim)\n",
    "    target_net = DQN(input_dim, output_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    def choose_action(state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return env.action_space.sample()  # 随机动作\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return torch.argmax(q_values).item()  # 选择最优动作\n",
    "    \n",
    "    def replay():\n",
    "        if len(memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(memory, batch_size)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        \n",
    "        q_values = policy_net(states).gather(1, actions)\n",
    "        next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + gamma * next_q_values\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    episodes = 500\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for t in range(200):  # 限定每个 episode 的最大步数\n",
    "            action = choose_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            replay()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    print(\"训练完成！\")\n",
    "    return policy_net  # 返回训练后的模型\n",
    "\n",
    "# 4. 评估智能体\n",
    "def evaluate_agent(env, policy_net, episodes=5):\n",
    "    total_rewards = []\n",
    "    optimal_params = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(200):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        optimal_params.append(state)  # 记录每个 episode 的最终参数\n",
    "    \n",
    "    best_episode_index = np.argmax(total_rewards)\n",
    "    best_params = optimal_params[best_episode_index]\n",
    "    best_reward = total_rewards[best_episode_index]\n",
    "    \n",
    "    print(f\"最优参数: {best_params}\")\n",
    "    print(f\"最优产量: {best_reward:.2f}\")\n",
    "    \n",
    "    return best_params, best_reward\n",
    "\n",
    "# 5. 运行强化学习\n",
    "if __name__ == \"__main__\":\n",
    "    env = FermentationEnv()\n",
    "    policy_net = train_dqn(env)  # 训练并返回训练后的模型\n",
    "    best_params, best_reward = evaluate_agent(env, policy_net)  # 评估智能体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.999871015548706}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "res = classifier(\"Today is a nice day.\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
