{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [下载和安装](#toc1_)    \n",
    "- 2. [教程](#toc2_)    \n",
    "- 3. [概要](#toc3_)    \n",
    "- 4. [神经网络搭建八股](#toc4_)    \n",
    "- 5. [basic](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[下载和安装](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm-haiku version: 0.0.11\n",
      "Jax version: 0.4.20\n",
      "tensorflow version: 2.10.0\n",
      "tensorflow_datasets version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "print(f\"dm-haiku version: {hk.__version__}\")\n",
    "\n",
    "import jax\n",
    "print(f\"Jax version: {jax.__version__}\")\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "print(f\"tensorflow_datasets version: {tfds.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[概要](#toc0_)\n",
    "```\n",
    "google:\n",
    "    Tensorflow(Sonnet)\n",
    "    Haiku(JAX)\n",
    "facebook:\n",
    "    Pytorch\n",
    "Microsoftware:\n",
    "    CNTK\n",
    "AWA:\n",
    "    MXnet\n",
    "\n",
    "dm-haiku教程：\n",
    "https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html\n",
    "https://zhuanlan.zhihu.com/p/471892075\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[神经网络搭建八股](#toc0_)\n",
    "```\n",
    "1. 定义网络结构，计算预测值(y_hat);\n",
    "2. 构造loss函数；\n",
    "3. 训练（迭代）：更新权重(w)和偏置(b)。\n",
    "```\n",
    "## 自己摸索（便于理解）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# 1.定义神经网络结构\n",
    "class Network(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mlp = hk.Sequential([hk.Linear(768, name='hidden1'),\n",
    "                            jax.nn.relu,\n",
    "                            hk.Linear(10, name='hidden2'),\n",
    "                            jax.nn.relu,\n",
    "                            jax.nn.softmax])\n",
    "        logits = mlp(x)\n",
    "        return logits\n",
    "# 2.初始化网络获得params:w,b\n",
    "model = hk.transform(lambda x: Network(name='TestNetwork')(x))\n",
    "params = model.init(rng=rng, x=jnp.ones((256, 28*28), dtype=jnp.float32))\n",
    "# params\n",
    "\n",
    "# 3.定义loss和优化器：自动包括损失函数、反向传播（自动微分、更新权重）\n",
    "def loss(x, params, y):\n",
    "    logits = model.apply(x=x, params=params, rng=None)\n",
    "    cce = jnp.mean(-jnp.sum(jnp.log(logits) * y))\n",
    "    return cce\n",
    "\n",
    "opt = optimizers.sgd(step_size=0.001)\n",
    "opt_state = opt.init_fn(params) # 需要接受网络结构的相关信息\n",
    "# opt_state\n",
    "\n",
    "# 4.准备数据集\n",
    "x_train = jnp.load(\"Minist/mnist_train_x.npy\")\n",
    "y_train = jnp.load(\"Minist/mnist_train_y.npy\")\n",
    "y_train = jax.nn.one_hot(x=y_train, num_classes=len(jnp.unique(y_train))) ## y_trian要做独热编码处理\n",
    "## x y配对并打乱顺序\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_train = tfds.as_numpy(ds_train)\n",
    "\n",
    "# 5.开始训练\n",
    "acc_list = []\n",
    "for epoch in range(100): \n",
    "    # epochs=100，所有数据迭代100次\n",
    "    step = 0\n",
    "    for batch in ds_train:\n",
    "        # 一个整体分为batch_size=256份，迭代256次去更新params\n",
    "        features = batch[0].reshape((-1, 28*28))\n",
    "        features = features/255\n",
    "        labels = batch[1].reshape((-1, 10))\n",
    "        # print(features.shape, labels.shape)\n",
    "\n",
    "        grads = jax.grad(loss, argnums=(1))(features, params, labels)       # argnums=(0,1,2,3)指定需要求导的自变量（这里是params)\n",
    "        opt_state = opt.update_fn(step, grads, opt_state)                   # 更新优化器的opt_state\n",
    "        params = opt.params_fn(opt_state)                                   # 用新的opt_state去生成新的params\n",
    "        step += 1\n",
    "\n",
    "    ## 计算acc\n",
    "    prediction = model.apply(x=x_train.reshape(-1,28*28)/255, params=params, rng=None)\n",
    "    # print(prediction.shape) # (60000, 10)\n",
    "    # print(y_trains) # (60000, 10)\n",
    "    pred_targets = jnp.argmax(prediction, axis=1)           # 返回最大数字的下标(预测)\n",
    "    y_targets = jnp.argmax(y_train, axis=1)                 # 返回最大数字的下标(真实)\n",
    "    ok = jnp.sum(pred_targets == y_targets)                 # 比较下表是否一致，是则ok否则不ok\n",
    "    acc = jnp.divide(ok, y_train.shape[0])                  # ok的占比\n",
    "    acc_list.append(acc)\n",
    "    print(acc)\n",
    "\n",
    "    ## 动态绘图\n",
    "    if epoch %10 == 0:\n",
    "        plt.clf()\n",
    "        plt.plot(acc_list)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('acc')\n",
    "        plt.pause(0.000001)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用jit（推荐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356667\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# 1定义神经网络结构\n",
    "class Network(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mlp = hk.Sequential([hk.Linear(768, name='hidden1'),\n",
    "                            jax.nn.relu,\n",
    "                            hk.Linear(10, name='hidden2'),\n",
    "                            jax.nn.relu,\n",
    "                            jax.nn.softmax])\n",
    "        logits = mlp(x)\n",
    "        return logits\n",
    "# 2初始化网络获得params:w,b\n",
    "model = hk.transform(lambda x: Network(name='TestNetwork')(x))\n",
    "params = model.init(rng=rng, x=jnp.ones((256, 28*28), dtype=jnp.float32))\n",
    "# params\n",
    "\n",
    "# 3定义loss和优化器：自动包括损失函数、反向传播（自动微分、更新权重）\n",
    "def loss(x, params, y):\n",
    "    logits = model.apply(x=x, params=params, rng=None)\n",
    "    cce = jnp.mean(-jnp.sum(jnp.log(logits) * y))\n",
    "    return cce\n",
    "\n",
    "opt = optimizers.sgd(step_size=0.001)\n",
    "opt_state = opt.init_fn(params) # 需要接受网络结构的相关信息\n",
    "# opt_state\n",
    "\n",
    "@jax.jit\n",
    "def update(step, params, features, labels, opt_state):\n",
    "    grads = jax.grad(loss, argnums=(1))(features, params, labels)       # argnums=(0,1,2,3)指定需要求导的自变量（这里是params)\n",
    "    opt_state = opt.update_fn(step, grads, opt_state)                   # 更新优化器的opt_state\n",
    "    params = opt.params_fn(opt_state)                                   # 用新的opt_state去生成新的params\n",
    "    return opt_state, params                               \n",
    "\n",
    "# 4准备数据集\n",
    "x_train = jnp.load(\"Minist/mnist_train_x.npy\")\n",
    "y_train = jnp.load(\"Minist/mnist_train_y.npy\")\n",
    "## y_trian要做独热编码处理\n",
    "y_train = jax.nn.one_hot(x=y_train, num_classes=len(jnp.unique(y_train)))\n",
    "## x y配对并打乱顺序\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_train = tfds.as_numpy(ds_train)\n",
    "\n",
    "# 5训练\n",
    "acc_list = []\n",
    "for epoch in range(100):\n",
    "    step = 0\n",
    "    for batch in ds_train:\n",
    "        features = batch[0].reshape((-1, 28*28))\n",
    "        features = features/255\n",
    "        labels = batch[1].reshape((-1, 10))\n",
    "\n",
    "        opt_state, params = update(step, params, features, labels, opt_state)   # 更新参数\n",
    "        # jnp.save('model_params/params.npy', params)                             # 保存params参数，此步骤严重耗时\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    ## 计算acc\n",
    "    # params = jnp.load('model_params/params.npy')            # 加载参数\n",
    "\n",
    "    prediction = model.apply(x=x_train.reshape(-1,28*28)/255, params=params, rng=None)\n",
    "    pred_targets = jnp.argmax(prediction, axis=1)           # 返回最大数字的下标(预测)\n",
    "    y_targets = jnp.argmax(y_train, axis=1)                 # 返回最大数字的下标(真实)\n",
    "    ok = jnp.sum(pred_targets == y_targets)                 # 比较下表是否一致，是则ok否则不ok\n",
    "    acc = jnp.divide(ok, y_train.shape[0])                  # ok的占比\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    ## 动态绘图\n",
    "    if epoch % 10 == 0:\n",
    "        print(acc)\n",
    "        plt.clf()\n",
    "        plt.plot(acc_list)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('acc')\n",
    "        plt.pause(0.000001)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------------------------------------------------------------------------------+-----------------+------------+------------+---------------+---------------+\n",
      "| Module                          | Config                                                                             | Module params   | Input      | Output     |   Param count |   Param bytes |\n",
      "+=================================+====================================================================================+=================+============+============+===============+===============+\n",
      "| test (Test)                     | Test(name='Test')                                                                  |                 | f32[2,3]   | f32[2,10]  |         4,210 |      16.84 KB |\n",
      "+---------------------------------+------------------------------------------------------------------------------------+-----------------+------------+------------+---------------+---------------+\n",
      "| test/sequential (Sequential)    | Sequential(                                                                        |                 | f32[2,3]   | f32[2,10]  |         4,210 |      16.84 KB |\n",
      "|  └ test (Test)                  |     layers=[Linear(output_size=300, name='hidden1'),                               |                 |            |            |               |               |\n",
      "|                                 |             <jax._src.custom_derivatives.custom_jvp object at 0x000002D5406FE470>, |                 |            |            |               |               |\n",
      "|                                 |             Linear(output_size=10, name='hidden2'),                                |                 |            |            |               |               |\n",
      "|                                 |             <function softmax at 0x000002D5407039A0>],                             |                 |            |            |               |               |\n",
      "|                                 | )                                                                                  |                 |            |            |               |               |\n",
      "+---------------------------------+------------------------------------------------------------------------------------+-----------------+------------+------------+---------------+---------------+\n",
      "| test/hidden1 (Linear)           | Linear(output_size=300, name='hidden1')                                            | w: f32[3,300]   | f32[2,3]   | f32[2,300] |         1,200 |       4.80 KB |\n",
      "|  └ test/sequential (Sequential) |                                                                                    | b: f32[300]     |            |            |               |               |\n",
      "|  └ test (Test)                  |                                                                                    |                 |            |            |               |               |\n",
      "+---------------------------------+------------------------------------------------------------------------------------+-----------------+------------+------------+---------------+---------------+\n",
      "| test/hidden2 (Linear)           | Linear(output_size=10, name='hidden2')                                             | w: f32[300,10]  | f32[2,300] | f32[2,10]  |         3,010 |      12.04 KB |\n",
      "|  └ test/sequential (Sequential) |                                                                                    | b: f32[10]      |            |            |               |               |\n",
      "|  └ test (Test)                  |                                                                                    |                 |            |            |               |               |\n",
      "+---------------------------------+------------------------------------------------------------------------------------+-----------------+------------+------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "\n",
    "# 定义网络结构和计算logits\n",
    "class Test(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mlp = hk.Sequential([\n",
    "            hk.Linear(300, name='hidden1'),\n",
    "            jax.nn.relu,\n",
    "            hk.Linear(10, name='hidden2'),\n",
    "            jax.nn.softmax\n",
    "        ])\n",
    "        return mlp(x)\n",
    "\n",
    "# 初始化网络和参数（w，b）\n",
    "net = hk.transform(lambda x: Test(name='Test')(x))\n",
    "x = jnp.ones((2,3))\n",
    "params = net.init(jax.random.PRNGKey(0), x=x)\n",
    "\n",
    "# 打印网络结构图\n",
    "print(hk.experimental.tabulate(net)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[hk.transform()](#toc0_)\n",
    "```\n",
    "# 网络实例化\n",
    "net = hk.transform()\n",
    "\n",
    "## 对象初始化w，b\n",
    "params = net.init(rng, x)\n",
    "\n",
    "## apply计算__call__函数体\n",
    "logits = net.apply(rng, params, x)\n",
    "```\n",
    "## def函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619]],      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hk.transforma(函数)\n",
    "\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "def forward(x):\n",
    "    mlp = hk.nets.MLP([300, 100, 10])\n",
    "    return mlp(x)\n",
    "forward = hk.transform(forward)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "x = jnp.ones([8, 28*28])\n",
    "\n",
    "params = forward.init(rng, x)\n",
    "logits = forward.apply(params, rng, x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @hk.transform装饰器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619]],      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @：装饰器修饰\n",
    "\n",
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@hk.transform\n",
    "def forward2(x):\n",
    "    mlp = hk.nets.MLP([300, 100, 10])\n",
    "    return mlp(x)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "x = jnp.ones([8, 28*28])\n",
    "\n",
    "params2 = forward2.init(rng, x)\n",
    "logits = forward2.apply(params2, rng, x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda匿名函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619]],      dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hk.transforma(lambda函数)\n",
    "\n",
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Forward3(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mlp = hk.nets.MLP([300, 100, 10])\n",
    "        return mlp(x)\n",
    "    \n",
    "rng = jax.random.PRNGKey(42)\n",
    "x = jnp.ones([8, 28*28])\n",
    "\n",
    "forward3 = hk.transform(lambda x: Forward3()(x))\n",
    "params3 = forward3.init(rng, x)\n",
    "logits = forward3.apply(params3, rng, x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dm-haiku优势\n",
    "```\n",
    "rng = jax.random.PRNkey(0)\n",
    "\n",
    "net = hk.transform()\n",
    "params = net.init(rng, x) # 初始化很简单\n",
    "net.apply(params, rng, x) # 运算起来也很多简单\n",
    "```\n",
    "```\n",
    "1. 先搭建模型函数model\n",
    "2. 用hk.transform转换模型函数，得到model_transform，如果正向inference不用随机数，可以再套一层hk.without_apply_rng\n",
    "model_transform.init()初始化模型，返回初始化参数\n",
    "3. 使用返回的参数，初始化optax包中的优化器\n",
    "4. 之后使用训练数据训练模型，代码为model_transform.apply()，根据logit计算出loss之后，使用jax.grad()计算出梯度，用opt.update更新优化器状态和参数更新值，用optax.apply_updates()更新模型参数 （或则手动用jax.tree_multimap更新网络参数） \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# 1定义神经网络结构\n",
    "class Network(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # mlp = hk.Sequential([hk.Linear(768, name='hidden1'),\n",
    "        #                     jax.nn.relu,\n",
    "        #                     hk.Linear(10, name='hidden2'),\n",
    "        #                     jax.nn.relu,\n",
    "        #                     jax.nn.softmax])\n",
    "        mlp = hk.Sequential([\n",
    "            hk.Conv2D()\n",
    "        ])\n",
    "        logits = mlp(x)\n",
    "        return logits\n",
    "# 2初始化网络获得params:w,b\n",
    "model = hk.transform(lambda x: Network(name='TestNetwork')(x))\n",
    "params = model.init(rng=rng, x=jnp.ones((256, 28*28), dtype=jnp.float32))\n",
    "# params\n",
    "\n",
    "# 3定义loss和优化器：自动包括损失函数、反向传播（自动微分、更新权重）\n",
    "def loss(x, params, y):\n",
    "    logits = model.apply(x=x, params=params, rng=None)\n",
    "    cce = jnp.mean(-jnp.sum(jnp.log(logits) * y))\n",
    "    return cce\n",
    "\n",
    "opt = optimizers.sgd(step_size=0.001)\n",
    "opt_state = opt.init_fn(params) # 需要接受网络结构的相关信息\n",
    "# opt_state\n",
    "\n",
    "@jax.jit\n",
    "def update(step, params, features, labels, opt_state):\n",
    "    grads = jax.grad(loss, argnums=(1))(features, params, labels)       # argnums=(0,1,2,3)指定需要求导的自变量（这里是params)\n",
    "    opt_state = opt.update_fn(step, grads, opt_state)                   # 更新优化器的opt_state\n",
    "    params = opt.params_fn(opt_state)                                   # 用新的opt_state去生成新的params\n",
    "    return opt_state, params                               \n",
    "\n",
    "# 4准备数据集\n",
    "x_train = jnp.load(\"Minist/mnist_train_x.npy\")\n",
    "y_train = jnp.load(\"Minist/mnist_train_y.npy\")\n",
    "## y_trian要做独热编码处理\n",
    "y_train = jax.nn.one_hot(x=y_train, num_classes=len(jnp.unique(y_train)))\n",
    "## x y配对并打乱顺序\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_train = tfds.as_numpy(ds_train)\n",
    "\n",
    "# 5训练\n",
    "acc_list = []\n",
    "for epoch in range(100):\n",
    "    step = 0\n",
    "    for batch in ds_train:\n",
    "        features = batch[0].reshape((-1, 28*28))\n",
    "        features = features/255\n",
    "        labels = batch[1].reshape((-1, 10))\n",
    "\n",
    "        opt_state, params = update(step, params, features, labels, opt_state)   # 更新参数\n",
    "        # jnp.save('model_params/params.npy', params)                             # 保存params参数，此步骤严重耗时\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    ## 计算acc\n",
    "    # params = jnp.load('model_params/params.npy')            # 加载参数\n",
    "\n",
    "    prediction = model.apply(x=x_train.reshape(-1,28*28)/255, params=params, rng=None)\n",
    "    pred_targets = jnp.argmax(prediction, axis=1)           # 返回最大数字的下标(预测)\n",
    "    y_targets = jnp.argmax(y_train, axis=1)                 # 返回最大数字的下标(真实)\n",
    "    ok = jnp.sum(pred_targets == y_targets)                 # 比较下表是否一致，是则ok否则不ok\n",
    "    acc = jnp.divide(ok, y_train.shape[0])                  # ok的占比\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    ## 动态绘图\n",
    "    if epoch % 10 == 0:\n",
    "        print(acc)\n",
    "        plt.clf()\n",
    "        plt.plot(acc_list)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('acc')\n",
    "        plt.pause(0.000001)\n",
    "        display.clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
