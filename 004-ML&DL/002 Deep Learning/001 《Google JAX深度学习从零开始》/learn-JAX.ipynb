{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [概要](#toc1_)    \n",
    "  - 1.1. [非纯函数](#toc1_1_)    \n",
    "  - 1.2. [纯函数](#toc1_2_)    \n",
    "- 2. [jax的一般特性](#toc2_)    \n",
    "  - 2.1. [加速-jit()](#toc2_1_)    \n",
    "  - 2.2. [自动微分-grad()](#toc2_2_)    \n",
    "    - 2.2.1. [同时获取函数值与导数-jax.value_and_grad()](#toc2_2_1_)    \n",
    "    - 2.2.2. [多元函数求导](#toc2_2_2_)    \n",
    "    - 2.2.3. [多返回值函数的求导](#toc2_2_3_)    \n",
    "    - 2.2.4. [高阶导函数](#toc2_2_4_)    \n",
    "  - 2.3. [自动向量化-vmap()/pmap()](#toc2_3_)    \n",
    "    - 2.3.1. [例1](#toc2_3_1_)    \n",
    "    - 2.3.2. [例2](#toc2_3_2_)    \n",
    "    - 2.3.3. [例3](#toc2_3_3_)    \n",
    "    - 2.3.4. [数组索引](#toc2_3_4_)    \n",
    "      - 2.3.4.1. [二维数组](#toc2_3_4_1_)    \n",
    "      - 2.3.4.2. [三维数组](#toc2_3_4_2_)    \n",
    "    - 2.3.5. [vmap操作](#toc2_3_5_)    \n",
    "        - 2.3.5.1.1. [vmap实战案例](#toc2_3_5_1_1_)    \n",
    "          - 2.3.5.1.1.1. [batch的实现](#toc2_3_5_1_1_1_)    \n",
    "- 3. [jax的高级特性](#toc3_)    \n",
    "  - 3.1. [jax.numpy特性](#toc3_1_)    \n",
    "    - 3.1.1. [赋值](#toc3_1_1_)    \n",
    "    - 3.1.2. [数组规范](#toc3_1_2_)    \n",
    "    - 3.1.3. [算数运算](#toc3_1_3_)    \n",
    "  - 3.2. [jax控制分支](#toc3_2_)    \n",
    "    - 3.2.1. [分支对grad影响](#toc3_2_1_)    \n",
    "    - 3.2.2. [分支对jit影响](#toc3_2_2_)    \n",
    "    - 3.2.3. [条件判断-jax.lax.cond(True, func1, func2, args)](#toc3_2_3_)    \n",
    "    - 3.2.4. [循环-jax.lax.while_loop()](#toc3_2_4_)    \n",
    "    - 3.2.5. [循环-jax.lax.fori_loop()](#toc3_2_5_)    \n",
    "  - 3.3. [jax.nn包含的函数](#toc3_3_)    \n",
    "  - 3.4. [jax.example_libraries](#toc3_4_)    \n",
    "- 4. [多层感知机](#toc4_)    \n",
    "  - 4.1. [准备数据集](#toc4_1_)    \n",
    "    - 4.1.1. [mnist](#toc4_1_1_)    \n",
    "    - 4.1.2. [独热码（one-hot）](#toc4_1_2_)    \n",
    "      - 4.1.2.1. [自定义函数实现](#toc4_1_2_1_)    \n",
    "      - 4.1.2.2. [jax.nn.one_hot()实现](#toc4_1_2_2_)    \n",
    "  - 4.2. [JAX实现全连接层](#toc4_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.19'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_datasets==4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[概要](#toc0_)\n",
    "```\n",
    "numpy:python中高效的科学计算模块，但是支持cpu加速；\n",
    "jax:google推出的可以利用cpu、gpu、tpu进行加速的计算模块：\n",
    "    random          产生随机数\n",
    "    numpy           数学计算，同python中的numpy模块，接口使用非常类似，可以无缝衔接\n",
    "    scipy           统计分析\n",
    "    nn              神经网络类的计算库\n",
    "    experimental    一些实验形式的内容\n",
    "    jit\n",
    "    grad\n",
    "\n",
    "JAX: 有个很强的特性，支支持“纯函数”，不支持OOP；\n",
    "    纯函数：\n",
    "        1. 相同输入总是返回相同输出：\n",
    "            返回值只和函数参数有关，与外部无关。（无论外部如何变化，函数的返回值都不会发生变化。）\n",
    "        2. 不产生副作用：\n",
    "            函数执行过程中对外部产生了可观察的变化，称之为 函数产生了副作用。\n",
    "        3. 不依赖于外部状态：\n",
    "            函数执行的过程中不会对外部产生可观察到的变化。\n",
    "为此google推出了dm-Haiku集成jax的library，支持OOP；\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[非纯函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实施函数计算\n",
      "First call: 4.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Second call: 5.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "实施函数计算\n",
      "Third call: [Array(5., dtype=float32, weak_type=True)]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 因为print的存在，编程了非纯函数\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def impure_print_side_effect(x):\n",
    "    print(f\"实施函数计算\")\n",
    "    return x\n",
    "\n",
    "print(f\"First call: {jax.jit(impure_print_side_effect)(4.0)}\")\n",
    "print('-'*100)\n",
    "\n",
    "print(f\"Second call: {jax.jit(impure_print_side_effect)(5.0)}\") # 与第一次数据类型一致，出发JIT缓存机制，不答应print内容了\n",
    "print('-'*100)\n",
    "\n",
    "print(f\"Third call: {jax.jit(impure_print_side_effect)([5.0])}\")\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call: 4.0\n",
      "Saved global: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "# 函数内部参数影响外部参数\n",
    "g = 0.\n",
    "def impure_saves_global(x):\n",
    "    global g\n",
    "    g = x\n",
    "    return x\n",
    "\n",
    "print(f\"First call: {jax.jit(impure_saves_global)(4.0)}\")\n",
    "\n",
    "print(f\"Saved global: {g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[纯函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "60.0\n",
      "[50.]\n"
     ]
    }
   ],
   "source": [
    "# 计算结果没有对外部函数做出任何影响，因此未纯函数\n",
    "def pure_uses_internal_state(x):\n",
    "    state = dict(even=0, odd=0)\n",
    "    for i in range(10):\n",
    "        state['even' if i % 2 == 0 else 'odd'] += x\n",
    "    \n",
    "    return state['even'] + state['odd']\n",
    "\n",
    "print(f\"{jax.jit(pure_uses_internal_state)(3.)}\")\n",
    "print(f\"{jax.jit(pure_uses_internal_state)(6.)}\")\n",
    "\n",
    "print(f\"{jax.jit(pure_uses_internal_state)(jnp.array([5.0]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function grad in module jax._src.api:\n",
      "\n",
      "grad(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False, reduce_axes: 'Sequence[AxisName]' = ()) -> 'Callable'\n",
      "    Creates a function that evaluates the gradient of ``fun``.\n",
      "    \n",
      "    Args:\n",
      "      fun: Function to be differentiated. Its arguments at positions specified by\n",
      "        ``argnums`` should be arrays, scalars, or standard Python containers.\n",
      "        Argument arrays in the positions specified by ``argnums`` must be of\n",
      "        inexact (i.e., floating-point or complex) type. It\n",
      "        should return a scalar (which includes arrays with shape ``()`` but not\n",
      "        arrays with shape ``(1,)`` etc.)\n",
      "      argnums: Optional, integer or sequence of integers. Specifies which\n",
      "        positional argument(s) to differentiate with respect to (default 0).\n",
      "      has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\n",
      "        first element is considered the output of the mathematical function to be\n",
      "        differentiated and the second element is auxiliary data. Default False.\n",
      "      holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\n",
      "        holomorphic. If True, inputs and outputs must be complex. Default False.\n",
      "      allow_int: Optional, bool. Whether to allow differentiating with\n",
      "        respect to integer valued inputs. The gradient of an integer input will\n",
      "        have a trivial vector-space dtype (float0). Default False.\n",
      "      reduce_axes: Optional, tuple of axis names. If an axis is listed here, and\n",
      "        ``fun`` implicitly broadcasts a value over that axis, the backward pass\n",
      "        will perform a ``psum`` of the corresponding gradient. Otherwise, the\n",
      "        gradient will be per-example over named axes. For example, if ``'batch'``\n",
      "        is a named batch axis, ``grad(f, reduce_axes=('batch',))`` will create a\n",
      "        function that computes the total gradient while ``grad(f)`` will create\n",
      "        one that computes the per-example gradient.\n",
      "    \n",
      "    Returns:\n",
      "      A function with the same arguments as ``fun``, that evaluates the gradient\n",
      "      of ``fun``. If ``argnums`` is an integer then the gradient has the same\n",
      "      shape and type as the positional argument indicated by that integer. If\n",
      "      argnums is a tuple of integers, the gradient is a tuple of values with the\n",
      "      same shapes and types as the corresponding arguments. If ``has_aux`` is True\n",
      "      then a pair of (gradient, auxiliary_data) is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> import jax\n",
      "    >>>\n",
      "    >>> grad_tanh = jax.grad(jax.numpy.tanh)\n",
      "    >>> print(grad_tanh(0.2))\n",
      "    0.961043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jax.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[jax的一般特性](#toc0_)\n",
    "## 2.1. <a id='toc2_1_'></a>[加速-jit()](#toc0_)\n",
    "```\n",
    "JIT：及时编译器。当采用标准的Python、Numpy函数，经过JIT编译后可在加速器上高效运行。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：0.04858279228210449\n",
      "耗时：0.05484318733215332\n",
      "耗时：0.0653378963470459\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x>0, x, alpha*jnp.exp(x) - alpha)\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(17), (10000000,))\n",
    "\n",
    "# 普通计算\n",
    "start = time.time()\n",
    "selu(x)\n",
    "stop = time.time(); print(f\"耗时：{stop - start}\")\n",
    "\n",
    "# 方法一：jax.jit()编译\n",
    "start = time.time()\n",
    "selu_jited = jax.jit(selu) # 利用jax.jit()编译\n",
    "selu_jited(x)\n",
    "stop = time.time(); print(f\"耗时：{stop - start}\")\n",
    "\n",
    "# 方法二： @jax.jit装饰器修饰\n",
    "@jax.jit\n",
    "def selu_jit(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x>0, x, alpha*jnp.exp(x) - alpha)\n",
    "start = time.time()\n",
    "selu_jit(x)\n",
    "stop = time.time(); print(f\"耗时：{stop - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[自动微分-grad()](#toc0_)\n",
    "```\n",
    "grad:\n",
    "    1. 必须使用浮点型数值计算\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function grad in module jax._src.api:\n",
      "\n",
      "grad(fun: 'Callable', argnums: 'int | Sequence[int]' = 0, has_aux: 'bool' = False, holomorphic: 'bool' = False, allow_int: 'bool' = False, reduce_axes: 'Sequence[AxisName]' = ()) -> 'Callable'\n",
      "    Creates a function that evaluates the gradient of ``fun``.\n",
      "    \n",
      "    Args:\n",
      "      fun: Function to be differentiated. Its arguments at positions specified by\n",
      "        ``argnums`` should be arrays, scalars, or standard Python containers.\n",
      "        Argument arrays in the positions specified by ``argnums`` must be of\n",
      "        inexact (i.e., floating-point or complex) type. It\n",
      "        should return a scalar (which includes arrays with shape ``()`` but not\n",
      "        arrays with shape ``(1,)`` etc.)\n",
      "      argnums: Optional, integer or sequence of integers. Specifies which\n",
      "        positional argument(s) to differentiate with respect to (default 0).\n",
      "      has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\n",
      "        first element is considered the output of the mathematical function to be\n",
      "        differentiated and the second element is auxiliary data. Default False.\n",
      "      holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\n",
      "        holomorphic. If True, inputs and outputs must be complex. Default False.\n",
      "      allow_int: Optional, bool. Whether to allow differentiating with\n",
      "        respect to integer valued inputs. The gradient of an integer input will\n",
      "        have a trivial vector-space dtype (float0). Default False.\n",
      "      reduce_axes: Optional, tuple of axis names. If an axis is listed here, and\n",
      "        ``fun`` implicitly broadcasts a value over that axis, the backward pass\n",
      "        will perform a ``psum`` of the corresponding gradient. Otherwise, the\n",
      "        gradient will be per-example over named axes. For example, if ``'batch'``\n",
      "        is a named batch axis, ``grad(f, reduce_axes=('batch',))`` will create a\n",
      "        function that computes the total gradient while ``grad(f)`` will create\n",
      "        one that computes the per-example gradient.\n",
      "    \n",
      "    Returns:\n",
      "      A function with the same arguments as ``fun``, that evaluates the gradient\n",
      "      of ``fun``. If ``argnums`` is an integer then the gradient has the same\n",
      "      shape and type as the positional argument indicated by that integer. If\n",
      "      argnums is a tuple of integers, the gradient is a tuple of values with the\n",
      "      same shapes and types as the corresponding arguments. If ``has_aux`` is True\n",
      "      then a pair of (gradient, auxiliary_data) is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> import jax\n",
      "    >>>\n",
      "    >>> grad_tanh = jax.grad(jax.numpy.tanh)\n",
      "    >>> print(grad_tanh(0.2))\n",
      "    0.961043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jax.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.) # 必须是浮点型\n",
    "\n",
    "Dfn = jax.grad(sum_logistic)\n",
    "\n",
    "Dfn_jited = jax.jit(Dfn) # 对导函数加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.86 ms ± 2.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit Dfn(x_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.66 µs ± 389 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit Dfn_jited(x_small) # 使用jit加速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. <a id='toc2_2_1_'></a>[同时获取函数值与导数-jax.value_and_grad()](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(1., dtype=float32, weak_type=True),\n",
       " Array(2., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def body_fn(x):\n",
    "    return x**2\n",
    "\n",
    "jax.value_and_grad(body_fn)(1.)\n",
    "# 函数值：1\n",
    "# 导函数值：2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. <a id='toc2_2_2_'></a>[多元函数求导](#toc0_)\n",
    "```\n",
    "argnums=(0,1,2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(3., dtype=float32, weak_type=True),\n",
       " Array(2., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def body_fun(x, y):\n",
    "    return x*y\n",
    "grad_body_fun = jax.grad(body_fun)\n",
    "x = (2.)\n",
    "y = (3.)\n",
    "\n",
    "dx, dy = (jax.grad(body_fun, argnums=(0, 1))(x, y))\n",
    "dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(12., dtype=float32, weak_type=True),\n",
       " Array(8., dtype=float32, weak_type=True),\n",
       " Array(6., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def body_fun(x, y, z):\n",
    "    return x*y*z\n",
    "\n",
    "x = 2.\n",
    "y = 3.\n",
    "z = 4.\n",
    "dx, dy, dz = jax.grad(body_fun, argnums=(0, 1, 2))(x, y, z)   # dx, dy, dz\n",
    "# dx, dy = jax.grad(body_fun, argnums=(0, 1))(x, y, z)          # dx, dy\n",
    "# dx = jax.grad(body_fun, argnums=(0))(x, y, z)                 # dx, dy, dz\n",
    "dx, dy, dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. <a id='toc2_2_3_'></a>[多返回值函数的求导](#toc0_)\n",
    "```\n",
    "has_aux=True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(3., dtype=float32, weak_type=True),\n",
       " Array(13., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def body_fn(x, y):\n",
    "    return x*y, x**2+y**2\n",
    "\n",
    "x = 2.\n",
    "y = 3.\n",
    "jax.grad(body_fn, has_aux=True)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. <a id='toc2_2_4_'></a>[高阶导函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "fun = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
    "\n",
    "dx = jax.grad(fun) # 一阶导函数\n",
    "dx(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(10., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddx = jax.grad(dx) # 二阶导函数\n",
    "ddx(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dddx = jax.grad(ddx) # 三阶导函数\n",
    "dddx(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. <a id='toc2_3_'></a>[自动向量化-vmap()/pmap()](#toc0_)\n",
    "```\n",
    "什么是向量化？\n",
    "    vmap 在单张GPU的多个CUDA核心上并行计算\n",
    "    pmap 在单台机器的多个GPU计算卡上并行计算\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. <a id='toc2_3_1_'></a>[例1](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "\n",
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：0.0020017623901367188\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "x_small = jnp.arange(1024000.)\n",
    "Dfn = (jax.grad(sum_logistic))\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"耗时：{stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：0.0019969940185546875\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "x_small = jnp.arange(1024000.)\n",
    "Dfn = jax.vmap(jax.grad(sum_logistic))\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"耗时：{stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：0.003172159194946289\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "x_small = jnp.arange(1024000.)\n",
    "Dfn = jax.jit(jax.vmap(jax.grad(sum_logistic)))\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"耗时：{stop - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. <a id='toc2_3_2_'></a>[例2](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6, dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def vec_vec(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "x = jnp.array([1, 2, 1])\n",
    "y = jnp.array([2, 1, 2])\n",
    "\n",
    "vec_vec(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. <a id='toc2_3_3_'></a>[例3](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "jax.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. <a id='toc2_3_4_'></a>[数组索引](#toc0_)\n",
    "```\n",
    "知乎教程：https://zhuanlan.zhihu.com/p/476098317\n",
    "规律：可见索引的规律如下：索引的轴维度消失，前后维度合并。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.1. <a id='toc2_3_4_1_'></a>[二维数组](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 0.45765412, -1.8838878 ],\n",
       "        [-1.7656637 , -0.32822677],\n",
       "        [ 0.04516343,  1.7529849 ]], dtype=float32),\n",
       " (3, 2))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(55), (3, 2))\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.45765412, dtype=float32), Array(-1.8838878, dtype=float32))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引第0行，第0/1个元素, 类似矩阵索引。\n",
    "x[0,0], x[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.45765412, -1.8838878 ], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引一整行:\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.45765412, -1.7656637 ,  0.04516343], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引一整列:\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45765412 -1.8838878 ]\n",
      "[-1.7656637  -0.32822677]\n",
      "[0.04516343 1.7529849 ]\n"
     ]
    }
   ],
   "source": [
    "# 对axes=0 索引：\n",
    "for i in range(x.shape[0]):\n",
    "    print(x[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45765412 -1.7656637   0.04516343]\n",
      "[-1.8838878  -0.32822677  1.7529849 ]\n"
     ]
    }
   ],
   "source": [
    "# 对axes=1 每个元素索引：\n",
    "for i in range(x.shape[1]):\n",
    "    print(x[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.2. <a id='toc2_3_4_2_'></a>[三维数组](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-1.2363433 ,  0.711691  ],\n",
       "        [ 1.1999513 , -0.3833862 ],\n",
       "        [-1.9615409 , -0.91742545],\n",
       "        [-0.2196888 ,  1.1890033 ]],\n",
       "\n",
       "       [[-1.6743991 ,  0.1835342 ],\n",
       "        [-1.372812  , -1.2838745 ],\n",
       "        [-0.56314117,  0.11130438],\n",
       "        [-0.9001647 , -0.612242  ]],\n",
       "\n",
       "       [[-0.2662137 , -1.3849598 ],\n",
       "        [-1.0626214 , -0.24122705],\n",
       "        [-0.14088325, -0.3180565 ],\n",
       "        [ 0.36471063,  0.46731815]]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成一个三维数组\n",
    "x = jax.random.normal(jax.random.PRNGKey(55), (3, 4, 2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.2363433 ,  0.711691  ],\n",
       "       [ 1.1999513 , -0.3833862 ],\n",
       "       [-1.9615409 , -0.91742545],\n",
       "       [-0.2196888 ,  1.1890033 ]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(x.shape[0]):\n",
    "#     print(x[i,:,:])\n",
    "x[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.2363433,  0.711691 ],\n",
       "       [-1.6743991,  0.1835342],\n",
       "       [-0.2662137, -1.3849598]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引所有矩阵的第0行组成的矩阵：\n",
    "x[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.2363433 ,  1.1999513 , -1.9615409 , -0.2196888 ],\n",
       "       [-1.6743991 , -1.372812  , -0.56314117, -0.9001647 ],\n",
       "       [-0.2662137 , -1.0626214 , -0.14088325,  0.36471063]],      dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引所有矩阵的第0列组成的矩阵的转置：\n",
    "x[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. <a id='toc2_3_5_'></a>[vmap操作](#toc0_)\n",
    "```\n",
    "vmap(fun: 'F', in_axes: 'int | None | Sequence[Any]' = 0, out_axes: 'Any' = 0, axis_name: 'AxisName | None' = None, axis_size: 'int | None' = None, spmd_axis_name: 'AxisName | tuple[AxisName, ...] | None' = None) -> 'F'\n",
    "    Vectorizing map. Creates a function which maps ``fun`` over argument axes.\n",
    "\n",
    "fun: 代表你需要进行向量化操作的具体函数；\n",
    "in_axes：输入格式为元组，代表fun中每个输入参数中，使用哪一个维度进行向量化；\n",
    "out_axes: 经过fun计算后，每组输出在哪个维度输出。\n",
    "\n",
    "在介绍了numpy多维数组的索引之后，vmap的in_axes参数的功能就一目了然了。其本质就是对某轴进行索引，得到n个新的数组，将这些数组传递给fun函数进行操作后叠加，用通用公式表示：\n",
    "\n",
    "# 1. 定义某种函数，以dot为例:\n",
    "fun = lambda x,y :  jnp.dot(x, y)\n",
    "\n",
    "# 2. 切片后数据有fun进行操作\n",
    "for i in range(x.shape[in_axes]):\n",
    "\tfun(x[:, i, :], y[:, i, :])\n",
    "\t\n",
    "# 3. 所有数组stack在out_axes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 0.40211347,  1.0316547 ,  0.24331902],\n",
       "        [-1.1584883 , -1.2835754 , -0.56345284],\n",
       "        [-0.01159265,  0.17644508, -0.5234676 ],\n",
       "        [-0.01921092, -0.66263014, -0.22824208]], dtype=float32),\n",
       " (4, 3),\n",
       " Array([[ 0.6122652 ,  1.1225883 ],\n",
       "        [ 1.1373317 , -0.8127325 ],\n",
       "        [-0.890405  ,  0.12623145]], dtype=float32),\n",
       " (3, 2))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义jnp.dot(x,y)函数\n",
    "f = lambda x,y : jnp.dot(x,y)\n",
    "\n",
    "# 初始化x,y样本\n",
    "x = jax.random.normal(jax.random.PRNGKey(55), (4, 3))\n",
    "y = jax.random.normal(jax.random.PRNGKey(42), (3, 2))\n",
    "x, x.shape, y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 1.2028812 , -0.35633698],\n",
       "        [-1.667452  , -0.3284274 ],\n",
       "        [ 0.65967697, -0.2224945 ],\n",
       "        [-0.5621646 ,  0.48816377]], dtype=float32),\n",
       " (4, 2))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先看看原始的jnp.dot的计算结果的shape和我们直接用矩阵乘法预期一致：\n",
    "z = f(x,y)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.2028812 , -0.35633698],\n",
       "       [-1.667452  , -0.3284274 ],\n",
       "       [ 0.65967697, -0.2224945 ],\n",
       "       [-0.5621646 ,  0.48816377]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来，我们使用vmap来对x的第0轴索引后产生的每个新数组，与y进行jnp.dot():\n",
    "jax.vmap(f, in_axes=(0,None), out_axes=0)(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40211347 1.0316547  0.24331902]\n",
      "[ 1.2028812  -0.35633698]\n",
      "===============================\n",
      "[-1.1584883  -1.2835754  -0.56345284]\n",
      "[-1.667452  -0.3284274]\n",
      "===============================\n",
      "[-0.01159265  0.17644508 -0.5234676 ]\n",
      "[ 0.65967697 -0.2224945 ]\n",
      "===============================\n",
      "[-0.01921092 -0.66263014 -0.22824208]\n",
      "[-0.5621646   0.48816377]\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "# x 第0维有4个array\n",
    "for i in range(x.shape[0]):\n",
    "\t\n",
    "\tprint(x[i, :])\n",
    "\tprint(jnp.dot(x[i,:], y))\n",
    "\tprint(\"===============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.2028812 , -1.667452  ,  0.65967697, -0.5621646 ],\n",
       "       [-0.35633698, -0.3284274 , -0.2224945 ,  0.48816377]],      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(f, (None, 1), 0)(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.5.1.1. <a id='toc2_3_5_1_1_'></a>[vmap实战案例](#toc0_)\n",
    "###### 2.3.5.1.1.1. <a id='toc2_3_5_1_1_1_'></a>[batch的实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义函数：\n",
    "f = lambda x,w : jnp.dot(w,x)\n",
    "\n",
    "# 定义x, batch_x, w。\n",
    "x = jax.random.normal(jax.random.PRNGKey(55), (5, 3))\n",
    "x_batch = jax.random.normal(jax.random.PRNGKey(55), (4, 5, 3))\n",
    "w = jax.random.normal(jax.random.PRNGKey(42), (100, x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 激活值a=w@x:\n",
    "jnp.dot(w, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.8 µs ± 6.47 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# 这样手写batch, 使用for loop:\n",
    "def for_loop_batch(x_batch):\n",
    "    for x in x_batch:\n",
    "        # print(x)\n",
    "        jnp.dot(w, x)    # shape: (100, 3)\n",
    "\n",
    "# for_loop_batch(x_batch)\n",
    "%timeit for_loop_batch(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 ms ± 71.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# batch_a = jax.vmap(f, in_axes=(0,None), out_axes=0)(x_batch, w)\n",
    "# print(batch_a.shape)\n",
    "%timeit jax.vmap(f, in_axes=(0, None), out_axes=0)(x_batch, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[jax的高级特性](#toc0_)\n",
    "## 3.1. <a id='toc3_1_'></a>[jax.numpy特性](#toc0_)\n",
    "```\n",
    "jax.numpy和Numpy用法非常相似，可以做到无缝切换；\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. <a id='toc3_1_1_'></a>[赋值](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([1, 2, 3], dtype=int32), (3,), array([1, 2, 3]), (3,))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([1,2,3])\n",
    "y = np.array([1,2,3])\n",
    "x, x.shape, y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.arange(10), np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]], dtype=int32),\n",
       " array([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.arange(10).reshape(2,5), np.arange(10).reshape(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32),\n",
       " array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linspace(0,9,10), np.linspace(0,9,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]], dtype=int32),\n",
       " [Array([[0, 1, 2, 3, 4]], dtype=int32),\n",
       "  Array([[5, 6, 7, 8, 9]], dtype=int32)])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tem = jnp.arange(10).reshape(2, 5)\n",
    "tem, jnp.split(tem, 2, axis=0)\n",
    "# jnp.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. <a id='toc3_1_2_'></a>[数组规范](#toc0_)\n",
    "```\n",
    "jax.numpy.arrary被设置好后就不能改变，只能通过at的方式进行更改。\n",
    "jax.numpy.ndarray.at:\n",
    "    x = x.at[idx].set(y)            ->          x[idx] = y\n",
    "    x = x.at[idx].add(y)            ->          x[idx] += y\n",
    "    x = x.at[idx].multiply(y)       ->          x[idx] *= y\n",
    "    x = x.at[idx].divide(y)         ->          x[idx] /= y\n",
    "    x = x.at[idx].power(y)          ->          x[idx] **= y\n",
    "    x = x.at[idx].min(y)            ->          x[idx] = minimum(x[idx], y)\n",
    "    x = x.at[idx].max(y)            ->          x[idx] = maximum(x[idx], y)\n",
    "    x = x.at[idx].apply(ufunc)      ->          ufunc.at(x, idx)\n",
    "    x = x.at[idx].get()             ->          x = x[idx]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'index_update' from 'jax.ops' (d:\\ProgramFiles\\miniconda3\\envs\\tensorflow2\\lib\\site-packages\\jax\\ops\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkStation\\PyhtonWorkStation\\SmallTools\\Library\\004-ML&DL\\LearnAlphaFlod\\MLP-Frame\\LearnJAX.ipynb 单元格 59\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnJAX.ipynb#Y201sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnJAX.ipynb#Y201sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnJAX.ipynb#Y201sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m index_update\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnJAX.ipynb#Y201sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m jax_array \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mzeros((\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), dtype\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnJAX.ipynb#Y201sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m jax_array\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'index_update' from 'jax.ops' (d:\\ProgramFiles\\miniconda3\\envs\\tensorflow2\\lib\\site-packages\\jax\\ops\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax_array = jnp.zeros((3,3), dtype=jnp.float32)\n",
    "jax_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_array[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkStation\\PyhtonWorkStation\\SmallTools\\Library\\004-ML&DL\\LearnAlphaFlod\\MLP-Frame\\LearnJAX.ipynb 单元格 61\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnJAX.ipynb#Y205sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 直接更改将报错\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnJAX.ipynb#Y205sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m jax_array[\u001b[39m1\u001b[39;49m, :] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "File \u001b[1;32md:\\ProgramFiles\\miniconda3\\envs\\tensorflow2\\lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:270\u001b[0m, in \u001b[0;36m_unimplemented_setitem\u001b[1;34m(self, i, x)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unimplemented_setitem\u001b[39m(\u001b[39mself\u001b[39m, i, x):\n\u001b[0;32m    266\u001b[0m   msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object does not support item assignment. JAX arrays are \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mimmutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mor another .at[] method: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 270\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)))\n",
      "\u001b[1;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "# 直接更改将报错\n",
    "jax_array[1, :] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_jax_array = jax_array.at[1,:].set(1.0)\n",
    "new_jax_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. <a id='toc3_1_3_'></a>[算数运算](#toc0_)\n",
    "```\n",
    "jnp.add()\n",
    "jnp.subtract()\n",
    "jnp.multiply()\n",
    "jnp.divide()\n",
    "jnp.power()\n",
    "jnp.exp()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.add(3,2)\n",
    "jnp.subtract(3,2)\n",
    "jnp.multiply(3,2)\n",
    "jnp.divide(3,3)\n",
    "jnp.power(2,2)\n",
    "jnp.exp(2)\n",
    "jnp.log(100)\n",
    "jnp.log10(100)\n",
    "jnp.sin(2)\n",
    "jnp.cos(2)\n",
    "jnp.tan(3)\n",
    "# jnp.dot([1,2,3], [3,4,5]) # 报错\n",
    "jnp.dot(jnp.array([1,2,3]), jnp.array([3,4,5]))\n",
    "jnp.where(True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 1.0545162 , -0.96928865],\n",
       "        [-0.5946021 , -0.03188572],\n",
       "        [ 2.4109333 , -1.8784491 ],\n",
       "        [-0.7847696 , -0.31370842],\n",
       "        [ 0.3337089 ,  1.7677035 ],\n",
       "        [-1.0277646 ,  1.4111718 ],\n",
       "        [-0.5084971 , -0.5263775 ],\n",
       "        [ 0.5031504 ,  1.0549793 ],\n",
       "        [-0.08740733,  0.7958167 ],\n",
       "        [ 2.6565616 , -0.5822906 ]], dtype=float32),\n",
       " Array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=float32),\n",
       " Array([1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(seed=0) # rng:随机数生成器\n",
    "\n",
    "input = jax.random.normal(rng, shape=(10, 2))\n",
    "\n",
    "# weight = jax.random.normal(rng, shape=(2, 5))\n",
    "weight = jnp.ones(shape=(2, 5))\n",
    "\n",
    "# bias = jax.random.normal(rng, shape=(5,))\n",
    "bias = jnp.ones(shape=(5,))\n",
    "\n",
    "input, weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.08522755,  0.08522755,  0.08522755,  0.08522755,  0.08522755],\n",
       "       [-0.62648785, -0.62648785, -0.62648785, -0.62648785, -0.62648785],\n",
       "       [ 0.5324842 ,  0.5324842 ,  0.5324842 ,  0.5324842 ,  0.5324842 ],\n",
       "       [-1.0984781 , -1.0984781 , -1.0984781 , -1.0984781 , -1.0984781 ],\n",
       "       [ 2.1014125 ,  2.1014125 ,  2.1014125 ,  2.1014125 ,  2.1014125 ],\n",
       "       [ 0.38340724,  0.38340724,  0.38340724,  0.38340724,  0.38340724],\n",
       "       [-1.0348747 , -1.0348747 , -1.0348747 , -1.0348747 , -1.0348747 ],\n",
       "       [ 1.5581298 ,  1.5581298 ,  1.5581298 ,  1.5581298 ,  1.5581298 ],\n",
       "       [ 0.70840937,  0.70840937,  0.70840937,  0.70840937,  0.70840937],\n",
       "       [ 2.074271  ,  2.074271  ,  2.074271  ,  2.074271  ,  2.074271  ]],      dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.dot(input, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.0852275 ,  1.0852275 ,  1.0852275 ,  1.0852275 ,  1.0852275 ],\n",
       "       [ 0.37351215,  0.37351215,  0.37351215,  0.37351215,  0.37351215],\n",
       "       [ 1.5324842 ,  1.5324842 ,  1.5324842 ,  1.5324842 ,  1.5324842 ],\n",
       "       [-0.09847808, -0.09847808, -0.09847808, -0.09847808, -0.09847808],\n",
       "       [ 3.1014125 ,  3.1014125 ,  3.1014125 ,  3.1014125 ,  3.1014125 ],\n",
       "       [ 1.3834072 ,  1.3834072 ,  1.3834072 ,  1.3834072 ,  1.3834072 ],\n",
       "       [-0.03487468, -0.03487468, -0.03487468, -0.03487468, -0.03487468],\n",
       "       [ 2.5581298 ,  2.5581298 ,  2.5581298 ,  2.5581298 ,  2.5581298 ],\n",
       "       [ 1.7084093 ,  1.7084093 ,  1.7084093 ,  1.7084093 ,  1.7084093 ],\n",
       "       [ 3.074271  ,  3.074271  ,  3.074271  ,  3.074271  ,  3.074271  ]],      dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.dot(input, weight) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.swapaxes(msa_act, -2, -3)\n",
    "jnp.transpose(act, [1, 0, 2])\n",
    "jnp.expand_dims(msa_mask, axis=-1)\n",
    "jnp.asarray(loss)\n",
    "jnp.minimum(num_iter, self.config.num_recycle)\n",
    "jnp.einsum('bqa,ahc->bqhc', q_data, q_weights) * key_dim**(-0.5)\n",
    "jnp.where(bias, logits, _SOFTMAX_MASK)\n",
    "jnp.concatenate([evoformer_input['msa'], template_activations], axis=0)\n",
    "[jnp.zeros_like(x) for x in unit_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[jax控制分支](#toc0_)\n",
    "```\n",
    "grad：python分支和jax分支都支持\n",
    "jit：支支持jax分支\n",
    "\n",
    "jax控制流：\n",
    "    lax.cond: 等同于if\n",
    "    lax.while_loop: 等同于while\n",
    "    lax.fori_loop:等同于for\n",
    "    lax.scan：对数组进行操作的函数\n",
    "```\n",
    "### 3.2.1. <a id='toc3_2_1_'></a>[分支对grad影响](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(12., dtype=float32, weak_type=True),\n",
       " Array(-4., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    if x < 3:\n",
    "        return 3.0 * x**2\n",
    "    else:\n",
    "        return -4.0 * x\n",
    "    \n",
    "Df = jax.grad(f)\n",
    "Df(2.0), Df(3.0)\n",
    "# 不影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. <a id='toc3_2_2_'></a>[分支对jit影响](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jited = jax.jit(f)\n",
    "f_jited(2.0) # 会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. <a id='toc3_2_3_'></a>[条件判断-jax.lax.cond(True, func1, func2, args)](#toc0_)\n",
    "```\n",
    "jax.lax.cond(判断条件, True对应的执行函数, False对应的执行)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(12., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将python分支改写成jax分支\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def laxf(x):\n",
    "    return jax.lax.cond(x<3, lambda x: 3.0 * x**2, lambda x:-4*x, x)\n",
    "\n",
    "Dlaxf = jax.grad(laxf)\n",
    "laxf_jited = jax.jit(Dlaxf)\n",
    "laxf_jited(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. <a id='toc3_2_4_'></a>[循环-jax.lax.while_loop()](#toc0_)\n",
    "```\n",
    "jax.lax.while_loop(cond_fun, body_fun, init_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(17, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_val = 0\n",
    "\n",
    "def cond_fun(x):\n",
    "    return x < 17\n",
    "\n",
    "def body_fun(x):\n",
    "    return x + 1\n",
    "\n",
    "jax.lax.while_loop(cond_fun, body_fun, init_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. <a id='toc3_2_5_'></a>[循环-jax.lax.fori_loop()](#toc0_)\n",
    "```\n",
    "jax.lax.fori_loop(start, stop, body_fun, init_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(45, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_val = 0\n",
    "start = 0\n",
    "stop = 10\n",
    "body_fun = lambda i,x : x + i\n",
    "\n",
    "jax.lax.fori_loop(start, stop, body_fun, init_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[jax.nn包含的函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.nn.one_hot()\n",
    "# jax.nn.normalize()\n",
    "# jax.nn.sigmoid()\n",
    "# jax.nn.tanh()\n",
    "# jax.nn.softmax()\n",
    "# jax.nn.relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[jax.example_libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  jax.example_libraries.optimizers as optimezers\n",
    "# import jax.example_libraries.stax as stax\n",
    "\n",
    "# optimezers.adam()\n",
    "# optimezers.sgd()\n",
    "# optimezers.adagrad()\n",
    "# optimezers.rmsprop()\n",
    "\n",
    "# stax.BatchNorm()\n",
    "# stax.Dense()\n",
    "# stax.Dropout()\n",
    "# stax.Conv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[多层感知机](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. <a id='toc4_1_'></a>[准备数据集](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. <a id='toc4_1_1_'></a>[mnist](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "X_train = jnp.load('Minist/mnist_train_x.npy')\n",
    "y_train = jnp.load(\"Minist/mnist_train_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       " (60000, 28, 28))"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([5, 0, 4, ..., 5, 6, 8], dtype=uint8), (60000,))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trian, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. <a id='toc4_1_2_'></a>[独热码（one-hot）](#toc0_)\n",
    "```\n",
    "离散数据之间若无相关关系，则在数值表示上最好也不要有顺序等相关关系，所有用独热码的形式表示。\n",
    "```\n",
    "#### 4.1.2.1. <a id='toc4_1_2_1_'></a>[自定义函数实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_3 = y_train[0:3] # 取前三行\n",
    "y_train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[5],\n",
       "       [0],\n",
       "       [4]], dtype=uint8)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_3[:,None] # 行向量编程列向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.arange(10) # 0-9的10个行向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[False, False, False, False, False,  True, False, False, False,\n",
       "        False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False,  True, False, False, False, False,\n",
       "        False]], dtype=bool)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_3[:,None] == jnp.arange(10) # 匹配上的显示True，否则为False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([False, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array([False,False,True]) # 默认变成了bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array([False,False,True], dtype=jnp.float32) # 指定数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([5, 0, 4, 1, 9], dtype=uint8),\n",
       " Array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用函数框起来\n",
    "def one_hot_nojit(x, k=10, dtype=jnp.float32):\n",
    "    return jnp.array(x[:,None] == jnp.arange(k), dtype)\n",
    "\n",
    "y_trian[0:5], one_hot_nojit(y_trian)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.2. <a id='toc4_1_2_2_'></a>[jax.nn.one_hot()实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jax.nn.one_hot()\n",
    "## 简单实用\n",
    "\n",
    "jax.nn.one_hot(y_train, num_classes=10, dtype=jnp.float32)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接训练mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, random\n",
    "from jax.example_libraries import stax, optimizers\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定义网络结构，并计算出预测值(y_hat)\n",
    " \n",
    "# {Dense(1024) -> ReLU}x2 -> Dense(10) -> LogSoftmax\n",
    "init_random_params, predict = stax.serial(\n",
    "    stax.Dense(1024), stax.Relu,\n",
    "    stax.Dense(1024), stax.Relu,\n",
    "    stax.Dense(10), stax.LogSoftmax\n",
    "    )\n",
    "## 初始化网络参数\n",
    "_, init_params = init_random_params(rng=random.PRNGKey(0), input_shape=(-1, 28*28))\n",
    "# init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构造损失函数：真实值(y) - 预测值(y_hat)\n",
    "\n",
    "@jit\n",
    "def loss(params, batch):\n",
    "    \"\"\" Cross-entropy loss over a minibatch. \"\"\"\n",
    "    inputs, targets = batch\n",
    "    return jnp.mean(jnp.sum(-targets * predict(params, inputs), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 定义优化器：用来更新权重（w)和偏置(b)\n",
    "\n",
    "opt = optimizers.adam(step_size=2e-4)\n",
    "# opt_state = opt.init_fn(网络参数)        # 初始化优化器参数\n",
    "# params = opt.params_fn(opt_state)                # 获得优化器参数 \n",
    "# opt_state = opt.update_fn()\n",
    "\n",
    "## 传入网络的参数后初始化优化器参数\n",
    "opt_state = opt.init_fn(init_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Training set accuracy: 0.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def pred_check(params, batch):\n",
    "    \"\"\" Correct predictions over a minibatch. \"\"\"\n",
    "    inputs, targets = batch\n",
    "    predict_result = predict(params, inputs)\n",
    "    predicted_class = jnp.argmax(predict_result, axis=1)\n",
    "    targets = jnp.argmax(targets, axis=1)\n",
    "    return jnp.sum(predicted_class == targets)\n",
    "\n",
    "\n",
    "# 准备数据集\n",
    "x_train = jnp.load(\"Minist/mnist_train_x.npy\")\n",
    "y_train = jnp.load(\"Minist/mnist_train_y.npy\")\n",
    "## y_trian要做独热编码处理\n",
    "y_train = jax.nn.one_hot(x=y_train, num_classes=len(jnp.unique(y_train)))\n",
    "\n",
    "## x y配对并打乱顺序\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_train = tfds.as_numpy(ds_train)\n",
    "\n",
    "\n",
    "# 4. 训练过程\n",
    "x = []\n",
    "train_acc = [] # 训练准确度暂存列表\n",
    "for epoch in range(5):\n",
    "    itercount = 0\n",
    "    for batch_raw in ds_train:\n",
    "        data = batch_raw[0].reshape((-1, 28 * 28))\n",
    "        targets = batch_raw[1].reshape((-1, 10))\n",
    "        # 更新权重和偏置\n",
    "        opt_state = opt.update_fn(itercount,\n",
    "                                  grad(loss)(opt.params_fn(opt_state), (data,targets)), \n",
    "                                  opt_state\n",
    "                                  )\n",
    "        itercount += 1\n",
    "    params = opt.params_fn(opt_state)\n",
    "    #上面是训练部分，这里是存档部分，这里直接仿照numpy进行存档即可\n",
    "\n",
    "    #上面是载入部分，直接仿照numpy中数据进行载入即可\n",
    "    #params = jnp.load(\"params.npy\",allow_pickle =True)\n",
    "    # Train Acc\n",
    "    correct_preds = 0.0\n",
    "    for batch_raw in ds_train:\n",
    "        data = batch_raw[0].reshape((-1, 28 * 28))\n",
    "        targets = batch_raw[1]\n",
    "        correct_preds += pred_check(params, (data, targets))\n",
    "\n",
    "    acc = correct_preds / float(len(y_train))\n",
    "    train_acc.append(acc)\n",
    "    print(f\"{epoch}) Training set accuracy: {acc}\")\n",
    "\n",
    "    # # 绘图\n",
    "    x.append(epoch)\n",
    "    plt.clf()\n",
    "    plt.plot(x, train_acc)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('acc')\n",
    "    plt.title('train curve')\n",
    "    plt.pause(0.0001)  # 暂停一段时间，不然画的太快会卡住显示不出来\n",
    "    display.clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
