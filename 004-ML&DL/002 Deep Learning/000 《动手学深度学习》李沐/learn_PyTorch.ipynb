{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [概述](#toc1_)    \n",
    "- 2. [环境配置](#toc2_)    \n",
    "  - 2.1. [conda1](#toc2_1_)    \n",
    "  - 2.2. [conda2](#toc2_2_)    \n",
    "  - 2.3. [conda/uv](#toc2_3_)    \n",
    "- 3. [utils](#toc3_)    \n",
    "  - 3.1. [deepspore](#toc3_1_)    \n",
    "  - 3.2. [save to utils.py](#toc3_2_)    \n",
    "  - 3.3. [实验可重复性](#toc3_3_)    \n",
    "  - 3.4. [Metrics和Visualization](#toc3_4_)    \n",
    "    - 3.4.1. [Metrics tracker](#toc3_4_1_)    \n",
    "      - 3.4.1.1. [v1](#toc3_4_1_1_)    \n",
    "    - 3.4.2. [可视化](#toc3_4_2_)    \n",
    "  - 3.5. [GPU](#toc3_5_)    \n",
    "  - 3.6. [Timer](#toc3_6_)    \n",
    "    - 3.6.1. [CPU计时器](#toc3_6_1_)    \n",
    "    - 3.6.2. [gpu计时器](#toc3_6_2_)    \n",
    "  - 3.7. [Callback](#toc3_7_)    \n",
    "  - 3.8. [Trainer](#toc3_8_)    \n",
    "  - 3.9. [ParametersSize](#toc3_9_)    \n",
    "  - 3.10. [numpy和pytorch计算速度比较](#toc3_10_)    \n",
    "- 4. [安装GPU驱动](#toc4_)    \n",
    "  - 4.1. [安装策略](#toc4_1_)    \n",
    "  - 4.2. [首先确认内核版本和发行版本，再确认显卡型号](#toc4_2_)    \n",
    "  - 4.3. [安装驱动-CUDA Driver](#toc4_3_)    \n",
    "    - 4.3.1. [下载CUDA Driver](#toc4_3_1_)    \n",
    "    - 4.3.2. [禁用nouveau](#toc4_3_2_)    \n",
    "    - 4.3.3. [安装CUDA Driver](#toc4_3_3_)    \n",
    "    - 4.3.4. [查看显卡是否安装成功](#toc4_3_4_)    \n",
    "    - 4.3.5. [查看nvcc](#toc4_3_5_)    \n",
    "  - 4.4. [全局驱动和全局CUDA Toolkit和CuDNN](#toc4_4_)    \n",
    "    - 4.4.1. [下载对应的CUDA Toolkit版本](#toc4_4_1_)    \n",
    "    - 4.4.2. [安装CUDA Toolkit](#toc4_4_2_)    \n",
    "    - 4.4.3. [下载对应的CuDNN](#toc4_4_3_)    \n",
    "    - 4.4.4. [安装CuDNN](#toc4_4_4_)    \n",
    "  - 4.5. [安装对应版本的Pytorch](#toc4_5_)    \n",
    "  - 4.6. [全局驱动个人CUDA Toolkit](#toc4_6_)    \n",
    "  - 4.7. [GPU测试程序](#toc4_7_)    \n",
    "    - 4.7.1. [在GPU上验证Trainer](#toc4_7_1_)    \n",
    "    - 4.7.2. [GPU burn压力测试](#toc4_7_2_)    \n",
    "- 5. [Pytorch模块介绍](#toc5_)    \n",
    "  - 5.1. [导入模块](#toc5_1_)    \n",
    "- 6. [数据封装和加载](#toc6_)    \n",
    "  - 6.1. [torchvison.datasets获得Dataset](#toc6_1_)    \n",
    "  - 6.2. [Dataset](#toc6_2_)    \n",
    "    - 6.2.1. [TensorDataset()](#toc6_2_1_)    \n",
    "    - 6.2.2. [重载Dataset类](#toc6_2_2_)    \n",
    "    - 6.2.3. [Pytoch.utils.data.Dataset类分析和总结](#toc6_2_3_)    \n",
    "    - 6.2.4. [Subset](#toc6_2_4_)    \n",
    "    - 6.2.5. [random_split](#toc6_2_5_)    \n",
    "    - 6.2.6. [ConcateDataset](#toc6_2_6_)    \n",
    "    - 6.2.7. [IterableDataset](#toc6_2_7_)    \n",
    "      - 6.2.7.1. [流式数据加载](#toc6_2_7_1_)    \n",
    "      - 6.2.7.2. [动态生成数据](#toc6_2_7_2_)    \n",
    "      - 6.2.7.3. [无限数据流](#toc6_2_7_3_)    \n",
    "      - 6.2.7.4. [多线程数据加载与分布式支持](#toc6_2_7_4_)    \n",
    "      - 6.2.7.5. [使用注意事项](#toc6_2_7_5_)    \n",
    "  - 6.3. [DataLoader](#toc6_3_)    \n",
    "    - 6.3.1. [估计数据加载时间](#toc6_3_1_)    \n",
    "    - 6.3.2. [collate_fn处理不等长tensor](#toc6_3_2_)    \n",
    "    - 6.3.3. [重载DataLoader](#toc6_3_3_)    \n",
    "- 7. [张量(Tensors)](#toc7_)    \n",
    "  - 7.1. [Tensors定义](#toc7_1_)    \n",
    "  - 7.2. [Tensors属性](#toc7_2_)    \n",
    "    - 7.2.1. [数据类型(dtype)](#toc7_2_1_)    \n",
    "      - 7.2.1.1. [转化格式](#toc7_2_1_1_)    \n",
    "    - 7.2.2. [设备(device)](#toc7_2_2_)    \n",
    "    - 7.2.3. [维度(size/shape)](#toc7_2_3_)    \n",
    "      - 7.2.3.1. [标量](#toc7_2_3_1_)    \n",
    "      - 7.2.3.2. [一维张量](#toc7_2_3_2_)    \n",
    "      - 7.2.3.3. [多维张量](#toc7_2_3_3_)    \n",
    "    - 7.2.4. [特殊的一维张量](#toc7_2_4_)    \n",
    "      - 7.2.4.1. [一维张量的例子](#toc7_2_4_1_)    \n",
    "      - 7.2.4.2. [区分行向量和列向量](#toc7_2_4_2_)    \n",
    "        - 7.2.4.2.1. [示例](#toc7_2_4_2_1_)    \n",
    "      - 7.2.4.3. [小结](#toc7_2_4_3_)    \n",
    "  - 7.3. [Tensors操作](#toc7_3_)    \n",
    "    - 7.3.1. [索引和切片](#toc7_3_1_)    \n",
    "    - 7.3.2. [修改维度](#toc7_3_2_)    \n",
    "      - 7.3.2.1. [[: None], [None, :]                            ](#toc7_3_2_1_)    \n",
    "      - 7.3.2.2. [reshape函数](#toc7_3_2_2_)    \n",
    "      - 7.3.2.3. [view函数](#toc7_3_2_3_)    \n",
    "      - 7.3.2.4. [transpose函数](#toc7_3_2_4_)    \n",
    "        - 7.3.2.4.1. [二维](#toc7_3_2_4_1_)    \n",
    "        - 7.3.2.4.2. [三维](#toc7_3_2_4_2_)    \n",
    "      - 7.3.2.5. [permute函数](#toc7_3_2_5_)    \n",
    "        - 7.3.2.5.1. [二维](#toc7_3_2_5_1_)    \n",
    "        - 7.3.2.5.2. [三维](#toc7_3_2_5_2_)    \n",
    "      - 7.3.2.6. [unsqueeze函数增加维度](#toc7_3_2_6_)    \n",
    "        - 7.3.2.6.1. [1维](#toc7_3_2_6_1_)    \n",
    "        - 7.3.2.6.2. [多维度](#toc7_3_2_6_2_)    \n",
    "      - 7.3.2.7. [squeeze函数减少维度](#toc7_3_2_7_)    \n",
    "      - 7.3.2.8. [拼接 (concat)](#toc7_3_2_8_)    \n",
    "      - 7.3.2.9. [拆分 (split)](#toc7_3_2_9_)    \n",
    "      - 7.3.2.10. [分块 (chunk)](#toc7_3_2_10_)    \n",
    "      - 7.3.2.11. [拼接 (stack)](#toc7_3_2_11_)    \n",
    "        - 7.3.2.11.1. [cat和stack的比较](#toc7_3_2_11_1_)    \n",
    "      - 7.3.2.12. [广播 (expand)](#toc7_3_2_12_)    \n",
    "      - 7.3.2.13. [repeat](#toc7_3_2_13_)    \n",
    "      - 7.3.2.14. [repeat_interleave](#toc7_3_2_14_)    \n",
    "        - 7.3.2.14.1. [expand和repeat对比](#toc7_3_2_14_1_)    \n",
    "      - 7.3.2.15. [填充padding和打包packing](#toc7_3_2_15_)    \n",
    "  - 7.4. [线性代数运算](#toc7_4_)    \n",
    "    - 7.4.1. [数值运算](#toc7_4_1_)    \n",
    "    - 7.4.2. [数值运算-乘法](#toc7_4_2_)    \n",
    "      - 7.4.2.1. [哈达玛积](#toc7_4_2_1_)    \n",
    "      - 7.4.2.2. [点积（Dot Product）](#toc7_4_2_2_)    \n",
    "      - 7.4.2.3. [矩阵-向量积](#toc7_4_2_3_)    \n",
    "      - 7.4.2.4. [矩阵-矩阵积](#toc7_4_2_4_)    \n",
    "      - 7.4.2.5. [批量矩阵乘法](#toc7_4_2_5_)    \n",
    "      - 7.4.2.6. [乘总结](#toc7_4_2_6_)    \n",
    "    - 7.4.3. [统计运算](#toc7_4_3_)    \n",
    "  - 7.5. [广播机制 (Broadcasting)](#toc7_5_)    \n",
    "    - 7.5.1. [广播规则](#toc7_5_1_)    \n",
    "  - 7.6. [Pytorch的计算图 和 自动微分 (autograd)](#toc7_6_)    \n",
    "    - 7.6.1. [反向传播 (backward)-批量求梯度，但未进行参数更新](#toc7_6_1_)    \n",
    "    - 7.6.2. [仅计算梯度 (求导计算)](#toc7_6_2_)    \n",
    "  - 7.7. [自动微积-autograd](#toc7_7_)    \n",
    "    - 7.7.1. [自己探索](#toc7_7_1_)    \n",
    "      - 7.7.1.1. [标量-一阶导数（得标量）](#toc7_7_1_1_)    \n",
    "      - 7.7.1.2. [标量/向量-一阶导数（得向量）](#toc7_7_1_2_)    \n",
    "      - 7.7.1.3. [向量/向量-一阶导数（得矩阵）](#toc7_7_1_3_)    \n",
    "      - 7.7.1.4. [求高阶导数](#toc7_7_1_4_)    \n",
    "    - 7.7.2. [一个简单的例子](#toc7_7_2_)    \n",
    "    - 7.7.3. [计算另一个](#toc7_7_3_)    \n",
    "    - 7.7.4. [非标量变量的反向传播](#toc7_7_4_)    \n",
    "    - 7.7.5. [分离计算](#toc7_7_5_)    \n",
    "    - 7.7.6. [Python控制流的梯度计算](#toc7_7_6_)    \n",
    "  - 7.8. [概率论](#toc7_8_)    \n",
    "- 8. [神经网络-训练八股](#toc8_)    \n",
    "  - 8.1. [现线性回归模型于训练过程-从零开始](#toc8_1_)    \n",
    "    - 8.1.1. [虚拟出数据](#toc8_1_1_)    \n",
    "    - 8.1.2. [读取数据](#toc8_1_2_)    \n",
    "    - 8.1.3. [初始化模型参数](#toc8_1_3_)    \n",
    "    - 8.1.4. [定义模型](#toc8_1_4_)    \n",
    "    - 8.1.5. [定义损失函数](#toc8_1_5_)    \n",
    "    - 8.1.6. [定义优化算法](#toc8_1_6_)    \n",
    "    - 8.1.7. [训练](#toc8_1_7_)    \n",
    "  - 8.2. [现线性回归模型于训练过程-简洁实现](#toc8_2_)    \n",
    "    - 8.2.1. [虚拟数据](#toc8_2_1_)    \n",
    "    - 8.2.2. [读取数据](#toc8_2_2_)    \n",
    "    - 8.2.3. [定义模型](#toc8_2_3_)    \n",
    "    - 8.2.4. [初始化模型参数](#toc8_2_4_)    \n",
    "    - 8.2.5. [定义损失函数](#toc8_2_5_)    \n",
    "    - 8.2.6. [定义优化算法](#toc8_2_6_)    \n",
    "    - 8.2.7. [训练](#toc8_2_7_)    \n",
    "    - 8.2.8. [参数保存](#toc8_2_8_)    \n",
    "    - 8.2.9. [重载](#toc8_2_9_)    \n",
    "  - 8.3. [分类-softmax](#toc8_3_)    \n",
    "    - 8.3.1. [快速实现](#toc8_3_1_)    \n",
    "    - 8.3.2. [从头实现](#toc8_3_2_)    \n",
    "    - 8.3.3. [交叉熵损失](#toc8_3_3_)    \n",
    "  - 8.4. [专题-模型定义（计算预测值y_hat）](#toc8_4_)    \n",
    "    - 8.4.1. [块：torch.nn模块](#toc8_4_1_)    \n",
    "      - 8.4.1.1. [Sequential、ModuleList、ModuleDict](#toc8_4_1_1_)    \n",
    "      - 8.4.1.2. [比较](#toc8_4_1_2_)    \n",
    "    - 8.4.2. [块：自定义](#toc8_4_2_)    \n",
    "      - 8.4.2.1. [自定义块](#toc8_4_2_1_)    \n",
    "      - 8.4.2.2. [顺序块](#toc8_4_2_2_)    \n",
    "      - 8.4.2.3. [效率](#toc8_4_2_3_)    \n",
    "    - 8.4.3. [模型结构/组成](#toc8_4_3_)    \n",
    "      - 8.4.3.1. [.children()](#toc8_4_3_1_)    \n",
    "      - 8.4.3.2. [.named_children()](#toc8_4_3_2_)    \n",
    "      - 8.4.3.3. [.modules()](#toc8_4_3_3_)    \n",
    "      - 8.4.3.4. [.named_modules()](#toc8_4_3_4_)    \n",
    "      - 8.4.3.5. [删除和添加](#toc8_4_3_5_)    \n",
    "      - 8.4.3.6. [替换](#toc8_4_3_6_)    \n",
    "      - 8.4.3.7. [add_module()](#toc8_4_3_7_)    \n",
    "    - 8.4.4. [模型：参数管理](#toc8_4_4_)    \n",
    "      - 8.4.4.1. [参数访问](#toc8_4_4_1_)    \n",
    "        - 8.4.4.1.1. [state_dict](#toc8_4_4_1_1_)    \n",
    "        - 8.4.4.1.2. [parameters](#toc8_4_4_1_2_)    \n",
    "        - 8.4.4.1.3. [named_parameters](#toc8_4_4_1_3_)    \n",
    "      - 8.4.4.2. [参数初始化](#toc8_4_4_2_)    \n",
    "        - 8.4.4.2.1. [内置初始化](#toc8_4_4_2_1_)    \n",
    "        - 8.4.4.2.2. [自定义初始化](#toc8_4_4_2_2_)    \n",
    "        - 8.4.4.2.3. [参数绑定](#toc8_4_4_2_3_)    \n",
    "    - 8.4.5. [层：自定义](#toc8_4_5_)    \n",
    "      - 8.4.5.1. [不带参数的层](#toc8_4_5_1_)    \n",
    "      - 8.4.5.2. [带参数的层](#toc8_4_5_2_)    \n",
    "  - 8.5. [专题-损失函数 (loss_fn)](#toc8_5_)    \n",
    "    - 8.5.1. [均方误差](#toc8_5_1_)    \n",
    "    - 8.5.2. [交叉熵](#toc8_5_2_)    \n",
    "      - 8.5.2.1. [快速实现](#toc8_5_2_1_)    \n",
    "      - 8.5.2.2. [从头实现](#toc8_5_2_2_)    \n",
    "    - 8.5.3. [自定义](#toc8_5_3_)    \n",
    "  - 8.6. [专题-反向传播（求梯度）](#toc8_6_)    \n",
    "  - 8.7. [专题-更新权重（优化算法）](#toc8_7_)    \n",
    "    - 8.7.1. [小批量随机梯度下降（SGD）](#toc8_7_1_)    \n",
    "    - 8.7.2. [adam](#toc8_7_2_)    \n",
    "    - 8.7.3. [RMSprop](#toc8_7_3_)    \n",
    "    - 8.7.4. [学习率调度器](#toc8_7_4_)    \n",
    "      - 8.7.4.1. [StepLR： 按照固定的步长调整学习率](#toc8_7_4_1_)    \n",
    "      - 8.7.4.2. [MultiStepLR： 在指定的里程碑（milestones）上调整学习率](#toc8_7_4_2_)    \n",
    "      - 8.7.4.3. [ExponentialLR： 以指数衰减的方式调整学习率](#toc8_7_4_3_)    \n",
    "      - 8.7.4.4. [CosineAnnealingLR： 余弦退火调整学习率](#toc8_7_4_4_)    \n",
    "      - 8.7.4.5. [ReduceLROnPlateau： 当指标停止改善时，降低学习率](#toc8_7_4_5_)    \n",
    "      - 8.7.4.6. [LambdaLR： 使用自定义的函数来调整学习率](#toc8_7_4_6_)    \n",
    "      - 8.7.4.7. [自定义](#toc8_7_4_7_)    \n",
    "  - 8.8. [专题-训练](#toc8_8_)    \n",
    "    - 8.8.1. [开始训练](#toc8_8_1_)    \n",
    "    - 8.8.2. [自己探索](#toc8_8_2_)    \n",
    "      - 8.8.2.1. [lr的影响](#toc8_8_2_1_)    \n",
    "      - 8.8.2.2. [不同模型的效率](#toc8_8_2_2_)    \n",
    "    - 8.8.3. [K折交叉验证](#toc8_8_3_)    \n",
    "  - 8.9. [可视化训练过程](#toc8_9_)    \n",
    "- 9. [在 GPU 上训练](#toc9_)    \n",
    "  - 9.1. [查看GPU配置](#toc9_1_)    \n",
    "  - 9.2. [单机单卡（GPU）](#toc9_2_)    \n",
    "  - 9.3. [单机多卡（GPU）](#toc9_3_)    \n",
    "    - 9.3.1. [DP](#toc9_3_1_)    \n",
    "    - 9.3.2. [DDP](#toc9_3_2_)    \n",
    "      - 9.3.2.1. [在colab上测试可用](#toc9_3_2_1_)    \n",
    "  - 9.4. [多机多卡（GPU）- 分布式训练](#toc9_4_)    \n",
    "- 10. [模型和参数的保存与加载](#toc10_)    \n",
    "  - 10.1. [加载和保存-张量](#toc10_1_)    \n",
    "  - 10.2. [加载和保存-模型参数](#toc10_2_)    \n",
    "  - 10.3. [safetensor](#toc10_3_)    \n",
    "- 11. [神经网络类型](#toc11_)    \n",
    "  - 11.1. [CNN](#toc11_1_)    \n",
    "    - 11.1.1. [概述](#toc11_1_1_)    \n",
    "    - 11.1.2. [简单CNN](#toc11_1_2_)    \n",
    "      - 11.1.2.1. [从头实现](#toc11_1_2_1_)    \n",
    "        - 11.1.2.1.1. [卷积计算过程](#toc11_1_2_1_1_)    \n",
    "        - 11.1.2.1.2. [从头卷积层](#toc11_1_2_1_2_)    \n",
    "      - 11.1.2.2. [简洁实现](#toc11_1_2_2_)    \n",
    "      - 11.1.2.3. [填充和步幅](#toc11_1_2_3_)    \n",
    "      - 11.1.2.4. [多输入和多输出通道](#toc11_1_2_4_)    \n",
    "      - 11.1.2.5. [Pooling (汇聚层)](#toc11_1_2_5_)    \n",
    "        - 11.1.2.5.1. [平均Pooling](#toc11_1_2_5_1_)    \n",
    "        - 11.1.2.5.2. [最大Pooling](#toc11_1_2_5_2_)    \n",
    "    - 11.1.3. [LeNet](#toc11_1_3_)    \n",
    "    - 11.1.4. [AlexNet](#toc11_1_4_)    \n",
    "    - 11.1.5. [VGG](#toc11_1_5_)    \n",
    "    - 11.1.6. [NiN](#toc11_1_6_)    \n",
    "    - 11.1.7. [GoogLeNet](#toc11_1_7_)    \n",
    "    - 11.1.8. [批量规范化](#toc11_1_8_)    \n",
    "    - 11.1.9. [ResNet](#toc11_1_9_)    \n",
    "      - 11.1.9.1. [从头实现](#toc11_1_9_1_)    \n",
    "  - 11.2. [序列数据](#toc11_2_)    \n",
    "    - 11.2.1. [什么是序列](#toc11_2_1_)    \n",
    "    - 11.2.2. [语言模型](#toc11_2_2_)    \n",
    "    - 11.2.3. [文本预处理](#toc11_2_3_)    \n",
    "      - 11.2.3.1. [下载《Time machine》并读取数据](#toc11_2_3_1_)    \n",
    "      - 11.2.3.2. [词元化（Tokenization）](#toc11_2_3_2_)    \n",
    "      - 11.2.3.3. [词表（Vocabulary）](#toc11_2_3_3_)    \n",
    "      - 11.2.3.4. [整合所有功能](#toc11_2_3_4_)    \n",
    "      - 11.2.3.5. [文本编码与向量化](#toc11_2_3_5_)    \n",
    "        - 11.2.3.5.1. [word2vec](#toc11_2_3_5_1_)    \n",
    "    - 11.2.4. [语言模型数据集](#toc11_2_4_)    \n",
    "      - 11.2.4.1. [顺序采样 (Sequential Sampling)](#toc11_2_4_1_)    \n",
    "      - 11.2.4.2. [随机采样 (Random Sampling)](#toc11_2_4_2_)    \n",
    "      - 11.2.4.3. [PyTorch分装的顺序或随机采样](#toc11_2_4_3_)    \n",
    "      - 11.2.4.4. [总结](#toc11_2_4_4_)    \n",
    "      - 11.2.4.5. [包装](#toc11_2_4_5_)    \n",
    "  - 11.3. [RNN](#toc11_3_)    \n",
    "    - 11.3.1. [RNN-循环神经网络原理](#toc11_3_1_)    \n",
    "      - 11.3.1.1. [从头实现网络](#toc11_3_1_1_)    \n",
    "      - 11.3.1.2. [简洁实现](#toc11_3_1_2_)    \n",
    "      - 11.3.1.3. [训练和预测](#toc11_3_1_3_)    \n",
    "      - 11.3.1.4. [warm-up 预热期](#toc11_3_1_4_)    \n",
    "      - 11.3.1.5. [深层RNN](#toc11_3_1_5_)    \n",
    "      - 11.3.1.6. [双向RNN](#toc11_3_1_6_)    \n",
    "    - 11.3.2. [GRU](#toc11_3_2_)    \n",
    "      - 11.3.2.1. [从头实现](#toc11_3_2_1_)    \n",
    "      - 11.3.2.2. [简洁实现](#toc11_3_2_2_)    \n",
    "    - 11.3.3. [LSTM](#toc11_3_3_)    \n",
    "      - 11.3.3.1. [从头实现](#toc11_3_3_1_)    \n",
    "      - 11.3.3.2. [简洁实现](#toc11_3_3_2_)    \n",
    "    - 11.3.4. [Encoder-Decoder框架](#toc11_3_4_)    \n",
    "      - 11.3.4.1. [Encoder部分](#toc11_3_4_1_)    \n",
    "      - 11.3.4.2. [Decoder部分](#toc11_3_4_2_)    \n",
    "      - 11.3.4.3. [Encoder-Decoder（合并编码器和解码器）](#toc11_3_4_3_)    \n",
    "  - 11.4. [seq2seq (Sequence to sequence learning)](#toc11_4_)    \n",
    "    - 11.4.1. [机器翻译与数据集](#toc11_4_1_)    \n",
    "      - 11.4.1.1. [下载和预处理数据集](#toc11_4_1_1_)    \n",
    "      - 11.4.1.2. [词元化](#toc11_4_1_2_)    \n",
    "      - 11.4.1.3. [词表](#toc11_4_1_3_)    \n",
    "      - 11.4.1.4. [截断和填充](#toc11_4_1_4_)    \n",
    "      - 11.4.1.5. [集合](#toc11_4_1_5_)    \n",
    "    - 11.4.2. [编码器-解码器架构](#toc11_4_2_)    \n",
    "    - 11.4.3. [序列到序列学习](#toc11_4_3_)    \n",
    "    - 11.4.4. [损失函数](#toc11_4_4_)    \n",
    "      - 11.4.4.1. [掩码](#toc11_4_4_1_)    \n",
    "      - 11.4.4.2. [带掩码的softmax交叉熵损失](#toc11_4_4_2_)    \n",
    "    - 11.4.5. [训练](#toc11_4_5_)    \n",
    "    - 11.4.6. [预测](#toc11_4_6_)    \n",
    "  - 11.5. [Attention](#toc11_5_)    \n",
    "    - 11.5.1. [实例数据](#toc11_5_1_)    \n",
    "    - 11.5.2. [无注意力的方式-如平均汇聚](#toc11_5_2_)    \n",
    "    - 11.5.3. [非参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc11_5_3_)    \n",
    "    - 11.5.4. [参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc11_5_4_)    \n",
    "    - 11.5.5. [注意力分数函数-计算q和k相似度](#toc11_5_5_)    \n",
    "      - 11.5.5.1. [加性注意力 (Additive Attention)-计算q、k相似度](#toc11_5_5_1_)    \n",
    "      - 11.5.5.2. [缩放点积注意力 (Scaled Dot-Product Attention)-计算q、k相似度](#toc11_5_5_2_)    \n",
    "    - 11.5.6. [自注意力机制-q、k和v相同](#toc11_5_6_)    \n",
    "    - 11.5.7. [多头注意力机制-h个q、k和v对](#toc11_5_7_)    \n",
    "    - 11.5.8. [attention-seq2seq](#toc11_5_8_)    \n",
    "  - 11.6. [Transformer](#toc11_6_)    \n",
    "    - 11.6.1. [简洁实现](#toc11_6_1_)    \n",
    "    - 11.6.2. [位置编码](#toc11_6_2_)    \n",
    "      - 11.6.2.1. [绝对位置编码](#toc11_6_2_1_)    \n",
    "      - 11.6.2.2. [相对位置编码](#toc11_6_2_2_)    \n",
    "      - 11.6.2.3. [可学习的位置编码](#toc11_6_2_3_)    \n",
    "    - 11.6.3. [基于位置的前馈网络](#toc11_6_3_)    \n",
    "    - 11.6.4. [残差连接和层规范化](#toc11_6_4_)    \n",
    "    - 11.6.5. [编码器](#toc11_6_5_)    \n",
    "    - 11.6.6. [解码器](#toc11_6_6_)    \n",
    "    - 11.6.7. [基于Transformer的Seq2Seq网络](#toc11_6_7_)    \n",
    "  - 11.7. [BERT](#toc11_7_)    \n",
    "    - 11.7.1. [BERT encode block](#toc11_7_1_)    \n",
    "    - 11.7.2. [Masked Language Modeling](#toc11_7_2_)    \n",
    "    - 11.7.3. [Next Sentence Prediction](#toc11_7_3_)    \n",
    "    - 11.7.4. [BERT模型](#toc11_7_4_)    \n",
    "    - 11.7.5. [Datasets for Pre-training](#toc11_7_5_)    \n",
    "      - 11.7.5.1. [生成下一句预测任务的数据](#toc11_7_5_1_)    \n",
    "      - 11.7.5.2. [生成遮蔽语言模型任务的数据](#toc11_7_5_2_)    \n",
    "      - 11.7.5.3. [将文本转换为预训练数据集](#toc11_7_5_3_)    \n",
    "      - 11.7.5.4. [创建数据集](#toc11_7_5_4_)    \n",
    "    - 11.7.6. [预训练BERT](#toc11_7_6_)    \n",
    "    - 11.7.7. [用BERT表示文本](#toc11_7_7_)    \n",
    "  - 11.8. [用BERT做微调](#toc11_8_)    \n",
    "    - 11.8.1. [情感分析](#toc11_8_1_)    \n",
    "      - 11.8.1.1. [使用RNN](#toc11_8_1_1_)    \n",
    "      - 11.8.1.2. [使用CNN](#toc11_8_1_2_)    \n",
    "    - 11.8.2. [自然语言推断](#toc11_8_2_)    \n",
    "      - 11.8.2.1. [使用Attention](#toc11_8_2_1_)    \n",
    "      - 11.8.2.2. [微调BERT](#toc11_8_2_2_)    \n",
    "  - 11.9. [后BERT](#toc11_9_)    \n",
    "    - 11.9.1. [BERT的改进模型](#toc11_9_1_)    \n",
    "  - 11.10. [GPT](#toc11_10_)    \n",
    "  - 11.11. [T5](#toc11_11_)    \n",
    "  - 11.12. [BART](#toc11_12_)    \n",
    "  - 11.13. [mBART](#toc11_13_)    \n",
    "  - 11.14. [MoE](#toc11_14_)    \n",
    "    - 11.14.1. [基于Transformer实现MoE](#toc11_14_1_)    \n",
    "    - 11.14.2. [小项目](#toc11_14_2_)    \n",
    "  - 11.15. [Mamba](#toc11_15_)    \n",
    "- 12. [==============](#toc12_)    \n",
    "- 13. [炼丹心得](#toc13_)    \n",
    "  - 13.1. [关于调参](#toc13_1_)    \n",
    "  - 13.2. [模型选择](#toc13_2_)    \n",
    "  - 13.3. [离散数据](#toc13_3_)    \n",
    "    - 13.3.1. [one-hot](#toc13_3_1_)    \n",
    "    - 13.3.2. [embedding](#toc13_3_2_)    \n",
    "      - 13.3.2.1. [使用 torch.nn.Embedding](#toc13_3_2_1_)    \n",
    "      - 13.3.2.2. [初始化 Embedding 层](#toc13_3_2_2_)    \n",
    "  - 13.4. [BN和LN](#toc13_4_)    \n",
    "  - 13.5. [掩码 (mask)](#toc13_5_)    \n",
    "    - 13.5.1. [简单演示](#toc13_5_1_)    \n",
    "      - 13.5.1.1. [忽略填充](#toc13_5_1_1_)    \n",
    "      - 13.5.1.2. [加权忽略](#toc13_5_1_2_)    \n",
    "    - 13.5.2. [注意力机制中的掩码](#toc13_5_2_)    \n",
    "      - 13.5.2.1. [Padding Mask](#toc13_5_2_1_)    \n",
    "      - 13.5.2.2. [Causal Mask](#toc13_5_2_2_)    \n",
    "    - 13.5.3. [掩码注意力计算](#toc13_5_3_)    \n",
    "  - 13.6. [MLP、FC、FNN、CNN、RNN](#toc13_6_)    \n",
    "  - 13.7. [优化显存使用](#toc13_7_)    \n",
    "    - 13.7.1. [删除中间暂时不用的变量](#toc13_7_1_)    \n",
    "    - 13.7.2. [混合精度训练(Mixed Precision Training)](#toc13_7_2_)    \n",
    "    - 13.7.3. [梯度检查点（Gradient Checkpointing）](#toc13_7_3_)    \n",
    "    - 13.7.4. [分块计算 (Chunking)](#toc13_7_4_)    \n",
    "      - 13.7.4.1. [简单演示](#toc13_7_4_1_)    \n",
    "      - 13.7.4.2. [重要演示](#toc13_7_4_2_)    \n",
    "  - 13.8. [模型参数量](#toc13_8_)    \n",
    "  - 13.9. [大模型微调](#toc13_9_)    \n",
    "  - 13.10. [加速器](#toc13_10_)    \n",
    "    - 13.10.1. [deepspeed](#toc13_10_1_)    \n",
    "      - 13.10.1.1. [数据并行](#toc13_10_1_1_)    \n",
    "      - 13.10.1.2. [模型并行](#toc13_10_1_2_)    \n",
    "      - 13.10.1.3. [混合并行](#toc13_10_1_3_)    \n",
    "    - 13.10.2. [huggingface trainer and accelerate](#toc13_10_2_)    \n",
    "      - 13.10.2.1. [数据并行](#toc13_10_2_1_)    \n",
    "      - 13.10.2.2. [模型并行](#toc13_10_2_2_)    \n",
    "      - 13.10.2.3. [混合并行](#toc13_10_2_3_)    \n",
    "- 14. [PyTorch做迁移学习](#toc14_)    \n",
    "  - 14.1. [Fine-tuning](#toc14_1_)    \n",
    "    - 14.1.1. [小的lr](#toc14_1_1_)    \n",
    "    - 14.1.2. [停止计算梯度](#toc14_1_2_)    \n",
    "  - 14.2. [torchvision的应用案例](#toc14_2_)    \n",
    "  - 14.3. [迁移学习案例](#toc14_3_)    \n",
    "- 15. [Metrics](#toc15_)    \n",
    "  - 15.1. [TorchMetrics](#toc15_1_)    \n",
    "    - 15.1.1. [准确率、精确率、召回率和F1分数](#toc15_1_1_)    \n",
    "    - 15.1.2. [自定义计算指标](#toc15_1_2_)    \n",
    "    - 15.1.3. [于PyTorch Lightning联合使用](#toc15_1_3_)    \n",
    "  - 15.2. [分类问题的评估指标](#toc15_2_)    \n",
    "    - 15.2.1. [混淆矩阵](#toc15_2_1_)    \n",
    "      - 15.2.1.1. [二分类混淆矩阵](#toc15_2_1_1_)    \n",
    "      - 15.2.1.2. [多分类混淆矩阵](#toc15_2_1_2_)    \n",
    "      - 15.2.1.3. [可视化混淆矩阵](#toc15_2_1_3_)    \n",
    "      - 15.2.1.4. [混淆矩阵的优点与局限性](#toc15_2_1_4_)    \n",
    "    - 15.2.2. [准确率 (Accuracy)](#toc15_2_2_)    \n",
    "    - 15.2.3. [精确率 (Precision)](#toc15_2_3_)    \n",
    "    - 15.2.4. [召回率 (Recall)](#toc15_2_4_)    \n",
    "    - 15.2.5. [F1-Score](#toc15_2_5_)    \n",
    "    - 15.2.6. [ROC 曲线和 AUC (Area Under Curve)](#toc15_2_6_)    \n",
    "    - 15.2.7. [多分类问题指标](#toc15_2_7_)    \n",
    "  - 15.3. [回归问题的评估指标](#toc15_3_)    \n",
    "    - 15.3.1. [平均绝对误差 (MAE)](#toc15_3_1_)    \n",
    "    - 15.3.2. [均方误差 (MSE)](#toc15_3_2_)    \n",
    "    - 15.3.3. [均方根误差 (RMSE)](#toc15_3_3_)    \n",
    "    - 15.3.4. [R² (决定系数)](#toc15_3_4_)    \n",
    "  - 15.4. [Metrics tracker](#toc15_4_)    \n",
    "- 16. [Benchmark](#toc16_)    \n",
    "  - 16.1. [确定 Benchmark 目标](#toc16_1_)    \n",
    "  - 16.2. [Benchmark模板](#toc16_2_)    \n",
    "- 17. [PyTorch lightning](#toc17_)    \n",
    "  - 17.1. [训练逻辑](#toc17_1_)    \n",
    "  - 17.2. [Data.py](#toc17_2_)    \n",
    "  - 17.3. [Model.py](#toc17_3_)    \n",
    "  - 17.4. [ModelWrapper.py](#toc17_4_)    \n",
    "    - 17.4.1. [Training and vlidation](#toc17_4_1_)    \n",
    "    - 17.4.2. [Validation](#toc17_4_2_)    \n",
    "    - 17.4.3. [Test](#toc17_4_3_)    \n",
    "    - 17.4.4. [Prediction](#toc17_4_4_)    \n",
    "      - 17.4.4.1. [PyTorch lightning自身Trainer直接predict](#toc17_4_4_1_)    \n",
    "      - 17.4.4.2. [PyTorch lightning加载权重后预测](#toc17_4_4_2_)    \n",
    "      - 17.4.4.3. [提取权重后加载至纯PyTorch模型](#toc17_4_4_3_)    \n",
    "- 18. [Torchvision](#toc18_)    \n",
    "  - 18.1. [Models](#toc18_1_)    \n",
    "    - 18.1.1. [可用模型](#toc18_1_1_)    \n",
    "    - 18.1.2. [下载模型和权重](#toc18_1_2_)    \n",
    "    - 18.1.3. [模型加载权重](#toc18_1_3_)    \n",
    "    - 18.1.4. [总结](#toc18_1_4_)    \n",
    "  - 18.2. [Dataset](#toc18_2_)    \n",
    "- 19. [多模态 (ML, MultiModal Learning)](#toc19_)    \n",
    "  - 19.1. [特征融合](#toc19_1_)    \n",
    "    - 19.1.1. [concatenate融合](#toc19_1_1_)    \n",
    "    - 19.1.2. [加权融合](#toc19_1_2_)    \n",
    "    - 19.1.3. [元素级融合](#toc19_1_3_)    \n",
    "    - 19.1.4. [张量融合](#toc19_1_4_)    \n",
    "    - 19.1.5. [注意力机制融合](#toc19_1_5_)    \n",
    "    - 19.1.6. [高阶融合](#toc19_1_6_)    \n",
    "  - 19.2. [简单示例](#toc19_2_)    \n",
    "- 20. [Few-shot learning](#toc20_)    \n",
    "  - 20.1. [Siamese Network](#toc20_1_)    \n",
    "- 21. [matplotlib](#toc21_)    \n",
    "  - 21.1. [字体](#toc21_1_)    \n",
    "  - 21.2. [显示中文](#toc21_2_)    \n",
    "- 22. [argparse](#toc22_)    \n",
    "- 23. [ml_collections](#toc23_)    \n",
    "  - 23.1. [概述](#toc23_1_)    \n",
    "  - 23.2. [详细使用](#toc23_2_)    \n",
    "- 24. [functools](#toc24_)    \n",
    "  - 24.1. [partial](#toc24_1_)    \n",
    "- 25. [copy](#toc25_)    \n",
    "  - 25.1. [列表类型的拷贝](#toc25_1_)    \n",
    "  - 25.2. [字典类型的拷贝](#toc25_2_)    \n",
    "- 26. [tqdm](#toc26_)    \n",
    "  - 26.1. [基础循环封装](#toc26_1_)    \n",
    "  - 26.2. [手动控制进度](#toc26_2_)    \n",
    "  - 26.3. [多进度条嵌套](#toc26_3_)    \n",
    "  - 26.4. [进阶功能与优化](#toc26_4_)    \n",
    "    - 26.4.1. [动态调整参数](#toc26_4_1_)    \n",
    "    - 26.4.2. [与Pandas结合](#toc26_4_2_)    \n",
    "    - 26.4.3. [Jupyter Notebook适配](#toc26_4_3_)    \n",
    "    - 26.4.4. [多线程/多进程支持](#toc26_4_4_)    \n",
    "    - 26.4.5. [自定义进度条格式](#toc26_4_5_)    \n",
    "- 27. [callback](#toc27_)    \n",
    "  - 27.1. [基于getattr实现](#toc27_1_)    \n",
    "- 28. [typing](#toc28_)    \n",
    "  - 28.1. [基础类型注释](#toc28_1_)    \n",
    "    - 28.1.1. [变量注解](#toc28_1_1_)    \n",
    "    - 28.1.2. [函数注解](#toc28_1_2_)    \n",
    "  - 28.2. [容器类型注解](#toc28_2_)    \n",
    "    - 28.2.1. [标准容器](#toc28_2_1_)    \n",
    "    - 28.2.2. [嵌套容器](#toc28_2_2_)    \n",
    "  - 28.3. [高级类型](#toc28_3_)    \n",
    "    - 28.3.1. [泛型与类型变量](#toc28_3_1_)    \n",
    "    - 28.3.2. [回调函数类型](#toc28_3_2_)    \n",
    "  - 28.4. [结构化类型](#toc28_4_)    \n",
    "    - 28.4.1. [类型别名](#toc28_4_1_)    \n",
    "- 29. [collections](#toc29_)    \n",
    "  - 29.1. [namedtuple（具名元组）](#toc29_1_)    \n",
    "  - 29.2. [deque（双端队列）](#toc29_2_)    \n",
    "  - 29.3. [ defaultdict（默认字典）](#toc29_3_)    \n",
    "  - 29.4. [ OrderedDict（有序字典）](#toc29_4_)    \n",
    "  - 29.5. [Counter（计数器）](#toc29_5_)    \n",
    "- 30. [multiprocessing](#toc30_)    \n",
    "  - 30.1. [map and map_async](#toc30_1_)    \n",
    "  - 30.2. [starmap and starmap_async](#toc30_2_)    \n",
    "    - 30.2.1. [starmap（同步阻塞）](#toc30_2_1_)    \n",
    "    - 30.2.2. [starmap_async（异步非阻塞）](#toc30_2_2_)    \n",
    "  - 30.3. [apply and apply_aysnc](#toc30_3_)    \n",
    "    - 30.3.1. [apply](#toc30_3_1_)    \n",
    "    - 30.3.2. [apply_async](#toc30_3_2_)    \n",
    "- 31. [itertools](#toc31_)    \n",
    "- 32. [pip打包](#toc32_)    \n",
    "  - 32.1. [README.md](#toc32_1_)    \n",
    "  - 32.2. [setup.py](#toc32_2_)    \n",
    "  - 32.3. [方式一：本地安装（开发模式）](#toc32_3_)    \n",
    "  - 32.4. [方式二：打包为分发文件，再安装](#toc32_4_)    \n",
    "  - 32.5. [上传到 PyPI（可选）](#toc32_5_)    \n",
    "  - 32.6. [维护与更新](#toc32_6_)    \n",
    "    - 32.6.1. [依赖更新](#toc32_6_1_)    \n",
    "    - 32.6.2. [卸载旧版](#toc32_6_2_)    \n",
    "- 33. [转格式](#toc33_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[概述](#toc0_)\n",
    "写这个笔记主要为了记录学习过程，包括知识的总结、归纳和反思等。作为一个非科班出生的生物人，仅凭着对人工智能的热爱开始了自学这条路。前路漫漫不敢想，也不曾觉得以后能端这碗饭。只是，羡慕网上像智慧君、李沐这样的人，能够从事如此炫酷的工作，能把自己的热爱开发成一生从事的职业。仔细想想如果自己不做点什么或是不为此努力点什么，就觉得坐立不安、难以入眠。同时深知，这个过程会是无比艰辛，在百无聊赖之际，记录学习的过程或许会是一种苦中作乐的方式。\n",
    "\n",
    "- d2l EN (及时更新): [https://d2l.ai/index.html](https://d2l.ai/index.html)\n",
    "\n",
    "- d2l ZH: [https://zh-v2.d2l.ai/](https://zh-v2.d2l.ai/)\n",
    "\n",
    "\n",
    "机器学习分类：\n",
    "- 监督学习\n",
    "- 半监督学习：少量样本\n",
    "- 自监督学习\n",
    "- 强化学习\n",
    "- 生成数据\n",
    "  - 对抗模型\n",
    "  - 扩散模型\n",
    "- 无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[环境配置](#toc0_)\n",
    "\n",
    "- PyTorch官方教程 [https://pytorch.org/](https://pytorch.org/)\n",
    "\n",
    "- PyTorch lightning官方教程 [https://lightning.ai/docs/pytorch/stable/](https://lightning.ai/docs/pytorch/stable/)\n",
    "\n",
    "- 尽量用conda配置环境，不要conda和pip混搭。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. <a id='toc2_1_'></a>[conda1](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environmental name \n",
    "name=\"pytorch1\"\n",
    "\n",
    "# Create environment and entry the environment\n",
    "conda create -n $name -y && conda activate $name\n",
    "\n",
    "# Install ipykernel and related packages via conda\n",
    "conda install ipykernel matplotlib pandas seaborn -y\n",
    "\n",
    "# Download and install CUDATOOLkit containing CUDA and nvcc and etc. via conda on NVIDIA channel\n",
    "## 方法一：\n",
    "# conda install nvidia::cuda-toolkit -y\n",
    "## 方法二（推荐）, with ncvv and etc.\n",
    "conda install nvidia/label/cuda-12.4.0::cuda -y -c nvidia/label/cuda-12.4.0\n",
    "\n",
    "# Install PyTorch\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y\n",
    "\n",
    "# Instll packages \n",
    "conda install esri::torch-geometric lightning deepspeed torchmetrics huggingface_hub -c conda-forge -y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[conda2](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"pytorch\"\n",
    "\n",
    "conda create -n $name -y && conda activate $name \n",
    "\n",
    "conda install -y \\\n",
    "    nvidia/label/cuda-12.4.0::cuda \\\n",
    "    pytorch::pytorch \\\n",
    "    pytorch::torchvision \\\n",
    "    pytorch::torchaudio \\\n",
    "    conda-forge::torchmetrics \\\n",
    "    conda-forge::deepspeed \\\n",
    "    conda-forge::mpi4py \\\n",
    "    conda-forge::pytorch-lightning \\\n",
    "    esri::torch-geometric \\\n",
    "    conda-forge::huggingface_hub \\\n",
    "    anaconda::ipykernel \\\n",
    "    conda-forge::matplotlib \\\n",
    "    anaconda::pandas \\\n",
    "    anaconda::seaborn \\\n",
    "    anaconda::numpy \\\n",
    "    anaconda::scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. <a id='toc2_3_'></a>[conda/uv](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash \n",
    "name=\"deeplearning\"\n",
    "\n",
    "# create the environment\n",
    "conda create -n $name -y python && conda activate $name\n",
    "\n",
    "# Install uv tool\n",
    "pip install uv \n",
    "\n",
    "# Install packages via uv\n",
    "## 1. Dev LLms\n",
    "# uv pip install \\\n",
    "#     ipykernel \\\n",
    "#     numpy pandas \\\n",
    "#     scipy statsmodels pingouin scikit-posthocs \\\n",
    "#     matplotlib seaborn statannotations \\\n",
    "#     scikit-learn shap \\\n",
    "#     d2l \\\n",
    "#     torch torchvision torchaudio deepspeed \\\n",
    "#     openai \\\n",
    "#     transformers[torch] datasets evaluate\n",
    "\n",
    "## 2. 可以进行安装，提高成功率。\n",
    "uv pip install ipykernel numpy pandas scipy statsmodels pingouin scikit-posthocs matplotlib seaborn statannotations scikit-learn torch torchvision torchaudio deepspeed transformers[torch] datasets evaluate openai\n",
    "uv pip install shap\n",
    "uv pip install d2l==0.16.4  # https://pypi.org/project/d2l/#history,不同Python版本尝试不同的d2l版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[utils](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[deepspore](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Yu Zhao\n",
      "Version: 0.1.2\n"
     ]
    }
   ],
   "source": [
    "import deepspore \n",
    "\n",
    "\n",
    "# Print the information\n",
    "print(f\"Author: {deepspore.__author__}\")\n",
    "print(f\"Version: {deepspore.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[save to utils.py](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存代码值utils.py模块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save \n",
    "import json\n",
    "\n",
    "\n",
    "def ipynb2(ipynb_file:str, label:str, output_file:str) -> None:\n",
    "    '''\n",
    "    Collect code blocks started with label, such as \"#@save\" in ipynb format file to one file named with outputfile.\n",
    "    Args:\n",
    "        ipynb_file: str\n",
    "        label: str, such as %%bash, #@save and etc.\n",
    "        output_file: str\n",
    "    \n",
    "    # Demo\n",
    "    >>> extract_save_blocks(ipynb_file= 'learn_PyTorch.ipynb', label= '#@save', output_file= 'utils/utils.py')\n",
    "    '''\n",
    "    \n",
    "    with open(ipynb_file, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    saved_code_blocks = []\n",
    "\n",
    "    for cell in notebook.get('cells', []):\n",
    "        if cell.get('cell_type') == 'code':\n",
    "            source_lines = cell.get('source', [])\n",
    "            if source_lines and source_lines[0].lstrip().startswith(label):\n",
    "                code_block = ''.join(source_lines)\n",
    "                saved_code_blocks.append(code_block)\n",
    "\n",
    "    if saved_code_blocks:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write(\"# This file is generated from saved notebook code blocks\\n\\n\")\n",
    "            f_out.write(\"\\n\\n\\n\".join(saved_code_blocks))\n",
    "        print(f\"Saved {len(saved_code_blocks)} block(s) to {output_file}\")\n",
    "    else:\n",
    "        print(f\"No {label} blocks found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ipynb2 in module deepspore.ipynb:\n",
      "\n",
      "ipynb2(ipynb_file: str, label: str, output_file: str) -> None\n",
      "    Collect code blocks started with label, such as \"#@save\" in ipynb format file to one file named with outputfile.\n",
      "    Args:\n",
      "        ipynb_file: str\n",
      "        label: str, such as %%bash, #@save and etc.\n",
      "        output_file: str\n",
      "\n",
      "    # Demo:\n",
      "    >>> extract_save_blocks(ipynb_file= 'learn_PyTorch.ipynb', label= '#@save', output_file= 'utils/utils.py')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.ipynb import ipynb2\n",
    "\n",
    "\n",
    "help(ipynb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12 block(s) to utils/utils.py\n"
     ]
    }
   ],
   "source": [
    "# 用法示例\n",
    "ipynb2(ipynb_file= 'learn_PyTorch.ipynb', label= '#@save', output_file= 'utils/utils.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[实验可重复性](#toc0_)\n",
    "整个代码框架中很多地方使用到随机数的，为了实验的可重复性需要固定随机种子；  \n",
    "另外，有研究表明GPU中的CUDA变成也有很多地方对实验结果的稳定性很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import random \n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed: int = 42)-> None:\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # GPU operation have separate seed\n",
    "    if torch.cuda.is_available():  \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "    # We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"Set seed {seed} for reproducibility.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_seed in module deepspore.training:\n",
      "\n",
      "set_seed(seed: int = 42) -> None\n",
      "    Function for setting the seed.\n",
      "    Args:\n",
      "        seed: int, default is 42.\n",
      "\n",
      "    Demo:\n",
      "    >>>set_seed(seed= 123)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import set_seed\n",
    "\n",
    "\n",
    "help(set_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 42 for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "# Call the function to set random seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[Metrics和Visualization](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. <a id='toc3_4_1_'></a>[Metrics tracker](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1.1. <a id='toc3_4_1_1_'></a>[v1](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        # 示例输出结构\n",
    "        history = {\n",
    "            'epoch': {\n",
    "                'train_loss_epoch': [0.5, 0.4, 0.3],          # 每个epoch的指标\n",
    "                'train_acc_epoch': [0.8, 0.85, 0.9],\n",
    "                'val_loss_epoch': [0.6, 0.5, 0.4],\n",
    "                'val_acc_epoch': [0.7, 0.75, 0.8]\n",
    "            },\n",
    "            'step': {\n",
    "                'train_loss_step': [0.55, 0.45, 0.35],        # 每个step的指标\n",
    "                'train_acc_step': [0.78, 0.83, 0.88],\n",
    "                'val_loss_step': [0.62, 0.52, 0.34],          # 验证阶段一般只在epoch结束时计算\n",
    "                'val_acc_step': [0.72, 0.77, 0.77],\n",
    "            },\n",
    "        }     \n",
    "        '''\n",
    "        self._metrics = {}                          # 存储指标计算函数\n",
    "        self._epoch_buffer = defaultdict(list)      # Epoch级别累积\n",
    "        self._step_buffer = defaultdict(list)       # Step级别累积\n",
    "        self._history = {\n",
    "            'epoch': defaultdict(list),             # 按阶段和指标名存储epoch指标\n",
    "            \"step\": defaultdict(list)               # 按阶段和指标名存储step指标\n",
    "        }\n",
    "        self.current_stage = 'train'                # 当前阶段标识\n",
    "\n",
    "    def add_metric(self, name, metric_fn):\n",
    "        \"\"\"注册指标（如损失、准确率）\"\"\"\n",
    "        self._metrics[name] = metric_fn\n",
    "\n",
    "    def set_stage(self, stage):\n",
    "        \"\"\"设置当前阶段（train/val/test）\"\"\"\n",
    "        self.current_stage = stage\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"更新缓冲区（需传入指标函数所需的参数），紧邻每个batch之后计算。\"\"\"\n",
    "        for name, fn in self._metrics.items():\n",
    "            value = fn(**kwargs)\n",
    "            self._epoch_buffer[name].append(value)  # 累积到epoch\n",
    "            self._step_buffer[name].append(value)   # 累积到step\n",
    "\n",
    "    def compute_epoch_metrics(self):\n",
    "        \"\"\"计算并返回当前阶段的Epoch平均指标\"\"\"\n",
    "        epoch_metrics = {}\n",
    "        for name, values in self._epoch_buffer.items():\n",
    "            avg_value = self._compute_avg(values)\n",
    "            epoch_metrics[name] = avg_value\n",
    "            self._history['epoch'][f\"{self.current_stage}_{name}_epoch\"].append(avg_value)\n",
    "        self._epoch_buffer.clear()  # 清空Epoch缓冲区\n",
    "        return epoch_metrics\n",
    "\n",
    "    def compute_step_metrics(self):\n",
    "        \"\"\"计算并返回当前阶段的Step平均指标（自动清空Step缓冲区）\"\"\"\n",
    "        step_metrics = {}\n",
    "        for name, values in self._step_buffer.items():\n",
    "            avg_value = self._compute_avg(values)\n",
    "            step_metrics[name] = avg_value\n",
    "            self._history['step'][f\"{self.current_stage}_{name}_step\"].append(avg_value)\n",
    "        self._step_buffer.clear()  # 清空Step缓冲区\n",
    "        return step_metrics\n",
    "\n",
    "    def _compute_avg(self, values):\n",
    "        \"\"\"通用平均值计算（支持标量和张量）\"\"\"\n",
    "        if not values:\n",
    "            return 0.0  # 避免空列表\n",
    "        if isinstance(values[0], (int, float)):\n",
    "            return sum(values) / len(values)\n",
    "        elif isinstance(values[0], torch.Tensor):\n",
    "            return torch.stack(values).mean(dim=0)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type: {type(values[0])}\")\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"获取所有历史记录（用于可视化）\"\"\"\n",
    "        return self._history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MetricTracker in module deepspore.training:\n",
      "\n",
      "class MetricTracker(builtins.object)\n",
      " |  Document metrics of training process.\n",
      " |\n",
      " |  Demo:\n",
      " |  >>>metric_tracker = MetricTracker()\n",
      " |  >>>metric_tracker.add_metric(name='acc', metric_fn=metric_fn)\n",
      " |  >>>metric_tracker.set_stage(stage='train')\n",
      " |  >>>metric_tracker.update()\n",
      " |  >>>metric_tracker.step_metrics()\n",
      " |  >>>metric_tracker.epoch_metrics()\n",
      " |  >>>metric_tracker.get_history()\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self)\n",
      " |      # 示例输出结构\n",
      " |      history = {\n",
      " |          'epoch': {\n",
      " |              'train_loss_epoch': [0.5, 0.4, 0.3],          # 每个epoch的指标\n",
      " |              'train_acc_epoch': [0.8, 0.85, 0.9],\n",
      " |              'val_loss_epoch': [0.6, 0.5, 0.4],\n",
      " |              'val_acc_epoch': [0.7, 0.75, 0.8]\n",
      " |          },\n",
      " |          'step': {\n",
      " |              'train_loss_step': [0.55, 0.45, 0.35],        # 每个step的指标\n",
      " |              'train_acc_step': [0.78, 0.83, 0.88],\n",
      " |              'val_loss_step': [0.62, 0.52, 0.34],          # 验证阶段一般只在epoch结束时计算\n",
      " |              'val_acc_step': [0.72, 0.77, 0.77],\n",
      " |          },\n",
      " |      }\n",
      " |\n",
      " |  add_metric(self, name, metric_fn)\n",
      " |      注册指标（如损失、准确率）\n",
      " |\n",
      " |  compute_epoch_metrics(self)\n",
      " |      计算并返回当前阶段的Epoch平均指标\n",
      " |\n",
      " |  compute_step_metrics(self)\n",
      " |      计算并返回当前阶段的Step平均指标（自动清空Step缓冲区）\n",
      " |\n",
      " |  get_history(self)\n",
      " |      获取所有历史记录（用于可视化）\n",
      " |\n",
      " |  set_stage(self, stage)\n",
      " |      设置当前阶段（train/val/test）\n",
      " |\n",
      " |  update(self, **kwargs)\n",
      " |      更新缓冲区（需传入指标函数所需的参数），紧邻每个batch之后计算。\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import MetricTracker\n",
    "\n",
    "\n",
    "help(MetricTracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. <a id='toc3_4_2_'></a>[可视化](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def set_plt_default():\n",
    "    plt.rcdefaults()\n",
    "    \n",
    "\n",
    "def set_plt_rcParams(**kswargs):\n",
    "    # 设置字体栈（优先级从高到低）\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = [\n",
    "        'Times New Roman',   # 英文优先使用\n",
    "        'SimSun',            # 中文宋体\n",
    "        # 'SimHei',            # 备用中文字体黑体\n",
    "        # 'Noto Sans CJK SC'   # 最后回退\n",
    "    ]\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "    plt.rcParams['pdf.fonttype'] = 42           # ai可编辑的字体格式\n",
    "    plt.rcParams['figure.figsize'] = (3, 3)     # figsize\n",
    "    plt.rcParams['savefig.format'] = \"svg\"      # svg格式\n",
    "    plt.rcParams['savefig.transparent'] = True  # 背景是否透明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_plt_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_plt_default in module deepspore.matplotlib_config:\n",
      "\n",
      "set_plt_default() -> None\n",
      "    Set default for plt.\n",
      "    Demo:\n",
      "    >>>set_plt_default()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.matplotlib_config import set_plt_default, set_plt_rcParams\n",
    "\n",
    "\n",
    "help(set_plt_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function set_plt_rcParams in module deepspore.matplotlib_config:\n",
      "\n",
      "set_plt_rcParams(figsize: tuple = (3, 3), format: str = 'svg', **kswargs) -> None\n",
      "    Set configure for plt.\n",
      "    Demo:\n",
      "    >>>set_plt_rcParams()\n",
      "    >>>set_plt_rcParams()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(set_plt_rcParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Visualization:\n",
    "    '''接受MetricTracker计算的_history，自动绘图。'''\n",
    "\n",
    "    def refresh_plot(self, history: defaultdict[list]):\n",
    "        '''再jupyter中持续刷新展示图片'''\n",
    "        plt.close()                                 # close figure （推荐）\n",
    "        fig = self._show(history)\n",
    "        display.display(fig)                        # 在jupyter中展示 （推荐）\n",
    "        display.clear_output(wait= True)             # 等待 （必须） \n",
    "\n",
    "    def _show(self, history: defaultdict[list]):\n",
    "        '''根据实验：train、val、test等，指标：loss、acc、f1等自动绘图'''\n",
    "        experiments, metrics = self._get_config(history)\n",
    "        fig, axess = plt.subplots(nrows= len(history.keys()), ncols= len(metrics))        \n",
    "        for i, strategy in enumerate(history.keys()):\n",
    "            for j, metric in enumerate(metrics):\n",
    "                for experiment in experiments:\n",
    "                    axess[i][j].plot(history[strategy][f\"{experiment}_{metric}_{strategy}\"], label= f\"{experiment}_{metric}\")\n",
    "                    axess[i][j].legend()\n",
    "                    axess[i][j].set_xlabel(strategy)\n",
    "                    axess[i][j].set_ylabel(metric)\n",
    "                    axess[i][j].set_title(f\"{metric} curve\")\n",
    "        fig.tight_layout()         \n",
    "        return fig           \n",
    "\n",
    "    def _get_config(self, history):\n",
    "        '''获得实验：train、val、test等，指标：loss、acc、f1等'''\n",
    "        experiments = set()\n",
    "        metrics = set()\n",
    "        for i in next(iter(history.values())).keys():           # 只取第一个值\n",
    "            experiment_name, metrics_name, _ = i.split(\"_\")\n",
    "            experiments.add(experiment_name)\n",
    "            metrics.add(metrics_name)\n",
    "        return experiments, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Visualization in module deepspore.training:\n",
      "\n",
      "class Visualization(builtins.object)\n",
      " |  接受MetricTracker计算的_history，自动绘图。\n",
      " |\n",
      " |  Demo:\n",
      " |  >>>visualization = Visualization()\n",
      " |  >>>visualization.refresh_plot(history= history)\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  refresh_plot(self, history: collections.defaultdict[list])\n",
      " |      再jupyter中持续刷新展示图片\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import Visualization \n",
    "\n",
    "\n",
    "help(Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 示例输出结构\n",
    "history = {\n",
    "    'epoch': {\n",
    "        'train_loss_epoch': [0.5, 0.4, 0.3],          # 每个epoch的指标\n",
    "        'train_acc_epoch': [0.8, 0.85, 0.9],\n",
    "        'val_loss_epoch': [0.6, 0.5, 0.4],\n",
    "        'val_acc_epoch': [0.7, 0.75, 0.8]\n",
    "    },\n",
    "    'step': {\n",
    "        'train_loss_step': [0.55, 0.45, 0.35],        # 每个step的指标\n",
    "        'train_acc_step': [0.78, 0.83, 0.88],\n",
    "        'val_loss_step': [0.62, 0.52, 0.34],          # 验证阶段一般只在epoch结束时计算\n",
    "        'val_acc_step': [0.72, 0.77, 0.77],\n",
    "    },\n",
    "}\n",
    "\n",
    "visualization = Visualization()\n",
    "visualization.refresh_plot(history= history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. <a id='toc3_5_'></a>[GPU](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the device that will be used throughout this notebook\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"如果GPU可用，则返回GPU设备，否则返回CPU设备\"\"\"\n",
    "    if torch.cuda.is_avaliable():\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    print('Only CPU.')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的GPU设备\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function try_gpu in module deepspore.training:\n",
      "\n",
      "try_gpu(i: int = 0)\n",
      "    Try get gpu.\n",
      "    >>>try_gpu(i=0)\n",
      "\n",
      "Help on function try_all_gpus in module deepspore.training:\n",
      "\n",
      "try_all_gpus()\n",
      "    Try all GPUs.\n",
      "    >>>try_all_gpus()\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepspore.training import try_gpu, try_all_gpus \n",
    "\n",
    "\n",
    "help(try_gpu), help(try_all_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from d2l import torch as d2l\n",
    "import time \n",
    "from IPython import display \n",
    "\n",
    "\n",
    "def use_svg_display():\n",
    "    '''设置matplotlib的输出格式为svg'''\n",
    "    display.set_matplotlib_formats('svg')\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 如果idx是slice，则返回self.data[idx]\n",
    "        if isinstance(idx, slice):\n",
    "            return self.data[idx]\n",
    "        else: \n",
    "            return self.data[idx]  \n",
    "\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置 Matplotlib 坐标轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "       \n",
    "\n",
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        # d2l.use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes,]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"\n",
    "    计算正确预测的数量\n",
    "    Args:\n",
    "        y_hat: 预测的标签\n",
    "        y: 真实的标签\n",
    "    Returns:\n",
    "        float: 正确预测的数量\n",
    "    \"\"\"\n",
    "    # 如果 y_hat 的维度大于1（即多分类情况），并且第二维的大小大于1，表示每个样本有多个类别的预测分数。\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        # 这将 y_hat 转换为形状为 (batch_size,) 的张量，其中每个元素表示预测的类别标签。\n",
    "        y_hat = d2l.argmax(y_hat, axis=1)\n",
    "    # 将 y_hat 转换为与 y 相同的数据类型，以确保类型匹配。\n",
    "    # 使用 == 运算符逐元素比较预测值与真实标签，生成一个布尔张量 cmp，其中每个元素为 True 表示预测正确，False 表示预测错误。\n",
    "    cmp = d2l.astype(y_hat, y.dtype) == y\n",
    "    # 将布尔张量 cmp 转换为与 y 相同的数据类型（通常是整数类型），其中 True 转换为 1，False 转换为 0。\n",
    "    # 使用 d2l.reduce_sum 对转换后的张量进行求和，得到正确预测的总数。\n",
    "    # 将结果转换为浮点数并返回。\n",
    "    return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))\n",
    "\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    \"\"\"\n",
    "    计算模型在数据集上的准确率\n",
    "    Args:\n",
    "        net: 模型\n",
    "        data_iter: 数据集\n",
    "    Returns:\n",
    "        float: 准确率\n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), d2l.size(y))\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. <a id='toc3_6_'></a>[Timer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import time \n",
    "\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Timer in module deepspore.training:\n",
      "\n",
      "class Timer(builtins.object)\n",
      " |  Record multiple running times.\n",
      " |\n",
      " |  Demo:\n",
      " |  >>>timer = Timer()\n",
      " |  >>>timer.start()\n",
      " |  >>>timer.stop()\n",
      " |  >>>timer.sum()\n",
      " |  >>>timer.avg()\n",
      " |  >>>timer.cumsum()\n",
      " |  >>>timer.to_date(seconds= timer.sum())\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  avg(self) -> float\n",
      " |      Return the average time.\n",
      " |\n",
      " |  cumsum(self) -> list\n",
      " |      Return the accumulated time.\n",
      " |\n",
      " |  start(self)\n",
      " |      Start the timer.\n",
      " |\n",
      " |  stop(self)\n",
      " |      Stop the timer and record the time in a list.\n",
      " |\n",
      " |  sum(self) -> float\n",
      " |      Return the sum of time.\n",
      " |\n",
      " |  to_date(self, seconds) -> None\n",
      " |      Translate seconds to date format.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import Timer \n",
    "\n",
    "\n",
    "help(Timer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1. <a id='toc3_6_1_'></a>[CPU计时器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import time\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    Record multiple running times.\n",
    "    \n",
    "    Demo:\n",
    "    >>>timer = Timer()\n",
    "    >>>timer.start()\n",
    "    >>>timer.stop()\n",
    "    >>>timer.sum()\n",
    "    >>>timer.avg()\n",
    "    >>>timer.cumsum()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self) -> float:\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self) -> float:\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self) -> list:\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "    \n",
    "    def to_date(self, seconds) -> None:\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        print('='*20, '\\n', f\"Total：\\n {days} d \\n {hours} h \\n {minutes} m \\n {remaining_seconds} s\")\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Timer in module deepspore.training:\n",
      "\n",
      "class Timer(builtins.object)\n",
      " |  Record multiple running times.\n",
      " |\n",
      " |  Demo:\n",
      " |  >>>timer = Timer()\n",
      " |  >>>timer.start()\n",
      " |  >>>timer.stop()\n",
      " |  >>>timer.sum()\n",
      " |  >>>timer.avg()\n",
      " |  >>>timer.cumsum()\n",
      " |  >>>timer.to_date(seconds= timer.sum())\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  avg(self) -> float\n",
      " |      Return the average time.\n",
      " |\n",
      " |  cumsum(self) -> list\n",
      " |      Return the accumulated time.\n",
      " |\n",
      " |  start(self)\n",
      " |      Start the timer.\n",
      " |\n",
      " |  stop(self)\n",
      " |      Stop the timer and record the time in a list.\n",
      " |\n",
      " |  sum(self) -> float\n",
      " |      Return the sum of time.\n",
      " |\n",
      " |  to_date(self, seconds) -> None\n",
      " |      Translate seconds to date format.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import Timer \n",
    "\n",
    "\n",
    "help(Timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== \n",
      " Total：\n",
      " 0.0 d \n",
      " 0.0 h \n",
      " 0.0 m \n",
      " 4.000745534896851 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "timer = Timer()\n",
    "time.sleep(1)\n",
    "timer.stop()\n",
    "time.sleep(2)\n",
    "timer.stop()\n",
    "# timer.sum()\n",
    "timer.to_date(seconds= timer.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. <a id='toc3_6_2_'></a>[gpu计时器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "GPU time: 0.00236s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "计算GPU的时间是不同于计算CPU的的时间的\n",
    "'''\n",
    "\n",
    "import torch \n",
    "\n",
    "\n",
    "class gpuTimer():\n",
    "    def __init__(self):\n",
    "        # CUDA is asynchronous, so we need to use different timing functions\n",
    "        self.start = torch.cuda.Event(enable_timing=True)\n",
    "        self.end = torch.cuda.Event(enable_timing=True)\n",
    "        self.start.record()\n",
    "\n",
    "    def __call__(self):\n",
    "        self.end.record()\n",
    "        torch.cuda.synchronize()  # Waits for everything to finish running on the GPU\n",
    "        print(\"⚡\"*20, f\"\\nGPU time: {0.001 * self.start.elapsed_time(self.end):6.5f}s\")  # Milliseconds to seconds\n",
    "\n",
    "\n",
    "# Demo for Timer on GPU devices\n",
    "timer_on_gpu = gpuTimer()\n",
    "a = torch.arange(45).reshape(3, 3, 5)\n",
    "b = torch.arange(45).reshape(3, 5, 3)\n",
    "c = torch.bmm(a, b)\n",
    "timer_on_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. <a id='toc3_7_'></a>[Callback](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "from abc import ABC \n",
    "\n",
    "\n",
    "class Callback(ABC):\n",
    "    '''callback template'''\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def on_step_begin(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def on_step_end(self, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoCallback(Callback):\n",
    "    '''Call function'''\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        print(\"Runing on_train_begin ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Callback in module deepspore.training:\n",
      "\n",
      "class Callback(builtins.object)\n",
      " |  callback template\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  on_epoch_begin(self, **kwargs)\n",
      " |\n",
      " |  on_epoch_end(self, **kwargs)\n",
      " |\n",
      " |  on_step_begin(self, **kwargs)\n",
      " |\n",
      " |  on_step_end(self, **kwargs)\n",
      " |\n",
      " |  on_train_begin(self, **kwargs)\n",
      " |\n",
      " |  on_train_end(self, **kwargs)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import Callback \n",
    "\n",
    "\n",
    "help(Callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. <a id='toc3_8_'></a>[Trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import torch \n",
    "from tqdm import tqdm \n",
    "import pickle \n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            device: str = \"auto\",\n",
    "            train_dataloader: torch.utils.data.DataLoader = None, \n",
    "            val_dataloader: torch.utils.data.DataLoader = None, \n",
    "            model: torch.nn.Module = None, \n",
    "            loss_fn:torch.nn.modules.loss = None, \n",
    "            optimizer: torch.optim.Optimizer = None, \n",
    "            is_tqdm: bool = True, \n",
    "            callbacks: list = [],\n",
    "    ):\n",
    "        # basic sets\n",
    "        self.device = self._get_device(device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.model = self._get_model(model)\n",
    "        self.loss_fn = loss_fn \n",
    "        self.optimizer = optimizer\n",
    "        self.is_tqdm = is_tqdm\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "        # set metrics_tracker \n",
    "        self.metrics_tracker = MetricTracker()\n",
    "        self.metrics_tracker.add_metric('loss', lambda **kw: kw['loss'])  # 直接从kwargs获取loss\n",
    "        self.metrics_tracker.add_metric('acc', lambda **kw: (kw['y_hat'].argmax(1) == kw['y']).float().mean().item())\n",
    "\n",
    "        # visualization\n",
    "        self.visualization = Visualization()\n",
    "\n",
    "    def _get_device(self, device: str) -> torch.device:\n",
    "        '''CPU or GPUs.'''\n",
    "        if device == \"auto\":\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            device = torch.device(device)\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Runing on {device} ...\")\n",
    "        print(\"=\" * 100)\n",
    "        return device\n",
    "    \n",
    "    def _get_model(self, model) -> torch.nn.Module:\n",
    "        '''Move the mode to device.'''\n",
    "        model = torch.nn.DataParallel(model).to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def _disable_visualization(self) -> bool:\n",
    "        '''Weather show tqdm.'''\n",
    "        if self.is_tqdm:\n",
    "            return False \n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def _call_callbacks(self, method_name: str, **kwargs):\n",
    "        '''Run the method of callback from callback dict with default order.'''\n",
    "        for callback in self.callbacks:\n",
    "            if hasattr(callback, method_name):\n",
    "                method = getattr(callback, method_name, f\"{method_name} is not exist!\")\n",
    "                return method(**kwargs)\n",
    "        \n",
    "    def train(self, epochs: int, **kwargs):\n",
    "        '''Main loop.'''\n",
    "        self._call_callbacks(method_name= \"on_train_begin\", **kwargs)\n",
    "\n",
    "        with tqdm(range(epochs), desc= \"Training epoch\", unit= \"epoch\", disable= self._disable_visualization()) as pbar:\n",
    "            for epoch in pbar:\n",
    "                # train\n",
    "                self._call_callbacks(method_name= \"on_epoch_begin\", **kwargs)\n",
    "                train_logs = self._train_step()\n",
    "                train_logs = {\"train_\"+name: value for name, value in train_logs.items()}\n",
    "                self._call_callbacks(method_name= \"on_epoch_end\", **kwargs)\n",
    "\n",
    "                # val\n",
    "                val_logs = self._validate_step()\n",
    "                val_logs = {\"val_\"+name: value for name, value in val_logs.items()}    \n",
    "\n",
    "                # update show progress bar or visualization\n",
    "                pbar.set_postfix({**train_logs, **val_logs})\n",
    "                if self._disable_visualization():\n",
    "                    self.visualization.refresh_plot(history= self.metrics_tracker.get_history())\n",
    "\n",
    "        self._call_callbacks(method_name= \"on_train_end\", **kwargs)\n",
    "\n",
    "    def _train_step(self, **kwargs) -> dict:\n",
    "        '''On train step.'''\n",
    "        self.model.train() \n",
    "        self.metrics_tracker.set_stage(\"train\") ## for train\n",
    "\n",
    "        for X, y in self.train_dataloader:\n",
    "            self._call_callbacks(method_name= \"on_step_begin\", **kwargs)\n",
    "\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.model(X)\n",
    "            loss = self.loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.metrics_tracker.update(y_hat= y_hat, y= y, loss= loss.item()) ## update for train\n",
    "            self.metrics_tracker.compute_step_metrics() ## on step level with train\n",
    "            self._call_callbacks(method_name= \"on_step_end\", **kwargs)\n",
    "\n",
    "        train_metrics = self.metrics_tracker.compute_epoch_metrics() ## on epoch level with train\n",
    "        return train_metrics\n",
    "\n",
    "    def _validate_step(self) -> dict:\n",
    "        '''On validate step.'''\n",
    "        self.model.eval()\n",
    "        self.metrics_tracker.set_stage(\"val\") ## for val\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.val_dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                y_hat = self.model(X)\n",
    "                loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "                self.metrics_tracker.update(y_hat= y_hat, y= y, loss= loss.item()) ## update for val\n",
    "                self.metrics_tracker.compute_step_metrics() ## on step level with val\n",
    "\n",
    "            val_metrics = self.metrics_tracker.compute_epoch_metrics() ## on epoch level with val\n",
    "        return val_metrics\n",
    "    \n",
    "    def save_metrics(self, file_path: str):\n",
    "        '''Save the history with pickle format.'''\n",
    "        history = self.metrics_tracker.get_history()\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "\n",
    "    def save_checkpoint(self, file_path: str):\n",
    "        '''Save checkpoint.'''\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, file_path)\n",
    "\n",
    "    def load_checkpoint(self, file_path):\n",
    "        '''Load checkpoint.'''\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.model.load_state_dict(state_dict= checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.metrics_tracker._history\n",
    "# trainer.metrics_tracker.get_history()\n",
    "# trainer.save_metrics(file_path= \"./cache/metrics_tracker_history.pickle\")\n",
    "# trainer.save_checkpoint(file_path= './cache/checkpoint.pt')\n",
    "# trainer.load_checkpoint(file_path= './cache/checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module deepspore.training:\n",
      "\n",
      "class Trainer(builtins.object)\n",
      " |  Trainer(\n",
      " |      device='auto',\n",
      " |      train_dataloader=None,\n",
      " |      val_dataloader=None,\n",
      " |      model=None,\n",
      " |      loss_fn=None,\n",
      " |      optimizer=None,\n",
      " |      is_tqdm=True,\n",
      " |      callbacks: list = []\n",
      " |  )\n",
      " |\n",
      " |  Trainer.\n",
      " |\n",
      " |  Demo:\n",
      " |  >>>trainer = Trainer()\n",
      " |  >>>\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      device='auto',\n",
      " |      train_dataloader=None,\n",
      " |      val_dataloader=None,\n",
      " |      model=None,\n",
      " |      loss_fn=None,\n",
      " |      optimizer=None,\n",
      " |      is_tqdm=True,\n",
      " |      callbacks: list = []\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  load_checkpoint(self, file_path)\n",
      " |      Load checkpoint.\n",
      " |\n",
      " |  save_checkpoint(self, file_path)\n",
      " |      Save checkpoint.\n",
      " |\n",
      " |  save_metrics(self, file_path)\n",
      " |      Save the history with pickle format.\n",
      " |\n",
      " |  train(self, epochs: int = 2, **kwargs)\n",
      " |      Main loop.\n",
      " |      >>>train(epochs= 30)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import Trainer \n",
    "\n",
    "\n",
    "help(Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer2 in module deepspore.training:\n",
      "\n",
      "class Trainer2(Trainer)\n",
      " |  From Trainer, train visualization per step, valid visualization per epoch.\n",
      " |  Demo:\n",
      " |  >>>trainer = Trainer2()\n",
      " |  >>>trainer.train(epochs=3)\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Trainer2\n",
      " |      Trainer\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  train(self, epochs: int = 2, **kwargs)\n",
      " |      Main loop.\n",
      " |      >>>train(epochs= 30)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Trainer:\n",
      " |\n",
      " |  load_checkpoint(self, file_path)\n",
      " |      Load checkpoint.\n",
      " |\n",
      " |  save_checkpoint(self, file_path)\n",
      " |      Save checkpoint.\n",
      " |\n",
      " |  save_metrics(self, file_path)\n",
      " |      Save the history with pickle format.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Trainer:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import Trainer2\n",
    "\n",
    "# Trainer2?\n",
    "help(Trainer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Runing on cuda ...\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepspore.training.Trainer2 at 0x7f323978cad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. <a id='toc3_9_'></a>[ParametersSize](#toc0_)\n",
    "PyTorch 在进行深度学习训练的时候，有 4 大部分的显存开销：\n",
    "  - `模型参数(parameters)` ；\n",
    "  - `模型参数的梯度(gradients)` ；\n",
    "  - `中间激活值(intermediate activations) 或者叫中间结果(intermediate results)`；\n",
    "  - `优化器状态(optimizer states)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x) # 编码\n",
    "        x = self.decoder(x) # 解码\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 32\n",
    "model = Model(d_model=value*64, nhead=value, dim_feedforward=1024, dropout=0.1, batch_first=True, num_layers=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量: 1880.686592M\n",
      "可训练参数数量: 1880.686592M\n",
      "模型大小: 7174.25 MB\n"
     ]
    }
   ],
   "source": [
    "# 计算模型的总参数数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "# 计算可训练的参数数量\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"总参数数量: {total_params / 1000000}M\")\n",
    "print(f\"可训练参数数量: {trainable_params / 1000000}M\")\n",
    "\n",
    "# 计算模型大小（以MB为单位）\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "\n",
    "print(f\"模型大小: {size_all_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save \n",
    "import torch \n",
    "\n",
    "\n",
    "class GetModelSize:\n",
    "    '''\n",
    "    Calculate the parameter numbers and sizes of model.\n",
    "\n",
    "    Demo:\n",
    "    >>>get_model_size = GetModelSize()\n",
    "    >>>get_model_size.parameter_numbers(model= model)\n",
    "    >>>get_model_size.parameter_sizes(model= model)\n",
    "    '''\n",
    "\n",
    "    def parameter_numbers(self, model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def parameter_sizes(self, model, dtype=torch.float32):\n",
    "        bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "        total_params = self.parameter_numbers(model)\n",
    "        total_size = total_params * bytes_per_param\n",
    "        print(f'{total_params/1000000} M parameters')\n",
    "        print(f'{total_size/(1024*1024):.2f} MB')\n",
    "        # return total_params, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880.686592 M parameters\n",
      "7174.25 MB\n"
     ]
    }
   ],
   "source": [
    "from deepspore.training import GetModelSize\n",
    "\n",
    "\n",
    "get_model_size = GetModelSize()\n",
    "get_model_size.parameter_numbers(model= model)\n",
    "get_model_size.parameter_sizes(model= model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. <a id='toc3_10_'></a>[numpy和pytorch计算速度比较](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Datas on cpu with arrary\n",
    "a = np.random.rand(1000, 1000)\n",
    "b = np.random.rand(1000, 1000)\n",
    "\n",
    "# Datas on cpu with tensor\n",
    "at = torch.Tensor(a).to('cpu')\n",
    "bt = torch.Tensor(b).to('cpu')\n",
    "\n",
    "# Datas on gpu with tensor\n",
    "at_gpu = torch.Tensor(a).to('cuda:0')\n",
    "bt_gpu = torch.Tensor(b).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 ms ± 49.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a + b   # On cpu via numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.1 μs ± 737 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit at + bt # On cpu via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.010480832099914551 s\n"
     ]
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "stop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "at_gpu + bt_gpu\n",
    "stop.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "print(f'Time: {0.001 * start.elapsed_time(stop)} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[安装GPU驱动](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以CentOS8安装NVIDIA Tesla A100为例，下载CUDA Toolkit和CuDNN，需要注意cudnn的版本必须与cuda的版本相匹配：\n",
    "\n",
    "  1. NVIDIA Driver：NVIDIA驱动是NVIDIA显卡的`驱动程序`，它是CUDA和CuDNN的前提条件。显卡驱动下载地址：https://www.nvidia.com/Download/index.aspx。\n",
    "\n",
    "  2. CUDA Toolkit：CUDA Toolkit是一个`开发工具包`，其中包含了CUDA编译器、IDE、调试器等工具，以及CUDA程序所需的各种库文件和头文件，每个版本的CUDA Toolkit 都对应一个最低版本的显卡驱动版本（CUDA Driver）。\n",
    "\n",
    "  3. NVCC：其实就是`CUDA的编译器`,可以从CUDA Toolkit的/bin目录中获取,类似于gcc就是c语言的编译器。\n",
    "\n",
    "  4. CUDA Deep Neural Network (cuDNN)：CuDNN是NVIDIA提供的一个`深度神经网络加速库`，它包含了一系列高性能的基本函数和算法，用于加速深度学习任务的计算；CuDNN需要与CUDA Toolkit一起使用，以优化深度学习任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. <a id='toc4_1_'></a>[安装策略](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方式一 `全局驱动，各自cuda`：`只安装NVIDIA Tesla A100的driver，每个用户自己利用conda安装CUDA Toolkit、cuDNN和对应的Pytorch版本（推荐），但是得注意选择兼容型号。（推荐）`\n",
    "\n",
    "- 方式二 `全局驱动，全局cuda`：`安装Driver、CUDA Toolkit (全局安装)`\n",
    "    \n",
    "- 方式三 `docker`：`安装Driver、NVIDIA docker (docker虚拟容器)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. <a id='toc4_2_'></a>[首先确认内核版本和发行版本，再确认显卡型号](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看linux内核版本、架构\n",
      "Linux 135.91.205.202.cau.edu.cn 4.18.0-348.7.1.el8_5.x86_64 #1 SMP Wed Dec 22 13:25:12 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\n",
      "发行版本\n",
      "CentOS Linux release 8.1.1911 (Core) \n",
      "显卡型号 （硬件层面）\n",
      "2f:00.0 3D controller: NVIDIA Corporation Device 20b0 (rev a1)\n",
      "86:00.0 3D controller: NVIDIA Corporation Device 20b0 (rev a1)\n",
      "验证系统是否安装gcc编译器\n",
      "gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4)\n",
      "Copyright (C) 2018 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "echo 查看linux内核版本、架构\n",
    "uname -a\n",
    "# Linux 135.91.205.202.cau.edu.cn 4.18.0-147.el8.x86_64 #1 SMP Wed Dec 4 21:51:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\n",
    "# x86_64\n",
    "\n",
    "echo 发行版本\n",
    "cat /etc/redhat-release\n",
    "# CentOS Linux release 8.1.1911 (Core)\n",
    "# CentOS\n",
    "\n",
    "echo 显卡型号 （硬件层面）\n",
    "lspci | grep -i nvidia\n",
    "# 04:00.0 3D controller: NVIDIA Corporation GK208M [GeForce GT 730M] (rev a1)\n",
    "\n",
    "echo 验证系统是否安装gcc编译器\n",
    "gcc --version\n",
    "\n",
    "# sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. <a id='toc4_3_'></a>[安装驱动-CUDA Driver](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. <a id='toc4_3_1_'></a>[下载CUDA Driver](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 从NVIDIA官网下辖\n",
    "# https://www.nvidia.cn/Download/index.aspx?lang=cn\n",
    "\n",
    "# 2. 通过dnf search nvidia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. <a id='toc4_3_2_'></a>[禁用nouveau](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 貌似在centos8上默认就禁用了，我没改，直接查看了lsmod | grep nouveau命令，发现没有输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. <a id='toc4_3_3_'></a>[安装CUDA Driver](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'sudo' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x *.run\n",
    "# !sudo ./*.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4. <a id='toc4_3_4_'></a>[查看显卡是否安装成功](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 18 13:05:45 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:2F:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             35W /  400W |   37523MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          Off |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             36W /  400W |   37523MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    953178      C   ...osy/miniconda3/envs/vllm/bin/python      37514MiB |\n",
      "|    1   N/A  N/A    953179      C   ...osy/miniconda3/envs/vllm/bin/python      37514MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5. <a id='toc4_3_5_'></a>[查看nvcc](#toc0_)\n",
    "```shell\n",
    "nvcc只是CUDA Toolkit中的一个软件。此时，只是安装了驱动程序，没有安装CUDA Toolkit，所以无法查看nvcc。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Feb_27_16:19:38_PST_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.99\n",
      "Build cuda_12.4.r12.4/compiler.33961263_0\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "# source /bmp/backup/zhaosy/miniconda3/etc/profile.d/conda.sh\n",
    "# conda activate pytorch \n",
    "nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. <a id='toc4_4_'></a>[全局驱动和全局CUDA Toolkit和CuDNN](#toc0_)\n",
    "```shell\n",
    "不推荐一开始作为root为Linux全局配置CUDA Toolkit，每个用户和软件使用的CUDA Toolkit版本可能不一样。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. <a id='toc4_4_1_'></a>[下载对应的CUDA Toolkit版本](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc -V # 查看是否安装好CUDA Toolkit\n",
    "\n",
    "wget https://us.download.nvidia.cn/tesla/535.129.03/NVIDIA-Linux-x86_64-535.129.03.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. <a id='toc4_4_2_'></a>[安装CUDA Toolkit](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 卸载之前安装的cuda\n",
    "sudo dnf remove nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "chmod +x NVIDIA-Linux-x86_64-535.129.03.run\n",
    "sudo sh NVIDIA-Linux-x86_64-535.129.03.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. <a id='toc4_4_3_'></a>[下载对应的CuDNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://link.zhihu.com/?target=https%3A//developer.nvidia.com/rdp/cudnn-download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4. <a id='toc4_4_4_'></a>[安装CuDNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. <a id='toc4_5_'></a>[安装对应版本的Pytorch](#toc0_)\n",
    "\n",
    "在Pytorch的官网进行查询，按照条件检索符合要求的软件版本，最主要的是对应的cuda版本号。\n",
    "\n",
    "[https://pytorch.org/](https://pytorch.org/)\n",
    "\n",
    "![PyTorch](./Pytorch_Pictures/Install_PyTorch/PyTorch_website.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# https://pytorch.org/\n",
    "# CUDA 12.1\n",
    "conda create -n pytorch-gpu -y && conda activate pytorch-gpu \n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia # CUDA 12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. <a id='toc4_6_'></a>[全局驱动个人CUDA Toolkit](#toc0_)\n",
    "\n",
    "- 全局A100驱动\n",
    "\n",
    "- conda下cuda toolkit、pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviroment and actiate environment\n",
    "conda create -n pytorch-gpu && conda activate pytorch-gpu \n",
    "\n",
    "# Install cudatoolkit via conda containing nvcc etc.\n",
    "# Instead of :\n",
    "# conda install cudatoolkit\n",
    "# or \n",
    "# conda install cuda-nvcc\n",
    "conda install cuda -y  -c nvidia/label/cuda-12.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or:\n",
    "\n",
    "```bash\n",
    "chmod +x cuda_12.4.0_550.54.14_linux.run\n",
    "\n",
    "# Install CudaToolkit as  Non-root user (the toolkitpath is not /usr/...)\n",
    "./cuda_12.4.0_550.54.14_linux.run  --toolkit --toolkitpath=$HOME/ProgramFiles/cuda-12.4 --defaultroot=$HOME/ProgramFiles/cuda-12.4\n",
    "\n",
    "\n",
    "\n",
    "# 方便切换版本\n",
    "ln -s $HOME/ProgramFiles/cuda-12.4 $HOME/ProgramFiles/cuda\n",
    "\n",
    "# 设置 CUDA_HOME\n",
    "export CUDA_HOME=$HOME/ProgramFiles/cuda\n",
    "\n",
    "# 添加到 PATH\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "\n",
    "# 添加库路径\n",
    "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. <a id='toc4_7_'></a>[GPU测试程序](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1. <a id='toc4_7_1_'></a>[在GPU上验证Trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from deepspore.training import Trainer, Callback, MetricTracker, Visualization\n",
    "\n",
    "from deepspore.training import set_seed\n",
    "set_seed(42)  # 设置随机种子以确保结果可复现\n",
    "\n",
    "# 设置matplotlib的默认配置\n",
    "from deepspore.matplotlib_config import set_plt_rcParams\n",
    "set_plt_rcParams(figsize=(6, 6))\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "dbs = './data/'\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), ])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root= dbs, train= True, download= True, transform= transforms)\n",
    "test_dataset = torchvision.datasets.MNIST(root= dbs, train= False, download= True, transform= transforms)\n",
    "## 迭代型数据方式\n",
    "train_iter = data.DataLoader(dataset= train_dataset, batch_size= 128, shuffle= True)    # train需要shuffle\n",
    "test_iter = data.DataLoader(dataset= test_dataset, batch_size= 128)                     # test不需要shuffle训练\n",
    "\n",
    "\n",
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        return self.network(inputs)\n",
    "\n",
    "\n",
    "# Trainer\n",
    "## callback\n",
    "class PrintCallback(Callback):\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        print(\"Runing on train begin ...\")\n",
    "\n",
    "\n",
    "# lr 0.01 -> 0.5\n",
    "net = Net()  \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params= net.parameters(), lr=0.5)   \n",
    "\n",
    "trainer = Trainer(\n",
    "    device= 'auto', \n",
    "    train_dataloader= train_iter, \n",
    "    val_dataloader= test_iter, \n",
    "    model= net, \n",
    "    loss_fn= loss_fn, \n",
    "    optimizer= opt, \n",
    "    is_tqdm= False, \n",
    "    callbacks= [PrintCallback(), ]\n",
    ")\n",
    "\n",
    "# trainer.train(epochs= 30, steps=None)\n",
    "trainer.train(epochs= 30, steps=30)\n",
    "\n",
    "# trainer.metrics_tracker._history\n",
    "# trainer.metrics_tracker.get_history()\n",
    "# trainer.save_metrics(file_path= \"./cache/metrics_tracker_history.pickle\")\n",
    "# trainer.save_checkpoint(file_path= './cache/checkpoint.pt')\n",
    "# trainer.load_checkpoint(file_path= './cache/checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2. <a id='toc4_7_2_'></a>[GPU burn压力测试](#toc0_)\n",
    "```shell\n",
    "李沐在装机配置后，进行GPU压力测试所用的程序为GPU_burn（可从github上下载）\n",
    "```\n",
    "\n",
    "- gpu_burn: \n",
    "  - github地址：`git clone https://github.com/wilicc/gpu-burn.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# git clone\n",
    "# git clone https://github.com/wilicc/gpu-burn.git\n",
    "\n",
    "cd gpu-burn\n",
    "\n",
    "# make \n",
    "make\n",
    "\n",
    "# 或\n",
    "# make CUDAPATH=~/minicnoda3/pytorch-gpu/\n",
    "\n",
    "# help\n",
    "gpu_burn --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# 2h * 60min * 60s = 7200s with tensor core (avaliable)\n",
    "gpu_burn -tc $(( 3 * 24 * 60 * 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[Pytorch模块介绍](#toc0_)\n",
    "## 5.1. <a id='toc5_1_'></a>[导入模块](#toc0_)\n",
    "```python\n",
    "torchvision\n",
    "  models\n",
    "  datasets\n",
    "  transforms\n",
    "  utils\n",
    "\n",
    "  \n",
    "torch\n",
    "  utils\n",
    "    data            # 数据加载相关\n",
    "      TensorDataset\n",
    "      Dataset\n",
    "      DataLoader\n",
    "  nn\n",
    "    functional\n",
    "    Sequential\n",
    "    DataParallel\n",
    "    Linear\n",
    "    Softmax\n",
    "  optim\n",
    "    SGD\n",
    "    Adam\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version:  2.6.0+cu124\n",
      "torchvision version: 0.21.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# 现成的数据库\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "\n",
    "# 数据加载\n",
    "from torch.utils import data                                             # from torch.utils import data\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader          # data.Dataset, data.TensorDataset, data.DataLoader\n",
    "\n",
    "# 神经网络结构\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "# import torch.nn.DataParallel\n",
    "from torch.nn import DataParallel\n",
    "from torch import distributed as dist\n",
    "\n",
    "# 优化器\n",
    "from torch import optim \n",
    "\n",
    "print('pytorch version: ', torch.__version__)\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[数据封装和加载](#toc0_)\n",
    "\n",
    "PyTorch为我们提供的`Dataset`和`DataLoader`类分别负责可被Pytorhc使用的数据集的`创建`以及向训练`传递数据`的任务。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. <a id='toc6_1_'></a>[torchvison.datasets获得Dataset](#toc0_)\n",
    "\n",
    "* `tochvision`主要处理图像数据，包含一些常用的数据集、模型、转换函数等。  torchvision独立于PyTorch，需要专门安装。\n",
    "\n",
    "  * torchvision.`models`: 提供深度学习中各种经典的网络结构、预训练好的模型，如：Alex-Net、VGG、ResNet、Inception等。\n",
    "\n",
    "  * torchvision.`datasets`：提供常用的数据集，设计上继承 torch.utils.data.Dataset，主要包括：MNIST、CIFAR10/100、ImageNet、COCO等。\n",
    "\n",
    "  * torchvision.`transforms`：提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。\n",
    "  \n",
    "  * torchvision.`utils`：工具类，如保存张量作为图像到磁盘，给一个小批量创建一个图像网格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:04<00:00, 6.19MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 1.14MB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:05<00:00, 790kB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 19.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.FashionMNIST,\n",
       " torchvision.datasets.mnist.FashionMNIST)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "# 数据集下载路径\n",
    "dbs = './data/'\n",
    "\n",
    "trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),                  # PIL转换为tensor格式\n",
    "    torchvision.transforms.Normalize((0.5,), (1.0,))    # 标准化\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=trans, \n",
    "#   target_transform=False\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=trans, \n",
    "#   target_transform=False\n",
    ")\n",
    "\n",
    "type(train_dataset), type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset FashionMNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ./data/\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(1.0,))\n",
       "            ),\n",
       " Dataset FashionMNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data/\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(1.0,))\n",
       "            ))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 封装成torch使用的dataset格式数据\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. <a id='toc6_2_'></a>[Dataset](#toc0_)\n",
    "### 6.2.1. <a id='toc6_2_1_'></a>[TensorDataset()](#toc0_)\n",
    "\n",
    "- `TensorDataset`是一个现成的类，用于将数据表示为`张量列表`。\n",
    "\n",
    "- 如果你只是想创建一个包含输入特征和标签的数据集，可以直接使用 TensorDataset：\n",
    "\n",
    "  - `dataset = torch.utils.data.TensorDataset( input_features, labels )` # 按照下标顺序将input_features和labels值对应起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataset.TensorDataset,\n",
       " <torch.utils.data.dataset.TensorDataset at 0x7f1795401e50>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# 1. 自建数据集 (Tensor格式的数据)\n",
    "features = torch.tensor([i for i in range(1000)])   # 必须是tensor格式的额数据\n",
    "labels = features * 2                               # labels = torch.mul(features, 2)\n",
    "\n",
    "# 2. 构建dataset数据集\n",
    "datasets = TensorDataset(features, labels) \n",
    "type(datasets), datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0), (tensor(0), tensor(0)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features0, labels0 = datasets[0] # 取第一个数据对\n",
    "\n",
    "features0, labels0, datasets.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(1), tensor(2)), (tensor(1), tensor(2)))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[1], datasets.__getitem__(1) # 取第二个数据对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.__len__()  # 数据对的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. <a id='toc6_2_2_'></a>[重载Dataset类](#toc0_)\n",
    "\n",
    "- `torch.utils.data.Dataset`是一个抽象类，用于定义新类型的自定义数据集：\n",
    "\n",
    "  - 重载`__init__(self, *args, **kwargs)`: 初始化方法，可以在其中加载你的数据；\n",
    "\n",
    "  - 重载`__len(self)__`: 返回数据集的长度；\n",
    "\n",
    "  - 重载`__getitem__(self, index)`: 根据索引返回数据集中的一个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.MyData at 0x7f17953fb610>,\n",
       " (tensor(0), tensor(0)),\n",
       " (tensor(1), tensor(1)),\n",
       " (tensor(1), tensor(1)),\n",
       " (tensor(2), tensor(2)),\n",
       " (tensor(2), tensor(2)),\n",
       " 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# 1. 重载Dataset类\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, nums:int= 15):\n",
    "        '''初始化参数，耗时的操作初始化时候就完成。'''\n",
    "        self.nums = nums\n",
    "        self.features = torch.arange(self.nums)\n",
    "        self.labels = torch.arange(self.nums)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''返回数据集的总数目。'''\n",
    "        return self.nums\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''耗时的工作初始化时就一步完成，此处依据index或idx查找并返回对应的数据即可。'''        \n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "\n",
    "# 2. 利用重载的Dataset创建数据集\n",
    "datasets = MyData()\n",
    "datasets, datasets[0], datasets[1], datasets.__getitem__(1), datasets[2], datasets.__getitem__(2), datasets.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. <a id='toc6_2_3_'></a>[Pytoch.utils.data.Dataset类分析和总结](#toc0_)\n",
    "\n",
    "- 在PyTorch中数据的封装格式为torch.utils.data.Dataset类；\n",
    "\n",
    "- 第一种方式：直接加载`torchvision.datasets`中对应的数据库生成Dataset格式\n",
    "\n",
    "- 第二种方式：自定义\n",
    "  - 利用`Tensordataset(features, labels)`函数将features和labels配对并生成Dataset格式 (推荐，我觉得更加方便)\n",
    "  \n",
    "  - 重载`Dataset`类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4. <a id='toc6_2_4_'></a>[Subset](#toc0_)\n",
    "\n",
    "用于从数据集中`抽取`子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子集大小: 3\n",
      "子集中的第一个样本: (tensor(1), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset \n",
    "\n",
    "\n",
    "subset = Subset(dataset= datasets, indices= [1, 2, 3])     # 从datasets中抽取indices=[1, 2, 3]的子集\n",
    "\n",
    "print(\"子集大小:\", len(subset))                             # 输出: 3\n",
    "print(\"子集中的第一个样本:\", subset[0])                      # 输出: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5. <a id='toc6_2_5_'></a>[random_split](#toc0_)\n",
    "按比例随机划分数据集，常用于划分训练集、验证集和测试集。\n",
    "\n",
    "为什么固定 random_split：\n",
    "- 可重复实验：固定随机数种子后，实验结果可复现。\n",
    "- 调试方便：划分后的数据集一致性便于调试和对比结果。\n",
    "\n",
    "固定 random_split 的方法：\n",
    "- 方法 1：使用 torch.manual_seed，设置全局随机数种子，让分割结果可复现。\n",
    "- 方法 2：使用 Generator 显式指定种子，通过 torch.Generator 显式控制随机数生成器的种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 10\n",
      "验证集大小: 3\n",
      "测试集大小: 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# 使用 Generator 设置随机数种子\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    dataset= datasets, \n",
    "    lengths= [10, 3, 2], \n",
    "    generator= torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"验证集大小: {len(validation_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6. <a id='toc6_2_6_'></a>[ConcateDataset](#toc0_)\n",
    "\n",
    "将多个数据集拼接成一个数据集。\n",
    "\n",
    "使用场景：多个数据源时方便整合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x7f8026f614f0>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "combined_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.7. <a id='toc6_2_7_'></a>[IterableDataset](#toc0_)\n",
    "对于特别大的数据集（不能一次性加载到内存中），可以使用 IterableDataset 实现流式加载。\n",
    "\n",
    "- 流式数据：当数据无法一次性加载到内存中时，例如从文件、网络或数据库流式读取的数据。\n",
    "- 动态数据生成：当数据是实时生成的，而不是固定的，比如从传感器读取数据或模拟生成数据。\n",
    "- 超大数据集：处理非常大的数据集，避免内存爆炸。\n",
    "- 与普通的 Dataset 不同，IterableDataset 不需要实现 `__len__` 和 `__getitem__` 方法，而是通过实现 `__iter__` 方法来定义数据生成逻辑。\n",
    "\n",
    "IterableDataset 的设计与传统的 Dataset 有所不同：\n",
    "\n",
    "- 不需要实现 `__getitem__` 方法，因为数据是通过迭代生成的。\n",
    "- 必须实现 `__iter__` 方法，返回一个迭代器，用于逐条生成数据。\n",
    "- 无需实现 `__len__` 方法，但可以实现 `__len__` 来支持数据集大小统计。\n",
    "\n",
    "与普通 Dataset 的对比\n",
    "|特性|Dataset|IterableDataset|\n",
    "|:-|:-|:-|\n",
    "|访问方式|随机访问（通过索引 `__getitem__`）|顺序访问（通过 `__iter__` 迭代）|\n",
    "|适用场景|静态、小型数据集|流式、动态生成或超大数据集|\n",
    "|内存管理|可加载到内存中|流式加载，减少内存占用|\n",
    "|是否支持索引|支持|不支持|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.1. <a id='toc6_2_7_1_'></a>[流式数据加载](#toc0_)\n",
    "如果数据存储在一个非常大的文件中，可以使用 `IterableDataset` 来流式读取数据，而不是一次性将数据加载到内存中。\n",
    "\n",
    "关键点：\n",
    "\n",
    "- 数据是按行流式读取的，每次只加载一部分到内存。\n",
    "- 使用 DataLoader 对数据按批次进行处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次数据: tensor([0, 1, 2])\n",
      "批次数据: tensor([3, 4, 5])\n",
      "批次数据: tensor([6, 7, 8])\n",
      "批次数据: tensor([9])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class FileDataset(IterableDataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                yield line.strip()  # 每次返回一行数据\n",
    "\n",
    "\n",
    "# 创建数据集和 DataLoader\n",
    "file_path = 'example.txt'  # 假设文件内容非常大\n",
    "dataset = FileDataset(file_path)\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"批次数据:\", batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.2. <a id='toc6_2_7_2_'></a>[动态生成数据](#toc0_)\n",
    "如果数据是动态生成的，比如需要实时生成斐波那契数列或伪随机数序列，可以使用 `IterableDataset`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斐波那契批次: tensor([0, 1, 1])\n",
      "斐波那契批次: tensor([2, 3, 5])\n",
      "斐波那契批次: tensor([ 8, 13, 21])\n",
      "斐波那契批次: tensor([34])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class FibonacciDataset(IterableDataset):\n",
    "    def __init__(self, max_length):\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        a, b = 0, 1\n",
    "        for _ in range(self.max_length):\n",
    "            yield a\n",
    "            a, b = b, a + b\n",
    "\n",
    "\n",
    "# 创建数据集和 DataLoader\n",
    "dataset = FibonacciDataset(max_length=10)\n",
    "dataloader = DataLoader(dataset, batch_size=3)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"斐波那契批次:\", batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.3. <a id='toc6_2_7_3_'></a>[无限数据流](#toc0_)\n",
    "有时我们需要一个无限的数据流，例如训练生成器模型时使用的随机数据流。\n",
    "\n",
    "关键点：\n",
    "\n",
    "- 数据集是无限的，可以动态生成。\n",
    "- 控制数据生成的批次或次数由外部逻辑实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机数批次: tensor([0.6394, 0.0250, 0.2750, 0.2232, 0.7365], dtype=torch.float64)\n",
      "随机数批次: tensor([0.6767, 0.8922, 0.0869, 0.4219, 0.0298], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class RandomDataset(IterableDataset):\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield random.random()  # 无限生成随机数\n",
    "\n",
    "\n",
    "# 创建数据集和 DataLoader\n",
    "dataset = RandomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size= 5)\n",
    "\n",
    "\n",
    "# 仅读取两批数据\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(\"随机数批次:\", batch)\n",
    "    if i == 1:  # 控制只读取两批\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.4. <a id='toc6_2_7_4_'></a>[多线程数据加载与分布式支持](#toc0_)\n",
    "如果需要在分布式或多线程环境中使用 IterableDataset，可以通过 torch.utils.data.get_worker_info 获取工作线程的信息，并实现分片逻辑。\n",
    "\n",
    "1. 顺序保障：如果数据需要特定顺序（如时序数据），要在 `__iter__` 方法中维护顺序。\n",
    "2. 分布式支持：\n",
    "    - 对于多进程或分布式训练，需要实现 `__iter__` 方法中的分片逻辑。\n",
    "    - 可以使用 `torch.utils.data.get_worker_info` 获取当前进程的 ID 和数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次数据: tensor([ 0,  4,  8, 12, 16])\n",
      "批次数据: tensor([ 1,  5,  9, 13, 17])\n",
      "批次数据: tensor([ 2,  6, 10, 14, 18])\n",
      "批次数据: tensor([ 3,  7, 11, 15, 19])\n",
      "批次数据: tensor([20, 24, 28, 32, 36])\n",
      "批次数据: tensor([21, 25, 29, 33, 37])\n",
      "批次数据: tensor([22, 26, 30, 34, 38])\n",
      "批次数据: tensor([23, 27, 31, 35, 39])\n",
      "批次数据: tensor([40, 44, 48, 52, 56])\n",
      "批次数据: tensor([41, 45, 49, 53, 57])\n",
      "批次数据: tensor([42, 46, 50, 54, 58])\n",
      "批次数据: tensor([43, 47, 51, 55, 59])\n",
      "批次数据: tensor([60, 64, 68, 72, 76])\n",
      "批次数据: tensor([61, 65, 69, 73, 77])\n",
      "批次数据: tensor([62, 66, 70, 74, 78])\n",
      "批次数据: tensor([63, 67, 71, 75, 79])\n",
      "批次数据: tensor([80, 84, 88, 92, 96])\n",
      "批次数据: tensor([81, 85, 89, 93, 97])\n",
      "批次数据: tensor([82, 86, 90, 94, 98])\n",
      "批次数据: tensor([83, 87, 91, 95, 99])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import get_worker_info, IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class DistributedDataset(IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        \n",
    "        if worker_info is None:\n",
    "            # 单线程，返回全部数据\n",
    "            return iter(self.data)\n",
    "        else:\n",
    "            # 多线程，按线程数分片\n",
    "            worker_id = worker_info.id\n",
    "            num_workers = worker_info.num_workers\n",
    "            return iter(self.data[worker_id::num_workers])\n",
    "\n",
    "\n",
    "dataset = DistributedDataset(range(100))\n",
    "dataloader = DataLoader(dataset, num_workers=4, batch_size=5)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"批次数据:\", batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.5. <a id='toc6_2_7_5_'></a>[使用注意事项](#toc0_)\n",
    "1. 不可随机访问：\n",
    "\n",
    "   - IterableDataset 不支持通过索引访问数据（没有 `__getitem__` 方法）。\n",
    "   - 只能顺序生成数据。\n",
    "\n",
    "2. 分布式与多线程支持：\n",
    "\n",
    "   - 如果需要并行加载数据，需在 `__iter__` 方法中处理数据分片。\n",
    "\n",
    "3. 效率问题：\n",
    "\n",
    "   - 适合处理内存不足的场景，但如果数据可以一次性加载到内存中，Dataset 会更高效。\n",
    "\n",
    "4. 批次大小：\n",
    "   - 使用 DataLoader 的 batch_size 参数，IterableDataset 的数据流可以按批次返回。\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. <a id='toc6_3_'></a>[DataLoader](#toc0_)\n",
    "\n",
    "1. 先将自制的数据集利用data.TensorDataset生成`dataset`；\n",
    "\n",
    "2. 再用data.DataLoader加载到dataset成最终可用的带有batch_size的格式`DataLoader`，方便后续的训练\n",
    "\n",
    "3. 先测试以下数据加载的速度，必须比训练计算所耗的时间小，否则将降低训练效率；\n",
    "\n",
    "4. 当数据加载时间很长时可以预加载，缩短时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# 加载torchvison数据集（格式化好的torch.utils.data.Dataset）\n",
    "train_iter = DataLoader(\n",
    "    dataset = datasets,             # Dataset\n",
    "    batch_size = 5,                 # batch size\n",
    "    shuffle = True,                 # 打乱顺序\n",
    "    num_workers = 3,                # 线程数\n",
    "    drop_last = False,              # 是否删除最后一个不是整数的batch\n",
    "    # collate_fn=collate_function     # 处理函数，可以处理不等长数据等等\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataloader.DataLoader,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f18f0620810>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_iter), train_iter        # 直接打印看不到内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽取: 1 [tensor([ 4, 10,  9, 14,  5]), tensor([ 4, 10,  9, 14,  5])]\n",
      "随机抽取: 2 [tensor([ 1, 13,  3,  7,  6]), tensor([ 1, 13,  3,  7,  6])]\n",
      "随机抽取: 3 [tensor([ 8, 11,  2,  0, 12]), tensor([ 8, 11,  2,  0, 12])]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_iter):  # 小批量的batch_size数据\n",
    "    if batch_idx == 10:\n",
    "        break\n",
    "    print('随机抽取:', batch_idx+1, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1. <a id='toc6_3_1_'></a>[估计数据加载时间](#toc0_)\n",
    "\n",
    "估计加载数据所需时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== \n",
      " Total：\n",
      " 0.0 d \n",
      " 0.0 h \n",
      " 0.0 m \n",
      " 0.16623139381408691 s\n"
     ]
    }
   ],
   "source": [
    "# 读完一个epoch的一个batch，耗时\n",
    "timer = cpuTimer()\n",
    "for X, y in train_iter:\n",
    "    break \n",
    "timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== \n",
      " Total：\n",
      " 0.0 d \n",
      " 0.0 h \n",
      " 0.0 m \n",
      " 0.16842293739318848 s\n"
     ]
    }
   ],
   "source": [
    "# 读完一个epoch的所有batch，耗时\n",
    "timer = cpuTimer()\n",
    "for X, y in train_iter:\n",
    "    continue \n",
    "timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. <a id='toc6_3_2_'></a>[collate_fn处理不等长tensor](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 样本的来源\n",
    "DataLoader 会调用 `Dataset.__getitem__` 获取 `batch_size` 个样本。这些样本是 collate_fn 的输入，形式是一个 Python `列表`，其中每个元素是` __getitem__ `方法返回的结果。\n",
    "\n",
    "2. 默认行为  \n",
    "如果不指定 collate_fn，DataLoader 会尝试自动将样本堆叠成张量：\n",
    "  - 如果样本是 torch.Tensor，会沿第 0 维堆叠（使用 torch.stack）。\n",
    "  - 如果样本是其他可组合的类型（如 dict 或 list），会递归地组合它们的内容。\n",
    "  - 如果样本形状不一致，默认行为会失败。\n",
    "\n",
    "  ```python\n",
    "  # Dataset 提供样本： DataLoader 根据 batch_size 从 Dataset 调用 __getitem__，返回一个列表 batch。\n",
    "  batch = [dataset[i] for i in range(batch_size)]\n",
    "\n",
    "  # 调用 collate_fn： 将这个 batch 传入 collate_fn，进行处理：\n",
    "  processed_batch = collate_fn(batch)\n",
    "  ```\n",
    "\n",
    "3. 自定义 collate_fn 的作用   \n",
    "自定义 collate_fn 可以覆盖默认行为，定义自己的逻辑来处理复杂的数据结构或变长数据。例如：  \n",
    "    - 对变长序列进行填充。\n",
    "    - 按需调整数据的结构或类型。\n",
    "    - 返回额外的辅助信息（如序列长度）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 collate_fn 来处理字典和标签\n",
    "def collate_function(batch):\n",
    "    # 分别提取 batch 中的 msa, pair 和 labels\n",
    "    msa_batch = [item[0]['msa'] for item in batch]\n",
    "    pair_batch = [item[0]['pair'] for item in batch]\n",
    "    labels_batch = [item[1] for item in batch]\n",
    "\n",
    "    # 找到 batch 中最长的 num_residues\n",
    "    max_residues = max([msa.shape[1] for msa in msa_batch])\n",
    "\n",
    "    # 对 MSA 特征填充 num_residues 维度，使其维度一致\n",
    "    padded_msa_batch = []\n",
    "    for msa in msa_batch:\n",
    "        pad_size = max_residues - msa.shape[1]\n",
    "        padded_msa = torch.nn.functional.pad(msa, (0, 0, 0, pad_size))  # 填充第二维度\n",
    "        padded_msa_batch.append(padded_msa)\n",
    "\n",
    "    # 对 Pair 特征填充 num_residues 维度，使其维度一致\n",
    "    padded_pair_batch = []\n",
    "    for pair in pair_batch:\n",
    "        pad_size = max_residues - pair.shape[0]\n",
    "        padded_pair = torch.nn.functional.pad(pair, (0, 0, 0, pad_size, 0, pad_size))  # 填充第一和第二维度\n",
    "        padded_pair_batch.append(padded_pair)\n",
    "\n",
    "    # 将 MSA 和 Pair 特征堆叠为批量数据\n",
    "    padded_msa_batch = torch.stack(padded_msa_batch)\n",
    "    padded_pair_batch = torch.stack(padded_pair_batch)\n",
    "\n",
    "    # 将标签堆叠为批量数据\n",
    "    labels_batch = torch.stack(labels_batch)\n",
    "\n",
    "    # 返回批量化后的字典和标签\n",
    "    return {'msa': padded_msa_batch, 'pair': padded_pair_batch}, labels_batch\n",
    "\n",
    "train_iter = data.DataLoader(\n",
    "    dataset=train_datasets, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    num_workers=20, \n",
    "    collate_fn=collate_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3. <a id='toc6_3_3_'></a>[重载DataLoader](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data \n",
    "\n",
    "\n",
    "class RebuildDataLoader(data.DataLoader):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        return \n",
    "    \n",
    "    # 重载 __iter__ 方法来控制数据加载方式\n",
    "    def __iter__(self):\n",
    "        # 你可以在这里实现自定义的加载逻辑，比如控制每个批次的顺序\n",
    "        iterator = super().__iter__()\n",
    "        for batch in iterator:\n",
    "            # 可以在这里处理或过滤批次数据\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[张量(Tensors)](#toc0_)\n",
    "## 7.1. <a id='toc7_1_'></a>[Tensors定义](#toc0_)\n",
    "\n",
    "PyTorch 的一大作用就是可以代替 Numpy 库，所以首先介绍 Tensors ，也就是张量，它相当于 Numpy 的多维数组(ndarrays)。\n",
    "\n",
    "* 两者的区别就是：\n",
    "    * `数学或物理`概念：张量 (`Tensors`)\n",
    "    \n",
    "    * `编程`概念：数组 (`Array`)\n",
    "    \n",
    "* 总结\n",
    "\n",
    "|函数名称|注释|\n",
    "|:-|:-|\n",
    "|torch.tensor()|tensor|\n",
    "|torch.asarray()||\n",
    "|torch.from_numpy()|numpy2tensor|\n",
    "|torch.empty(size)|垃圾数|\n",
    "|torch.zeros(size)|0|\n",
    "|torch.ones(size)|1|\n",
    "|`torch.full(size,fill_value)`|fill_value|\n",
    "|torch.rand(size)|随机数|\n",
    "|torch.randn(size)|标准正态分布|\n",
    "|torch.normal(mean,std,size)|正态分布|\n",
    "|torch.arange(start,end,step,size)|数组|\n",
    "|.reshape(size)|重塑|\n",
    "|.numpy()|转为numpy的ndarray|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor()\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], dtype= torch.float32)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asarray()\n",
    "\n",
    "x = torch.asarray([1.0, 2.0, 3.0], dtype= torch.float32)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.from_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]]),\n",
       " tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy转tensor, from_numpy()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(0, 15).reshape(5, 3)\n",
    "\n",
    "x, torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4684e-32,  3.0845e-41, -3.9247e+29],\n",
       "        [ 4.5593e-41,  1.4013e-45,  0.0000e+00],\n",
       "        [ 4.2039e-45,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty()\n",
    "\n",
    "torch.empty(size= (5, 3), dtype= torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros()\n",
    "\n",
    "torch.zeros(size= (5, 3)) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ones()\n",
    "\n",
    "torch.ones(size= (5, 3), dtype= torch.float32) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.full(size, fill_value)\n",
    "\n",
    "可以用来做mask的`填充`，填充任意数值fill_value，而不只是0或1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "torch.full(size= (2, 3), fill_value= torch.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.rand()，产生随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8854, 0.5739, 0.2666],\n",
       "        [0.6274, 0.2696, 0.4414],\n",
       "        [0.2969, 0.8317, 0.1053],\n",
       "        [0.2695, 0.3588, 0.1994],\n",
       "        [0.5472, 0.0062, 0.9516]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand()\n",
    "\n",
    "torch.rand(size= (5, 3), dtype= torch.float32) # 随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.randn()，标准正态分布随机数，产生正态分布随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1859, -0.8860, -0.7150,  0.1280, -0.1603],\n",
       "        [-2.2161, -0.6858, -0.3295, -0.2747, -1.2552],\n",
       "        [-0.7813,  0.2293, -1.2754, -1.9245,  0.4336]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randn()\n",
    "\n",
    "torch.randn(size= (3, 5), dtype= torch.float32) # 标准正态分布随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.normal()，正态分布随机数，产生mean和std的size个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6641, -0.4337, -0.4201, -0.9500, -1.0014],\n",
       "        [-0.7719,  1.3434,  0.9560, -1.0110, -0.3568],\n",
       "        [ 0.7147, -0.2398,  0.2163,  0.5484, -0.4415]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean= 0, std= 1, size= (3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.arange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arange()\n",
    "\n",
    "torch.arange(3) # 0, 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape()\n",
    "\n",
    "torch.arange(start= 0, end= 15, step= 1).reshape(5, 3) # reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .numpy()，将tensor转化为numpy的ndarray格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]),\n",
       " array([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor转化为numpy\n",
    "\n",
    "x = torch.arange(start= 0, end= 15, step= 1).reshape(5, 3)\n",
    "\n",
    "x, x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. <a id='toc7_2_'></a>[Tensors属性](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. <a id='toc7_2_1_'></a>[数据类型(dtype)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.int8\n",
    "torch.int16         # \n",
    "torch.int32         # torch.IntTensor()\n",
    "torch.int64         # torch.LongTensor()\n",
    "torch.float16       # \n",
    "torch.float32       # torch.FloatTensor()\n",
    "torch.float64       # torch.DoubleTensor()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12, 5]), device(type='cpu'), torch.float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.normal(\n",
    "    mean= 0, \n",
    "    std= 1, \n",
    "    size= (128, 12, 5), \n",
    "    dtype= torch.float32, \n",
    "    device= 'cpu'\n",
    ")\n",
    "\n",
    "x.shape, x.device, x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1.1. <a id='toc7_2_1_1_'></a>[转化格式](#toc0_)\n",
    "\n",
    "方法一：\n",
    "\n",
    "|函数|备注|\n",
    "|-|-|\n",
    "|tensor.double()：|把一个张量tensor转为torch.float64 数据类型|\n",
    "|tensor.float()：|把一个张量tensor转为torch.float32 数据类型|\n",
    "|tensor.int()：|把一个张量tensor转为torch.int32 数据类型|\n",
    "|tensor.long(): |把一个张量tensor转为torch.int64 数据类型|\n",
    "\n",
    "方法二： \n",
    "\n",
    "|类型|函数|备注|\n",
    "|-|-|-|\n",
    "|float to int|||\n",
    "||x.int()|float to int32|\n",
    "||x.long()|float to int64|\n",
    "|int to float|||\n",
    "||x.float()|int32 to float|\n",
    "||x.double()|int64 to float|\n",
    "\n",
    "方法三：a.to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.float64, torch.int32, torch.int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, x.float().dtype, x.double().dtype, x.int().dtype, x.long().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, x.to(torch.float).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. <a id='toc7_2_2_'></a>[设备(device)](#toc0_)\n",
    "PyTorch识别的设备类型: cpu, cuda:0, cuda:1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12, 5]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    '''列出cpu或所有的gpu [cuda:0, cuda:1]'''\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{i}\")\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "x = torch.normal(\n",
    "    mean=0, \n",
    "    std=1, \n",
    "    size=(128, 12, 5), \n",
    "    dtype=torch.float32, \n",
    "    device=try_gpu()\n",
    ")\n",
    "\n",
    "x.shape, x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. <a id='toc7_2_3_'></a>[维度(size/shape)](#toc0_)\n",
    "\n",
    "- 查看张量维度：\n",
    "\n",
    "    |函数名称|注释|\n",
    "    |:-|:-|\n",
    "    |x.size()||\n",
    "    |x.shape||\n",
    "\n",
    "- tensor([[[]]]) `直接`表示:\n",
    "  - 几个`[[[`，表示几个维度\n",
    "  - 没有`[]`，表示0维，即标量\n",
    "\n",
    "- Size[] `属性`：\n",
    "  - `[2, 3]`: 几个数字即几个维度\n",
    "  - `[2, 3]`: 每个数字表示对应维度的元素数量\n",
    "  - `[]`: 0维表示标量\n",
    "  - `[2]`: 一维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3.1. <a id='toc7_2_3_1_'></a>[标量](#toc0_)\n",
    "dim=0 表示 `标量`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), torch.Size([]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1)\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3.2. <a id='toc7_2_3_2_'></a>[一维张量](#toc0_)\n",
    "dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]), torch.Size([1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1])   # 相比较标量，只是多了一个[]\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4, 5]), torch.Size([5]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3.3. <a id='toc7_2_3_3_'></a>[多维张量](#toc0_)\n",
    "dim 大于等于 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9., 10., 11.]]),\n",
       " torch.Size([2, 6]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[ 0,  1,  2,  3,  4,  5],\n",
    "                  [ 6,  7,  8,  9, 10, 11]], dtype= torch.float32)\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5]],\n",
       " \n",
       "         [[ 6,  7],\n",
       "          [ 8,  9],\n",
       "          [10, 11]]]),\n",
       " torch.Size([2, 3, 2]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[ 0,  1],\n",
    "                   [ 2,  3],\n",
    "                   [ 4,  5]],\n",
    "                  [[ 6,  7],\n",
    "                   [ 8,  9],\n",
    "                   [10, 11]]])\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0,  1],\n",
       "           [ 2,  3],\n",
       "           [ 4,  5]]],\n",
       " \n",
       " \n",
       "         [[[ 6,  7],\n",
       "           [ 8,  9],\n",
       "           [10, 11]]]]),\n",
       " torch.Size([2, 1, 3, 2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(2, 1, 3, 2)\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0,  1],\n",
       "           [ 2,  3],\n",
       "           [ 4,  5]],\n",
       " \n",
       "          [[ 6,  7],\n",
       "           [ 8,  9],\n",
       "           [10, 11]]],\n",
       " \n",
       " \n",
       "         [[[12, 13],\n",
       "           [14, 15],\n",
       "           [16, 17]],\n",
       " \n",
       "          [[18, 19],\n",
       "           [20, 21],\n",
       "           [22, 23]]]]),\n",
       " torch.Size([2, 2, 3, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24).reshape(2, 2, 3, 2)\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4. <a id='toc7_2_4_'></a>[特殊的一维张量](#toc0_)\n",
    "`在PyTorch中，一维张量（Tensor）通常表示一个向量，它可以被视为一行或一列的数值。然而，在大多数情况下，一维张量并不明确区分是行向量还是列向量。这是因为一维张量在数学运算中通常是按照向量的规则来处理的，而不是像矩阵那样区分行和列。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4.1. <a id='toc7_2_4_1_'></a>[一维张量的例子](#toc0_)\n",
    "\n",
    "假设我们有一个一维张量 `tensor`，它的形状为 `(n,)`，其中 `n` 表示张量中的元素数量。这样的张量可以表示为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 创建一个一维张量\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "print(tensor)  # 输出: tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4.2. <a id='toc7_2_4_2_'></a>[区分行向量和列向量](#toc0_)\n",
    "\n",
    "尽管一维张量本身没有明确的行向量或列向量的概念，但在某些情况下，我们可能需要将其视为行向量或列向量来进行`矩阵运算`。这可以通过增加一个额外的维度来实现：\n",
    "\n",
    "- **行向量**：可以通过 `.unsqueeze(-1)` 方法增加一个维度来表示行向量，形状变为 `(n, 1)`。\n",
    "- **列向量**：可以通过 `.unsqueeze(0)` 方法增加一个维度来表示列向量，形状变为 `(1, n)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.4.2.1. <a id='toc7_2_4_2_1_'></a>[示例](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4, 5]), torch.Size([5]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个一维张量\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "vector, vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Vector:\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# 转换为行向量\n",
    "row_vector = vector.unsqueeze(-1)\n",
    "\n",
    "print(\"Row Vector:\", row_vector, row_vector.shape, sep='\\n')  # 输出: tensor([[1], [2], [3], [4], [5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Vector:\n",
      "tensor([[1, 2, 3, 4, 5]])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# 转换为列向量\n",
    "column_vector = vector.unsqueeze(0)\n",
    "\n",
    "print(\"Column Vector:\", column_vector, column_vector.shape, sep='\\n')  # 输出: tensor([[1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4.3. <a id='toc7_2_4_3_'></a>[小结](#toc0_)\n",
    "\n",
    "在PyTorch中，一维张量通常表示向量，没有明确的行向量或列向量之分。如果需要明确表示行向量或列向量，可以通过增加维度的方式来进行转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. <a id='toc7_3_'></a>[Tensors操作](#toc0_)\n",
    "### 7.3.1. <a id='toc7_3_1_'></a>[索引和切片](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]),\n",
       " torch.Size([5, 3]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15).reshape(5, 3)\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 1行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1] # 2行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3] # 1-3行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  3,  6,  9, 12])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0] # 1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4,  7, 10, 13])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1] # 2列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 6])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3, 0] # 1-3行，1列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. <a id='toc7_3_2_'></a>[修改维度](#toc0_)\n",
    "\n",
    "* 形状/维度：其实整个张量运算就是线性代数中的矩阵运算，所以最重要是明白`矩阵运算前后`的`形状/维度`。  \n",
    "\n",
    "* 高阶张量由若干低阶张量构成，如\n",
    "    * 结构为 (n, c, h, w)的 4 阶张量由 n 个结构为 (c, h, w) 的 3 阶张量构成，\n",
    "    * 结构为 (c, h, w)的 3 阶张量由 c 个结构为 (h, w) 的 2 阶张量构成，\n",
    "    * 结构为 (h, w)的 2 阶张量又由 h 个长度为 w 的 1 阶张量构成，h 为行数，w 为列数。\n",
    "\n",
    "* 修改形状/维度：reshape和view都是用来重塑tensor的shape的。view只适合对满足连续性条件（contiguous）的tensor进行操作，而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。view能干的reshape都能干，如果view不能干就可以用reshape来处理。\n",
    "\n",
    "- 维度`依次重排`：\n",
    "\n",
    "    - `.reshape()`\n",
    "\n",
    "    - `.view()`\n",
    "\n",
    "- 维度重组或转换或`挪动`：\n",
    "\n",
    "    - `permute()`\n",
    "\n",
    "    - `transpose()`\n",
    "\n",
    "- 参考：[https://blog.csdn.net/weixin_44115575/article/details/140742574](https://blog.csdn.net/weixin_44115575/article/details/140742574)\n",
    "\n",
    "  - 技术层面上的实现：一个张量是由`头部信息`部分和`数据存储`部分组成，头部信息部分存储了张量的`形状 (shape)`、`步长 (stride)`、`数据类型 (dtype)`等信息，数据存储部分存储了张量的实际数据。reshape、view、transpose和permute操作都是基于头部信息部分进行操作的，不会改变数据存储部分的数据。只是view处理前可能需要contiguous()一下；transpose和permute操作在交换维度的时候，需要考虑步长stride的重新计算；交换维度后，对对应长stride进行对应的交换，只是视图变了，数据存储部分的数据没有变。\n",
    "\n",
    "  - 应用层面上的理解：\n",
    "\n",
    "    - 图像数据处理：经常需要将图像的维度进行重排，如将 (H, W, C) 转换为 (C, H, W)，或者将 (H, W, C) 转换为 (C, H, W)。\n",
    "        ```python \n",
    "        # 假设图像数据为 (Batch, Channels, Height, Width)\n",
    "        image_tensor = torch.randn(32, 3, 64, 64)\n",
    "\n",
    "        # 转换为 (Batch, Height, Width, Channels)\n",
    "        image_tensor_permuted = image_tensor.permute(0, 2, 3, 1)\n",
    "        ```\n",
    "\n",
    "    - 自然语言处理:在自然语言处理任务中，RNN 或 Transformer 模型可能需要特定的输入维度顺序。例如，输入数据可能需要以 (sequence_length, batch_size, features) 的格式提供。\n",
    "        ```python\n",
    "        # 假设输入数据为 (Batch, Sequence Length, Features)\n",
    "        input_tensor = torch.randn(32, 10, 128)\n",
    "\n",
    "        # 转换为 (Sequence Length, Batch, Features)\n",
    "        input_tensor_permuted = input_tensor.permute(1, 0, 2)\n",
    "        ``` \n",
    "\n",
    "    - 多维数据分析:在处理多维数据时，某些操作可能需要特定的维度顺序。例如，计算某个维度上的均值或标准差时，可能需要先调整维度顺序。\n",
    "        ```python\n",
    "        # 假设有一个 4D 张量\n",
    "        data_tensor = torch.randn(5, 10, 15, 20)\n",
    "\n",
    "        # 需要在第一个维度上进行某种操作，可以先调整维度顺序\n",
    "        data_tensor_permuted = data_tensor.permute(1, 0, 2, 3)\n",
    "        ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.1. <a id='toc7_3_2_1_'></a>[[: None], [None, :]](#toc0_)                             [&#8593;](#toc0_)\n",
    "含义：[None, :] 是利用 Python 的`切片语法`为张量增加一个新维度。\n",
    "- None：相当于在第 0 维增加一个新维度。\n",
    "- :：表示保留张量原本的所有元素。\n",
    "\n",
    "特点:\n",
    "- 只能增加维度（例如将 1D 张量变为 2D 张量）。\n",
    "- 增加的维度的大小为 1，方便用于广播操作。\n",
    "\n",
    "\n",
    "含义：reshape(-1, 1) 是用于调整张量形状的通用操作\n",
    " - -1：表示自动推导该维度的大小（根据张量总元素个数计算）。\n",
    " - 1：将张量变形为具有 1 列的 2D 张量。\n",
    "\n",
    "特点：\n",
    "- 更灵活，可以同时调整多个维度的大小。\n",
    "- 可以改变张量的形状，不局限于增加维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12]),\n",
       " torch.Size([1, 12]),\n",
       " torch.Size([1, 12]),\n",
       " torch.Size([12, 1]),\n",
       " torch.Size([12, 1]))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(12)\n",
    "\n",
    "x1 = x[None, :]\n",
    "x2 = x.reshape(1, -1)\n",
    "\n",
    "x3 = x[:, None]\n",
    "x4 = x.reshape(-1, 1)\n",
    "\n",
    "x.shape, x1.shape, x2.shape, x3.shape, x4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.2. <a id='toc7_3_2_2_'></a>[reshape函数](#toc0_)\n",
    "从左往右拉直，然后依次排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       " torch.Size([15]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15)\n",
    "\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape\n",
    "\n",
    "X.reshape(3, 5), X.reshape(3, 5).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.3. <a id='toc7_3_2_3_'></a>[view函数](#toc0_)\n",
    "从左往右拉直，然后依次排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view\n",
    "\n",
    "X.view(3, 5), X.view(3, 5).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.4. <a id='toc7_3_2_4_'></a>[transpose函数](#toc0_)\n",
    "\n",
    "`transpose()`函数`一次进行两个维度`的交换，参数是 0, 1, 2, 3, … ，随着待转换张量的阶数上升参数越来越多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.4.1. <a id='toc7_3_2_4_1_'></a>[二维](#toc0_)\n",
    "二维下transpose和T效果相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 阶张量，结构为 (h, w)，\n",
    "# 对应 transpose() 函数中的参数是 (0, 1) 两个索引，\n",
    "# 进行 transpose(0, 1) 操作就是在交换 h, w 两个维度，\n",
    "# 得到的结果与常见的矩阵转置相同。\n",
    "\n",
    "X = torch.arange(15).reshape(3, 5)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10],\n",
       "        [ 1,  6, 11],\n",
       "        [ 2,  7, 12],\n",
       "        [ 3,  8, 13],\n",
       "        [ 4,  9, 14]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10],\n",
       "        [ 1,  6, 11],\n",
       "        [ 2,  7, 12],\n",
       "        [ 3,  8, 13],\n",
       "        [ 4,  9, 14]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.4.2. <a id='toc7_3_2_4_2_'></a>[三维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [12, 13, 14, 15]],\n",
       " \n",
       "         [[ 4,  5,  6,  7],\n",
       "          [16, 17, 18, 19]],\n",
       " \n",
       "         [[ 8,  9, 10, 11],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(0, 1).shape, X.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]),\n",
       " tensor([[[ 0, 12],\n",
       "          [ 4, 16],\n",
       "          [ 8, 20]],\n",
       " \n",
       "         [[ 1, 13],\n",
       "          [ 5, 17],\n",
       "          [ 9, 21]],\n",
       " \n",
       "         [[ 2, 14],\n",
       "          [ 6, 18],\n",
       "          [10, 22]],\n",
       " \n",
       "         [[ 3, 15],\n",
       "          [ 7, 19],\n",
       "          [11, 23]]]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(0, 2).shape, X.transpose(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3]),\n",
       " tensor([[[ 0,  4,  8],\n",
       "          [ 1,  5,  9],\n",
       "          [ 2,  6, 10],\n",
       "          [ 3,  7, 11]],\n",
       " \n",
       "         [[12, 16, 20],\n",
       "          [13, 17, 21],\n",
       "          [14, 18, 22],\n",
       "          [15, 19, 23]]]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(1, 2).shape, X.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]),\n",
       " tensor([[[ 0, 12],\n",
       "          [ 4, 16],\n",
       "          [ 8, 20]],\n",
       " \n",
       "         [[ 1, 13],\n",
       "          [ 5, 17],\n",
       "          [ 9, 21]],\n",
       " \n",
       "         [[ 2, 14],\n",
       "          [ 6, 18],\n",
       "          [10, 22]],\n",
       " \n",
       "         [[ 3, 15],\n",
       "          [ 7, 19],\n",
       "          [11, 23]]]))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape, X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.5. <a id='toc7_3_2_5_'></a>[permute函数](#toc0_)\n",
    "\n",
    "`permute()`函数`一次可以进行多个维度`的交换或者可以成为维度重新排列，参数是 0, 1, 2, 3, … ，随着待转换张量的阶数上升参数越来越多，本质上可以理解为多个 transpose() 操作的叠加，因此理解 permute() 函数的关键在于理解 transpose() 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.5.1. <a id='toc7_3_2_5_1_'></a>[二维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]),\n",
       " tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15).reshape(3, 5)\n",
    "\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]),\n",
       " tensor([[ 0,  5, 10],\n",
       "         [ 1,  6, 11],\n",
       "         [ 2,  7, 12],\n",
       "         [ 3,  8, 13],\n",
       "         [ 4,  9, 14]]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(1, 0).shape, X.permute(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.5.2. <a id='toc7_3_2_5_2_'></a>[三维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [12, 13, 14, 15]],\n",
       " \n",
       "         [[ 4,  5,  6,  7],\n",
       "          [16, 17, 18, 19]],\n",
       " \n",
       "         [[ 8,  9, 10, 11],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(1, 0, 2).shape, X.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3]),\n",
       " tensor([[[ 0,  4,  8],\n",
       "          [ 1,  5,  9],\n",
       "          [ 2,  6, 10],\n",
       "          [ 3,  7, 11]],\n",
       " \n",
       "         [[12, 16, 20],\n",
       "          [13, 17, 21],\n",
       "          [14, 18, 22],\n",
       "          [15, 19, 23]]]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(0, 2, 1).shape, X.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]),\n",
       " tensor([[[ 0, 12],\n",
       "          [ 4, 16],\n",
       "          [ 8, 20]],\n",
       " \n",
       "         [[ 1, 13],\n",
       "          [ 5, 17],\n",
       "          [ 9, 21]],\n",
       " \n",
       "         [[ 2, 14],\n",
       "          [ 6, 18],\n",
       "          [10, 22]],\n",
       " \n",
       "         [[ 3, 15],\n",
       "          [ 7, 19],\n",
       "          [11, 23]]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(2, 1, 0).shape, X.permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.6. <a id='toc7_3_2_6_'></a>[unsqueeze函数增加维度](#toc0_)\n",
    "unsqueeze 用于在`指定位置插入`一个大小为 `1` 的新维度。它不会改变数据本身，只是改变张量的形状。  \n",
    "增加/插入大小为`1`的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.6.1. <a id='toc7_3_2_6_1_'></a>[1维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size: torch.Size([4])\n",
      "tensor([1, 2, 3, 4])\n",
      "x1 size: torch.Size([4])\n",
      "tensor([[1, 2, 3, 4]])\n",
      "x1 size: torch.Size([4])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(f'x size: {x.shape}', x, sep='\\n')\n",
    "\n",
    "x1 = torch.unsqueeze(input=x, dim=0)\n",
    "print(f'x1 size: {x.shape}', x1, sep='\\n')          # 1 x 4\n",
    "\n",
    "x1 = torch.unsqueeze(input=x, dim=1)\n",
    "print(f'x1 size: {x.shape}', x1, sep='\\n')          # 4 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.6.2. <a id='toc7_3_2_6_2_'></a>[多维度](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]),\n",
       " tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).reshape(3, 3)\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 3]),\n",
       " tensor([[[0, 1, 2],\n",
       "          [3, 4, 5],\n",
       "          [6, 7, 8]]]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.unsqueeze(input=x, dim=0)\n",
    "\n",
    "x1.shape, x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 3]),\n",
       " tensor([[[0, 1, 2]],\n",
       " \n",
       "         [[3, 4, 5]],\n",
       " \n",
       "         [[6, 7, 8]]]))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.unsqueeze(input=x, dim=1)\n",
    "\n",
    "x2.shape, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 1]),\n",
       " tensor([[[0],\n",
       "          [1],\n",
       "          [2]],\n",
       " \n",
       "         [[3],\n",
       "          [4],\n",
       "          [5]],\n",
       " \n",
       "         [[6],\n",
       "          [7],\n",
       "          [8]]]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = torch.unsqueeze(input=x, dim=2)\n",
    "\n",
    "x3.shape, x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.7. <a id='toc7_3_2_7_'></a>[squeeze函数减少维度](#toc0_)\n",
    "squeeze 用于`移除大小为 1` 的维度。它不会改变数据本身，只是改变张量的形状。  \n",
    "移除大小为`1`的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 3]), torch.Size([3, 3]), torch.Size([3, 1, 3]))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).reshape(3, 1, 3)\n",
    "\n",
    "x.shape, torch.squeeze(x, 1).shape, torch.squeeze(x, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.8. <a id='toc7_3_2_8_'></a>[拼接 (concat)](#toc0_)\n",
    "- 作用：torch.cat 用于将一组张量在`已有的维度`上拼接，而`不会创建新的维度`。\n",
    "- 拼接维度：你可以指定沿哪个维度进行拼接。\n",
    "- 输入要求：输入的张量在被拼接的维度以外的维度上`形状必须相同`。\n",
    "- 不增加新维度：拼接后张量的总维度与输入张量`相同`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 两个形状相同的张量\n",
    "a = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "\n",
    "b = torch.tensor([[5, 6], \n",
    "                  [7, 8]])\n",
    "\n",
    "# 沿dim=0拼接 (纵向)\n",
    "cat_result_0 = torch.cat((a, b), dim=0)\n",
    "print(cat_result_0)\n",
    "# 输出:\n",
    "# tensor([[1, 2],\n",
    "#         [3, 4],\n",
    "#         [5, 6],\n",
    "#         [7, 8]])\n",
    "\n",
    "# 沿dim=1拼接 (横向)\n",
    "cat_result_1 = torch.cat((a, b), dim=1)\n",
    "print(cat_result_1)\n",
    "# 输出:\n",
    "# tensor([[1, 2, 5, 6],\n",
    "#         [3, 4, 7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2, 3]), torch.Size([2, 4, 3]), torch.Size([2, 2, 6]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(shape=(2, 2, 3))\n",
    "\n",
    "dim1 = torch.cat([x, x], dim=0)\n",
    "dim2 = torch.cat([x, x], dim=1)\n",
    "dim3 = torch.cat([x, x], dim=2)\n",
    "\n",
    "dim1.shape, dim2.shape, dim3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.9. <a id='toc7_3_2_9_'></a>[拆分 (split)](#toc0_)\n",
    "- 功能: 将张量分成指定大小的块。\n",
    "- 参数:\n",
    "    - input: 要分割的张量。 \n",
    "    - split_size_or_sections: 每个块的大小，或者一个列表，指定每个块的大小。\n",
    "    - dim: 沿着哪个维度进行分割。\n",
    "- 特点: 可以指定每个块的大小，或者通过列表指定每个块的具体大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 0: tensor([0, 1, 2])\n",
      "块 1: tensor([3, 4, 5])\n",
      "块 2: tensor([6, 7, 8])\n",
      "块 3: tensor([9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "tensor = torch.arange(10)\n",
    "splits = torch.split(tensor, 3)  # 每个块大小为3\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"块 {i}: {split}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.10. <a id='toc7_3_2_10_'></a>[分块 (chunk)](#toc0_)\n",
    "- 作用：torch.chunk 将张量沿着指定维度分割成多个较小的张量。\n",
    "- 分割维度：指定沿着哪个维度进行分割。\n",
    "- 分割大小：指定每个小张量的大小。\n",
    "- 输入要求：除了分割维度外，其他维度的大小必须相同。\n",
    "x = torch.arange(12).reshape(2, 6)\n",
    "\n",
    "x, torch.chunk(x, chunks=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 0: tensor([0, 1, 2, 3])\n",
      "块 1: tensor([4, 5, 6, 7])\n",
      "块 2: tensor([8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "tensor = torch.arange(10)\n",
    "chunks = torch.chunk(tensor, 3)  # 分成3块\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"块 {i}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.11. <a id='toc7_3_2_11_'></a>[拼接 (stack)](#toc0_)\n",
    "- 作用：torch.stack 将一组张量沿一个`新维度`拼接，这个新维度是会在指定的位置创建出来的。\n",
    "- 增加新维度：输出的张量的总维度会比输入张量`多一维`。\n",
    "- 输入要求：所有输入张量必须形状`完全相同`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# 使用相同的张量a和b\n",
    "# 两个形状相同的张量\n",
    "a = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "\n",
    "b = torch.tensor([[5, 6], \n",
    "                  [7, 8]])\n",
    "\n",
    "stack_result = torch.stack((a, b), dim=0)\n",
    "print(stack_result)\n",
    "# 输出:\n",
    "# tensor([[[1, 2],\n",
    "#          [3, 4]],\n",
    "# \n",
    "#         [[5, 6],\n",
    "#          [7, 8]]])\n",
    "\n",
    "# 增加了新的第0维度，结果形状为 (2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.11.1. <a id='toc7_3_2_11_1_'></a>[cat和stack的比较](#toc0_)\n",
    "|操作\t|torch.cat\t|torch.stack|\n",
    "|-|-|-|\n",
    "|作用\t|沿`现有`维度拼接张量\t|在`新`维度上拼接张量|\n",
    "|维度\t|`不增加新维度`，输出张量与输入张量维度相同\t|`增加新维度`，输出张量比输入张量多一维|\n",
    "|输入要求\t|`除了拼接维度外，其他维度大小必须相同`\t|`所有输入张量的形状必须完全相同`|\n",
    "|用例\t|沿着现有维度连接多组数据\t|将多组相同形状的数据堆叠为一个`新张量`|\n",
    "\n",
    "torch.cat 更适合在相同维度上拼接数据，而 torch.stack 则用于将数据沿新维度进行组织。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.12. <a id='toc7_3_2_12_'></a>[广播 (expand)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 功能：通过改变张量的视图（view）来扩展张量的维度。\n",
    "- 特点：\n",
    "    - 不会复制数据。\n",
    "    - 只改变张量的视图，使其在需要的维度上 \"看起来\" 是扩展的。\n",
    "    - 扩展的维度必须是` 1` 或者是`可以广播`的。\n",
    "- 用途：适用于需要广播操作的情况，可以减少内存开销。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 广播机制：在进行广播操作时，expand 可以用于将一个较小的张量扩展为与另一个张量相同的形状。\n",
    "- 不会复制数据，只改变张量的视图，使其在需要的维度上 \"看起来\" 是扩展的，扩展的维度必须是 1 或者是可以广播的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 假设有一个形状为 (5, 1) 的张量\n",
    "a = torch.tensor([[1], \n",
    "                  [2], \n",
    "                  [3], \n",
    "                  [4], \n",
    "                  [5]])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3],\n",
       "        [4, 4, 4],\n",
       "        [5, 5, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 扩展为形状 (5, 3)，以便与形状为 (5, 3) 的张量进行运算\n",
    "a_expanded = a.expand(5, 3)\n",
    "\n",
    "a_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 总结：\n",
    "    - 不复制数据：expand 不会实际复制数据，而是通过调整步长来实现扩展。这意味着扩展后的张量与原始张量共享相同的数据。\n",
    "    - 只能扩展大小为 1 的维度：expand 只能扩展那些原始大小为 1 的维度。如果尝试扩展其他维度，会引发错误。\n",
    "    - expand 是一个高效的操作，用于在不复制数据的情况下扩展张量的维度。它在需要进行广播操作或匹配特定形状要求时非常有用。通过理解 expand 的工作原理和应用场景，可以更好地利用它来优化张量操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.13. <a id='toc7_3_2_13_'></a>[repeat](#toc0_)\n",
    "\n",
    "- 功能：通过真正复制数据来重复张量的内容。\n",
    "- 特点：\n",
    "    - 会实际分配新的内存来存储重复的数据。\n",
    "    - 数据被实际复制，因此内存占用会增加。\n",
    "- 用途：适用于需要明确复制张量内容的情况。\n",
    "- 注意：repeat 会创建一个新的张量，与原始张量无关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 假设有一个形状为 (5, 1) 的张量\n",
    "a = torch.tensor([[1], \n",
    "                  [2], \n",
    "                  [3], \n",
    "                  [4], \n",
    "                  [5]])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5]]),\n",
       " tensor([[1, 1],\n",
       "         [2, 2],\n",
       "         [3, 3],\n",
       "         [4, 4],\n",
       "         [5, 5]]))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 表示每个维度的重复次数。可以是多个整型参数，也可以是一个整型元组\n",
    "# 每个维度指定一个整数，表示在该维度上重复多少次。\n",
    "# 重复后的维度大小为原始大小乘以该维度的重复次数。\n",
    "## dim0重复2次，dim1重复1次\n",
    "y = a.repeat((2, 1))\n",
    "## dim0重复1次，dim1重复2次\n",
    "y1 = a.repeat(1, 2)\n",
    "\n",
    "y, y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.14. <a id='toc7_3_2_14_'></a>[repeat_interleave](#toc0_)\n",
    "- 功能：将张量的元素重复指定次数，并返回一个新的张量。\n",
    "- 参数：\n",
    "    - input: 要重复的张量。\n",
    "    - repeats: 每个元素重复的次数。\n",
    "    - dim: 沿着哪个维度进行重复。\n",
    "- 返回：一个新的张量，形状与输入张量相同，但元素被重复。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,\n",
       "          9,  9, 10, 10, 11, 11]),\n",
       " tensor([[ 0,  1,  2,  3],\n",
       "         [ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[ 0,  0,  1,  1,  2,  2,  3,  3],\n",
       "         [ 4,  4,  5,  5,  6,  6,  7,  7],\n",
       "         [ 8,  8,  9,  9, 10, 10, 11, 11]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(12).reshape(3, 4)\n",
    "\n",
    "x, torch.repeat_interleave(x, repeats=2), torch.repeat_interleave(x, repeats=2, dim=0), torch.repeat_interleave(x, repeats=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.14.1. <a id='toc7_3_2_14_1_'></a>[expand和repeat对比](#toc0_)\n",
    "- 总结\n",
    "  - 如果只是为了广播操作，建议使用 expand，效率更高。\n",
    "  - 如果需要实际的数据重复，使用 repeat。\n",
    "\n",
    "- 比较\n",
    "\n",
    "  |特性|expand|repeat|\n",
    "  |-|-|-|\n",
    "  |内存占用|低，只是改变视图|高，数据实际复制|\n",
    "  |数据共享|是，返回共享内存的视图|否，返回一个新的张量|\n",
    "  |使用场景|适用于广播操作，不需要实际复制数据|适用于需要显式复制数据的情况|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.15. <a id='toc7_3_2_15_'></a>[填充padding和打包packing](#toc0_)\n",
    "\n",
    "在 PyTorch 中，处理变长序列时，常用以下函数来进行填充（padding）和打包（packing）操作:\n",
    "  - pad_sequence 用于将序列填充到相同长度，\n",
    "  - pack_padded_sequence 将填充后的序列压紧以去除填充部分，经过 RNN 处理后，使用 pad_packed_sequence 将压紧的序列解包恢复，\n",
    "  - torch.nn.functional.pad 则用于对张量进行一般性的填充操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "torch.nn.functional.pad(input, pad, mode='constant', value=0)   # 用于在张量的指定维度前后(上下左右)添加数值(padding几个单位距离)，以改变其形状。\n",
    "# input：需要填充的 N 维张量。\n",
    "# pad：一个包含偶数个元素的元组，表示各维度的填充长度。\n",
    "# mode：填充模式，可选 'constant'（常数填充）、'reflect'（反射填充）等。\n",
    "# value：仅在 mode='constant' 时有效，表示填充值。\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0],\n",
       "         [0, 1, 2, 0],\n",
       "         [0, 3, 4, 0],\n",
       "         [0, 0, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 2, 0, 0],\n",
       "         [0, 0, 3, 4, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "x = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "\n",
    "F.pad(input=x, pad=(1, 1, 1, 1), mode='constant', value=0), F.pad(input=x, pad=(2, 2, 2, 2), mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)     # 用于将一系列可变长度的张量填充为相同长度，以便进行批处理。\n",
    "# sequences：变长序列的列表。\n",
    "# batch_first：如果为 True，输出形状为 (batch_size, max_length, *)，否则为 (max_length, batch_size, *)。\n",
    "# padding_value：填充值，默认值为 0。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 0],\n",
       "        [6, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils import rnn \n",
    "\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5])\n",
    "c = torch.tensor([6])\n",
    "\n",
    "padded = rnn.pad_sequence(sequences=[a, b, c], batch_first=True, padding_value=0)\n",
    "\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True, enforce_sorted=False)   # 用于将填充后的序列压紧，去除填充部分，以提高 RNN 的计算效率。\n",
    "# input：填充后的序列张量。\n",
    "# lengths：每个序列的实际长度列表。\n",
    "# batch_first：如果为 True，输入形状应为 (batch_size, max_length, *)。\n",
    "# enforce_sorted：如果为 True，序列应按长度递减排序。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [4, 5, 0],\n",
       "         [6, 0, 0]]),\n",
       " PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([0, 1, 2]), unsorted_indices=tensor([0, 1, 2])))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils import rnn \n",
    "\n",
    "\n",
    "lengths = torch.tensor([3, 2, 1])\n",
    "\n",
    "padded_packed = rnn.pack_padded_sequence(input=padded, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "padded, padded_packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=True, padding_value=0, total_length=None)    # 用于将压紧的序列解包，恢复为填充后的形式，便于后续处理。\n",
    "# sequence：PackedSequence 对象。\n",
    "# batch_first：如果为 True，输出形状为 (batch_size, max_length, *)。\n",
    "# padding_value：填充值，默认值为 0。\n",
    "# total_length：如果不是 None，输出将被填充到该长度。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([0, 1, 2]), unsorted_indices=tensor([0, 1, 2])),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 0],\n",
       "         [6, 0, 0]]),\n",
       " tensor([3, 2, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils import rnn  \n",
    "\n",
    "unpacked, unpacked_lengths = rnn.pad_packed_sequence(sequence=padded_packed, batch_first=True, padding_value=0, total_length=None)\n",
    "padded_packed, unpacked, unpacked_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. <a id='toc7_4_'></a>[线性代数运算](#toc0_)\n",
    "PyTorch的运算很大一块是`线性代数运算-矩阵运算`，所以需要搞清楚每一步计算前后矩阵的`形状/维度`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. <a id='toc7_4_1_'></a>[数值运算](#toc0_)\n",
    "\n",
    "- 自动做广播：\n",
    "    - x, y的size维度对应的维度数值必须为`无 (不是0)`或`1`，才能被广播。\n",
    "\n",
    "\n",
    "|操作|函数|\n",
    "|:-|:-|\n",
    "|+|torch.add(X, Y)|\n",
    "|-|torch.sub(X, Y)|\n",
    "|*|torch.mul(X, Y|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.],\n",
       "         [12., 13., 14.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5, 3, dtype=torch.float32)\n",
    "y = torch.arange(0, 15, 1, dtype=torch.float32).reshape(5, 3)\n",
    "z = torch.arange(0, 15, 1, dtype=torch.float32).reshape(3, 5)\n",
    "\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.],\n",
       "         [ 7.,  8.,  9.],\n",
       "         [10., 11., 12.],\n",
       "         [13., 14., 15.]]),\n",
       " tensor([[ 1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.],\n",
       "         [ 7.,  8.,  9.],\n",
       "         [10., 11., 12.],\n",
       "         [13., 14., 15.]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y, torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2, 1, 1, 9, 9]),\n",
       " torch.Size([10, 1, 9, 9, 1, 1]),\n",
       " torch.Size([10, 2, 9, 9, 9, 9]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动做广播 (很重要)\n",
    "# 两个tensor的维度数完全相同，但对应为维度数值不同时：\n",
    "# x或y中的一个必须是1，才能被自动做广播。\n",
    "\n",
    "x = torch.randn(size=(10, 2, 1, 1, 9, 9))   # 被自动广播成 (10, 2, 9, 9, 9, 9)\n",
    "y = torch.randn(size=(10, 1, 9, 9, 1, 1))   # 被自动广播成 (10, 2, 9, 9, 9, 9)\n",
    "\n",
    "x.size(), y.size(), (x+y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2, 1, 1, 9, 9]),\n",
       " torch.Size([9, 1, 1]),\n",
       " torch.Size([10, 2, 1, 9, 9, 9]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动做广播 (很重要)\n",
    "# 两个tensor的维度数不同，且对应为维度数值不同时：\n",
    "# x或y中的一个必须是1，才能被自动做广播，且\n",
    "# 短的维度会被自动广播成长的一样。\n",
    "\n",
    "x = torch.randn(size=(10, 2, 1, 1, 9, 9))   # 被自动广播成 (10, 2, 1, 9, 9, 9)\n",
    "y = torch.randn(size=(9, 1, 1))             # 被自动广播成 (10, 2, 1, 9, 9, 9)\n",
    "\n",
    "x.size(), y.size(), (x+y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1.,   0.,  -1.],\n",
       "         [ -2.,  -3.,  -4.],\n",
       "         [ -5.,  -6.,  -7.],\n",
       "         [ -8.,  -9., -10.],\n",
       "         [-11., -12., -13.]]),\n",
       " tensor([[  1.,   0.,  -1.],\n",
       "         [ -2.,  -3.,  -4.],\n",
       "         [ -5.,  -6.,  -7.],\n",
       "         [ -8.,  -9., -10.],\n",
       "         [-11., -12., -13.]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y, torch.sub(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * 3 # 数乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(x, 3) # 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, z) # 矩阵相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. <a id='toc7_4_2_'></a>[数值运算-乘法](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.1. <a id='toc7_4_2_1_'></a>[哈达玛积](#toc0_)\n",
    "* 按照`元素`进行乘法\n",
    "* 乘前形状必须相同，乘后不改变形状\n",
    "* x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 0,  1,  4],\n",
       "         [ 9, 16, 25]]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(6).reshape(2, 3)\n",
    "\n",
    "x, x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.2. <a id='toc7_4_2_2_'></a>[点积（Dot Product）](#toc0_)\n",
    "* 按照元素进行乘法后相加\n",
    "* 乘前形状一样，乘后`标量`\n",
    "* `torch.dot(x, x)` # dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([0, 1, 4]), tensor(5))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.arange(3)\n",
    "\n",
    "x, x * x, torch.dot(x, x) # 打印， 哈德玛积， 点积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.3. <a id='toc7_4_2_3_'></a>[矩阵-向量积](#toc0_)\n",
    "* 矩阵乘法的特殊\n",
    "* 乘后`向量`\n",
    "* `torch.mv(A, x)` # matrix-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([4]), torch.Size([3]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12, dtype= torch.float32).reshape(3, 4)\n",
    "x = torch.ones(4, dtype= torch.float32)\n",
    "\n",
    "A.shape, x.shape, torch.mv(A, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]),)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A * x).shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.4. <a id='toc7_4_2_4_'></a>[矩阵-矩阵积](#toc0_)\n",
    "* 乘后**矩阵**\n",
    "* `torch.matmul(X, Y)`  # 矩阵乘法，`支持广播`\n",
    "* `X @ Y`               # 同上\n",
    "* `torch.mm(X, Y)`      # 矩阵乘法，`不支持广播`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]),\n",
       " torch.Size([5, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15).reshape(3, 5)\n",
    "Y = torch.arange(15).reshape(5, 3)\n",
    "\n",
    "X.shape, Y.shape, (X @ Y).shape, torch.mm(X, Y).shape, torch.matmul(X, Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.5. <a id='toc7_4_2_5_'></a>[批量矩阵乘法](#toc0_)\n",
    "\n",
    "* A: (b x n x m) \n",
    "* B: (b x m x p)\n",
    "* `torch.bmm(A, B)`   # b x n x p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 5]), torch.Size([3, 5, 3]), torch.Size([3, 3, 3]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(45).reshape(3, 3, 5)\n",
    "Y = torch.arange(45).reshape(3, 5, 3)\n",
    "\n",
    "X.shape, Y.shape, torch.bmm(X, Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.6. <a id='toc7_4_2_6_'></a>[乘总结](#toc0_)\n",
    "\n",
    "参考PyTorch lightning 总结：[https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/01-introduction-to-pytorch.html](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/01-introduction-to-pytorch.html)\n",
    "\n",
    "|乘法|函数|\n",
    "|:-|:-|\n",
    "|哈德玛积|A * B|\n",
    "|点积|dot(A, B)|\n",
    "|矩阵-向量|mv(A, x)|\n",
    "|矩阵-矩阵|matmul(A, B) 或  A @ B，同时mm(A, B)不支持广播|\n",
    "|批量矩阵乘法|`bmm(A, B)`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3. <a id='toc7_4_3_'></a>[统计运算](#toc0_)\n",
    "\n",
    "|操作|注释|\n",
    "|:-|:-|\n",
    "|torch.mean()|取平均|\n",
    "|torch.median()||\n",
    "|torch.mode()||\n",
    "|torch.min()||\n",
    "|torch.max()||\n",
    "|torch.std()||\n",
    "|torch.var()||\n",
    "|torch.squar()||\n",
    "|torch.`argmax()`||\n",
    "|torch.`argmin()`||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14.])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15, dtype= torch.float32)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.), tensor(7.))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x), x.mean() # 平均数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.), tensor(7.))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.median(x), x.median() # 中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(x), x.min()   # 最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14.), tensor(14.))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x), x.max()   # 最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.mode(\n",
       " values=tensor(0.),\n",
       " indices=tensor(0)),\n",
       " torch.return_types.mode(\n",
       " values=tensor(0.),\n",
       " indices=tensor(0)))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mode(x), x.mode() # 众数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.4721), tensor(4.4721))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(x), x.std()   # 标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(20.), tensor(20.))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(x), x.var()   # 方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]),\n",
       " tensor([[2, 2, 2, 2, 2]]),\n",
       " tensor([[4],\n",
       "         [4],\n",
       "         [4]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15).reshape(3, 5)\n",
    "\n",
    "# dim = 0, 表示从上往下\n",
    "# dim = 1, 表示从左往右\n",
    "# keepdim = True, 表示保持原始维度信息\n",
    "\n",
    "x, torch.argmax(x, dim=0, keepdim=True), torch.argmax(x, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 2, 2, 2]), tensor([4, 4, 4]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim = False， 表示丢弃原始维度信息\n",
    "\n",
    "torch.argmax(x, dim=0, keepdim=False), torch.argmax(x, dim=1, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == torch.argmax(x, dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[torch.argmax(x, dim=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[torch.argmax(x, dim=0)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. <a id='toc7_5_'></a>[广播机制 (Broadcasting)](#toc0_)\n",
    "\n",
    "PyTorch中的广播机制是指在进行张量运算时，如果两个张量的形状不完全相同但可以通过扩展其中一个张量的尺寸来使它们能够兼容地进行操作，则这个过程就被称为广播（Broadcasting）。这种机制使得不同形状的张量可以一起进行数学运算，而不需要显式地调整张量的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. <a id='toc7_5_1_'></a>[广播规则](#toc0_)\n",
    "\n",
    "1. `从右向左`比较两个张量的维度大小。\n",
    "2. 如果两个维度大小`相等`，或者某一方的维度大小为`1`，则可以进行广播。\n",
    "3. 如果遇到维度大小不一致的情况，并且不符合上述条件，则无法进行广播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **标量与张量的运算**\n",
    "\n",
    "   如果一个标量与一个张量进行运算，那么该标量会被广播到张量的各个元素上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size: torch.Size([3])\n",
      "tensor([2., 4., 6.]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "scalar = 2\n",
    "tensor = torch.tensor([1., 2., 3.])\n",
    "print(f'tensor size: {tensor.shape}')\n",
    "\n",
    "result = scalar * tensor\n",
    "\n",
    "print(result, result.shape)  # 输出 tensor([2., 4., 6.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **维度大小为1的张量**\n",
    "\n",
    "   如果一个张量的一个维度大小为1，那么这个维度可以被扩展到匹配另一个张量的相应维度大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1 size: torch.Size([3, 1])\n",
      "tensor2 size: torch.Size([3])\n",
      "tensor([[2., 3., 4.],\n",
      "        [3., 4., 5.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1.], [2.], [3.]])  # 形状为 (3, 1) 表示： 3 x 1\n",
    "print(f'tensor1 size: {tensor1.shape}')\n",
    "tensor2 = torch.tensor([1., 2., 3.])        # 形状为 (3,)   表示： 1 x 3\n",
    "print(f'tensor2 size: {tensor2.shape}')\n",
    "result = tensor1 + tensor2\n",
    "\n",
    "print(result)  # 输出 tensor([[2., 3., 4.],\n",
    "#                     [3., 4., 5.],\n",
    "#                     [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **不同维度的张量**\n",
    "\n",
    "   当两个张量的维度不同时，较小的张量会在前面添加维度大小为1的维度，然后进行广播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1 size: torch.Size([128, 10, 20, 100])\n",
      "tensor2 size: torch.Size([1, 100])\n",
      "torch.Size([128, 10, 20, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(size=(128, 10, 20, 100))    # 形状为 (128, 10, 20, 100)        \n",
    "print(f'tensor1 size: {tensor1.shape}')\n",
    "\n",
    "# tensor2 = torch.randn(size=(100,))                # 形状为 (1, 100)  表示: 1 x 100，广播为：(128, 10, 20, 100) \n",
    "tensor2 = torch.randn(size=(1,100))                # 形状为 (1, 100)  表示: 1 x 100，广播为：(128, 10, 20, 100) \n",
    "print(f'tensor2 size: {tensor2.shape}')\n",
    "\n",
    "result = tensor1 + tensor2\n",
    "\n",
    "print(result.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **不兼容的维度**\n",
    "\n",
    "   如果两个张量的对应维度大小不一致，并且不能通过扩展为1来解决，那么就不能进行广播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1., 2.], [3., 4.]])  # 形状为 (2, 2), 2 x 2\n",
    "tensor2 = torch.tensor([1., 2., 3.])          # 形状为 (3,),   1 x 3\n",
    "# 下面的代码会抛出错误\n",
    "try:\n",
    "    result = tensor1 + tensor2\n",
    "except RuntimeError as e:\n",
    "    print(e)  # 张量形状不匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 小结\n",
    "\n",
    "PyTorch中的广播机制允许开发人员使用更简洁的代码来处理不同形状的张量之间的运算。这种机制在实现复杂的神经网络架构时非常有用，因为它减少了手动调整张量形状的需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. <a id='toc7_6_'></a>[Pytorch的计算图 和 自动微分 (autograd)](#toc0_)\n",
    "\n",
    "- PyTorch是动态图，即`计算图 (有向无环图（DAG），每个节点表示一个张量或运算)`的搭建和运算是同时的，随时可以输出结果；而TensorFlow是静态图。\n",
    "\n",
    "- 在pytorch的计算图里只有两种元素：`数据（tensor）`和 `运算（operation）`\n",
    "\n",
    "  - 运算包括了：加减乘除、开方、幂指对、三角函数等可求导运算\n",
    "\n",
    "  - 数据可分为：`叶子节点（leaf node`）和`非叶子节点`；\n",
    "    - 叶子节点：计算图的起点，是直接由用户创建的张量，通常具有 requires_grad=True 属性，用于存储梯度信息；可以通过 `is_leaf` 属性判断某个张量是否为叶子节点\n",
    "    - 非叶子节点：由叶子节点`通过运算生成的中间张量`，不直接存储梯度，这些张量的 grad 属性默认是 None，但可以通过 retain_grad() 方法显式保存它们的梯度。\n",
    "    - 叶子节点是用户创建的节点，不依赖其它节点；它们表现出来的区别在于`用y.backward()进行反向传播`结束之后，`非叶子节点的梯度会被释放掉`，只保留叶子节点的梯度，这样就节省了内存。如果想要保留非叶子节点的梯度，可以使用`retain_grad()`方法。\n",
    "\n",
    "- torch.tensor节点 具有如下属性：\n",
    "  - 查看 是否为叶子节点 `is_leaf`\n",
    "  - 查看 是否可以求导 `requires_grad`\n",
    "  - 查看 运算名称 `grad_fn`\n",
    "  - 查看 导数值 `grad`\n",
    "\n",
    "- 针对requires_grad属性，自己定义的叶子节点默认为False，而非叶子节点默认为True，神经网络中的权重默认为True。判断哪些节点是True/False的一个原则就是从你需要求导的叶子节点到loss节点之间是一条可求导的通路。\n",
    "\n",
    "---\n",
    "\n",
    "- PyTorch提供两种求梯度的方法：`backward()` 和 `torch.autograd.grad()` ，他们的区别在于前者是给叶子节点填充.grad字段，而后者是直接返回梯度给你，我会在后面举例说明。还需要知道y.backward()其实等同于`torch.autograd.backward(y)`。\n",
    "\n",
    "\n",
    "![PyTorch的计算图](./Pytorch_Pictures/PyTorch_graphacial_demo/graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1. <a id='toc7_6_1_'></a>[反向传播 (backward)-批量求梯度，但未进行参数更新](#toc0_)\n",
    "\n",
    "计算`所有节点 (Tensor)` 的梯度并存储在节点的`grad属性中`，但未进行节点参数更新 (是优化函数干的事)。\n",
    "\n",
    "- `y.backward()` 或 `torch.autograd.backward(y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]], requires_grad=True),\n",
       " tensor([[-0.7109,  0.6400,  0.1948, -0.3200,  1.3037],\n",
       "         [-1.3823,  1.3554,  0.3006,  1.3434,  1.2845],\n",
       "         [-0.1937, -0.7454, -1.6233, -0.3062, -0.8822]], requires_grad=True),\n",
       " tensor(16.0991, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.ones(size= (3, 5), dtype= torch.float32, requires_grad= True)       # 自定义需要存储梯度\n",
    "x2 = torch.randn(size= (3, 5), dtype= torch.float32, requires_grad= True)      # 默认是不存储梯度\n",
    "\n",
    "y = torch.add(x1**2, x2**3).sum()   # 应变量必须是标量\n",
    "\n",
    "x1, x2, y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 查看is_leaf属性\n",
    "x1.is_leaf, x2.is_leaf, y.is_leaf   \n",
    "# x1, x2是叶子节点，y不是叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 查看requires_grad属性\n",
    "x1.requires_grad, x2.requires_grad, y.requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, <SumBackward0 at 0x7fce06bff0d0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 查看grad_fn属性\n",
    "x1.grad_fn, x2.grad_fn, y.grad_fn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32820/1085795512.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  x1.grad, x2.grad, y.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 查看grad属性\n",
    "x1.grad, x2.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "# torch.autograd.backward(y)  # 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32820/1085795512.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  x1.grad, x2.grad, y.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]]),\n",
       " tensor([[4.0998e+00, 3.6832e+00, 5.1059e+00, 5.9216e-05, 6.2451e+00],\n",
       "         [2.4456e+00, 3.4693e+00, 9.5398e-02, 7.5255e-01, 9.7835e-03],\n",
       "         [7.6321e+00, 7.4899e-01, 8.9373e-01, 2.3219e+00, 2.8837e+00]]),\n",
       " None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 查看grad属性\n",
    "x1.grad, x2.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2. <a id='toc7_6_2_'></a>[仅计算梯度 (求导计算)](#toc0_)\n",
    "\n",
    "和backward不同，torch.autograd.grad只是计算`应变量 (output)` 对`自变量 (input)`的`导数 (梯度)`；`应变量必须是标量`。\n",
    "\n",
    "- `torch.autograd.grad(output=y, input=x, retain_grad=False/True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.]],\n",
       " \n",
       "         [[3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.]],\n",
       " \n",
       "         [[3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.]]]),)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(size= (3, 3, 5), dtype= torch.float32, requires_grad= True) # 必须是float类型\n",
    "y = (x**3).sum()\n",
    "\n",
    "torch.autograd.grad(outputs= y, inputs= x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7. <a id='toc7_7_'></a>[自动微积-autograd](#toc0_)\n",
    "```shell\n",
    "深度学习框架可以自动计算导数：\n",
    "```\n",
    "|操作|函数|\n",
    "|:-|:-|\n",
    "|1. 我们首先将梯度附加到想要对其计算偏导数的变量上，|x.requires_grad_(True)|\n",
    "|2. 然后记录目标值的计算，|y = x * x (grad_fn)|\n",
    "|3. 执行它的反向传播函数(求梯度)，|y.backward()|\n",
    "|4. 并访得到的梯度。|x.grad|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1. <a id='toc7_7_1_'></a>[自己探索](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.1. <a id='toc7_7_1_1_'></a>[标量-一阶导数（得标量）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2., requires_grad=True), tensor(4., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, dtype=torch.float32, requires_grad=True)  # 标量\n",
    "y = x**2    \n",
    "                                                    # 标量\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为None\n",
    "x.grad, x.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y对x进行求导\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为2 * 2 = 4\n",
    "x.grad                                                          # 标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 2*x # y关于x的一阶导函数就是2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造新的关于x的函数：z = x**3\n",
    "z = x**2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为：\n",
    "x.grad.zero_()\n",
    "x.grad # 应该为0才对，需要手动清零# x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z关于x求导\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为：\n",
    "x.grad # 应该为4，但是残留的4 + 本次的4 = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.2. <a id='toc7_7_1_2_'></a>[标量/向量-一阶导数（得向量）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0, dtype=torch.float32, requires_grad=True)  # 向量\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14., grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.dot(x, x)                                              # 标量\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "\n",
    "x.grad                                                          # 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.3. <a id='toc7_7_1_3_'></a>[向量/向量-一阶导数（得矩阵）](#toc0_)\n",
    "\n",
    "- pytorch只能对标量/标量，标量/向量求导，`即x可以为标量也可以为向量，但是y必须为标量`\n",
    "\n",
    "- `只需要先将y转变为标量，对分别求导没影响的就是求和`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.arange(4, dtype=torch.float32, requires_grad=True)    # 向量\n",
    "\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = i ** 2                                                      # 向量\n",
    "\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h.backward()      # 报错\n",
    "h.sum().backward()  # 正常\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.4. <a id='toc7_7_1_4_'></a>[求高阶导数](#toc0_)\n",
    "\n",
    "- 利用`torch.autograd.grad(outputs=y, inputs=x, create_grad=True)`\n",
    "\n",
    "- 保留计算图 (链表指针), `create_grad=True `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12., grad_fn=<MulBackward0>),)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2, dtype=torch.float32, requires_grad=True)\n",
    "y = x**3\n",
    "grad1 = torch.autograd.grad(outputs=y, inputs=x, create_graph=True) # create_graph=True, 必须保留计算图才能进行后续的高阶导数计算\n",
    "\n",
    "grad1 # 3 * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12., grad_fn=<MulBackward0>),)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad2 = torch.autograd.grad(outputs=grad1, inputs=x, create_graph=True)\n",
    "\n",
    "grad2 # 6 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.),)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad3 = torch.autograd.grad(outputs=grad2, inputs=x)\n",
    "\n",
    "grad3 # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2. <a id='toc7_7_2_'></a>[一个简单的例子](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 定义变量\n",
    "x = torch.arange(4.0)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在我们计算关于的梯度之前，需要一个地方来存储梯度。\n",
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "\n",
    "x.grad                  # 默认值是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在计算。\n",
    "y = 2 * torch.dot(x, x)\n",
    "\n",
    "y                       # x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。\n",
    "y.backward()            # [4x, 4x, 4x, 4x] 导函数\n",
    "\n",
    "x.grad                  # [4*0, 4*1, 4*2, 4*3] 导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x         # [4x, 4x, 4x, 4x] 导函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.3. <a id='toc7_7_3_'></a>[计算另一个](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor(6., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.4. <a id='toc7_7_4_'></a>[非标量变量的反向传播](#toc0_)\n",
    "\n",
    "- 当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 \n",
    "\n",
    "- 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。\n",
    "\n",
    "- 然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的**偏导数之和**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x * x \n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0.]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, tensor([0., 2., 4., 6.]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum().backward(), x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.5. <a id='toc7_7_5_'></a>[分离计算](#toc0_)\n",
    "\n",
    "- 有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "\n",
    "- 这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.6. <a id='toc7_7_6_'></a>[Python控制流的梯度计算](#toc0_)\n",
    "\n",
    "- 使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们计算梯度。\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a，因此可以用d/a验证梯度是否正确。\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8. <a id='toc7_8_'></a>[概率论](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正太函数分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7607, -0.3525, -0.6538,  0.3404, -1.0323, -0.4394,  0.0676,  0.7667,\n",
      "         1.7211, -0.0475, -1.1181,  0.7353, -1.7605,  1.4901, -0.9803,  0.8902,\n",
      "        -0.0931, -1.7484,  2.3997,  0.6524,  0.6782, -1.1440, -1.3923,  0.1212,\n",
      "        -1.0076, -1.7668, -1.4322, -0.6901,  0.2830,  0.5470,  1.4634,  0.6256,\n",
      "         1.2161, -1.3545, -1.2281, -0.6693,  0.2557,  0.2750, -0.1981,  0.4620,\n",
      "         0.5137, -0.3635,  0.7580,  0.6187, -2.0609, -1.9659, -0.0752,  0.7554,\n",
      "        -0.6792, -1.2573, -0.1298, -0.4564,  0.3095,  1.2856, -0.7012,  0.5607,\n",
      "        -1.0115,  1.1368,  1.0839,  1.5874, -1.1725,  0.8335, -1.8986,  1.1627,\n",
      "         1.5963, -1.7788,  1.4887,  2.6236,  0.0521, -0.5584, -1.2956, -1.0912,\n",
      "         1.0101,  0.6228,  0.3619,  1.4112,  0.1833,  0.4523, -0.5056,  0.2020,\n",
      "         1.4686,  1.8315, -0.8283,  0.6796, -0.1077,  0.0794, -0.2321,  1.2689,\n",
      "        -0.5188,  0.6315,  1.2953, -0.0427,  0.0622, -0.6244, -0.6351, -1.3894,\n",
      "         0.1629, -0.7895, -0.0437,  1.6747])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f350d9c5df0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "data = torch.normal(mean= 0, std= 1, size= (100,), dtype= torch.float32, device= device)\n",
    "print(data)\n",
    "plt.figure()\n",
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[神经网络-训练八股](#toc0_)\n",
    "\n",
    "|步骤|计算|操作|\n",
    "|:-|:-|:-|\n",
    "|1|定义网络模型|->计算出`y_hat`|\n",
    "|2|选择损失函数|->计算`loss值`、求梯度|\n",
    "|3|选择优化器|->`更新`网络权重参数|\n",
    "|4|训练|->实施1、2、3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. <a id='toc8_1_'></a>[现线性回归模型于训练过程-从零开始](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. <a id='toc8_1_1_'></a>[虚拟出数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([1.6710, 0.3170])\n",
      "label: tensor([6.4774])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "\n",
    "\n",
    "def synthetic_data(w, b, num_examples):  \n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(mean= 0, std= 1, size= (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(mean= 0, std= 0.01, size= y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "print('features:', features[0])\n",
    "print('label:', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"231.442187pt\" height=\"169.678125pt\" viewBox=\"0 0 231.442187 169.678125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-05-18T15:08:53.316260</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 169.678125 \n",
       "L 231.442187 169.678125 \n",
       "L 231.442187 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "L 28.942188 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m18bb6ce550\" d=\"M 0 0.5 \n",
       "C 0.132602 0.5 0.25979 0.447317 0.353553 0.353553 \n",
       "C 0.447317 0.25979 0.5 0.132602 0.5 0 \n",
       "C 0.5 -0.132602 0.447317 -0.25979 0.353553 -0.353553 \n",
       "C 0.25979 -0.447317 0.132602 -0.5 0 -0.5 \n",
       "C -0.132602 -0.5 -0.25979 -0.447317 -0.353553 -0.353553 \n",
       "C -0.447317 -0.25979 -0.5 -0.132602 -0.5 0 \n",
       "C -0.5 0.132602 -0.447317 0.25979 -0.353553 0.353553 \n",
       "C -0.25979 0.447317 -0.132602 0.5 0 0.5 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p149bb96678)\">\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.095479\" y=\"47.161377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.851713\" y=\"83.62544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.208597\" y=\"87.238868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.729918\" y=\"40.397538\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.573273\" y=\"74.594754\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.717888\" y=\"60.945456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.324851\" y=\"78.7769\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.745046\" y=\"79.445989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.425785\" y=\"38.908429\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.929806\" y=\"116.402206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.433764\" y=\"88.486938\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.993977\" y=\"81.085971\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.88151\" y=\"87.997522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.448528\" y=\"88.393808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.731191\" y=\"76.554431\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.365738\" y=\"73.519191\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.023584\" y=\"74.321666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.343054\" y=\"93.603985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.480389\" y=\"88.551763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"82.783874\" y=\"35.366294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.40635\" y=\"58.043827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.163633\" y=\"103.28369\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.402431\" y=\"57.861658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.67707\" y=\"81.847122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.810681\" y=\"77.415171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.459828\" y=\"90.031881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"74.291277\" y=\"35.628451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.353451\" y=\"92.726373\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.567219\" y=\"65.921645\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.252503\" y=\"67.827082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.357592\" y=\"73.868138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.556005\" y=\"81.497491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.588391\" y=\"57.255578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.873451\" y=\"56.665809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.53247\" y=\"72.549742\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.363962\" y=\"85.38564\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.858176\" y=\"58.074153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.627228\" y=\"51.205587\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.274568\" y=\"46.708364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.191399\" y=\"111.673949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.057851\" y=\"75.989104\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.467418\" y=\"67.739194\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.631192\" y=\"59.798297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.163912\" y=\"74.989388\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.397586\" y=\"59.375105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.25971\" y=\"59.267597\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.847122\" y=\"93.854999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.1543\" y=\"71.690158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.433433\" y=\"87.617628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.147698\" y=\"63.623364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.640488\" y=\"73.829216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.738102\" y=\"89.36582\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.302647\" y=\"94.878713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.085924\" y=\"98.631002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.452017\" y=\"69.606532\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.770517\" y=\"77.322051\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.949251\" y=\"72.458828\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"75.617311\" y=\"44.551745\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.242985\" y=\"71.231531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.456331\" y=\"87.507944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.942623\" y=\"89.154946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.84967\" y=\"115.023299\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.293778\" y=\"57.941606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.996861\" y=\"64.990028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.079155\" y=\"124.951583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.600268\" y=\"47.266292\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.097375\" y=\"82.798065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"171.793196\" y=\"84.905216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.647189\" y=\"63.342156\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.612985\" y=\"71.311502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.608414\" y=\"71.440978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.656532\" y=\"109.509315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.409678\" y=\"76.930463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.748237\" y=\"81.979596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.86734\" y=\"45.388764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.971797\" y=\"96.679841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.111144\" y=\"39.170635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.522205\" y=\"104.843924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"215.364915\" y=\"136.669454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.039242\" y=\"64.485384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.026192\" y=\"92.242205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.829908\" y=\"58.495525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.715044\" y=\"43.485036\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.536737\" y=\"66.785002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.411851\" y=\"75.943248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.084039\" y=\"105.039855\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.865947\" y=\"90.586554\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.345475\" y=\"70.239975\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.715025\" y=\"86.234712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.761775\" y=\"72.272405\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"90.937862\" y=\"42.120415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.620502\" y=\"52.733002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.71258\" y=\"91.534395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.038561\" y=\"78.277075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.303632\" y=\"80.007046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.680851\" y=\"123.495815\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.102381\" y=\"87.647679\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.983381\" y=\"68.147087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.287017\" y=\"77.973753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.728341\" y=\"102.953634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.255003\" y=\"48.117778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.176357\" y=\"79.685957\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.072006\" y=\"105.065753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.081318\" y=\"94.163776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.789688\" y=\"39.249632\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.093641\" y=\"94.996263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.020821\" y=\"110.657126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.413335\" y=\"56.029453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.839972\" y=\"95.454916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.064537\" y=\"95.256836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.861671\" y=\"79.859375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.424467\" y=\"86.897922\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.523298\" y=\"94.19315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"174.220273\" y=\"100.574648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.411392\" y=\"95.633883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.245321\" y=\"77.128793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.974341\" y=\"42.823713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.346943\" y=\"71.69238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.730968\" y=\"43.229075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.599003\" y=\"84.788153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.921167\" y=\"72.66032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.318025\" y=\"58.450722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"194.960884\" y=\"131.832798\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.851446\" y=\"54.265354\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.738185\" y=\"79.788226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.232769\" y=\"39.581108\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.609002\" y=\"74.271061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.604959\" y=\"96.641912\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.290707\" y=\"95.322537\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.920401\" y=\"37.140992\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.569027\" y=\"88.381482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.631163\" y=\"82.87251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.473124\" y=\"114.644034\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.089957\" y=\"81.887664\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"171.2269\" y=\"110.831563\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.389193\" y=\"79.688158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.145673\" y=\"72.500684\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.439791\" y=\"64.647256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.448923\" y=\"83.772278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.538471\" y=\"126.892836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.310948\" y=\"52.077508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.07802\" y=\"71.031701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.269235\" y=\"77.309642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.764291\" y=\"95.714536\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.771941\" y=\"78.662995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.05873\" y=\"52.853989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.825865\" y=\"85.024525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.04984\" y=\"65.790707\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.985694\" y=\"73.972357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.372596\" y=\"73.366803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.97065\" y=\"103.786181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.344855\" y=\"70.019699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"93.447802\" y=\"46.796222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.990588\" y=\"71.131398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.143886\" y=\"67.727624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.66859\" y=\"87.823199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.899883\" y=\"74.458643\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.576442\" y=\"119.338468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.434056\" y=\"65.114343\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.373461\" y=\"90.545777\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"74.95746\" y=\"32.072086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.379211\" y=\"89.646188\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.659058\" y=\"131.976622\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.916788\" y=\"37.519916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.311539\" y=\"68.471924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.407896\" y=\"58.213502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.98083\" y=\"67.209455\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.578836\" y=\"83.648026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.530611\" y=\"81.247013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.676106\" y=\"78.386436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"188.590076\" y=\"110.099002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.701229\" y=\"79.870857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.85927\" y=\"86.82456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.10514\" y=\"55.800128\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.751938\" y=\"95.543365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.73323\" y=\"80.716385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.229499\" y=\"49.869361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.169676\" y=\"78.313592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"181.657673\" y=\"108.965791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.523107\" y=\"89.407648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.105095\" y=\"102.732158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.184204\" y=\"130.289923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.883837\" y=\"77.297087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.828564\" y=\"77.985982\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.511402\" y=\"86.539024\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.205736\" y=\"88.497173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.420465\" y=\"82.445164\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.183605\" y=\"100.566278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.508009\" y=\"78.2606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.071365\" y=\"104.95213\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.33869\" y=\"90.006435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.497756\" y=\"66.676858\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.538572\" y=\"44.376892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.319541\" y=\"50.800963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.013391\" y=\"73.130755\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.436765\" y=\"90.054365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.851902\" y=\"88.832337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.298429\" y=\"109.265351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.81924\" y=\"105.520126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.125532\" y=\"45.042519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.520817\" y=\"71.352702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.616857\" y=\"87.712444\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.361615\" y=\"93.431903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.955365\" y=\"115.084394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.302701\" y=\"65.664837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.00782\" y=\"41.543072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.22642\" y=\"65.483171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.02213\" y=\"73.541159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.117686\" y=\"107.280884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.369294\" y=\"81.808232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.787771\" y=\"92.773059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.140729\" y=\"110.615348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.002826\" y=\"87.620506\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.425689\" y=\"95.841866\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.446106\" y=\"67.122825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.509686\" y=\"77.513579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.088532\" y=\"65.123268\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.731574\" y=\"108.43805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.832592\" y=\"59.570065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.975699\" y=\"71.318317\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.907336\" y=\"103.523987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.159527\" y=\"43.963427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.55038\" y=\"71.800816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.639005\" y=\"50.486663\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.806314\" y=\"80.491925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.015768\" y=\"101.020893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.847569\" y=\"103.960201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.777105\" y=\"64.961187\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"178.333764\" y=\"108.101154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.172839\" y=\"71.272436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.066135\" y=\"54.533553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.714613\" y=\"77.715356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.050538\" y=\"85.805685\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.294464\" y=\"84.457284\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.123238\" y=\"110.462091\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.150669\" y=\"40.894705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.184716\" y=\"69.718393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.640153\" y=\"99.022385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.863809\" y=\"47.724193\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.626844\" y=\"94.62609\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.241278\" y=\"69.007238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"178.5462\" y=\"115.545579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.599468\" y=\"50.701739\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.598236\" y=\"93.231596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.028281\" y=\"48.494835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.972378\" y=\"60.52896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"175.093025\" y=\"115.579612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.674163\" y=\"57.422106\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"81.816997\" y=\"48.033332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.268201\" y=\"92.460497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.982868\" y=\"94.537197\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.366972\" y=\"105.885217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.981166\" y=\"89.998881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.909952\" y=\"57.369795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.800072\" y=\"109.391435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.3025\" y=\"68.146646\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.112621\" y=\"77.863288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.019004\" y=\"82.309716\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.437598\" y=\"64.998791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.85551\" y=\"62.189196\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.337286\" y=\"90.072432\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.260642\" y=\"75.777629\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.736644\" y=\"88.600963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.817745\" y=\"62.371521\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.558975\" y=\"81.635209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.054067\" y=\"54.280767\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.664876\" y=\"73.743823\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.419599\" y=\"70.005569\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.615409\" y=\"67.434763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.973077\" y=\"76.827061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.851912\" y=\"83.246127\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.465775\" y=\"62.814925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.69414\" y=\"82.432944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.5567\" y=\"66.571063\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.865298\" y=\"61.460029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.895165\" y=\"84.160395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.69307\" y=\"68.818434\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.775156\" y=\"52.552853\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.131616\" y=\"95.612473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.314575\" y=\"60.307027\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.398024\" y=\"65.64585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.260952\" y=\"90.950681\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.303587\" y=\"67.75297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.26854\" y=\"53.050234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"184.810641\" y=\"106.433803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.187906\" y=\"49.907413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.298809\" y=\"81.185502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.691714\" y=\"64.86224\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.551724\" y=\"63.739985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"190.442405\" y=\"101.59375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.636939\" y=\"110.318736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"189.434988\" y=\"122.525807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"83.010608\" y=\"38.326326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.135154\" y=\"91.321178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.262778\" y=\"77.393835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.275451\" y=\"49.56985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.832572\" y=\"85.533263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.746231\" y=\"72.671207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.820777\" y=\"99.386596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.683545\" y=\"67.775845\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.196624\" y=\"74.056585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.203098\" y=\"75.661508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.907138\" y=\"61.146314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.719176\" y=\"105.44496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"90.349765\" y=\"37.429393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.196461\" y=\"78.800497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.30426\" y=\"76.243697\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.995264\" y=\"65.434945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.206289\" y=\"95.744662\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.555366\" y=\"60.535636\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.926729\" y=\"87.970989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.454368\" y=\"86.192893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.242903\" y=\"57.788411\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.740043\" y=\"75.358756\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.248791\" y=\"35.551553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.69951\" y=\"78.30334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.329754\" y=\"64.711556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.247098\" y=\"101.88973\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.406229\" y=\"97.819789\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.031335\" y=\"88.734081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.843379\" y=\"45.330227\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.434758\" y=\"44.209348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.13027\" y=\"76.632356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.58724\" y=\"108.769978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.484005\" y=\"100.102314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.946035\" y=\"88.620482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.437023\" y=\"96.788656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.015895\" y=\"60.901465\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.169374\" y=\"93.607883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.803278\" y=\"74.933473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.571926\" y=\"82.22746\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.47718\" y=\"95.62631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.262069\" y=\"60.61157\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.718515\" y=\"81.496522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.181732\" y=\"41.516842\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.331169\" y=\"88.36295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.890814\" y=\"70.030588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.969004\" y=\"78.772412\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.483839\" y=\"69.940603\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.305641\" y=\"72.17007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.064401\" y=\"108.453568\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.168083\" y=\"66.977319\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"186.070028\" y=\"121.685772\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.440592\" y=\"96.364357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.615906\" y=\"83.36262\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.334251\" y=\"77.685671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.832708\" y=\"77.021255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.584402\" y=\"81.933449\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.030805\" y=\"56.89179\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.2568\" y=\"109.930138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"81.284677\" y=\"46.477058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.833589\" y=\"107.725896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.628025\" y=\"95.836344\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.51822\" y=\"25.80849\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.05078\" y=\"83.93642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.24022\" y=\"41.798879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"176.812922\" y=\"100.699816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"206.112711\" y=\"132.059178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.459017\" y=\"61.119264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.340598\" y=\"102.003628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.568646\" y=\"90.938355\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"178.466042\" y=\"135.076353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.576191\" y=\"89.697253\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.493513\" y=\"84.102082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.957632\" y=\"70.643879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.199816\" y=\"77.271077\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"175.249178\" y=\"118.26173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.033036\" y=\"100.42625\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.798255\" y=\"85.449113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.32953\" y=\"91.51232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.504324\" y=\"98.445219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.286187\" y=\"69.827219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.735623\" y=\"96.551888\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.230718\" y=\"69.991721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.659505\" y=\"77.144345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.011388\" y=\"53.943991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.674035\" y=\"42.491325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.566322\" y=\"71.860096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.753018\" y=\"71.452624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.905282\" y=\"87.64505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.361549\" y=\"59.370096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.572454\" y=\"71.761607\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.975877\" y=\"59.617879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.789529\" y=\"76.742222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.474806\" y=\"104.341687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.264875\" y=\"68.026442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.512114\" y=\"87.193492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.937993\" y=\"69.71948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.383548\" y=\"45.774082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.136088\" y=\"64.811021\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.571522\" y=\"54.048753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.810063\" y=\"47.273691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.294015\" y=\"79.306596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.120782\" y=\"90.852565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.06859\" y=\"66.468719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.146701\" y=\"63.262394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.221559\" y=\"65.665216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.708983\" y=\"89.563288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.273493\" y=\"66.439287\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.837173\" y=\"83.170556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.803884\" y=\"65.386806\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.632525\" y=\"94.60237\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.695338\" y=\"77.253336\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.70951\" y=\"40.351655\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.687648\" y=\"79.915883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.793452\" y=\"52.793148\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.863376\" y=\"74.635042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.501331\" y=\"91.09296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.99569\" y=\"73.305529\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.416351\" y=\"88.123721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.367629\" y=\"93.357726\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.029471\" y=\"96.797956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.194088\" y=\"101.376634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.180676\" y=\"59.779791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.57167\" y=\"83.17462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.986711\" y=\"58.26251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.046832\" y=\"93.280006\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.214217\" y=\"89.448882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.969972\" y=\"92.86561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.20047\" y=\"76.563584\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.767122\" y=\"73.075454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.681444\" y=\"59.21978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.292355\" y=\"66.063946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.960186\" y=\"93.067459\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.53253\" y=\"74.32969\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"177.84401\" y=\"120.319759\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.647964\" y=\"89.083226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"75.729887\" y=\"21.716144\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.927387\" y=\"67.650078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.175365\" y=\"65.223208\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.556223\" y=\"97.346334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.029483\" y=\"110.278113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.402947\" y=\"55.04702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.987071\" y=\"36.320377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.230393\" y=\"66.727421\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.536374\" y=\"54.699249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.740495\" y=\"68.524397\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"68.20164\" y=\"18.152251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.864201\" y=\"96.415787\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.393444\" y=\"53.098791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.272119\" y=\"55.945706\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.119748\" y=\"67.417656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.167812\" y=\"50.437323\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.387767\" y=\"53.971836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.477571\" y=\"60.758656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.02508\" y=\"108.205526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.285307\" y=\"67.258656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.331823\" y=\"82.418834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.486232\" y=\"77.876968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.91161\" y=\"82.143424\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.717006\" y=\"57.152768\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.891315\" y=\"94.68598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.848712\" y=\"112.619282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.33896\" y=\"84.338058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.909578\" y=\"105.269701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.05359\" y=\"90.123526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.726965\" y=\"103.768415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.333197\" y=\"90.380013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.915242\" y=\"55.457805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.672469\" y=\"80.84138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.840902\" y=\"70.947599\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.958121\" y=\"23.162991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.110344\" y=\"77.873558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.218008\" y=\"105.443857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.779349\" y=\"91.711486\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.503636\" y=\"44.255994\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.471323\" y=\"95.849579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.37937\" y=\"83.283928\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.675286\" y=\"47.93496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"76.540277\" y=\"41.384245\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.953678\" y=\"78.467352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.887518\" y=\"50.069211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.066197\" y=\"94.835505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.852849\" y=\"108.472892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.112367\" y=\"104.72846\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.869502\" y=\"106.281367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.342459\" y=\"51.793366\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.511957\" y=\"70.717656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.192634\" y=\"98.065695\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.357245\" y=\"93.409271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.565301\" y=\"91.783085\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.027944\" y=\"60.077816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.546238\" y=\"51.08775\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.902369\" y=\"70.340151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.13645\" y=\"64.663011\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.119843\" y=\"55.93901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.90781\" y=\"54.780247\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.844246\" y=\"88.241005\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.684121\" y=\"81.15877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.593471\" y=\"81.571363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.760849\" y=\"62.661631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.407119\" y=\"69.693332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.228326\" y=\"74.384589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.196136\" y=\"98.598832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.523503\" y=\"66.33793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"206.761358\" y=\"125.79795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.095197\" y=\"98.4751\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.539013\" y=\"64.56139\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.164185\" y=\"81.606658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.702628\" y=\"98.219326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.258917\" y=\"94.831201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.037722\" y=\"104.699339\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.170226\" y=\"89.41371\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.059483\" y=\"72.784708\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.702482\" y=\"87.411776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.989185\" y=\"52.687565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.075942\" y=\"50.725829\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.970726\" y=\"65.130989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.883673\" y=\"63.986565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.96272\" y=\"80.834248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.150811\" y=\"81.052541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.807994\" y=\"68.298331\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.10131\" y=\"63.476082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.210835\" y=\"84.375522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.352392\" y=\"73.802402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.34687\" y=\"91.968214\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.696498\" y=\"35.232469\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.963518\" y=\"78.597462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.385956\" y=\"70.788778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.184783\" y=\"59.685314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.853204\" y=\"90.993283\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.690509\" y=\"129.760231\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.434951\" y=\"95.79107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.306675\" y=\"49.856484\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.074512\" y=\"86.650945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.928025\" y=\"84.532026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.962191\" y=\"63.882353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.203076\" y=\"56.071804\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.794781\" y=\"91.749217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.275888\" y=\"88.082351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.2505\" y=\"105.130473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.045802\" y=\"51.890282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.08613\" y=\"58.570508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.192583\" y=\"59.913878\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.382741\" y=\"77.805222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.026704\" y=\"88.190008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.660371\" y=\"52.29818\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.102869\" y=\"74.407627\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.28041\" y=\"86.544145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.533615\" y=\"97.2812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.039942\" y=\"47.401588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.798571\" y=\"72.467508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.605425\" y=\"117.705185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.117282\" y=\"77.578839\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.863978\" y=\"68.35539\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.916677\" y=\"67.610349\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"70.69861\" y=\"29.829656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.49309\" y=\"101.365519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.800132\" y=\"65.258895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.875062\" y=\"77.986457\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"37.81946\" y=\"22.041043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.9688\" y=\"82.919953\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"184.717724\" y=\"139.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.23157\" y=\"93.679042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"177.22424\" y=\"110.310029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.395139\" y=\"96.225358\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.573684\" y=\"51.327124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.758672\" y=\"58.736199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.570633\" y=\"66.047748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.098949\" y=\"80.562145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"180.18112\" y=\"122.612181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.484323\" y=\"72.588527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.19446\" y=\"68.419977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.194549\" y=\"69.203448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.39048\" y=\"99.604671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.836511\" y=\"95.278895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.276288\" y=\"116.960095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.729036\" y=\"62.502891\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.406236\" y=\"75.34703\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.079402\" y=\"65.876611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.053701\" y=\"59.436446\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.071025\" y=\"94.729298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.838797\" y=\"65.936659\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.557707\" y=\"66.413173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.492099\" y=\"71.617763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.297919\" y=\"65.554642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.249566\" y=\"50.537598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.751482\" y=\"60.66267\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.829179\" y=\"85.850677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.804102\" y=\"69.59409\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.009757\" y=\"33.39075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.889071\" y=\"84.740389\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.930904\" y=\"56.896367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.109569\" y=\"80.144903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.085701\" y=\"78.820328\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.607475\" y=\"90.114192\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.197228\" y=\"90.689906\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.888189\" y=\"86.676935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.803833\" y=\"100.273606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.385509\" y=\"71.28935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.428015\" y=\"93.395327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.227251\" y=\"75.908799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"83.610668\" y=\"47.334452\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"77.11768\" y=\"42.681032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.21452\" y=\"68.254932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"194.397655\" y=\"121.483361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.216604\" y=\"92.04012\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.076437\" y=\"84.291729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.395224\" y=\"80.507211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"189.387072\" y=\"111.456825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.094041\" y=\"70.558834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.844024\" y=\"99.113743\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.045958\" y=\"79.203896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.793533\" y=\"43.082933\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.747365\" y=\"49.999154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.860075\" y=\"80.905868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.956165\" y=\"53.759361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.715826\" y=\"57.926678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.062914\" y=\"30.864672\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.261816\" y=\"84.073393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.33017\" y=\"85.092948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.086697\" y=\"59.609608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.664584\" y=\"85.599525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.832551\" y=\"37.233837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.292719\" y=\"57.501901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"197.885911\" y=\"136.476002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.417242\" y=\"93.124352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.590631\" y=\"79.8113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.826924\" y=\"62.819942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.21641\" y=\"81.136541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.116507\" y=\"106.779282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.198812\" y=\"73.861437\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.610014\" y=\"107.285008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.8329\" y=\"86.318162\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"85.447827\" y=\"41.339427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.662964\" y=\"98.494541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.15241\" y=\"55.854203\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.675945\" y=\"74.743863\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.303793\" y=\"96.890186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.650283\" y=\"68.410737\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.823973\" y=\"86.884353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.11893\" y=\"48.318827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.12326\" y=\"82.182124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.71922\" y=\"43.483263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.301609\" y=\"72.569612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.466356\" y=\"72.806161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.890497\" y=\"47.114905\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"69.84394\" y=\"49.194621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.667698\" y=\"66.412408\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.492479\" y=\"105.564466\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.887408\" y=\"100.306264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.067891\" y=\"106.950633\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.148809\" y=\"64.954008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.429482\" y=\"90.1105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.664646\" y=\"86.995488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.867215\" y=\"62.85089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.549793\" y=\"93.940841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.053745\" y=\"79.150901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.650576\" y=\"68.84641\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.230696\" y=\"101.662611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.701619\" y=\"49.77235\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.691186\" y=\"69.579363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.18138\" y=\"111.892028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.551708\" y=\"60.302929\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.733292\" y=\"103.65314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"58.010948\" y=\"15.765491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.198177\" y=\"94.586921\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.120971\" y=\"54.787315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.041368\" y=\"83.67882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.275511\" y=\"68.62897\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.086569\" y=\"82.079448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.750245\" y=\"67.164617\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.120777\" y=\"97.354337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.347228\" y=\"90.121634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.348334\" y=\"79.292481\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.444262\" y=\"61.17923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.839766\" y=\"64.295246\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.445458\" y=\"116.605219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.649238\" y=\"82.715272\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.715663\" y=\"92.86394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.159696\" y=\"70.888666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"65.476058\" y=\"13.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.099375\" y=\"69.149559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.263962\" y=\"42.032186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.166695\" y=\"57.837107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.732869\" y=\"75.377153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.679882\" y=\"131.895592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.939425\" y=\"59.141719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.714627\" y=\"111.353278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.763694\" y=\"96.236795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.906559\" y=\"64.276207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.604701\" y=\"60.290799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.973405\" y=\"106.724808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.629961\" y=\"85.050771\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.981322\" y=\"107.254549\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"180.513474\" y=\"111.184544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.836516\" y=\"95.462359\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.914484\" y=\"67.259161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.479348\" y=\"70.232714\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.272523\" y=\"49.957704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.135323\" y=\"66.102736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.628174\" y=\"83.945404\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.519095\" y=\"77.177288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.061573\" y=\"76.923153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.333124\" y=\"90.314048\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.929194\" y=\"62.515236\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.347144\" y=\"59.281458\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.947892\" y=\"101.318345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.997123\" y=\"64.226513\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.902053\" y=\"102.041242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.913735\" y=\"77.644053\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.177522\" y=\"91.387142\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.664466\" y=\"27.810463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.855771\" y=\"81.917821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.229796\" y=\"89.348764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"79.634661\" y=\"35.092983\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.891549\" y=\"65.048166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"93.968327\" y=\"43.871166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.907416\" y=\"75.364399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.486247\" y=\"76.088841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.810386\" y=\"72.693093\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.978057\" y=\"83.175531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.110827\" y=\"79.505546\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.016312\" y=\"100.975342\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.702803\" y=\"124.807206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.412038\" y=\"90.983812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.235486\" y=\"83.077619\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.534762\" y=\"83.761282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.610104\" y=\"87.596558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.090768\" y=\"84.604151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"187.20176\" y=\"116.228875\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.439972\" y=\"77.64884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.274607\" y=\"113.955505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.646856\" y=\"68.287306\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.591017\" y=\"71.528892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.994477\" y=\"74.031821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.041914\" y=\"98.097999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.900527\" y=\"48.35415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.638862\" y=\"86.280172\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.804202\" y=\"67.597294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.845581\" y=\"77.180758\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.671126\" y=\"64.412474\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"174.310451\" y=\"112.626968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.847771\" y=\"95.91145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.881409\" y=\"79.49606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.306849\" y=\"84.052712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.250617\" y=\"83.414741\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.994616\" y=\"75.428867\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.067052\" y=\"78.310834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.191582\" y=\"70.326748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.147501\" y=\"73.301386\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.13053\" y=\"71.147066\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.426643\" y=\"99.081935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.168215\" y=\"78.237334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.189183\" y=\"89.874683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.419576\" y=\"74.284852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.419249\" y=\"72.372303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.053958\" y=\"75.555399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.359279\" y=\"121.223819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.815822\" y=\"80.050575\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.911547\" y=\"78.240407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.923255\" y=\"75.147613\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.133718\" y=\"102.542952\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.329025\" y=\"87.033987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"93.422504\" y=\"45.301327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.517571\" y=\"91.822422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.024305\" y=\"76.642438\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.614931\" y=\"77.949601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"64.970981\" y=\"30.519615\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.081606\" y=\"73.579209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.838706\" y=\"82.110649\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.292935\" y=\"79.188468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.982502\" y=\"104.574014\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.096378\" y=\"61.68454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.129246\" y=\"62.090174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.758126\" y=\"64.382054\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.906803\" y=\"114.576675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.911888\" y=\"93.266282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.247932\" y=\"45.160321\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.519383\" y=\"104.21218\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.846885\" y=\"71.754109\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.148961\" y=\"68.087322\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.615576\" y=\"81.411749\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"90.316396\" y=\"50.130527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.132399\" y=\"50.705691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.188074\" y=\"63.967413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.547703\" y=\"57.232677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.194564\" y=\"84.365181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.661788\" y=\"77.832046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.034306\" y=\"113.094996\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.93483\" y=\"91.140991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.505665\" y=\"105.741195\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.334975\" y=\"68.392803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.794785\" y=\"75.12353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.469321\" y=\"90.649089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.66032\" y=\"104.070657\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.906548\" y=\"63.468873\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.338784\" y=\"82.86794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.549296\" y=\"85.608542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.546588\" y=\"97.206753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.718311\" y=\"68.260558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.147706\" y=\"56.226981\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.842273\" y=\"60.232185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.668063\" y=\"51.873542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"84.617334\" y=\"18.103516\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.451103\" y=\"93.246023\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.265351\" y=\"73.027207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.902407\" y=\"43.075637\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.954757\" y=\"67.577574\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.89097\" y=\"79.029583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"187.621362\" y=\"119.915368\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.612109\" y=\"83.74487\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.686826\" y=\"76.370095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.799684\" y=\"92.48442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.290279\" y=\"49.338792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.838858\" y=\"100.184448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.269882\" y=\"97.096052\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.038635\" y=\"67.031251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.041575\" y=\"87.363325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"84.014338\" y=\"21.562226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.515553\" y=\"69.816669\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.565861\" y=\"78.500167\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.95154\" y=\"89.430395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.14065\" y=\"91.456843\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.202022\" y=\"56.366234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.22455\" y=\"64.606874\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.958635\" y=\"78.651995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.252548\" y=\"72.41565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.756206\" y=\"107.662652\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.515326\" y=\"99.172621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.706676\" y=\"96.370217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.105019\" y=\"79.02333\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.779544\" y=\"89.016402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.200818\" y=\"104.754022\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.632423\" y=\"96.801986\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.282279\" y=\"111.927308\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.18855\" y=\"82.793202\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.458103\" y=\"41.859056\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.123526\" y=\"93.851658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.584503\" y=\"60.980031\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.909372\" y=\"74.145029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.550977\" y=\"94.163492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.472081\" y=\"68.159095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.929361\" y=\"102.06078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.826549\" y=\"57.863059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.134436\" y=\"85.641259\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.168939\" y=\"94.75956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.293359\" y=\"112.293427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.506265\" y=\"90.257794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.356788\" y=\"65.223327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.398334\" y=\"83.79076\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.570078\" y=\"75.850764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.328916\" y=\"55.738297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.610495\" y=\"72.706857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.378193\" y=\"78.429072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.583122\" y=\"73.384083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.425629\" y=\"88.150926\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.353432\" y=\"84.380879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.209797\" y=\"89.189101\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.685341\" y=\"62.18428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.618398\" y=\"71.018209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.333136\" y=\"70.038488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.322245\" y=\"72.531255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.823646\" y=\"74.393081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.608225\" y=\"53.162821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"202.783655\" y=\"121.830153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.99309\" y=\"80.320939\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.391942\" y=\"64.345249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.876304\" y=\"80.548704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.58977\" y=\"63.643561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.906441\" y=\"81.658512\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.266019\" y=\"101.152384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.281286\" y=\"66.060699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.366461\" y=\"92.592949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.516261\" y=\"67.56122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.92194\" y=\"90.036556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.144481\" y=\"57.539528\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.135178\" y=\"79.329016\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.908034\" y=\"78.861112\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.491266\" y=\"83.921422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.807697\" y=\"75.450786\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.051651\" y=\"66.182709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.019487\" y=\"98.983666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.111516\" y=\"65.131705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.162638\" y=\"69.319809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.660903\" y=\"78.456473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"91.734064\" y=\"51.867579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.351045\" y=\"82.685242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.6349\" y=\"73.055611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.068739\" y=\"84.355827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.63787\" y=\"87.599534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.409568\" y=\"93.319693\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.704529\" y=\"67.56309\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.300494\" y=\"63.42464\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.499743\" y=\"48.451007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.69255\" y=\"55.90116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.128581\" y=\"62.556877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.883643\" y=\"67.885119\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.241836\" y=\"77.585043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.797919\" y=\"87.156956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.0616\" y=\"58.183763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.015626\" y=\"82.54097\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.302346\" y=\"63.575948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.302998\" y=\"76.863967\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.911611\" y=\"99.133675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.123813\" y=\"66.921095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.865968\" y=\"52.645248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.78829\" y=\"79.424332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.709863\" y=\"63.33256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.644056\" y=\"39.476263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.256625\" y=\"86.515271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.738507\" y=\"99.654722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.612702\" y=\"91.443661\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.821672\" y=\"93.766817\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.388459\" y=\"72.543665\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.919281\" y=\"96.980838\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.278235\" y=\"93.142453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.94484\" y=\"130.32799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.820298\" y=\"37.928601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.650908\" y=\"62.978298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.414425\" y=\"73.937807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.594095\" y=\"88.439678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"170.995955\" y=\"109.732608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.47018\" y=\"97.713903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"91.416199\" y=\"51.397677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.637216\" y=\"69.78173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.319177\" y=\"71.655387\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.374822\" y=\"74.834836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.427259\" y=\"52.43887\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.449895\" y=\"78.713174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.719218\" y=\"70.531854\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.072781\" y=\"61.076881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.88235\" y=\"74.547729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.217137\" y=\"52.674687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.762105\" y=\"59.810942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.558407\" y=\"61.533266\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.053974\" y=\"105.538605\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.286644\" y=\"83.827882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.266477\" y=\"73.022932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.574237\" y=\"72.99407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.381512\" y=\"64.55689\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.693437\" y=\"83.376626\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.460302\" y=\"91.934398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.00611\" y=\"85.236683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.702211\" y=\"80.325857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.346003\" y=\"53.38592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.269631\" y=\"86.51086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.152682\" y=\"68.541043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.952226\" y=\"100.24009\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"171.111968\" y=\"118.103977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.53369\" y=\"90.561111\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.717881\" y=\"69.697515\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.815171\" y=\"62.456778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.308034\" y=\"94.957518\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.427407\" y=\"92.102869\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.582537\" y=\"66.853116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.54365\" y=\"90.435422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.207181\" y=\"71.333254\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.161298\" y=\"103.614709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.907013\" y=\"98.96007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.818717\" y=\"81.294635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.449835\" y=\"81.23083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.090995\" y=\"102.947303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.379022\" y=\"79.235428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.079829\" y=\"82.178295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.273181\" y=\"91.059508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.334522\" y=\"86.045153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.770433\" y=\"65.405451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.552052\" y=\"69.00258\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.765937\" y=\"82.574475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.607644\" y=\"115.920234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.087753\" y=\"89.551559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.319823\" y=\"77.133567\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.731657\" y=\"68.029976\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.131994\" y=\"84.591819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.73471\" y=\"95.674044\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.18093\" y=\"83.951852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.339812\" y=\"48.107719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.868383\" y=\"91.144692\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.915862\" y=\"77.966158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.038497\" y=\"101.867718\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.953976\" y=\"69.104578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.381854\" y=\"95.437832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.145448\" y=\"91.398395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.161531\" y=\"82.985289\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.390359\" y=\"113.470475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.705958\" y=\"87.582783\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.643029\" y=\"80.594545\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.61515\" y=\"75.882131\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.371491\" y=\"83.48542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.091775\" y=\"75.831255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.084409\" y=\"84.442152\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.313126\" y=\"66.885205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.356645\" y=\"87.367559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.569709\" y=\"75.389946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.88968\" y=\"62.102534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.395297\" y=\"89.780347\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.960033\" y=\"73.903159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.651549\" y=\"115.759169\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.600207\" y=\"99.445677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.972016\" y=\"100.660773\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.195861\" y=\"95.951702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.305644\" y=\"91.992748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.934713\" y=\"93.511792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.517035\" y=\"93.644631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.15317\" y=\"39.124589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.259368\" y=\"89.347345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.762384\" y=\"69.862951\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.40995\" y=\"101.115803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.643811\" y=\"68.037402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m2cbfe66e94\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"37.108605\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −4 -->\n",
       "      <g transform=\"translate(29.737511 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"84.839648\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(77.468554 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"132.57069\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(129.38944 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"180.301733\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(177.120483 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <defs>\n",
       "       <path id=\"m67ee9f9fc6\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"125.71812\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- −5 -->\n",
       "      <g transform=\"translate(7.2 129.517339) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"99.751174\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(15.579688 103.550393) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"73.784228\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(15.579688 77.583446) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"47.817281\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(9.217188 51.6165) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"21.850335\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(9.217188 25.649554) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 28.942188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 28.942188 7.2 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p149bb96678\">\n",
       "   <rect x=\"28.942188\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘图，查看分布\n",
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:, (1)].detach().numpy(), \n",
    "                labels.detach().numpy(), 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"231.442187pt\" height=\"169.678125pt\" viewBox=\"0 0 231.442187 169.678125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-05-18T15:08:56.251352</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 169.678125 \n",
       "L 231.442187 169.678125 \n",
       "L 231.442187 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "L 28.942188 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m823589f946\" d=\"M 0 0.5 \n",
       "C 0.132602 0.5 0.25979 0.447317 0.353553 0.353553 \n",
       "C 0.447317 0.25979 0.5 0.132602 0.5 0 \n",
       "C 0.5 -0.132602 0.447317 -0.25979 0.353553 -0.353553 \n",
       "C 0.25979 -0.447317 0.132602 -0.5 0 -0.5 \n",
       "C -0.132602 -0.5 -0.25979 -0.447317 -0.353553 -0.353553 \n",
       "C -0.447317 -0.25979 -0.5 -0.132602 -0.5 0 \n",
       "C -0.5 0.132602 -0.447317 0.25979 -0.353553 0.353553 \n",
       "C -0.25979 0.447317 -0.132602 0.5 0 0.5 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p4610e2cf3f)\">\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.049598\" y=\"47.161377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.963107\" y=\"83.62544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.927232\" y=\"87.238868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.499187\" y=\"40.397538\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.738829\" y=\"74.594754\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"168.177498\" y=\"60.945456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.120588\" y=\"78.7769\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.100157\" y=\"79.445989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"167.203315\" y=\"38.908429\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"66.24924\" y=\"116.402206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"96.90644\" y=\"88.486938\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.287288\" y=\"81.085971\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.367315\" y=\"87.997522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.325299\" y=\"88.393808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.647682\" y=\"76.554431\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.127695\" y=\"73.519191\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.422087\" y=\"74.321666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.875464\" y=\"93.603985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.86115\" y=\"88.551763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.341229\" y=\"35.366294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.2019\" y=\"58.043827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.565749\" y=\"103.28369\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.72149\" y=\"57.861658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.517474\" y=\"81.847122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.731428\" y=\"77.415171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.879391\" y=\"90.031881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.688161\" y=\"35.628451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.386178\" y=\"92.726373\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.192063\" y=\"65.921645\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.646121\" y=\"67.827082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.295666\" y=\"73.868138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.981588\" y=\"81.497491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.834866\" y=\"57.255578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.896673\" y=\"56.665809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.227189\" y=\"72.549742\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.329422\" y=\"85.38564\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.058958\" y=\"58.074153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.342288\" y=\"51.205587\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.373088\" y=\"46.708364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"77.803974\" y=\"111.673949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.240795\" y=\"75.989104\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.312012\" y=\"67.739194\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.926694\" y=\"59.798297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.913672\" y=\"74.989388\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.635024\" y=\"59.375105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.512233\" y=\"59.267597\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.543894\" y=\"93.854999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.832725\" y=\"71.690158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.390046\" y=\"87.617628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.727296\" y=\"63.623364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.260721\" y=\"73.829216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.220665\" y=\"89.36582\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.38085\" y=\"94.878713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.725452\" y=\"98.631002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.94471\" y=\"69.606532\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.676253\" y=\"77.322051\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.217505\" y=\"72.458828\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.958828\" y=\"44.551745\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.123908\" y=\"71.231531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.629155\" y=\"87.507944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.86245\" y=\"89.154946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.219201\" y=\"115.023299\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"189.643748\" y=\"57.941606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.691932\" y=\"64.990028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.878438\" y=\"124.951583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.820831\" y=\"47.266292\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.795653\" y=\"82.798065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"185.46487\" y=\"84.905216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.289184\" y=\"63.342156\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.790243\" y=\"71.311502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.555683\" y=\"71.440978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"96.680579\" y=\"109.509315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.142542\" y=\"76.930463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.204389\" y=\"81.979596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.270455\" y=\"45.388764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"37.81946\" y=\"96.679841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"187.724045\" y=\"39.170635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.818021\" y=\"104.843924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.548633\" y=\"136.669454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.098167\" y=\"64.485384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"69.900495\" y=\"92.242205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.078741\" y=\"58.495525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.047477\" y=\"43.485036\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.94966\" y=\"66.785002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"170.966781\" y=\"75.943248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.77618\" y=\"105.039855\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.366653\" y=\"90.586554\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.319769\" y=\"70.239975\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.740655\" y=\"86.234712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.564353\" y=\"72.272405\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.289525\" y=\"42.120415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.977459\" y=\"52.733002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.326608\" y=\"91.534395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.793803\" y=\"78.277075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.637716\" y=\"80.007046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.535262\" y=\"123.495815\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.666409\" y=\"87.647679\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.243464\" y=\"68.147087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.233118\" y=\"77.973753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.615171\" y=\"102.953634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.83466\" y=\"48.117778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.245982\" y=\"79.685957\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.339264\" y=\"105.065753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.043811\" y=\"94.163776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.807544\" y=\"39.249632\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.697482\" y=\"94.996263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.091272\" y=\"110.657126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.549356\" y=\"56.029453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.928383\" y=\"95.454916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.897108\" y=\"95.256836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.102496\" y=\"79.859375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"51.276685\" y=\"86.897922\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.651787\" y=\"94.19315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.784394\" y=\"100.574648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.524805\" y=\"95.633883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"61.405454\" y=\"77.128793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"165.479099\" y=\"42.823713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.107527\" y=\"71.69238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.024353\" y=\"43.229075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.383267\" y=\"84.788153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.958005\" y=\"72.66032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.9598\" y=\"58.450722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.732142\" y=\"131.832798\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"204.588149\" y=\"54.265354\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.193145\" y=\"79.788226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.838601\" y=\"39.581108\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.481894\" y=\"74.271061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.181891\" y=\"96.641912\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.217305\" y=\"95.322537\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.981023\" y=\"37.140992\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.457846\" y=\"88.381482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.444136\" y=\"82.87251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.320171\" y=\"114.644034\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.182927\" y=\"81.887664\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.052671\" y=\"110.831563\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.614091\" y=\"79.688158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.600058\" y=\"72.500684\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.438867\" y=\"64.647256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.804829\" y=\"83.772278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"63.421576\" y=\"126.892836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.066561\" y=\"52.077508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.440322\" y=\"71.031701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.984559\" y=\"77.309642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.146232\" y=\"95.714536\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.036364\" y=\"78.662995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.580275\" y=\"52.853989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.086729\" y=\"85.024525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.583032\" y=\"65.790707\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.297572\" y=\"73.972357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.872947\" y=\"73.366803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.728672\" y=\"103.786181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.589504\" y=\"70.019699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.80647\" y=\"46.796222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.874851\" y=\"71.131398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"170.34891\" y=\"67.727624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.207703\" y=\"87.823199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.000766\" y=\"74.458643\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"60.052055\" y=\"119.338468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.077031\" y=\"65.114343\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.30747\" y=\"90.545777\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.660782\" y=\"32.072086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.63782\" y=\"89.646188\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"74.008351\" y=\"131.976622\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.802539\" y=\"37.519916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.031993\" y=\"68.471924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"160.842431\" y=\"58.213502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.530854\" y=\"67.209455\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.319874\" y=\"83.648026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.749716\" y=\"81.247013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.943817\" y=\"78.386436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.952825\" y=\"110.099002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.80105\" y=\"79.870857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.17755\" y=\"86.82456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.775787\" y=\"55.800128\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.787785\" y=\"95.543365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.680858\" y=\"80.716385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.31618\" y=\"49.869361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.986793\" y=\"78.313592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.027008\" y=\"108.965791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.632561\" y=\"89.407648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.120077\" y=\"102.732158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"55.340667\" y=\"130.289923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.168331\" y=\"77.297087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.052556\" y=\"77.985982\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.276859\" y=\"86.539024\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.447145\" y=\"88.497173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.160475\" y=\"82.445164\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"77.770008\" y=\"100.566278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.059892\" y=\"78.2606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.837459\" y=\"104.95213\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.805963\" y=\"90.006435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.199906\" y=\"66.676858\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.479051\" y=\"44.376892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.643194\" y=\"50.800963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.651571\" y=\"73.130755\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.739921\" y=\"90.054365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.782681\" y=\"88.832337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.722082\" y=\"109.265351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.765036\" y=\"105.520126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.92917\" y=\"45.042519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.462902\" y=\"71.352702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.305053\" y=\"87.712444\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.404515\" y=\"93.431903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.110943\" y=\"115.084394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.583832\" y=\"65.664837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.929312\" y=\"41.543072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.699533\" y=\"65.483171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.61974\" y=\"73.541159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.611058\" y=\"107.280884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.971819\" y=\"81.808232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.09323\" y=\"92.773059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"80.631611\" y=\"110.615348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.394381\" y=\"87.620506\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.171816\" y=\"95.841866\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.954207\" y=\"67.122825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.024508\" y=\"77.513579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.642053\" y=\"65.123268\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.571848\" y=\"108.43805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"187.924902\" y=\"59.570065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.405573\" y=\"71.318317\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.291421\" y=\"103.523987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.894751\" y=\"43.963427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.396273\" y=\"71.800816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.090681\" y=\"50.486663\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.573461\" y=\"80.491925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.203491\" y=\"101.020893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.981394\" y=\"103.960201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.084978\" y=\"64.961187\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.747365\" y=\"108.101154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.659146\" y=\"71.272436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.896406\" y=\"54.533553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.622954\" y=\"77.715356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.506828\" y=\"85.805685\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.689788\" y=\"84.457284\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.744492\" y=\"110.462091\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.914233\" y=\"40.894705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.029819\" y=\"69.718393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.924367\" y=\"99.022385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.206793\" y=\"47.724193\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.371572\" y=\"94.62609\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.283811\" y=\"69.007238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.098857\" y=\"115.545579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"187.707201\" y=\"50.701739\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.111841\" y=\"93.231596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.278095\" y=\"48.494835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.721283\" y=\"60.52896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.206497\" y=\"115.579612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.264923\" y=\"57.422106\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.301552\" y=\"48.033332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.042311\" y=\"92.460497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.554385\" y=\"94.537197\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.620078\" y=\"105.885217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.920391\" y=\"89.998881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.870621\" y=\"57.369795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.159061\" y=\"109.391435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.698184\" y=\"68.146646\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.944293\" y=\"77.863288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.880527\" y=\"82.309716\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.515502\" y=\"64.998791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.94588\" y=\"62.189196\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"82.297836\" y=\"90.072432\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.01675\" y=\"75.777629\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.402728\" y=\"88.600963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.38606\" y=\"62.371521\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.955738\" y=\"81.635209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.671034\" y=\"54.280767\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.351852\" y=\"73.743823\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.858871\" y=\"70.005569\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.11235\" y=\"67.434763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.694098\" y=\"76.827061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.895866\" y=\"83.246127\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.46799\" y=\"62.814925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.1394\" y=\"82.432944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.65086\" y=\"66.571063\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.977175\" y=\"61.460029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.67534\" y=\"84.160395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.576162\" y=\"68.818434\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.764923\" y=\"52.552853\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.268663\" y=\"95.612473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.919849\" y=\"60.307027\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.769421\" y=\"65.64585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.102148\" y=\"90.950681\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.715173\" y=\"67.75297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.335523\" y=\"53.050234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.15796\" y=\"106.433803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.824974\" y=\"49.907413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.53108\" y=\"81.185502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.397239\" y=\"64.86224\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.153422\" y=\"63.739985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"177.608589\" y=\"101.59375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.390429\" y=\"110.318736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.027687\" y=\"122.525807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.667361\" y=\"38.326326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.64613\" y=\"91.321178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"96.587818\" y=\"77.393835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.031757\" y=\"49.56985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.687921\" y=\"85.533263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.260207\" y=\"72.671207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.050953\" y=\"99.386596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.476967\" y=\"67.775845\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.306363\" y=\"74.056585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.989877\" y=\"75.661508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.810489\" y=\"61.146314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.631908\" y=\"105.44496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.028653\" y=\"37.429393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.080218\" y=\"78.800497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.75805\" y=\"76.243697\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.333868\" y=\"65.434945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.884273\" y=\"95.744662\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.75585\" y=\"60.535636\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.694347\" y=\"87.970989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.355425\" y=\"86.192893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.322212\" y=\"57.788411\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"169.078373\" y=\"75.358756\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"203.677167\" y=\"35.551553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.900537\" y=\"78.30334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.003074\" y=\"64.711556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.375884\" y=\"101.88973\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.776485\" y=\"97.819789\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.548848\" y=\"88.734081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.623095\" y=\"45.330227\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"166.504097\" y=\"44.209348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.490532\" y=\"76.632356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"42.223604\" y=\"108.769978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.483809\" y=\"100.102314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.557705\" y=\"88.620482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.493029\" y=\"96.788656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.602751\" y=\"60.901465\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.314864\" y=\"93.607883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.405215\" y=\"74.933473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.2544\" y=\"82.22746\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.82136\" y=\"95.62631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.820357\" y=\"60.61157\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.426303\" y=\"81.496522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.750114\" y=\"41.516842\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.995442\" y=\"88.36295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.801026\" y=\"70.030588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.461677\" y=\"78.772412\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.195137\" y=\"69.940603\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.935805\" y=\"72.17007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"76.383706\" y=\"108.453568\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.861479\" y=\"66.977319\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.519279\" y=\"121.685772\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.620947\" y=\"96.364357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.402655\" y=\"83.36262\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.037137\" y=\"77.685671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.506589\" y=\"77.021255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"177.185268\" y=\"81.933449\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.590134\" y=\"56.89179\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.6819\" y=\"109.930138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.241482\" y=\"46.477058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.572872\" y=\"107.725896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.637728\" y=\"95.836344\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"212.651217\" y=\"25.80849\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.587829\" y=\"83.93642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"180.754959\" y=\"41.798879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.633346\" y=\"100.699816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.661397\" y=\"132.059178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.076599\" y=\"61.119264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.355169\" y=\"102.003628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.63579\" y=\"90.938355\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"62.976754\" y=\"135.076353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.202629\" y=\"89.697253\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.238987\" y=\"84.102082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"168.109165\" y=\"70.643879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.793745\" y=\"77.271077\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.972281\" y=\"118.26173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.883808\" y=\"100.42625\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.039785\" y=\"85.449113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.66428\" y=\"91.51232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.241492\" y=\"98.445219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.03768\" y=\"69.827219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"75.822165\" y=\"96.551888\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.904047\" y=\"69.991721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.273474\" y=\"77.144345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.75058\" y=\"53.943991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.731207\" y=\"42.491325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.123343\" y=\"71.860096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.473095\" y=\"71.452624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"76.20589\" y=\"87.64505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.625086\" y=\"59.370096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.623662\" y=\"71.761607\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.872865\" y=\"59.617879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.328513\" y=\"76.742222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.067496\" y=\"104.341687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.101\" y=\"68.026442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.540093\" y=\"87.193492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.715822\" y=\"69.71948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.45679\" y=\"45.774082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.390465\" y=\"64.811021\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.321209\" y=\"54.048753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.308722\" y=\"47.273691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.437076\" y=\"79.306596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.410571\" y=\"90.852565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.786144\" y=\"66.468719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.533711\" y=\"63.262394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.389089\" y=\"65.665216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.792984\" y=\"89.563288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.134847\" y=\"66.439287\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.283384\" y=\"83.170556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.321473\" y=\"65.386806\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.674786\" y=\"94.60237\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.015272\" y=\"77.253336\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.597825\" y=\"40.351655\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.524061\" y=\"79.915883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.986621\" y=\"52.793148\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.451529\" y=\"74.635042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.319855\" y=\"91.09296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.287857\" y=\"73.305529\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.69111\" y=\"88.123721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.736298\" y=\"93.357726\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.638866\" y=\"96.797956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"63.749778\" y=\"101.376634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.916784\" y=\"59.779791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.478063\" y=\"83.17462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.579643\" y=\"58.26251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.260889\" y=\"93.280006\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.856746\" y=\"89.448882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.132326\" y=\"92.86561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.64914\" y=\"76.563584\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.98032\" y=\"73.075454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.448037\" y=\"59.21978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.196027\" y=\"66.063946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.623498\" y=\"93.067459\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"167.48427\" y=\"74.32969\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.613633\" y=\"120.319759\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.849133\" y=\"89.083226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.136872\" y=\"21.716144\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.194044\" y=\"67.650078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.50902\" y=\"65.223208\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.345405\" y=\"97.346334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.290961\" y=\"110.278113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.149373\" y=\"55.04702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.172144\" y=\"36.320377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.482255\" y=\"66.727421\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.525141\" y=\"54.699249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.40025\" y=\"68.524397\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.578988\" y=\"18.152251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.351455\" y=\"96.415787\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.619062\" y=\"53.098791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.703068\" y=\"55.945706\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.422522\" y=\"67.417656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"179.246541\" y=\"50.437323\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.299476\" y=\"53.971836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.247584\" y=\"60.758656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.829639\" y=\"108.205526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.127188\" y=\"67.258656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.219419\" y=\"82.418834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.733263\" y=\"77.876968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.098952\" y=\"82.143424\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.306709\" y=\"57.152768\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.29035\" y=\"94.68598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"56.441672\" y=\"112.619282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.18383\" y=\"84.338058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.610361\" y=\"105.269701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.990516\" y=\"90.123526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.297022\" y=\"103.768415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.657461\" y=\"90.380013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.048586\" y=\"55.457805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.176953\" y=\"80.84138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.15855\" y=\"70.947599\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"188.818438\" y=\"23.162991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.81954\" y=\"77.873558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"52.788866\" y=\"105.443857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.073555\" y=\"91.711486\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.648184\" y=\"44.255994\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.24535\" y=\"95.849579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.719859\" y=\"83.283928\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.82831\" y=\"47.93496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.600949\" y=\"41.384245\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"170.925208\" y=\"78.467352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.491491\" y=\"50.069211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.285578\" y=\"94.835505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.612509\" y=\"108.472892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.57265\" y=\"104.72846\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.8394\" y=\"106.281367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.025283\" y=\"51.793366\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.112622\" y=\"70.717656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.392245\" y=\"98.065695\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.588362\" y=\"93.409271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.28\" y=\"91.783085\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.005108\" y=\"60.077816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.487075\" y=\"51.08775\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.931134\" y=\"70.340151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.782326\" y=\"64.663011\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.487126\" y=\"55.93901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.874491\" y=\"54.780247\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.387538\" y=\"88.241005\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.230036\" y=\"81.15877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.805315\" y=\"81.571363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.518051\" y=\"62.661631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.912117\" y=\"69.693332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.50853\" y=\"74.384589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.022979\" y=\"98.598832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.308852\" y=\"66.33793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.66247\" y=\"125.79795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.230846\" y=\"98.4751\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.927235\" y=\"64.56139\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.136891\" y=\"81.606658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.011061\" y=\"98.219326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.433355\" y=\"94.831201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.01999\" y=\"104.699339\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.124803\" y=\"89.41371\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.731162\" y=\"72.784708\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.449115\" y=\"87.411776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.901989\" y=\"52.687565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.230845\" y=\"50.725829\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.264013\" y=\"65.130989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.363999\" y=\"63.986565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.755118\" y=\"80.834248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.336992\" y=\"81.052541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.442112\" y=\"68.298331\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.985402\" y=\"63.476082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.662602\" y=\"84.375522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.727325\" y=\"73.802402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"91.581023\" y=\"91.968214\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"183.538607\" y=\"35.232469\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.842907\" y=\"78.597462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.752634\" y=\"70.788778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.254133\" y=\"59.685314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.991266\" y=\"90.993283\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"51.962785\" y=\"129.760231\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.49515\" y=\"95.79107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.077792\" y=\"49.856484\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.310112\" y=\"86.650945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.556629\" y=\"84.532026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.689037\" y=\"63.882353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.347151\" y=\"56.071804\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.809807\" y=\"91.749217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.976101\" y=\"88.082351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.36503\" y=\"105.130473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.369996\" y=\"51.890282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.0653\" y=\"58.570508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.694646\" y=\"59.913878\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.833458\" y=\"77.805222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.038152\" y=\"88.190008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.299748\" y=\"52.29818\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.303556\" y=\"74.407627\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.91774\" y=\"86.544145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.32546\" y=\"97.2812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"209.389793\" y=\"47.401588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"181.015184\" y=\"72.467508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.360497\" y=\"117.705185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.733952\" y=\"77.578839\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.36438\" y=\"68.35539\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.414222\" y=\"67.610349\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.119107\" y=\"29.829656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"80.299656\" y=\"101.365519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.623621\" y=\"65.258895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.323999\" y=\"77.986457\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.074237\" y=\"22.041043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.23561\" y=\"82.919953\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"63.623552\" y=\"139.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.357694\" y=\"93.679042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.611059\" y=\"110.310029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.936631\" y=\"96.225358\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.522297\" y=\"51.327124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.057859\" y=\"58.736199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.800353\" y=\"66.047748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.564327\" y=\"80.562145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.434737\" y=\"122.612181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.082975\" y=\"72.588527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.615517\" y=\"68.419977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.593645\" y=\"69.203448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.359412\" y=\"99.604671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.507215\" y=\"95.278895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"91.779967\" y=\"116.960095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.826354\" y=\"62.502891\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.243687\" y=\"75.34703\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.302767\" y=\"65.876611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.021664\" y=\"59.436446\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.267938\" y=\"94.729298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.014277\" y=\"65.936659\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.896302\" y=\"66.413173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.794339\" y=\"71.617763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.880567\" y=\"65.554642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"169.210858\" y=\"50.537598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.656678\" y=\"60.66267\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.911634\" y=\"85.850677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.673002\" y=\"69.59409\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"189.161769\" y=\"33.39075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.753796\" y=\"84.740389\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.063714\" y=\"56.896367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.889163\" y=\"80.144903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.337622\" y=\"78.820328\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.048405\" y=\"90.114192\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.60175\" y=\"90.689906\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.546441\" y=\"86.676935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.150959\" y=\"100.273606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.452988\" y=\"71.28935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.797849\" y=\"93.395327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.605712\" y=\"75.908799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.511792\" y=\"47.334452\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.202954\" y=\"42.681032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.34902\" y=\"68.254932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.52931\" y=\"121.483361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.118569\" y=\"92.04012\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.717182\" y=\"84.291729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.564481\" y=\"80.507211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.997267\" y=\"111.456825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.925205\" y=\"70.558834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.319636\" y=\"99.113743\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.278791\" y=\"79.203896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.434161\" y=\"43.082933\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"167.558883\" y=\"49.999154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.350653\" y=\"80.905868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.014254\" y=\"53.759361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.995502\" y=\"57.926678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"160.34066\" y=\"30.864672\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.452077\" y=\"84.073393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.964594\" y=\"85.092948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.420073\" y=\"59.609608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.2422\" y=\"85.599525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.279531\" y=\"37.233837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.299046\" y=\"57.501901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.090431\" y=\"136.476002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.285742\" y=\"93.124352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.883758\" y=\"79.8113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.341734\" y=\"62.819942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.614272\" y=\"81.136541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.947694\" y=\"106.779282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.874459\" y=\"73.861437\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.434917\" y=\"107.285008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.587661\" y=\"86.318162\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.568218\" y=\"41.339427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.334003\" y=\"98.494541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.792341\" y=\"55.854203\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.765284\" y=\"74.743863\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"65.950443\" y=\"96.890186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.953128\" y=\"68.410737\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.076785\" y=\"86.884353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"165.077753\" y=\"48.318827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.557638\" y=\"82.182124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.465336\" y=\"43.483263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.999096\" y=\"72.569612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.658194\" y=\"72.806161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.614113\" y=\"47.114905\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.097028\" y=\"49.194621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.168313\" y=\"66.412408\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.900855\" y=\"105.564466\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.904687\" y=\"100.306264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.283924\" y=\"106.950633\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.055145\" y=\"64.954008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.513609\" y=\"90.1105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.437905\" y=\"86.995488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.146056\" y=\"62.85089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.417844\" y=\"93.940841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.497704\" y=\"79.150901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.479509\" y=\"68.84641\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.86093\" y=\"101.662611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.037075\" y=\"49.77235\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.592107\" y=\"69.579363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.298391\" y=\"111.892028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.540151\" y=\"60.302929\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.701447\" y=\"103.65314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.675451\" y=\"15.765491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.119625\" y=\"94.586921\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.41555\" y=\"54.787315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.166349\" y=\"83.67882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.124819\" y=\"68.62897\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.697529\" y=\"82.079448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.465559\" y=\"67.164617\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.395364\" y=\"97.354337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.4787\" y=\"90.121634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.71934\" y=\"79.292481\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.647301\" y=\"61.17923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.39114\" y=\"64.295246\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"68.965886\" y=\"116.605219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.765768\" y=\"82.715272\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.540192\" y=\"92.86394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.89068\" y=\"70.888666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"165.89304\" y=\"13.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"176.577749\" y=\"69.149559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.099286\" y=\"42.032186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.240875\" y=\"57.837107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.595161\" y=\"75.377153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"54.132348\" y=\"131.895592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.467119\" y=\"59.141719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.744611\" y=\"111.353278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.646477\" y=\"96.236795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.454028\" y=\"64.276207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.72858\" y=\"60.290799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.81705\" y=\"106.724808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.66873\" y=\"85.050771\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.426944\" y=\"107.254549\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.781012\" y=\"111.184544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.073136\" y=\"95.462359\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.5756\" y=\"67.259161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.399948\" y=\"70.232714\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.661228\" y=\"49.957704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.02515\" y=\"66.102736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.587138\" y=\"83.945404\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.828517\" y=\"77.177288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.246521\" y=\"76.923153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.757621\" y=\"90.314048\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.148795\" y=\"62.515236\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.402578\" y=\"59.281458\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"71.00132\" y=\"101.318345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.57908\" y=\"64.226513\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.297553\" y=\"102.041242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.14116\" y=\"77.644053\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.532508\" y=\"91.387142\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"215.364915\" y=\"27.810463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.557131\" y=\"81.917821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.399975\" y=\"89.348764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.696899\" y=\"35.092983\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.216523\" y=\"65.048166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.750923\" y=\"43.871166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.567312\" y=\"75.364399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.841989\" y=\"76.088841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.497954\" y=\"72.693093\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"82.079833\" y=\"83.175531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.359196\" y=\"79.505546\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.410509\" y=\"100.975342\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.404638\" y=\"124.807206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.355983\" y=\"90.983812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.025712\" y=\"83.077619\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.753408\" y=\"83.761282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.473648\" y=\"87.596558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"76.848333\" y=\"84.604151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.54825\" y=\"116.228875\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.241611\" y=\"77.64884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.800053\" y=\"113.955505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.94458\" y=\"68.287306\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.352201\" y=\"71.528892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.151741\" y=\"74.031821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.390124\" y=\"98.097999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.295013\" y=\"48.35415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.282461\" y=\"86.280172\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.137694\" y=\"67.597294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.418614\" y=\"77.180758\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"180.585042\" y=\"64.412474\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.505276\" y=\"112.626968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.83676\" y=\"95.91145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.953483\" y=\"79.49606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.450492\" y=\"84.052712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.20357\" y=\"83.414741\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.473731\" y=\"75.428867\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.974924\" y=\"78.310834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.555073\" y=\"70.326748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.47176\" y=\"73.301386\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.135547\" y=\"71.147066\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.265214\" y=\"99.081935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.139364\" y=\"78.237334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.121391\" y=\"89.874683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.026376\" y=\"74.284852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.496048\" y=\"72.372303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.263738\" y=\"75.555399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"66.330597\" y=\"121.223819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.496239\" y=\"80.050575\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.746725\" y=\"78.240407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.019036\" y=\"75.147613\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.167031\" y=\"102.542952\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.733378\" y=\"87.033987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.758837\" y=\"45.301327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.225777\" y=\"91.822422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.013595\" y=\"76.642438\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.577436\" y=\"77.949601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.717064\" y=\"30.519615\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.419942\" y=\"73.579209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.874708\" y=\"82.110649\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.592761\" y=\"79.188468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"70.614825\" y=\"104.574014\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.928618\" y=\"61.68454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"173.787735\" y=\"62.090174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.897787\" y=\"64.382054\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.262547\" y=\"114.576675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.003667\" y=\"93.266282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.857955\" y=\"45.160321\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.470098\" y=\"104.21218\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.86453\" y=\"71.754109\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.791658\" y=\"68.087322\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.37824\" y=\"81.411749\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.522723\" y=\"50.130527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.619915\" y=\"50.705691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.740067\" y=\"63.967413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.503223\" y=\"57.232677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.90262\" y=\"84.365181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.082842\" y=\"77.832046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.714402\" y=\"113.094996\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.922321\" y=\"91.140991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.429953\" y=\"105.741195\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.251272\" y=\"68.392803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.652541\" y=\"75.12353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.281016\" y=\"90.649089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.225874\" y=\"104.070657\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.851862\" y=\"63.468873\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.874801\" y=\"82.86794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.361829\" y=\"85.608542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.679553\" y=\"97.206753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.433652\" y=\"68.260558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.707459\" y=\"56.226981\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.466984\" y=\"60.232185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.211002\" y=\"51.873542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"191.749207\" y=\"18.103516\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.151989\" y=\"93.246023\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.586849\" y=\"73.027207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.664094\" y=\"43.075637\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.632218\" y=\"67.577574\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.688456\" y=\"79.029583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.443717\" y=\"119.915368\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.064883\" y=\"83.74487\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"166.324197\" y=\"76.370095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.687512\" y=\"92.48442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"184.488892\" y=\"49.338792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"74.158453\" y=\"100.184448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.142167\" y=\"97.096052\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.452225\" y=\"67.031251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.464654\" y=\"87.363325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"180.956995\" y=\"21.562226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"169.849401\" y=\"69.816669\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.234793\" y=\"78.500167\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.43934\" y=\"89.430395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.511813\" y=\"91.456843\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.525078\" y=\"56.366234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.370516\" y=\"64.606874\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.840956\" y=\"78.651995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.986175\" y=\"72.41565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"67.812426\" y=\"107.662652\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.24277\" y=\"99.172621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.122348\" y=\"96.370217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.984574\" y=\"79.02333\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.10275\" y=\"89.016402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.469622\" y=\"104.754022\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.055326\" y=\"96.801986\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.311371\" y=\"111.927308\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.977943\" y=\"82.793202\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"181.183955\" y=\"41.859056\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.899333\" y=\"93.851658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.641859\" y=\"60.980031\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.884806\" y=\"74.145029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"82.031302\" y=\"94.163492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.943946\" y=\"68.159095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.227504\" y=\"102.06078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.542706\" y=\"57.863059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.00117\" y=\"85.641259\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.462926\" y=\"94.75956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.415666\" y=\"112.293427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.501266\" y=\"90.257794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.875095\" y=\"65.223327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.522966\" y=\"83.79076\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.38404\" y=\"75.850764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"191.302369\" y=\"55.738297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.952036\" y=\"72.706857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.052766\" y=\"78.429072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.100739\" y=\"73.384083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.68414\" y=\"88.150926\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.949254\" y=\"84.380879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.488359\" y=\"89.189101\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.403712\" y=\"62.18428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.634546\" y=\"71.018209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.618662\" y=\"70.038488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.950225\" y=\"72.531255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.796374\" y=\"74.393081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.97434\" y=\"53.162821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.619682\" y=\"121.830153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.26606\" y=\"80.320939\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.049219\" y=\"64.345249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.42358\" y=\"80.548704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.697011\" y=\"63.643561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.620693\" y=\"81.658512\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.167502\" y=\"101.152384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.36571\" y=\"66.060699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.537929\" y=\"92.592949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.695567\" y=\"67.56122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.542432\" y=\"90.036556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"168.065405\" y=\"57.539528\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.202409\" y=\"79.329016\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.17291\" y=\"78.861112\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.472423\" y=\"83.921422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.067353\" y=\"75.450786\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.600733\" y=\"66.182709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.687881\" y=\"98.983666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.388839\" y=\"65.131705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.364471\" y=\"69.319809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.523987\" y=\"78.456473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.684456\" y=\"51.867579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"66.500922\" y=\"82.685242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.931686\" y=\"73.055611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.580351\" y=\"84.355827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.128055\" y=\"87.599534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.042181\" y=\"93.319693\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.098316\" y=\"67.56309\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.452956\" y=\"63.42464\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"181.400577\" y=\"48.451007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.510389\" y=\"55.90116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.469543\" y=\"62.556877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.694968\" y=\"67.885119\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.093586\" y=\"77.585043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.199189\" y=\"87.156956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.044037\" y=\"58.183763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.598942\" y=\"82.54097\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.157904\" y=\"63.575948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.28948\" y=\"76.863967\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.132618\" y=\"99.133675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.697074\" y=\"66.921095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"182.751259\" y=\"52.645248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.206545\" y=\"79.424332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.563205\" y=\"63.33256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.903969\" y=\"39.476263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.87899\" y=\"86.515271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.267502\" y=\"99.654722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.307719\" y=\"91.443661\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.555387\" y=\"93.766817\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.017868\" y=\"72.543665\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.969671\" y=\"96.980838\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.293017\" y=\"93.142453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.994584\" y=\"130.32799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.427291\" y=\"37.928601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"166.354605\" y=\"62.978298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.483096\" y=\"73.937807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.54527\" y=\"88.439678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.541587\" y=\"109.732608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.425463\" y=\"97.713903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.188517\" y=\"51.397677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.721828\" y=\"69.78173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.177918\" y=\"71.655387\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.965602\" y=\"74.834836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"188.57917\" y=\"52.43887\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.583692\" y=\"78.713174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.018702\" y=\"70.531854\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.213885\" y=\"61.076881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.605197\" y=\"74.547729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.640975\" y=\"52.674687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.011495\" y=\"59.810942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.194426\" y=\"61.533266\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.221769\" y=\"105.538605\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.302124\" y=\"83.827882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.453243\" y=\"73.022932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.00141\" y=\"72.99407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.698996\" y=\"64.55689\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.138168\" y=\"83.376626\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.811148\" y=\"91.934398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.818379\" y=\"85.236683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.508163\" y=\"80.325857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.762443\" y=\"53.38592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.850859\" y=\"86.51086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.383111\" y=\"68.541043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"68.207159\" y=\"100.24009\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.417239\" y=\"118.103977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.672379\" y=\"90.561111\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.416054\" y=\"69.697515\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.346262\" y=\"62.456778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.383049\" y=\"94.957518\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.253293\" y=\"92.102869\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.880967\" y=\"66.853116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.0285\" y=\"90.435422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.009441\" y=\"71.333254\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.573869\" y=\"103.614709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.573603\" y=\"98.96007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.108641\" y=\"81.294635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.700306\" y=\"81.23083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.120846\" y=\"102.947303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.863831\" y=\"79.235428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"91.39912\" y=\"82.178295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.719783\" y=\"91.059508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.136266\" y=\"86.045153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.852558\" y=\"65.405451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.931748\" y=\"69.00258\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.923474\" y=\"82.574475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.203872\" y=\"115.920234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.399908\" y=\"89.551559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.549853\" y=\"77.133567\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.650045\" y=\"68.029976\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.900625\" y=\"84.591819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.236454\" y=\"95.674044\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.441901\" y=\"83.951852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.453651\" y=\"48.107719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.528815\" y=\"91.144692\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.272931\" y=\"77.966158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.235085\" y=\"101.867718\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.639236\" y=\"69.104578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.173343\" y=\"95.437832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.640558\" y=\"91.398395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.366991\" y=\"82.985289\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.357063\" y=\"113.470475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.19085\" y=\"87.582783\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.92588\" y=\"80.594545\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.507434\" y=\"75.882131\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.130443\" y=\"83.48542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.525102\" y=\"75.831255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.416936\" y=\"84.442152\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.333151\" y=\"66.885205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.185721\" y=\"87.367559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.657561\" y=\"75.389946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.15855\" y=\"62.102534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.341837\" y=\"89.780347\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.317039\" y=\"73.903159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"71.70301\" y=\"115.759169\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.533778\" y=\"99.445677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.90197\" y=\"100.660773\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.346168\" y=\"95.951702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.431077\" y=\"91.992748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.271051\" y=\"93.511792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.176293\" y=\"93.644631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"193.891924\" y=\"39.124589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.574687\" y=\"89.347345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.69041\" y=\"69.862951\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.956908\" y=\"101.115803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.691024\" y=\"68.037402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m6ff9f381ac\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6ff9f381ac\" x=\"69.573382\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(62.202288 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6ff9f381ac\" x=\"125.776441\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(122.595191 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6ff9f381ac\" x=\"181.979501\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(178.798251 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <defs>\n",
       "       <path id=\"m28622f4ef6\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"125.71812\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −5 -->\n",
       "      <g transform=\"translate(7.2 129.517339) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"99.751174\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(15.579688 103.550393) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"73.784228\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(15.579688 77.583446) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"47.817281\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(9.217188 51.6165) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"21.850335\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(9.217188 25.649554) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 28.942188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 28.942188 7.2 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p4610e2cf3f\">\n",
       "   <rect x=\"28.942188\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘图，查看分布\n",
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:, (0)].detach().numpy(), \n",
    "                labels.detach().numpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. <a id='toc8_1_2_'></a>[读取数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7269, -0.1631],\n",
      "        [ 2.0933, -0.5410],\n",
      "        [ 0.7562, -0.6686],\n",
      "        [-0.4302,  0.3302],\n",
      "        [-0.1591,  1.4465],\n",
      "        [ 0.7235, -0.8781],\n",
      "        [ 0.0123,  0.3597],\n",
      "        [ 1.0409, -2.0936],\n",
      "        [ 0.6744, -0.2588],\n",
      "        [-0.2561, -0.4138]]) \n",
      " tensor([[ 6.2141],\n",
      "        [10.2228],\n",
      "        [ 7.9691],\n",
      "        [ 2.2033],\n",
      "        [-1.0338],\n",
      "        [ 8.6305],\n",
      "        [ 2.9873],\n",
      "        [13.3875],\n",
      "        [ 6.4233],\n",
      "        [ 5.0989]])\n"
     ]
    }
   ],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)                                 # 把原来的indices顺序给打乱了\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. <a id='toc8_1_3_'></a>[初始化模型参数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(mean= 0, std= 0.01, size= (2,1), requires_grad= True)\n",
    "b = torch.zeros(size= (1,), requires_grad= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4. <a id='toc8_1_4_'></a>[定义模型](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b): \n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5. <a id='toc8_1_5_'></a>[定义损失函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  \n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.6. <a id='toc8_1_6_'></a>[定义优化算法](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  \n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.7. <a id='toc8_1_7_'></a>[训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000051\n",
      "epoch 2, loss 0.000056\n",
      "epoch 3, loss 0.000051\n",
      "epoch 4, loss 0.000050\n",
      "epoch 5, loss 0.000048\n",
      "epoch 6, loss 0.000052\n",
      "epoch 7, loss 0.000052\n",
      "epoch 8, loss 0.000049\n",
      "epoch 9, loss 0.000049\n",
      "epoch 10, loss 0.000049\n",
      "w的估计误差: tensor([-0.0010,  0.0004], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([-0.0013], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "num_epochs = 10\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n",
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. <a id='toc8_2_'></a>[现线性回归模型于训练过程-简洁实现](#toc0_)\n",
    "### 8.2.1. <a id='toc8_2_1_'></a>[虚拟数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. <a id='toc8_2_2_'></a>[读取数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3. <a id='toc8_2_3_'></a>[定义模型](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4. <a id='toc8_2_4_'></a>[初始化模型参数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5. <a id='toc8_2_5_'></a>[定义损失函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.6. <a id='toc8_2_6_'></a>[定义优化算法](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "trainer = optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.7. <a id='toc8_2_7_'></a>[训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000298\n",
      "epoch 2, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 5, loss 0.000100\n",
      "epoch 6, loss 0.000099\n",
      "epoch 7, loss 0.000099\n",
      "epoch 8, loss 0.000099\n",
      "epoch 9, loss 0.000100\n",
      "epoch 10, loss 0.000098\n",
      "w的估计误差： tensor([-0.0003,  0.0004])\n",
      "b的估计误差： tensor([-0.0004])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        y_hat = net(X)                  # 1. 计算y_hat\n",
    "        loss = loss_fn(y_hat ,y)        # 2. 计算loss值\n",
    "        trainer.zero_grad()\n",
    "        loss.backward()                 # 2. 求梯度           \n",
    "        trainer.step()                  # 3. 更新网络权重参数\n",
    "    train_loss = loss_fn(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {train_loss:f}')\n",
    "\n",
    "\n",
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.8. <a id='toc8_2_8_'></a>[参数保存](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": num_epochs, \n",
    "        'mode_state_dict': net.state_dict(), \n",
    "        'opt_state_dict': trainer.state_dict(), \n",
    "        'loss': 'loss'\n",
    "    }, \n",
    "    'Pytorch_params/line_params.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.9. <a id='toc8_2_9_'></a>[重载](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0603])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32820/1977999358.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load('./Pytorch_params/line_params.pt')\n"
     ]
    }
   ],
   "source": [
    "check_point = torch.load('./Pytorch_params/line_params.pt')\n",
    "\n",
    "new_net = net = nn.Sequential(nn.Linear(2, 1))\n",
    "new_net.load_state_dict(check_point['mode_state_dict'])\n",
    "\n",
    "new_opt = optim.SGD(new_net.parameters(), lr=0.03)\n",
    "new_opt.load_state_dict(check_point['opt_state_dict'])\n",
    "\n",
    "# Stop BN、Dropout ...\n",
    "new_net.eval()\n",
    "\n",
    "# 停止计算梯度，节省运算和内存\n",
    "with torch.no_grad():\n",
    "    pre = new_net(torch.Tensor([3.0, 2.1]))\n",
    "    print(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. <a id='toc8_3_'></a>[分类-softmax](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. <a id='toc8_3_1_'></a>[快速实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([0.0900, 0.2447, 0.6652]), tensor(1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(3, dtype= torch.float32)\n",
    "x_softmax = torch.nn.functional.softmax(x, dim= 0)\n",
    "\n",
    "x, x_softmax, x_softmax.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. <a id='toc8_3_2_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([0.0900, 0.2447, 0.6652]), tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    x_exp = torch.exp(x)\n",
    "    partition = x_exp.sum()\n",
    "    return x_exp / partition \n",
    "\n",
    "\n",
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x_sf = softmax(x)\n",
    "\n",
    "x, x_sf, x_sf.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3. <a id='toc8_3_3_'></a>[交叉熵损失](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1599)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.arange(3, dtype=torch.float32)\n",
    "y = torch.tensor([0, 2, 1], dtype= torch.float32)\n",
    "\n",
    "y_hat = torch.tensor([0.1, 0.3, 0.6])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss = loss_fn(y_hat, y)\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. <a id='toc8_4_'></a>[专题-模型定义（计算预测值y_hat）](#toc0_)\n",
    "PyTorch的`nn`模块，提供了`nn.Module`类，用于定义神经网络模型。`nn.Module`是所有神经网络模型的基类，提供了构建、初始化、前向传播等功能。有很多`nn.Module`的子类，如`nn.Sequential`、`nn.ModuleList`、`nn.ModuleDict`等，用于构建和组织神经网络模型。`nn`中有很多现成的模块可以直接调用，比如`nn.Linear`、`nn.Conv2d`、`nn.LSTM`等。当然，也可以自定义神经网络模型，如通过继承`nn.Module`类，并重写`__init__`和`forward`方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1. <a id='toc8_4_1_'></a>[块：torch.nn模块](#toc0_)\n",
    "#### 8.4.1.1. <a id='toc8_4_1_1_'></a>[Sequential、ModuleList、ModuleDict](#toc0_)\n",
    "1. nn.`Sequential`(module1, module2, module3, ...)\n",
    "    1. .append()\n",
    "    2. .extend()\n",
    "    3. .insert()\n",
    "    4. .pop()\n",
    "    5. `.add_module()`\n",
    "\n",
    "2. nn.`ModuleList`([module1, module2, modeul3, ...])\n",
    "    1. .append()    # 追加\n",
    "    2. .extend()    # 拼接两个ModuleList\n",
    "    3. .insert()    # 指定位置插入\n",
    "    4. `.add_module()`\n",
    "\n",
    "3. nn.`ModuleDict`({'m1': module1, 'm2': module2, 'm3': module3, ...})\n",
    "    1. clear()  # 清空ModuleDict\n",
    "    2. items()  # 返回可迭代key: value\n",
    "    3. keys()   # 返回keys\n",
    "    4. values() # 返回values\n",
    "    5. pop()    # 返回一对key: value，并从字典中删除\n",
    "    6. `add_module()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nn.ModuleDict), help(nn.ModuleList), help(nn.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (4): Tanh()\n",
       "  (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (6): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(786, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 256), \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(256, 10), \n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = nn.ModuleList([net, net, net])\n",
    "\n",
    "net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (m1): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       "  (m2): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       "  (m3): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = nn.ModuleDict(\n",
    "    {\n",
    "        'm1': net,\n",
    "        'm2': net, \n",
    "        'm3': net\n",
    "    }\n",
    ")\n",
    "\n",
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "model2:\n",
      "ModelWithModuleList(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "model3:\n",
      "ModelWithModuleDict(\n",
      "  (layers): ModuleDict(\n",
      "    (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "# Sequential 实现\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "\n",
    "# ModuleList 实现\n",
    "class ModelWithModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithModuleList, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model2 = ModelWithModuleList()\n",
    "\n",
    "\n",
    "# ModuleDict 实现\n",
    "class ModelWithModuleDict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithModuleDict, self).__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'fc1': nn.Linear(10, 20),\n",
    "            'activation': nn.ReLU(),\n",
    "            'fc2': nn.Linear(20, 1)\n",
    "        })\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers['fc1'](x)\n",
    "        x = self.layers['activation'](x)\n",
    "        x = self.layers['fc2'](x)\n",
    "        return x\n",
    "\n",
    "model3 = ModelWithModuleDict()\n",
    "\n",
    "\n",
    "print('model1:', model1, sep='\\n')\n",
    "print('model2:', model2, sep='\\n')\n",
    "print('model3:', model3, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1.2. <a id='toc8_4_1_2_'></a>[比较](#toc0_)\n",
    "\n",
    "|特性|Sequential|ModuleList|ModuleDict|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|子模块组织方式|按顺序组织|按列表形式组织|按键值对形式组织|\n",
    "|前向传播|自动实现，按顺序调用|需要手动实现，灵活|需要手动实现，可按键值灵活调用|\n",
    "|灵活性|低|中|高|\n",
    "|适用场景|简单顺序模型|动态模型、循环模型|非顺序复杂模型|\n",
    "|动态添加子模块|不支持|支持|支持|\n",
    "|子模块调用方式|固定顺序调用|按索引调用|按键名调用|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2. <a id='toc8_4_2_'></a>[块：自定义](#toc0_)\n",
    "通过继承`nn.Module`类，并重写`__init__`和`forward`方法，可以自定义神经网络模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2.1. <a id='toc8_4_2_1_'></a>[自定义块](#toc0_)\n",
    "\n",
    "* 从编程的角度看：块就是Class\n",
    "\n",
    "* `nn.Module`会自动调用`forward()`方法，我们也可以重写该方法，从而实现更加灵活的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''定义每个块或层'''\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''正向传播'''\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2.2. <a id='toc8_4_2_2_'></a>[顺序块](#toc0_)\n",
    "```\n",
    "Sequential就是顺序块，这里我们自己从头实现一边Sequential这个方法\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2.3. <a id='toc8_4_2_3_'></a>[效率](#toc0_)\n",
    "\n",
    "1. 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "2. 块可以包含代码。\n",
    "3. 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "4. 层和块的顺序连接由Sequential块处理。\n",
    "\n",
    "读者可能会开始担心操作效率的问题。 毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、 代码执行和许多其他的Python代码。 Python的问题全局解释器锁 是众所周知的。 在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.3. <a id='toc8_4_3_'></a>[模型结构/组成](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Init the Net()\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.1. <a id='toc8_4_3_1_'></a>[.children()](#toc0_)\n",
    "列出`第一级别`的module权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=2, out_features=3, bias=True),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=3, out_features=128, bias=True)\n",
       " ),\n",
       " Linear(in_features=128, out_features=2, bias=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net\n",
    "list(net.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.2. <a id='toc8_4_3_2_'></a>[.named_children()](#toc0_)\n",
    "列出`第一级别`的module权重名称和权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear \t Linear(in_features=2, out_features=3, bias=True)\n",
      "block \t Sequential(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      ")\n",
      "decode \t Linear(in_features=128, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, children in net.named_children():\n",
    "    print(name, '\\t', children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.3. <a id='toc8_4_3_3_'></a>[.modules()](#toc0_)\n",
    "依次列出`所有`的module权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (block): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  )\n",
      "  (decode): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "==========\n",
      "Linear(in_features=2, out_features=3, bias=True)\n",
      "==========\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      ")\n",
      "==========\n",
      "Linear(in_features=3, out_features=128, bias=True)\n",
      "==========\n",
      "Linear(in_features=128, out_features=2, bias=True)\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for module in net.modules():\n",
    "    print( module)\n",
    "    print('='*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.4. <a id='toc8_4_3_4_'></a>[.named_modules()](#toc0_)\n",
    "依次列出`所有`的module权重名和权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> Net(\n",
      "  (linear): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (block): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  )\n",
      "  (decode): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "==========\n",
      "linear >>> Linear(in_features=2, out_features=3, bias=True)\n",
      "==========\n",
      "block >>> Sequential(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      ")\n",
      "==========\n",
      "block.0 >>> Linear(in_features=3, out_features=128, bias=True)\n",
      "==========\n",
      "decode >>> Linear(in_features=128, out_features=2, bias=True)\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for name, module in net.named_modules():\n",
    "    print(name, '>>>', module)\n",
    "    print('='*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.5. <a id='toc8_4_3_5_'></a>[删除和添加](#toc0_)\n",
    "先利用 net`.children()`迭代话模型第一层级，再`列表化 (list())` 并进行`索引提取`，最终实现删除或添加的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "   (1): Sequential(\n",
       "     (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "   )\n",
       "   (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "   (3): Linear(in_features=256, out_features=2, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "class NetDel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.net = Net()                                              ## 会报错\n",
    "        self.netdel_list = list(Net().children())[0:-1]                 # 删除最后一个结构\n",
    "        self.netdel_list += [nn.Linear(128, 256), nn.Linear(256, 2)]    # 添加两个新的结构\n",
    "        self.netdel = nn.Sequential(*self.netdel_list)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.netdel(X)\n",
    "\n",
    "\n",
    "netdel = NetDel()\n",
    "# netdel\n",
    "list(netdel.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.6. <a id='toc8_4_3_6_'></a>[替换](#toc0_)\n",
    "直接访`问模型的具体层级`，`替换`即可。\n",
    "\n",
    "* 当通过 `Sequential类` 定义模型时，我们可以通过 `索引 (下标)` 来访问模型的任意层；\n",
    "\n",
    "* `自定义的重载nn.Module` 的layer1、layer2等等，需要net`.`layer1或net`.`layer2方式进行调用；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetMod(\n",
       "  (model): Net(\n",
       "    (linear): Linear(in_features=2, out_features=3, bias=True)\n",
       "    (block): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    )\n",
       "    (decode): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "class NetMod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Net()\n",
    "        in_features = self.model.decode.in_features\n",
    "        self.model.decode = nn.Linear(in_features=in_features, out_features=10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "\n",
    "netmod = NetMod()\n",
    "netmod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.7. <a id='toc8_4_3_7_'></a>[add_module()](#toc0_)\n",
    "`add_module()` 方法用于将子模块添加到当前模块中，并为其指定一个名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (block): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  )\n",
       "  (decode): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (add_demo): Linear(in_features=2, out_features=256, bias=True)\n",
       "  (final_demo): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.add_module(name='add_demo', module=nn.Linear(2, 256))\n",
    "net.add_module(name='final_demo', module=nn.Sequential(nn.Linear(256, 128), nn.Linear(128, 2)))\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.4. <a id='toc8_4_4_'></a>[模型：参数管理](#toc0_)\n",
    "\n",
    "* 其实可以将`nn.Sequential`视为Python的`list数据结构`，`按顺序`储存神经网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0865],\n",
       "        [-0.0746]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.4.1. <a id='toc8_4_4_1_'></a>[参数访问](#toc0_)\n",
    "\n",
    "* 我们从已有模型中访问参数；\n",
    "\n",
    "* 当通过 `Sequential类` 定义模型时，我们可以通过 `索引 (下标)` 来访问模型的任意层；\n",
    "\n",
    "* `自定义的重载nn.Module` 的layer1、layer2等等，需要net`.`layer1或net`.`layer2方式进行调用；\n",
    "\n",
    "* 这就像模型是一个列表一样，每层的参数都在其属性中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.1.1. <a id='toc8_4_4_1_1_'></a>[state_dict](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net # nn.Sequential类，可以直接用下标进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=8, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=8, out_features=1, bias=True))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0], net[1], net[2] # nn.Sequential类，可以直接用下标进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "                      [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "                      [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "                      [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "                      [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "                      [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "                      [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "                      [ 0.4440, -0.2937, -0.0911, -0.4929]])),\n",
       "             ('bias',\n",
       "              tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880]))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "        [ 0.4440, -0.2937, -0.0911, -0.4929]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data # 访问目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias.data # 访问目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "                      [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "                      [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "                      [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "                      [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "                      [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "                      [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "                      [ 0.4440, -0.2937, -0.0911, -0.4929]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0894, -0.1603, -0.1185,  0.0858, -0.0592, -0.1632,  0.1876, -0.0784]])),\n",
       "             ('2.bias', tensor([-0.1285]))])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以直接输出神经网络的所有层参数信息，net[1]是relu激活函数，没有参数，所以就显示无\n",
    "# 后续，torch.save(net.state_dict(), 'Pytorch_datasets/net_params)\n",
    "\n",
    "net.state_dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.1.2. <a id='toc8_4_4_1_2_'></a>[parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<generator object Module.parameters at 0x7f0dc565da80>,\n",
       " <bound method Module.parameters of Sequential(\n",
       "   (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=8, out_features=1, bias=True)\n",
       " )>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters(), net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
      "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
      "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
      "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
      "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
      "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
      "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
      "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
      "       requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[-0.0894, -0.1603, -0.1185,  0.0858, -0.0592, -0.1632,  0.1876, -0.0784]],\n",
      "       requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.1285], requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(type(param))\n",
    "    print(param)\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "    print(param.is_leaf)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
      "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
      "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
      "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
      "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
      "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
      "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
      "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "Parameter containing:\n",
      "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
      "       requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param  in net[0].parameters():\n",
    "    print(param)\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "    print(param.is_leaf)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.1.3. <a id='toc8_4_4_1_3_'></a>[named_parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight \t Parameter containing:\n",
      "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
      "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
      "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
      "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
      "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
      "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
      "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
      "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)\n",
      "0.bias \t Parameter containing:\n",
      "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
      "       requires_grad=True)\n",
      "2.weight \t Parameter containing:\n",
      "tensor([[-0.0894, -0.1603, -0.1185,  0.0858, -0.0592, -0.1632,  0.1876, -0.0784]],\n",
      "       requires_grad=True)\n",
      "2.bias \t Parameter containing:\n",
      "tensor([-0.1285], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# list(net.named_parameters())\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, '\\t', param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.4.2. <a id='toc8_4_4_2_'></a>[参数初始化](#toc0_)\n",
    "\n",
    "* 初始化，主要是为了不要再一开始训练就炸掉了，其实不用太迷信了。\n",
    "\n",
    "* 默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 \n",
    "\n",
    "* PyTorch的nn.init模块提供了多种预置初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.2.1. <a id='toc8_4_4_2_1_'></a>[内置初始化](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = net[0]\n",
    "nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.2.2. <a id='toc8_4_4_2_2_'></a>[自定义初始化](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.2.3. <a id='toc8_4_4_2_3_'></a>[参数绑定](#toc0_)\n",
    "```\n",
    "有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(),\n",
    "    shared, \n",
    "    nn.ReLU(),\n",
    "    shared, \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "\n",
    "net(X)\n",
    "\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "\n",
    "net[2].weight.data[0, 0] = 100\n",
    "\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.5. <a id='toc8_4_5_'></a>[层：自定义](#toc0_)\n",
    "深度学习成功背后的一个因素是神经网络的灵活性：我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。\n",
    "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。\n",
    "\n",
    "```python\n",
    "块和层其实并无本质的区别，因为都是torch.nn.Module的子类\n",
    "e.g. \n",
    "    全连接层（FC）\n",
    "    池化层（Pooling）\n",
    "    BN层\n",
    "    Dropout层\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.5.1. <a id='toc8_4_5_1_'></a>[不带参数的层](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0.,  1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "    \n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9.3132e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在，我们可以将层作为组件合并到更复杂的模型中。\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(8, 128), \n",
    "    CenteredLayer()\n",
    ")\n",
    "\n",
    "\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.5.2. <a id='toc8_4_5_2_'></a>[带参数的层](#toc0_)\n",
    "\n",
    "用到`nn.Parameter()`可以将参数加入神经网络中，便于自动管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.1466, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units)) \n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "  \n",
    "    \n",
    "linear = MyLinear(5, 3)\n",
    "# linear.weight\n",
    "\n",
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们还可以使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层。\n",
    "net = nn.Sequential(\n",
    "    MyLinear(64, 8), \n",
    "    MyLinear(8, 1)\n",
    ")\n",
    "\n",
    "\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. <a id='toc8_5_'></a>[专题-损失函数 (loss_fn)](#toc0_)\n",
    "损失函数的输入是 loss_fn(y_hat, y) ，即网络输出和真实标签对的数据，然后返回一个数值表示网络输出和真实标签的差距。\n",
    "\n",
    "  1. 均方误差\n",
    "\n",
    "  2. 交叉熵\n",
    "  \n",
    "  3. 自定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. <a id='toc8_5_1_'></a>[均方误差](#toc0_)\n",
    "回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. <a id='toc8_5_2_'></a>[交叉熵](#toc0_)\n",
    "分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.2.1. <a id='toc8_5_2_1_'></a>[快速实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 2, 1]),\n",
       " tensor([[0.1000, 0.3000, 0.6000],\n",
       "         [0.3000, 0.2000, 0.5000],\n",
       "         [0.0000, 0.1000, 0.9000]]),\n",
       " tensor(1.2372))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 3个样本，3个类别的索引\n",
    "y = torch.tensor([0, 2, 1])\n",
    "\n",
    "# 3个样本，3个类别，每个样本的概率\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], \n",
    "                      [0.3, 0.2, 0.5], \n",
    "                      [0.0, 0.1, 0.9]])\n",
    "\n",
    "y, y_hat, loss_fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.2.2. <a id='toc8_5_2_2_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 2, 1]),\n",
       " tensor([[0.1000, 0.3000, 0.6000],\n",
       "         [0.3000, 0.2000, 0.5000],\n",
       "         [0.0000, 0.1000, 0.9000]]),\n",
       " tensor([2.3026, 0.6931, 2.3026]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "\n",
    "# 3个样本，3个类别的索引\n",
    "y = torch.tensor([0, 2, 1])\n",
    "\n",
    "# 3个样本，3个类别，每个样本的概率\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], \n",
    "                      [0.3, 0.2, 0.5], \n",
    "                      [0.0, 0.1, 0.9]])\n",
    "\n",
    "y, y_hat, cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. <a id='toc8_5_3_'></a>[自定义](#toc0_)\n",
    "自己定义赏罚分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, y_hat):\n",
    "    '''例如真实值于预测值之差'''\n",
    "    error_values = y - y_hat\n",
    "    return error_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. <a id='toc8_6_'></a>[专题-反向传播（求梯度）](#toc0_)\n",
    "```\n",
    "求梯度（求偏导数）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 见autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7. <a id='toc8_7_'></a>[专题-更新权重（优化算法）](#toc0_)\n",
    "- 优化算法，在深度学习中是非常重要的一环。在对损失函数进行优化的时候，比较关注损失函数的凹凸性的问题。  \n",
    "- 可惜的是，在现有损失函数中，只有线性函数网络结构和softmax结构是凸函数，其它例如MLP、CNN、RNN、注意力等都是非凸函数。  \n",
    "- 并且，在优化过程中通常只是得到了局部最小值，而不是全局最小值；\n",
    "- 小批量随机梯度下降算法是最常用的优化算法；\n",
    "- 冲量对梯度做平滑；\n",
    "- Adam对梯度做平滑，且对梯度各纬度值重新做调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 原函数图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Function')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"161.420313pt\" height=\"170.754375pt\" viewBox=\"0 0 161.420313 170.754375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T09:59:22.781319</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 170.754375 \n",
       "L 161.420313 170.754375 \n",
       "L 161.420313 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "L 42.620312 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m3955d9ea25\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3955d9ea25\" x=\"81.624326\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(78.443076 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3955d9ea25\" x=\"149.486898\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(146.305648 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(95.460938 161.474688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m21e1f615e4\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m21e1f615e4\" x=\"42.620312\" y=\"126.606808\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(20.878125 130.406027) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m21e1f615e4\" x=\"42.620312\" y=\"93.402363\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 97.201582) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m21e1f615e4\" x=\"42.620312\" y=\"60.197918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(29.257812 63.997137) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m21e1f615e4\" x=\"42.620312\" y=\"26.993473\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 30.792691) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- y -->\n",
       "     <g transform=\"translate(14.798437 80.7175) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-79\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_7\">\n",
       "    <path d=\"M 47.69304 60.197918 \n",
       "L 48.710977 61.336992 \n",
       "L 50.06823 63.265924 \n",
       "L 51.42548 65.597492 \n",
       "L 53.122046 68.960653 \n",
       "L 55.497236 74.223956 \n",
       "L 60.926241 86.541322 \n",
       "L 62.962118 90.545485 \n",
       "L 64.658684 93.402364 \n",
       "L 66.015935 95.316709 \n",
       "L 67.373186 96.87056 \n",
       "L 68.730437 98.047244 \n",
       "L 69.748375 98.678438 \n",
       "L 70.766313 99.095749 \n",
       "L 71.784252 99.304222 \n",
       "L 72.802192 99.312165 \n",
       "L 73.82013 99.130978 \n",
       "L 75.177382 98.620286 \n",
       "L 76.534633 97.84017 \n",
       "L 78.231197 96.560293 \n",
       "L 80.267074 94.720068 \n",
       "L 85.356767 89.965806 \n",
       "L 87.053331 88.746799 \n",
       "L 88.410583 88.029771 \n",
       "L 89.767835 87.593164 \n",
       "L 90.785773 87.47357 \n",
       "L 91.803712 87.547239 \n",
       "L 92.821651 87.824559 \n",
       "L 93.839589 88.312769 \n",
       "L 94.857528 89.015799 \n",
       "L 96.214779 90.287734 \n",
       "L 97.57203 91.9337 \n",
       "L 98.929281 93.934282 \n",
       "L 100.625846 96.886623 \n",
       "L 102.661723 100.980855 \n",
       "L 105.376226 107.064319 \n",
       "L 110.126605 117.844073 \n",
       "L 112.162482 121.823736 \n",
       "L 113.519735 124.061687 \n",
       "L 114.876987 125.878509 \n",
       "L 115.894924 126.922303 \n",
       "L 116.912862 127.662687 \n",
       "L 117.930804 128.075472 \n",
       "L 118.609429 128.158125 \n",
       "L 119.288054 128.080341 \n",
       "L 119.966679 127.837457 \n",
       "L 120.645308 127.425543 \n",
       "L 121.663241 126.484183 \n",
       "L 122.681179 125.148719 \n",
       "L 123.699121 123.416557 \n",
       "L 124.717058 121.289651 \n",
       "L 126.074313 117.85176 \n",
       "L 127.431563 113.752931 \n",
       "L 129.128125 107.767401 \n",
       "L 130.824688 100.934129 \n",
       "L 132.860567 91.827467 \n",
       "L 135.91438 76.985201 \n",
       "L 140.664764 53.852137 \n",
       "L 142.700639 45.04904 \n",
       "L 144.397202 38.669416 \n",
       "L 145.754456 34.356072 \n",
       "L 147.111706 30.861146 \n",
       "L 148.129648 28.834828 \n",
       "L 149.147585 27.358125 \n",
       "L 149.147585 27.358125 \n",
       "\" clip-path=\"url(#pc59f7a1f92)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 42.620312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 22.318125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- Function -->\n",
       "    <g transform=\"translate(72.9025 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-46\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"52.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"115.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"178.777344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"233.757812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"272.966797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"300.75\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"361.931641\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pc59f7a1f92\">\n",
       "   <rect x=\"42.620312\" y=\"22.318125\" width=\"111.6\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "x = torch.arange(-1, 2, 0.01, dtype= torch.float32, requires_grad= True)\n",
    "y = x * torch.cos(torch.pi * x)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 导函数图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'grad')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"161.820313pt\" height=\"170.754375pt\" viewBox=\"0 0 161.820313 170.754375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T09:59:25.690317</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 170.754375 \n",
       "L 161.820313 170.754375 \n",
       "L 161.820313 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.020313 133.198125 \n",
       "L 154.620312 133.198125 \n",
       "L 154.620312 22.318125 \n",
       "L 43.020313 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mdf888e2b19\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf888e2b19\" x=\"82.024326\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(78.843076 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mdf888e2b19\" x=\"149.886898\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(146.705648 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(95.860938 161.474688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m1e850c32e8\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e850c32e8\" x=\"43.020313\" y=\"123.017028\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(21.278125 126.816247) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e850c32e8\" x=\"43.020313\" y=\"96.07128\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.657813 99.870498) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e850c32e8\" x=\"43.020313\" y=\"69.125531\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.657813 72.92475) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e850c32e8\" x=\"43.020313\" y=\"42.179782\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(29.657813 45.979001) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- y` -->\n",
       "     <g transform=\"translate(15.198438 83.2175) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-60\" d=\"M 1147 5119 \n",
       "L 2028 3950 \n",
       "L 1550 3950 \n",
       "L 531 5119 \n",
       "L 1147 5119 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-79\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-60\" x=\"59.179688\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_7\">\n",
       "    <path d=\"M 48.09304 109.54415 \n",
       "L 49.789604 115.668505 \n",
       "L 51.146855 119.755067 \n",
       "L 52.504107 123.060589 \n",
       "L 53.522046 125.005961 \n",
       "L 54.539984 126.485062 \n",
       "L 55.557924 127.49654 \n",
       "L 56.236549 127.913077 \n",
       "L 56.915174 128.12645 \n",
       "L 57.593801 128.140545 \n",
       "L 58.272426 127.960316 \n",
       "L 58.951051 127.591743 \n",
       "L 59.968989 126.701287 \n",
       "L 60.986928 125.430448 \n",
       "L 62.004866 123.811035 \n",
       "L 63.362118 121.171749 \n",
       "L 65.058684 117.234419 \n",
       "L 67.09456 111.840406 \n",
       "L 73.880817 93.203818 \n",
       "L 75.577382 89.448398 \n",
       "L 76.934633 86.94922 \n",
       "L 78.291884 84.972038 \n",
       "L 79.309823 83.86377 \n",
       "L 80.327762 83.095343 \n",
       "L 81.3457 82.678145 \n",
       "L 82.024326 82.598405 \n",
       "L 82.702952 82.678145 \n",
       "L 83.381577 82.916838 \n",
       "L 84.399516 83.5692 \n",
       "L 85.417454 84.565768 \n",
       "L 86.435393 85.89176 \n",
       "L 87.792644 88.137402 \n",
       "L 89.149896 90.873458 \n",
       "L 90.84646 94.87064 \n",
       "L 92.882337 100.288078 \n",
       "L 99.329281 118.070227 \n",
       "L 101.025846 121.878722 \n",
       "L 102.383098 124.387442 \n",
       "L 103.401036 125.894441 \n",
       "L 104.418975 127.041817 \n",
       "L 105.436913 127.799157 \n",
       "L 106.115538 128.074371 \n",
       "L 106.794165 128.158125 \n",
       "L 107.47279 128.044941 \n",
       "L 108.151415 127.730403 \n",
       "L 108.83004 127.211176 \n",
       "L 109.84798 126.044039 \n",
       "L 110.865918 124.409073 \n",
       "L 111.883857 122.309619 \n",
       "L 112.901795 119.755075 \n",
       "L 114.259047 115.668505 \n",
       "L 115.616299 110.853708 \n",
       "L 117.312862 103.920836 \n",
       "L 119.348741 94.497239 \n",
       "L 122.063241 80.684933 \n",
       "L 127.152938 54.474942 \n",
       "L 129.188817 45.279694 \n",
       "L 130.885376 38.725586 \n",
       "L 132.24263 34.397953 \n",
       "L 133.260567 31.766937 \n",
       "L 134.278505 29.714185 \n",
       "L 134.95713 28.687269 \n",
       "L 135.635755 27.946199 \n",
       "L 136.31438 27.500431 \n",
       "L 136.993005 27.358125 \n",
       "L 137.671634 27.526148 \n",
       "L 138.350259 28.009941 \n",
       "L 139.028884 28.813546 \n",
       "L 139.70751 29.939558 \n",
       "L 140.725451 32.235029 \n",
       "L 141.743389 35.255163 \n",
       "L 142.76133 38.98932 \n",
       "L 144.118577 45.045738 \n",
       "L 145.475831 52.272171 \n",
       "L 147.172393 62.811538 \n",
       "L 148.86896 74.811197 \n",
       "L 149.547585 79.959361 \n",
       "L 149.547585 79.959361 \n",
       "\" clip-path=\"url(#pcab73c39f3)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.020313 133.198125 \n",
       "L 43.020313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 154.620312 133.198125 \n",
       "L 154.620312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.020313 133.198125 \n",
       "L 154.620313 133.198125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.020313 22.318125 \n",
       "L 154.620313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- grad -->\n",
       "    <g transform=\"translate(85.05875 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-67\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"104.589844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"165.869141\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pcab73c39f3\">\n",
       "   <rect x=\"43.020313\" y=\"22.318125\" width=\"111.6\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def y(x):\n",
    "    y_hat = x * torch.cos(torch.pi * x)\n",
    "    x_grad = torch.autograd.grad(outputs= y_hat, inputs= x)\n",
    "    return x_grad[0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "x_grads = [y(i) for i in x]\n",
    "\n",
    "\n",
    "plt.figure(figsize= (2,2))\n",
    "plt.plot(x.detach().cpu().numpy(), x_grads)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y`')\n",
    "plt.title('grad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def refresh_plot(fig):\n",
    "    '''再jupyter中持续刷新展示图片'''\n",
    "    plt.close()                                 # close figure （推荐）\n",
    "    # plt.show()                                # 普通展示\n",
    "    display.display(fig)                        # 在jupyter中展示 （推荐）\n",
    "    display.clear_output(wait= True)             # 等待 （必须）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"161.420312pt\" height=\"170.754375pt\" viewBox=\"0 0 161.420312 170.754375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T10:31:47.581632</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 170.754375 \n",
       "L 161.420312 170.754375 \n",
       "L 161.420312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "L 42.620312 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m8e598f5f71\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #ff0000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pf7f76d8179)\">\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"149.147585\" y=\"27.358125\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"148.809404\" y=\"27.721463\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"148.404979\" y=\"28.240286\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"147.922742\" y=\"28.976534\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"147.349863\" y=\"30.012969\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"146.672566\" y=\"31.457033\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"145.876748\" y=\"33.44255\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"144.949053\" y=\"36.126587\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"143.878516\" y=\"39.67738\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"142.658836\" y=\"44.249013\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"141.291164\" y=\"49.940788\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"139.786945\" y=\"56.746178\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"138.169949\" y=\"64.507303\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"136.476355\" y=\"72.900725\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"134.751944\" y=\"81.476218\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"133.046477\" y=\"89.746902\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"131.406648\" y=\"97.295975\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"129.869922\" y=\"103.853557\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "     <use xlink:href=\"#m8e598f5f71\" x=\"128.461129\" y=\"109.316517\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mcb49fda76c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcb49fda76c\" x=\"81.511222\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(78.329972 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcb49fda76c\" x=\"149.147585\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(145.966335 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(95.460938 161.474688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m855112d3b8\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m855112d3b8\" x=\"42.620312\" y=\"126.6124\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(20.878125 130.411619) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m855112d3b8\" x=\"42.620312\" y=\"93.527642\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 97.326861) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m855112d3b8\" x=\"42.620312\" y=\"60.442883\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(29.257812 64.242102) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m855112d3b8\" x=\"42.620312\" y=\"27.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 31.157344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- y -->\n",
       "     <g transform=\"translate(14.798437 80.7175) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-79\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_7\">\n",
       "    <path d=\"M 47.69304 60.442883 \n",
       "L 48.707584 61.577851 \n",
       "L 50.060312 63.499831 \n",
       "L 51.413038 65.822995 \n",
       "L 53.10395 69.174033 \n",
       "L 55.471222 74.418364 \n",
       "L 60.88213 86.691332 \n",
       "L 62.911221 90.681062 \n",
       "L 64.602132 93.527643 \n",
       "L 65.954859 95.435087 \n",
       "L 67.307586 96.983338 \n",
       "L 68.660313 98.15578 \n",
       "L 69.674857 98.784699 \n",
       "L 70.689403 99.200505 \n",
       "L 71.703948 99.408228 \n",
       "L 72.718495 99.416141 \n",
       "L 73.73304 99.235608 \n",
       "L 75.085767 98.726756 \n",
       "L 76.438494 97.949452 \n",
       "L 78.129404 96.674189 \n",
       "L 80.158494 94.840597 \n",
       "L 85.231222 90.103472 \n",
       "L 86.922131 88.888859 \n",
       "L 88.274858 88.174415 \n",
       "L 89.627586 87.739382 \n",
       "L 90.642131 87.62022 \n",
       "L 91.656677 87.693622 \n",
       "L 92.671222 87.969943 \n",
       "L 93.685768 88.456393 \n",
       "L 94.700313 89.156889 \n",
       "L 96.05304 90.42424 \n",
       "L 97.405767 92.064273 \n",
       "L 98.758494 94.057644 \n",
       "L 100.449403 96.999343 \n",
       "L 102.478494 101.078817 \n",
       "L 105.183948 107.140353 \n",
       "L 109.918493 117.88125 \n",
       "L 111.947584 121.846568 \n",
       "L 113.300312 124.076453 \n",
       "L 114.65304 125.886726 \n",
       "L 115.667585 126.926758 \n",
       "L 116.682129 127.664473 \n",
       "L 117.696678 128.07577 \n",
       "L 118.373041 128.158125 \n",
       "L 119.049404 128.080621 \n",
       "L 119.725767 127.838613 \n",
       "L 120.402134 127.428183 \n",
       "L 121.416674 126.490217 \n",
       "L 122.431219 125.159567 \n",
       "L 123.445767 123.433648 \n",
       "L 124.460312 121.314409 \n",
       "L 125.813042 117.88891 \n",
       "L 127.165768 113.804855 \n",
       "L 128.856675 107.8409 \n",
       "L 130.547583 101.032259 \n",
       "L 132.576676 91.958422 \n",
       "L 135.620309 77.169656 \n",
       "L 140.354858 54.119976 \n",
       "L 142.383947 45.348611 \n",
       "L 144.074855 38.991982 \n",
       "L 145.427585 34.694186 \n",
       "L 146.780311 31.211857 \n",
       "L 147.794859 29.192843 \n",
       "L 148.809404 27.721463 \n",
       "L 148.809404 27.721463 \n",
       "\" clip-path=\"url(#pf7f76d8179)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_8\">\n",
       "    <path d=\"M 149.147585 27.358125 \n",
       "L 148.809404 27.721463 \n",
       "L 148.404979 28.240286 \n",
       "L 147.922742 28.976534 \n",
       "L 147.349863 30.012969 \n",
       "L 146.672566 31.457033 \n",
       "L 145.876748 33.44255 \n",
       "L 144.949053 36.126587 \n",
       "L 143.878516 39.67738 \n",
       "L 142.658836 44.249013 \n",
       "L 141.291164 49.940788 \n",
       "L 139.786945 56.746178 \n",
       "L 138.169949 64.507303 \n",
       "L 136.476355 72.900725 \n",
       "L 134.751944 81.476218 \n",
       "L 133.046477 89.746902 \n",
       "L 131.406648 97.295975 \n",
       "L 129.869922 103.853557 \n",
       "L 128.461129 109.316517 \n",
       "\" clip-path=\"url(#pf7f76d8179)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 42.620312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220312 133.198125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 22.318125 \n",
       "L 154.220312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- Function -->\n",
       "    <g transform=\"translate(72.9025 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-46\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"52.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"115.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"178.777344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"233.757812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"272.966797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"300.75\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"361.931641\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pf7f76d8179\">\n",
       "   <rect x=\"42.620312\" y=\"22.318125\" width=\"111.6\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display \n",
    "import time\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x * torch.cos(torch.pi * x)\n",
    "\n",
    "def gd(x, y=f, eta=0.01, iter:int=5):\n",
    "    x_list = [x]    # 先存第一个数\n",
    "    i = 1\n",
    "    for _ in range(iter):\n",
    "        x_tensor = torch.tensor(x, dtype= torch.float32, requires_grad= True)\n",
    "        x_grad = torch.autograd.grad(outputs= f(x_tensor), inputs= x_tensor)\n",
    "        x -= (eta * x_grad[0].item())\n",
    "        x_list.append(x)\n",
    "        i += 1\n",
    "    return x_list\n",
    "\n",
    "# 从x开始，迭代iter次\n",
    "def demo(x, y, eta, iter, c):\n",
    "    xx = gd(x= x, y= f, eta= eta, iter= iter )\n",
    "    yy = f(torch.tensor(xx))\n",
    "    return xx, yy.detach().cpu().numpy(), c\n",
    "\n",
    "\n",
    "x = torch.arange(-1, 2, 0.01, dtype= torch.float32, requires_grad= True)\n",
    "y = f(x)\n",
    "\n",
    "eta = 0.01\n",
    "iter = 20\n",
    "c = 'red'\n",
    "xx, yy, c = demo(x= 2, y= f, eta= eta, iter= iter, c= c)    # lr很小就接近收敛\n",
    "\n",
    "for i in range(1, iter):\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(2, 2))\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Function')\n",
    "\n",
    "    plt.scatter(x= xx[0:i], y= yy[0:i], c= c)\n",
    "    plt.plot(xx[:i], yy[:i], c= c)\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait= True)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"161.420312pt\" height=\"170.754375pt\" viewBox=\"0 0 161.420312 170.754375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T10:33:26.661326</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 170.754375 \n",
       "L 161.420312 170.754375 \n",
       "L 161.420312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "L 42.620312 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"ma399fd5153\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #0000ff\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pd32ebf46a0)\">\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"149.147585\" y=\"27.358125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"145.765771\" y=\"33.743224\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"136.311612\" y=\"73.722605\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"119.064051\" y=\"128.077111\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.232994\" y=\"128.153996\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.418352\" y=\"128.157951\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.380229\" y=\"128.158117\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.388226\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386553\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386905\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386831\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386844\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386844\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386845\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386845\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386846\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386846\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386846\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#ma399fd5153\" x=\"118.386847\" y=\"128.158125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m07d4d1ace8\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m07d4d1ace8\" x=\"81.511222\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(78.329972 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m07d4d1ace8\" x=\"149.147585\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(145.966335 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(95.460938 161.474688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m7b96f2919e\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b96f2919e\" x=\"42.620312\" y=\"126.612369\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(20.878125 130.411588) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b96f2919e\" x=\"42.620312\" y=\"93.527621\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 97.32684) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b96f2919e\" x=\"42.620312\" y=\"60.442873\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(29.257812 64.242092) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b96f2919e\" x=\"42.620312\" y=\"27.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 31.157344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- y -->\n",
       "     <g transform=\"translate(14.798437 80.7175) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-79\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_7\">\n",
       "    <path d=\"M 47.69304 60.442873 \n",
       "L 48.707584 61.577841 \n",
       "L 50.060312 63.499819 \n",
       "L 51.413038 65.822983 \n",
       "L 53.10395 69.17402 \n",
       "L 55.471222 74.41835 \n",
       "L 60.88213 86.691313 \n",
       "L 62.911221 90.681042 \n",
       "L 64.602132 93.527622 \n",
       "L 65.954859 95.435066 \n",
       "L 67.307586 96.983316 \n",
       "L 68.660313 98.155758 \n",
       "L 69.674857 98.784677 \n",
       "L 70.689403 99.200483 \n",
       "L 71.703948 99.408205 \n",
       "L 72.718495 99.416119 \n",
       "L 73.73304 99.235585 \n",
       "L 75.085767 98.726734 \n",
       "L 76.438494 97.94943 \n",
       "L 78.129404 96.674168 \n",
       "L 80.158494 94.840576 \n",
       "L 85.231222 90.103453 \n",
       "L 86.922131 88.88884 \n",
       "L 88.274858 88.174396 \n",
       "L 89.627586 87.739363 \n",
       "L 90.642131 87.620201 \n",
       "L 91.656677 87.693604 \n",
       "L 92.671222 87.969924 \n",
       "L 93.685768 88.456374 \n",
       "L 94.700313 89.15687 \n",
       "L 96.05304 90.42422 \n",
       "L 97.405767 92.064253 \n",
       "L 98.758494 94.057623 \n",
       "L 100.449403 96.999321 \n",
       "L 102.478494 101.078794 \n",
       "L 105.183948 107.140328 \n",
       "L 109.918493 117.881222 \n",
       "L 111.947584 121.846539 \n",
       "L 113.300312 124.076422 \n",
       "L 114.65304 125.886695 \n",
       "L 115.667585 126.926726 \n",
       "L 116.682129 127.664442 \n",
       "L 117.696678 128.075739 \n",
       "L 118.373041 128.158093 \n",
       "L 119.049404 128.08059 \n",
       "L 119.725767 127.838581 \n",
       "L 120.402134 127.428152 \n",
       "L 121.416674 126.490186 \n",
       "L 122.431219 125.159536 \n",
       "L 123.445767 123.433618 \n",
       "L 124.460312 121.314379 \n",
       "L 125.813042 117.888881 \n",
       "L 127.165768 113.804828 \n",
       "L 128.856675 107.840875 \n",
       "L 130.547583 101.032236 \n",
       "L 132.576676 91.958402 \n",
       "L 135.620309 77.16964 \n",
       "L 140.354858 54.119967 \n",
       "L 142.383947 45.348605 \n",
       "L 144.074855 38.991978 \n",
       "L 145.427585 34.694183 \n",
       "L 146.780311 31.211856 \n",
       "L 147.794859 29.192843 \n",
       "L 148.809404 27.721463 \n",
       "L 148.809404 27.721463 \n",
       "\" clip-path=\"url(#pd32ebf46a0)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_8\">\n",
       "    <path d=\"M 149.147585 27.358125 \n",
       "L 145.765771 33.743224 \n",
       "L 136.311612 73.722605 \n",
       "L 119.064051 128.077111 \n",
       "L 118.232994 128.153996 \n",
       "L 118.418352 128.157951 \n",
       "L 118.380229 128.158117 \n",
       "L 118.388226 128.158125 \n",
       "L 118.386553 128.158125 \n",
       "L 118.386905 128.158125 \n",
       "L 118.386831 128.158125 \n",
       "L 118.386844 128.158125 \n",
       "L 118.386844 128.158125 \n",
       "L 118.386845 128.158125 \n",
       "L 118.386845 128.158125 \n",
       "L 118.386846 128.158125 \n",
       "L 118.386846 128.158125 \n",
       "L 118.386846 128.158125 \n",
       "L 118.386847 128.158125 \n",
       "\" clip-path=\"url(#pd32ebf46a0)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 42.620312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220312 133.198125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 22.318125 \n",
       "L 154.220312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- Function -->\n",
       "    <g transform=\"translate(72.9025 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-46\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"52.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"115.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"178.777344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"233.757812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"272.966797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"300.75\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"361.931641\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pd32ebf46a0\">\n",
       "   <rect x=\"42.620312\" y=\"22.318125\" width=\"111.6\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 0.1\n",
    "iter = 20\n",
    "c = 'blue'\n",
    "xx, yy, c = demo(x= 2, y= f, eta= eta, iter= iter, c= c)    # lr很大就很快收敛\n",
    "\n",
    "for i in range(1, iter):\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(2, 2))\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Function')\n",
    "\n",
    "    plt.scatter(x= xx[0:i], y= yy[0:i], c= c)\n",
    "    plt.plot(xx[:i], yy[:i], c= c)\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait= True)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"161.420313pt\" height=\"170.754375pt\" viewBox=\"0 0 161.420313 170.754375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T10:34:03.317125</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 170.754375 \n",
       "L 161.420313 170.754375 \n",
       "L 161.420313 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "L 42.620312 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m37a8abbef7\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #0000ff\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p90beff6da9)\">\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"47.69304\" y=\"60.197918\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"54.479295\" y=\"71.911987\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"69.994593\" y=\"98.798852\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"73.210099\" y=\"99.26153\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.095637\" y=\"99.327472\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.406405\" y=\"99.332242\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.309704\" y=\"99.332713\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.339021\" y=\"99.332756\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.330058\" y=\"99.33276\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332791\" y=\"99.33276\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.331957\" y=\"99.332761\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332211\" y=\"99.332761\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332134\" y=\"99.33276\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332158\" y=\"99.33276\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.33215\" y=\"99.33276\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332153\" y=\"99.332761\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332152\" y=\"99.332761\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332152\" y=\"99.332761\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m37a8abbef7\" x=\"72.332151\" y=\"99.33276\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"me070db43ef\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#me070db43ef\" x=\"81.624326\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(78.443076 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me070db43ef\" x=\"149.486898\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(146.305648 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(95.460938 161.474688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m2f2ae7cfde\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f2ae7cfde\" x=\"42.620312\" y=\"126.606808\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(20.878125 130.406027) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f2ae7cfde\" x=\"42.620312\" y=\"93.402363\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 97.201582) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f2ae7cfde\" x=\"42.620312\" y=\"60.197918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(29.257812 63.997137) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f2ae7cfde\" x=\"42.620312\" y=\"26.993473\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 30.792691) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- y -->\n",
       "     <g transform=\"translate(14.798437 80.7175) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-79\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_7\">\n",
       "    <path d=\"M 47.69304 60.197918 \n",
       "L 48.710977 61.336992 \n",
       "L 50.06823 63.265924 \n",
       "L 51.42548 65.597492 \n",
       "L 53.122046 68.960653 \n",
       "L 55.497236 74.223956 \n",
       "L 60.926241 86.541322 \n",
       "L 62.962118 90.545485 \n",
       "L 64.658684 93.402364 \n",
       "L 66.015935 95.316709 \n",
       "L 67.373186 96.87056 \n",
       "L 68.730437 98.047244 \n",
       "L 69.748375 98.678438 \n",
       "L 70.766313 99.095749 \n",
       "L 71.784252 99.304222 \n",
       "L 72.802192 99.312165 \n",
       "L 73.82013 99.130978 \n",
       "L 75.177382 98.620286 \n",
       "L 76.534633 97.84017 \n",
       "L 78.231197 96.560293 \n",
       "L 80.267074 94.720068 \n",
       "L 85.356767 89.965806 \n",
       "L 87.053331 88.746799 \n",
       "L 88.410583 88.029771 \n",
       "L 89.767835 87.593164 \n",
       "L 90.785773 87.47357 \n",
       "L 91.803712 87.547239 \n",
       "L 92.821651 87.824559 \n",
       "L 93.839589 88.312769 \n",
       "L 94.857528 89.015799 \n",
       "L 96.214779 90.287734 \n",
       "L 97.57203 91.9337 \n",
       "L 98.929281 93.934282 \n",
       "L 100.625846 96.886623 \n",
       "L 102.661723 100.980855 \n",
       "L 105.376226 107.064319 \n",
       "L 110.126605 117.844073 \n",
       "L 112.162482 121.823736 \n",
       "L 113.519735 124.061687 \n",
       "L 114.876987 125.878509 \n",
       "L 115.894924 126.922303 \n",
       "L 116.912862 127.662687 \n",
       "L 117.930804 128.075472 \n",
       "L 118.609429 128.158125 \n",
       "L 119.288054 128.080341 \n",
       "L 119.966679 127.837457 \n",
       "L 120.645308 127.425543 \n",
       "L 121.663241 126.484183 \n",
       "L 122.681179 125.148719 \n",
       "L 123.699121 123.416557 \n",
       "L 124.717058 121.289651 \n",
       "L 126.074313 117.85176 \n",
       "L 127.431563 113.752931 \n",
       "L 129.128125 107.767401 \n",
       "L 130.824688 100.934129 \n",
       "L 132.860567 91.827467 \n",
       "L 135.91438 76.985201 \n",
       "L 140.664764 53.852137 \n",
       "L 142.700639 45.04904 \n",
       "L 144.397202 38.669416 \n",
       "L 145.754456 34.356072 \n",
       "L 147.111706 30.861146 \n",
       "L 148.129648 28.834828 \n",
       "L 149.147585 27.358125 \n",
       "L 149.147585 27.358125 \n",
       "\" clip-path=\"url(#p90beff6da9)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_8\">\n",
       "    <path d=\"M 47.69304 60.197918 \n",
       "L 54.479295 71.911987 \n",
       "L 69.994593 98.798852 \n",
       "L 73.210099 99.26153 \n",
       "L 72.095637 99.327472 \n",
       "L 72.406405 99.332242 \n",
       "L 72.309704 99.332713 \n",
       "L 72.339021 99.332756 \n",
       "L 72.330058 99.33276 \n",
       "L 72.332791 99.33276 \n",
       "L 72.331957 99.332761 \n",
       "L 72.332211 99.332761 \n",
       "L 72.332134 99.33276 \n",
       "L 72.332158 99.33276 \n",
       "L 72.33215 99.33276 \n",
       "L 72.332153 99.332761 \n",
       "L 72.332152 99.332761 \n",
       "L 72.332152 99.332761 \n",
       "L 72.332151 99.33276 \n",
       "\" clip-path=\"url(#p90beff6da9)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 42.620312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 22.318125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- Function -->\n",
       "    <g transform=\"translate(72.9025 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-46\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"52.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"115.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"178.777344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"233.757812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"272.966797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"300.75\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"361.931641\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p90beff6da9\">\n",
       "   <rect x=\"42.620312\" y=\"22.318125\" width=\"111.6\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 0.2\n",
    "iter = 20\n",
    "c = 'blue'\n",
    "xx, yy, c = demo(x= -1, y= f, eta= eta, iter= iter, c= c)    # lr很大就很快收敛\n",
    "\n",
    "for i in range(1, iter):\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(2, 2))\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Function')\n",
    "\n",
    "    plt.scatter(x= xx[0:i], y= yy[0:i], c= c)\n",
    "    plt.plot(xx[:i], yy[:i], c= c)\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait= True)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"161.420313pt\" height=\"170.754375pt\" viewBox=\"0 0 161.420313 170.754375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T10:34:09.486761</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 170.754375 \n",
       "L 161.420313 170.754375 \n",
       "L 161.420313 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "L 42.620312 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m083a38ff63\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #0000ff\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pf6d97266c9)\">\n",
       "     <use xlink:href=\"#m083a38ff63\" x=\"81.624326\" y=\"93.402363\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m083a38ff63\" x=\"64.658683\" y=\"93.402362\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m083a38ff63\" x=\"91.308253\" y=\"87.486515\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m083a38ff63\" x=\"92.60057\" y=\"87.746601\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m71def0c7e5\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m71def0c7e5\" x=\"81.624326\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(78.443076 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m71def0c7e5\" x=\"149.486898\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(146.305648 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(95.460938 161.474688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m3f36b40018\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3f36b40018\" x=\"42.620312\" y=\"126.606808\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(20.878125 130.406027) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3f36b40018\" x=\"42.620312\" y=\"93.402363\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 97.201582) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3f36b40018\" x=\"42.620312\" y=\"60.197918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(29.257812 63.997137) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3f36b40018\" x=\"42.620312\" y=\"26.993473\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 30.792691) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- y -->\n",
       "     <g transform=\"translate(14.798437 80.7175) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-79\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_7\">\n",
       "    <path d=\"M 47.69304 60.197918 \n",
       "L 48.710977 61.336992 \n",
       "L 50.06823 63.265924 \n",
       "L 51.42548 65.597492 \n",
       "L 53.122046 68.960653 \n",
       "L 55.497236 74.223956 \n",
       "L 60.926241 86.541322 \n",
       "L 62.962118 90.545485 \n",
       "L 64.658684 93.402364 \n",
       "L 66.015935 95.316709 \n",
       "L 67.373186 96.87056 \n",
       "L 68.730437 98.047244 \n",
       "L 69.748375 98.678438 \n",
       "L 70.766313 99.095749 \n",
       "L 71.784252 99.304222 \n",
       "L 72.802192 99.312165 \n",
       "L 73.82013 99.130978 \n",
       "L 75.177382 98.620286 \n",
       "L 76.534633 97.84017 \n",
       "L 78.231197 96.560293 \n",
       "L 80.267074 94.720068 \n",
       "L 85.356767 89.965806 \n",
       "L 87.053331 88.746799 \n",
       "L 88.410583 88.029771 \n",
       "L 89.767835 87.593164 \n",
       "L 90.785773 87.47357 \n",
       "L 91.803712 87.547239 \n",
       "L 92.821651 87.824559 \n",
       "L 93.839589 88.312769 \n",
       "L 94.857528 89.015799 \n",
       "L 96.214779 90.287734 \n",
       "L 97.57203 91.9337 \n",
       "L 98.929281 93.934282 \n",
       "L 100.625846 96.886623 \n",
       "L 102.661723 100.980855 \n",
       "L 105.376226 107.064319 \n",
       "L 110.126605 117.844073 \n",
       "L 112.162482 121.823736 \n",
       "L 113.519735 124.061687 \n",
       "L 114.876987 125.878509 \n",
       "L 115.894924 126.922303 \n",
       "L 116.912862 127.662687 \n",
       "L 117.930804 128.075472 \n",
       "L 118.609429 128.158125 \n",
       "L 119.288054 128.080341 \n",
       "L 119.966679 127.837457 \n",
       "L 120.645308 127.425543 \n",
       "L 121.663241 126.484183 \n",
       "L 122.681179 125.148719 \n",
       "L 123.699121 123.416557 \n",
       "L 124.717058 121.289651 \n",
       "L 126.074313 117.85176 \n",
       "L 127.431563 113.752931 \n",
       "L 129.128125 107.767401 \n",
       "L 130.824688 100.934129 \n",
       "L 132.860567 91.827467 \n",
       "L 135.91438 76.985201 \n",
       "L 140.664764 53.852137 \n",
       "L 142.700639 45.04904 \n",
       "L 144.397202 38.669416 \n",
       "L 145.754456 34.356072 \n",
       "L 147.111706 30.861146 \n",
       "L 148.129648 28.834828 \n",
       "L 149.147585 27.358125 \n",
       "L 149.147585 27.358125 \n",
       "\" clip-path=\"url(#pf6d97266c9)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_8\">\n",
       "    <path d=\"M 81.624326 93.402363 \n",
       "L 64.658683 93.402362 \n",
       "L 91.308253 87.486515 \n",
       "L 92.60057 87.746601 \n",
       "\" clip-path=\"url(#pf6d97266c9)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 42.620312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 22.318125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- Function -->\n",
       "    <g transform=\"translate(72.9025 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-46\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"52.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"115.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"178.777344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"233.757812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"272.966797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"300.75\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"361.931641\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pf6d97266c9\">\n",
       "   <rect x=\"42.620312\" y=\"22.318125\" width=\"111.6\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 0.5\n",
    "iter = 5\n",
    "c = 'blue'\n",
    "xx, yy, c = demo(x= 0, y= f, eta= eta, iter= iter, c= c)    # lr很大就很快收敛\n",
    "\n",
    "for i in range(1, iter):\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(2, 2))\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Function')\n",
    "\n",
    "    plt.scatter(x= xx[0:i], y= yy[0:i], c= c)\n",
    "    plt.plot(xx[:i], yy[:i], c= c)\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait= True)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"161.420312pt\" height=\"170.754375pt\" viewBox=\"0 0 161.420312 170.754375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T10:34:25.109541</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 170.754375 \n",
       "L 161.420312 170.754375 \n",
       "L 161.420312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "L 42.620312 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m1b629f4ed8\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #0000ff\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pa787a326f7)\">\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"149.147585\" y=\"27.358125\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"140.693049\" y=\"52.587302\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"101.847563\" y=\"99.75543\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"119.663502\" y=\"127.867877\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"115.70083\" y=\"126.95586\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"123.224002\" y=\"123.844642\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"107.575186\" y=\"112.691995\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"127.432694\" y=\"112.92466\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"98.551309\" y=\"93.730501\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"112.036452\" y=\"122.004467\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"127.318529\" y=\"113.304047\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"98.76526\" y=\"94.068457\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"112.580471\" y=\"122.937385\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "     <use xlink:href=\"#m1b629f4ed8\" x=\"126.924714\" y=\"114.579123\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mc5f196e524\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc5f196e524\" x=\"81.511222\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(78.329972 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc5f196e524\" x=\"149.147585\" y=\"133.198125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(145.966335 147.796563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(95.460938 161.474688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"mb63755a808\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb63755a808\" x=\"42.620312\" y=\"126.6124\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(20.878125 130.411619) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb63755a808\" x=\"42.620312\" y=\"93.527642\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 97.326861) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb63755a808\" x=\"42.620312\" y=\"60.442883\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(29.257812 64.242102) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb63755a808\" x=\"42.620312\" y=\"27.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 31.157344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- y -->\n",
       "     <g transform=\"translate(14.798437 80.7175) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-79\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_7\">\n",
       "    <path d=\"M 47.69304 60.442883 \n",
       "L 48.707584 61.577851 \n",
       "L 50.060312 63.499831 \n",
       "L 51.413038 65.822995 \n",
       "L 53.10395 69.174033 \n",
       "L 55.471222 74.418364 \n",
       "L 60.88213 86.691332 \n",
       "L 62.911221 90.681062 \n",
       "L 64.602132 93.527643 \n",
       "L 65.954859 95.435087 \n",
       "L 67.307586 96.983338 \n",
       "L 68.660313 98.15578 \n",
       "L 69.674857 98.784699 \n",
       "L 70.689403 99.200505 \n",
       "L 71.703948 99.408228 \n",
       "L 72.718495 99.416141 \n",
       "L 73.73304 99.235608 \n",
       "L 75.085767 98.726756 \n",
       "L 76.438494 97.949452 \n",
       "L 78.129404 96.674189 \n",
       "L 80.158494 94.840597 \n",
       "L 85.231222 90.103472 \n",
       "L 86.922131 88.888859 \n",
       "L 88.274858 88.174415 \n",
       "L 89.627586 87.739382 \n",
       "L 90.642131 87.62022 \n",
       "L 91.656677 87.693622 \n",
       "L 92.671222 87.969943 \n",
       "L 93.685768 88.456393 \n",
       "L 94.700313 89.156889 \n",
       "L 96.05304 90.42424 \n",
       "L 97.405767 92.064273 \n",
       "L 98.758494 94.057644 \n",
       "L 100.449403 96.999343 \n",
       "L 102.478494 101.078817 \n",
       "L 105.183948 107.140353 \n",
       "L 109.918493 117.88125 \n",
       "L 111.947584 121.846568 \n",
       "L 113.300312 124.076453 \n",
       "L 114.65304 125.886726 \n",
       "L 115.667585 126.926758 \n",
       "L 116.682129 127.664473 \n",
       "L 117.696678 128.07577 \n",
       "L 118.373041 128.158125 \n",
       "L 119.049404 128.080621 \n",
       "L 119.725767 127.838613 \n",
       "L 120.402134 127.428183 \n",
       "L 121.416674 126.490217 \n",
       "L 122.431219 125.159567 \n",
       "L 123.445767 123.433648 \n",
       "L 124.460312 121.314409 \n",
       "L 125.813042 117.88891 \n",
       "L 127.165768 113.804855 \n",
       "L 128.856675 107.8409 \n",
       "L 130.547583 101.032259 \n",
       "L 132.576676 91.958422 \n",
       "L 135.620309 77.169656 \n",
       "L 140.354858 54.119976 \n",
       "L 142.383947 45.348611 \n",
       "L 144.074855 38.991982 \n",
       "L 145.427585 34.694186 \n",
       "L 146.780311 31.211857 \n",
       "L 147.794859 29.192843 \n",
       "L 148.809404 27.721463 \n",
       "L 148.809404 27.721463 \n",
       "\" clip-path=\"url(#pa787a326f7)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_8\">\n",
       "    <path d=\"M 149.147585 27.358125 \n",
       "L 140.693049 52.587302 \n",
       "L 101.847563 99.75543 \n",
       "L 119.663502 127.867877 \n",
       "L 115.70083 126.95586 \n",
       "L 123.224002 123.844642 \n",
       "L 107.575186 112.691995 \n",
       "L 127.432694 112.92466 \n",
       "L 98.551309 93.730501 \n",
       "L 112.036452 122.004467 \n",
       "L 127.318529 113.304047 \n",
       "L 98.76526 94.068457 \n",
       "L 112.580471 122.937385 \n",
       "L 126.924714 114.579123 \n",
       "\" clip-path=\"url(#pa787a326f7)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 42.620312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 154.220313 133.198125 \n",
       "L 154.220313 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 133.198125 \n",
       "L 154.220312 133.198125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 22.318125 \n",
       "L 154.220312 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- Function -->\n",
       "    <g transform=\"translate(72.9025 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-46\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"52.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"115.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"178.777344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"233.757812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"272.966797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"300.75\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"361.931641\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pa787a326f7\">\n",
       "   <rect x=\"42.620312\" y=\"22.318125\" width=\"111.6\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 0.25\n",
    "iter = 15\n",
    "c = 'blue'\n",
    "xx, yy, c = demo(x= 2, y= f, eta= eta, iter= iter, c= c)    # lr很大就很快收敛\n",
    "\n",
    "for i in range(1, iter):\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(2, 2))\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Function')\n",
    "\n",
    "    plt.scatter(x= xx[0:i], y= yy[0:i], c= c)\n",
    "    plt.plot(xx[:i], yy[:i], c= c)\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait= True)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.1. <a id='toc8_7_1_'></a>[小批量随机梯度下降（SGD）](#toc0_)\n",
    "- 随机梯度下降法，利用单个样本进行估算所有样本的梯度，然后进行后续的优化。这是计算效率很低的方式，所有改成小批量的随机梯度下降，可以提高计算效率。  \n",
    "- `小批量随机梯度下降法`是最常用的优化算法；\n",
    "- batch_size是所有样本，就是`梯度下降`。\n",
    "- `动量 (momentum)` 可以起到缓冲的作用，使得优化方向不会不停跳动，而是考虑之前几步的方向，具体考虑前多少步依赖于值的大小，一般取值为：0.5, 0.90, 0.99 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0.99\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.optim.SGD(\n",
    "    params= net.parameters(), \n",
    "    lr= 0.01, \n",
    "    momentum= 0.99, \n",
    "    # weight_decay=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.2. <a id='toc8_7_2_'></a>[adam](#toc0_)\n",
    "- Adam其实就是非常平滑的SGD，只是其对lr不敏感；  \n",
    "- Adam未必比SGD效果更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.optim.Adam(\n",
    "    params= net.parameters(), \n",
    "    lr= 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.3. <a id='toc8_7_3_'></a>[RMSprop](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.99\n",
       "    centered: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.optim.RMSprop(\n",
    "    params= net.parameters(), \n",
    "    lr= 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.4. <a id='toc8_7_4_'></a>[学习率调度器](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.1. <a id='toc8_7_4_1_'></a>[StepLR： 按照固定的步长调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.2. <a id='toc8_7_4_2_'></a>[MultiStepLR： 在指定的里程碑（milestones）上调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.3. <a id='toc8_7_4_3_'></a>[ExponentialLR： 以指数衰减的方式调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.4. <a id='toc8_7_4_4_'></a>[CosineAnnealingLR： 余弦退火调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.5. <a id='toc8_7_4_5_'></a>[ReduceLROnPlateau： 当指标停止改善时，降低学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.6. <a id='toc8_7_4_6_'></a>[LambdaLR： 使用自定义的函数来调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "lambda1 = lambda epoch: 0.65 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.7. <a id='toc8_7_4_7_'></a>[自定义](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# torch.optim.lr_scheduler._LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.optim.lr_scheduler._LRScheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. <a id='toc8_8_'></a>[专题-训练](#toc0_)\n",
    "\n",
    "![Train step via pure PyTorch](./Pytorch_Pictures/PyTorch_graphacial_demo/Train_step_via_pure_PyTorch.jpg)\n",
    "\n",
    "```python\n",
    "训练的模板代码\n",
    "```\n",
    "```python\n",
    "net.train():\n",
    "    启用 Batch Normalization 和 Dropout。\n",
    "    如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()\n",
    "    model.train()作用： \n",
    "                        对BN层，保证BN层能够用到每一批数据的均值和方差，并进行计算更新；\n",
    "                        对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。\n",
    "\n",
    "net.eval()\n",
    "    不启用 Batch Normalization 和 Dropout。\n",
    "    如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。\n",
    "    model.eval()是保证BN层直接利用之前训练阶段得到的均值和方差，即测试过程中要保证BN层的均值和方差不变；\n",
    "                        对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。\n",
    "                        \n",
    "with torch.no_grad():\n",
    "    pass\n",
    "\n",
    "    无论是train() 还是eval() 模式，各层的gradient计算和存储都在进行且完全一致，在forward的时候会保存中间结果和创建计算图以为后续的\n",
    "    反向传播做准备。而with torch.no_grad()则主要是用于停止autograd模块的工作，在内存中不储存的forward计算结果和不构建计算图，以起到加速和节省显存的作用。它的作用是将该with语句包裹起来的部分停止梯度的更新，从而节省了GPU算力和显存，但是并不会影响dropout和BN层的行为。\n",
    "    若想节约算力，可在test阶段带上torch.no_grad()，示例代码：\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "import torch \n",
    "from torch import nn  \n",
    "# import torch.nn.functional as F \n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "dbs = './data/'\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root= dbs, \n",
    "    train= True, \n",
    "    download= True, \n",
    "    transform= torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root= dbs, \n",
    "    train= False, \n",
    "    download= True, \n",
    "    transform= torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(\n",
    "    dataset= train_dataset, \n",
    "    batch_size= 128, \n",
    "    shuffle= True\n",
    ")\n",
    "\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10), \n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.network(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程封装\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import IPython.display as display\n",
    "import os\n",
    "\n",
    "\n",
    "def train_steps(\n",
    "        epochs, \n",
    "        train_dataset, \n",
    "        train_iter, \n",
    "        test_dataset, \n",
    "        net, \n",
    "        loss_fn, \n",
    "        opt, \n",
    "        device, \n",
    "        train_figure = False, \n",
    "        resume = False, \n",
    "        PATH = 'Pytorch_params/weights'\n",
    "    ):\n",
    "    '''\n",
    "    参数记录:\n",
    "            epochs = epochs                         # epoch\n",
    "            train_dataset = train_dataset           # 全部train数据集\n",
    "            train_iter = train_iter                 # batch之后的train数据集\n",
    "            test_dataset = test_dataset             # 全部test数据集\n",
    "            net = net                               # 网络模型\n",
    "            loss_fn = loss_fn                       # 损失函数\n",
    "            opt = opt                               # 优化器\n",
    "            device = device                         # device GPU/CPU\n",
    "            train_figure = False                    # 可视化训练过程\n",
    "            resume = False                          # 断点续训\n",
    "    '''\n",
    "    # 拷贝数据和模型到device上\n",
    "    print('='*100)\n",
    "    print(f\"Runing on {device}\")\n",
    "    print('='*100)\n",
    "    ## 数据\n",
    "    train_all_data_gpu = train_dataset.data.to(device)                                      # .to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)                                # .to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)                                        # .to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)                                  # .to(device)\n",
    "    ## 模型\n",
    "    net.to(device)                                                                          # .to(device)\n",
    "\n",
    "    def dl_plot(epochs:int, epoch_list:list, train_loss_list:list, train_acc_list:list, test_acc_list:list):\n",
    "        '''绘图'''\n",
    "        plt.rcParams['font.sans-serif']=['Times new roman', 'Arial', 'KaiTi']\n",
    "        plt.style.context(['ggplot', 'seaborn'])\n",
    "        \n",
    "        plt.close()\n",
    "        fig = plt.figure(figsize=(3.0, 3.0))\n",
    "\n",
    "        # for y, label in zip([train_loss_list, train_acc_list, test_acc_list], ['train_loss', 'train_acc', 'test_acc']):\n",
    "        for y, label in zip([train_acc_list, test_acc_list], ['train_acc', 'test_acc']):\n",
    "            plt.plot(epoch_list, y, label=label)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((1, epochs))\n",
    "        plt.ylabel('Values')\n",
    "        plt.ylim((0, 1))\n",
    "        plt.yticks(torch.arange(0, 1, 0.05).numpy())\n",
    "        # plt.tight_layout()\n",
    "\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    best_test_acc = 0\n",
    "\n",
    "    # 断点续训\n",
    "    start_epoch = 0\n",
    "    if resume:\n",
    "        if os.path.isfile(PATH+'/last.pt'):\n",
    "            check_point = torch.load(PATH+'/last.pt')\n",
    "            start_epoch = check_point['epoch']\n",
    "            net.load_state_dict(check_point['model_state_dict'])\n",
    "            opt.load_state_dict(check_point['opt_state_dict'])\n",
    "        else:\n",
    "            print(f'没有训练记录。')\n",
    "        \n",
    "    print('start_epoch: ', start_epoch)\n",
    "    for epoch in range(start_epoch, epochs, 1):\n",
    "        net.train()                             # 训练模式\n",
    "        epoch_list.append(epoch+1)\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   ## 复制到device（GPU/CPU）上                    # .to(device)\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            y_hat = net(X)                      # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)            # 计算loss\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            loss.backward()                     # 计算梯度\n",
    "            opt.step()                          # 更新网络参数\n",
    "\n",
    "        net.eval()                              # 切换至评估模式\n",
    "                                                # 模型默认是net.train()\n",
    "                                                # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                                                # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad():                   # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            train_loss_list.append(train_loss.item())\n",
    "            # print(train_loss)\n",
    "\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) \n",
    "            train_acc_list.append(train_acc.item())\n",
    "            # print(train_acc)\n",
    "\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp))\n",
    "            test_acc_list.append(test_acc.item())\n",
    "            # print(test_acc)\n",
    "\n",
    "            if train_figure:\n",
    "                if epoch % 1 == 0:\n",
    "                    dl_plot(epochs, epoch_list, train_loss_list, train_acc_list, test_acc_list)\n",
    "            else:\n",
    "                print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "        # 保存权重参数：last.pt和best.pt\n",
    "        torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/last.pt') \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/best.pt') \n",
    "\n",
    "    stop = time.time()\n",
    "    print('='*100)\n",
    "    print(f\"耗时： {stop - start} seconds.\")\n",
    "    return (train_loss, train_acc, test_acc)\n",
    "    # return (epoch_list, train_loss_list, train_acc_list, test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程封装\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import IPython.display as display\n",
    "import os\n",
    "\n",
    "\n",
    "def training_step(\n",
    "        epochs, \n",
    "        train_dataset, \n",
    "        train_iter, \n",
    "        test_dataset, \n",
    "        net, \n",
    "        loss_fn, \n",
    "        opt, \n",
    "        device, \n",
    "        train_figure = False, \n",
    "        resume = False, \n",
    "        PATH = 'Pytorch_params/weights'):\n",
    "    '''\n",
    "    训练过程\n",
    "    params:\n",
    "            epochs = epochs                         # epoch\n",
    "            train_dataset = train_dataset           # 全部train数据集\n",
    "            train_iter = train_iter                 # batch之后的train数据集\n",
    "            test_dataset = test_dataset             # 全部test数据集\n",
    "            net = net                               # 网络模型\n",
    "            loss_fn = loss_fn                       # 损失函数\n",
    "            opt = opt                               # 优化器\n",
    "            device = device                         # device GPU/CPU\n",
    "            train_figure = False                    # 可视化训练过程\n",
    "            resume = False                          # 断点续训\n",
    "    return:\n",
    "            tra_loss, val_loss, val_acc, test_loss, test_acc\n",
    "    '''\n",
    "    # 拷贝数据和模型到device上\n",
    "    print('='*100)\n",
    "    print(f\"Runing on {device}\")\n",
    "    print('='*100)\n",
    "    ## 数据\n",
    "    train_all_data_gpu = train_dataset.data.to(device)                                      # .to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)                                # .to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)                                        # .to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)                                  # .to(device)\n",
    "    ## 模型\n",
    "    net.to(device)                                                                          # .to(device)\n",
    "\n",
    "    def dl_plot(epochs:int, epoch_list:list, train_loss_list:list, train_acc_list:list, test_acc_list:list):\n",
    "        '''绘图'''\n",
    "        plt.rcParams['font.sans-serif']=['Times new roman', 'Arial', 'KaiTi']\n",
    "        plt.style.context(['ggplot', 'seaborn'])\n",
    "        \n",
    "        plt.close()\n",
    "        fig = plt.figure(figsize=(3.0, 3.0))\n",
    "\n",
    "        # for y, label in zip([train_loss_list, train_acc_list, test_acc_list], ['train_loss', 'train_acc', 'test_acc']):\n",
    "        for y, label in zip([train_acc_list, test_acc_list], ['train_acc', 'test_acc']):\n",
    "            plt.plot(epoch_list, y, label=label)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((1, epochs))\n",
    "        plt.ylabel('Values')\n",
    "        plt.ylim((0, 1))\n",
    "        plt.yticks(torch.arange(0, 1, 0.05).numpy())\n",
    "        # plt.tight_layout()\n",
    "\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    best_test_acc = 0\n",
    "\n",
    "    # 断点续训\n",
    "    start_epoch = 0\n",
    "    if resume:\n",
    "        if os.path.isfile(PATH+'/last.pt'):\n",
    "            check_point = torch.load(PATH+'/last.pt')\n",
    "            start_epoch = check_point['epoch']\n",
    "            net.load_state_dict(check_point['model_state_dict'])\n",
    "            opt.load_state_dict(check_point['opt_state_dict'])\n",
    "        else:\n",
    "            print(f'没有训练记录。')\n",
    "        \n",
    "    print('start_epoch: ', start_epoch)\n",
    "    for epoch in range(start_epoch, epochs, 1):\n",
    "        net.train()                             # 训练模式\n",
    "        epoch_list.append(epoch+1)\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   ## 复制到device（GPU/CPU）上                    # .to(device)\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            y_hat = net(X)                      # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)            # 计算loss\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            loss.backward()                     # 计算梯度\n",
    "            opt.step()                          # 更新网络参数\n",
    "\n",
    "        net.eval()                              # 切换至评估模式\n",
    "                                                # 模型默认是net.train()\n",
    "                                                # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                                                # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad():                   # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            train_loss_list.append(train_loss.item())\n",
    "            # print(train_loss)\n",
    "\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) \n",
    "            train_acc_list.append(train_acc.item())\n",
    "            # print(train_acc)\n",
    "\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp))\n",
    "            test_acc_list.append(test_acc.item())\n",
    "            # print(test_acc)\n",
    "\n",
    "            if train_figure:\n",
    "                if epoch % 1 == 0:\n",
    "                    dl_plot(epochs, epoch_list, train_loss_list, train_acc_list, test_acc_list)\n",
    "            else:\n",
    "                print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "        # 保存权重参数：last.pt和best.pt\n",
    "        torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/last.pt') \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/best.pt') \n",
    "\n",
    "    stop = time.time()\n",
    "    print('='*100)\n",
    "    print(f\"耗时： {stop - start} seconds.\")\n",
    "    return (train_loss, train_acc, test_acc)\n",
    "    # return (epoch_list, train_loss_list, train_acc_list, test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.1. <a id='toc8_8_1_'></a>[开始训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"446.96125pt\" height=\"336.634375pt\" viewBox=\"0 0 446.96125 336.634375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-31T11:22:06.028338</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 336.634375 \n",
       "L 446.96125 336.634375 \n",
       "L 446.96125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 131.678125 \n",
       "L 217.91125 131.678125 \n",
       "L 217.91125 22.318125 \n",
       "L 43.78125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"ma3d0d8142f\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"51.69625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(48.515 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"91.27125\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(88.09 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"130.84625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(127.665 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"170.42125\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(167.24 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"209.99625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(206.815 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(115.618125 159.954687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m80a1a8a7aa\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"43.78125\" y=\"100.163955\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2.1 -->\n",
       "      <g transform=\"translate(20.878125 103.963174) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"43.78125\" y=\"63.071366\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 2.2 -->\n",
       "      <g transform=\"translate(20.878125 66.870585) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"43.78125\" y=\"25.978776\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2.3 -->\n",
       "      <g transform=\"translate(20.878125 29.777995) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 86.655937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_9\">\n",
       "    <path d=\"M 51.69625 29.610842 \n",
       "L 91.27125 38.424852 \n",
       "L 130.84625 59.465457 \n",
       "L 170.42125 89.018264 \n",
       "L 209.99625 126.707216 \n",
       "\" clip-path=\"url(#p0e6f75976b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_10\">\n",
       "    <path d=\"M 51.69625 27.289034 \n",
       "L 91.27125 33.079646 \n",
       "L 130.84625 48.344265 \n",
       "L 170.42125 72.653045 \n",
       "L 209.99625 105.88167 \n",
       "\" clip-path=\"url(#p0e6f75976b)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 131.678125 \n",
       "L 43.78125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 217.91125 131.678125 \n",
       "L 217.91125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 131.678125 \n",
       "L 217.91125 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 22.318125 \n",
       "L 217.91125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(100.53875 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 50.78125 126.678125 \n",
       "L 130.371875 126.678125 \n",
       "Q 132.371875 126.678125 132.371875 124.678125 \n",
       "L 132.371875 95.765625 \n",
       "Q 132.371875 93.765625 130.371875 93.765625 \n",
       "L 50.78125 93.765625 \n",
       "Q 48.78125 93.765625 48.78125 95.765625 \n",
       "L 48.78125 124.678125 \n",
       "Q 48.78125 126.678125 50.78125 126.678125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_11\">\n",
       "     <path d=\"M 52.78125 101.864062 \n",
       "L 62.78125 101.864062 \n",
       "L 72.78125 101.864062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(80.78125 105.364062) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_12\">\n",
       "     <path d=\"M 52.78125 116.820312 \n",
       "L 62.78125 116.820312 \n",
       "L 72.78125 116.820312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(80.78125 120.320312) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 265.63125 131.678125 \n",
       "L 439.76125 131.678125 \n",
       "L 439.76125 22.318125 \n",
       "L 265.63125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"273.54625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(270.365 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"313.12125\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(309.94 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"352.69625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(349.515 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"392.27125\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(389.09 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"431.84625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(428.665 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(337.468125 159.954687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"265.63125\" y=\"119.039796\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 0.3 -->\n",
       "      <g transform=\"translate(242.728125 122.839014) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"265.63125\" y=\"88.434013\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(242.728125 92.233232) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"265.63125\" y=\"57.82823\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(242.728125 61.627449) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"265.63125\" y=\"27.222447\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(242.728125 31.021666) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_24\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(236.648437 85.560625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_22\">\n",
       "    <path d=\"M 273.54625 81.496863 \n",
       "L 313.12125 88.488493 \n",
       "L 352.69625 95.268255 \n",
       "L 392.27125 55.104218 \n",
       "L 431.84625 27.289034 \n",
       "\" clip-path=\"url(#pf0a217bc11)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 273.54625 126.707216 \n",
       "L 313.12125 80.153444 \n",
       "L 352.69625 107.627885 \n",
       "L 392.27125 78.13114 \n",
       "L 431.84625 44.742735 \n",
       "\" clip-path=\"url(#pf0a217bc11)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 265.63125 131.678125 \n",
       "L 265.63125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 439.76125 131.678125 \n",
       "L 439.76125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 265.63125 131.678125 \n",
       "L 439.76125 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 265.63125 22.318125 \n",
       "L 439.76125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_25\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(323.703125 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_2\">\n",
       "    <g id=\"patch_13\">\n",
       "     <path d=\"M 272.63125 60.230625 \n",
       "L 350.03125 60.230625 \n",
       "Q 352.03125 60.230625 352.03125 58.230625 \n",
       "L 352.03125 29.318125 \n",
       "Q 352.03125 27.318125 350.03125 27.318125 \n",
       "L 272.63125 27.318125 \n",
       "Q 270.63125 27.318125 270.63125 29.318125 \n",
       "L 270.63125 58.230625 \n",
       "Q 270.63125 60.230625 272.63125 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 274.63125 35.416562 \n",
       "L 284.63125 35.416562 \n",
       "L 294.63125 35.416562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_26\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(302.63125 38.916562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_25\">\n",
       "     <path d=\"M 274.63125 50.372812 \n",
       "L 284.63125 50.372812 \n",
       "L 294.63125 50.372812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_27\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(302.63125 53.872812) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 43.78125 299.078125 \n",
       "L 217.91125 299.078125 \n",
       "L 217.91125 189.718125 \n",
       "L 43.78125 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_5\">\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"51.69625\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(48.515 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"119.23038\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(106.50538 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"186.764509\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(174.039509 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_31\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(120.030625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"43.78125\" y=\"273.998293\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_32\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(20.878125 277.797512) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"43.78125\" y=\"247.836237\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_33\">\n",
       "      <!-- 2.1 -->\n",
       "      <g transform=\"translate(20.878125 251.635456) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"43.78125\" y=\"221.674181\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_34\">\n",
       "      <!-- 2.2 -->\n",
       "      <g transform=\"translate(20.878125 225.4734) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"43.78125\" y=\"195.512125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_35\">\n",
       "      <!-- 2.3 -->\n",
       "      <g transform=\"translate(20.878125 199.311344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_36\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 254.055937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_33\">\n",
       "    <path d=\"M 51.69625 197.507707 \n",
       "L 51.763784 197.426682 \n",
       "L 51.898852 197.571517 \n",
       "L 51.966387 197.45999 \n",
       "L 52.101455 197.989057 \n",
       "L 52.168989 197.444833 \n",
       "L 52.236523 197.594034 \n",
       "L 52.371591 197.30268 \n",
       "L 52.574194 197.693523 \n",
       "L 52.641728 197.78116 \n",
       "L 52.709262 197.202193 \n",
       "L 52.776796 197.30898 \n",
       "L 52.84433 197.480574 \n",
       "L 52.911864 198.031035 \n",
       "L 52.979398 197.431485 \n",
       "L 53.046933 197.590167 \n",
       "L 53.114467 197.67269 \n",
       "L 53.182001 198.008829 \n",
       "L 53.317069 197.328129 \n",
       "L 53.384603 198.1861 \n",
       "L 53.519672 197.292637 \n",
       "L 53.587206 198.022614 \n",
       "L 53.65474 197.208493 \n",
       "L 53.722274 197.389007 \n",
       "L 53.789808 198.025234 \n",
       "L 53.857342 197.706185 \n",
       "L 53.924876 197.571455 \n",
       "L 53.99241 197.176807 \n",
       "L 54.195013 197.826756 \n",
       "L 54.262547 197.523176 \n",
       "L 54.330081 199.077379 \n",
       "L 54.397615 197.966477 \n",
       "L 54.465149 199.384203 \n",
       "L 54.532683 198.749348 \n",
       "L 54.600218 198.370481 \n",
       "L 54.667752 198.569707 \n",
       "L 54.735286 198.520244 \n",
       "L 54.80282 197.814094 \n",
       "L 54.937888 199.194582 \n",
       "L 55.005422 199.042574 \n",
       "L 55.072956 199.27436 \n",
       "L 55.140491 197.432982 \n",
       "L 55.208025 198.943522 \n",
       "L 55.275559 197.937285 \n",
       "L 55.343093 198.429425 \n",
       "L 55.410627 198.226956 \n",
       "L 55.545695 198.597152 \n",
       "L 55.61323 197.085365 \n",
       "L 55.815832 199.728639 \n",
       "L 56.018434 198.567212 \n",
       "L 56.153503 198.69452 \n",
       "L 56.221037 198.772177 \n",
       "L 56.288571 199.421254 \n",
       "L 56.558707 198.111125 \n",
       "L 56.626241 198.582993 \n",
       "L 56.693776 198.303927 \n",
       "L 56.828844 197.390754 \n",
       "L 56.896378 197.652355 \n",
       "L 56.963912 198.339543 \n",
       "L 57.031446 202.873659 \n",
       "L 57.09898 202.107067 \n",
       "L 57.166515 203.245914 \n",
       "L 57.234049 202.088105 \n",
       "L 57.301583 202.460111 \n",
       "L 57.369117 203.521924 \n",
       "L 57.436651 203.176428 \n",
       "L 57.571719 201.844717 \n",
       "L 57.639253 202.762568 \n",
       "L 57.706788 201.898298 \n",
       "L 57.774322 203.117733 \n",
       "L 57.841856 202.327688 \n",
       "L 57.90939 203.338915 \n",
       "L 57.976924 203.167196 \n",
       "L 58.044458 201.430172 \n",
       "L 58.111992 201.983003 \n",
       "L 58.179526 201.879273 \n",
       "L 58.247061 204.02392 \n",
       "L 58.314595 202.673247 \n",
       "L 58.449663 203.66913 \n",
       "L 58.517197 203.108377 \n",
       "L 58.584731 203.606193 \n",
       "L 58.652265 201.429486 \n",
       "L 58.719799 204.849206 \n",
       "L 58.787334 202.976141 \n",
       "L 58.854868 201.403226 \n",
       "L 58.922402 204.638378 \n",
       "L 58.989936 201.895179 \n",
       "L 59.05747 202.465226 \n",
       "L 59.125004 204.537455 \n",
       "L 59.327607 202.162581 \n",
       "L 59.395141 202.267559 \n",
       "L 59.462675 203.275604 \n",
       "L 59.530209 202.874657 \n",
       "L 59.597743 202.094904 \n",
       "L 59.665277 206.949441 \n",
       "L 59.732811 203.421251 \n",
       "L 59.800346 209.144175 \n",
       "L 59.86788 206.590784 \n",
       "L 59.935414 204.646299 \n",
       "L 60.002948 206.546435 \n",
       "L 60.070482 205.370226 \n",
       "L 60.138016 204.713852 \n",
       "L 60.20555 208.025913 \n",
       "L 60.273084 206.370537 \n",
       "L 60.340619 207.207737 \n",
       "L 60.408153 207.095774 \n",
       "L 60.475687 202.3784 \n",
       "L 60.543221 206.68416 \n",
       "L 60.610755 204.123782 \n",
       "L 60.678289 205.656092 \n",
       "L 60.745823 204.104197 \n",
       "L 60.880892 205.400166 \n",
       "L 60.948426 201.906968 \n",
       "L 61.151028 210.950624 \n",
       "L 61.218562 209.176922 \n",
       "L 61.353631 204.253336 \n",
       "L 61.421165 204.890311 \n",
       "L 61.623767 208.308285 \n",
       "L 61.758835 205.616795 \n",
       "L 61.826369 205.673432 \n",
       "L 61.893904 205.435096 \n",
       "L 61.961438 206.491795 \n",
       "L 62.16404 202.06852 \n",
       "L 62.299108 205.058849 \n",
       "L 62.366642 216.31146 \n",
       "L 62.434177 214.13494 \n",
       "L 62.501711 217.575743 \n",
       "L 62.569245 212.203804 \n",
       "L 62.636779 214.813208 \n",
       "L 62.704313 217.458478 \n",
       "L 62.906916 212.172928 \n",
       "L 62.97445 217.13238 \n",
       "L 63.041984 212.263559 \n",
       "L 63.109518 217.262183 \n",
       "L 63.177052 214.675359 \n",
       "L 63.244586 218.920865 \n",
       "L 63.31212 217.410511 \n",
       "L 63.379654 212.009443 \n",
       "L 63.447189 212.051733 \n",
       "L 63.514723 212.875335 \n",
       "L 63.582257 220.114913 \n",
       "L 63.649791 214.500459 \n",
       "L 63.717325 217.104062 \n",
       "L 63.784859 219.007504 \n",
       "L 63.852393 215.19064 \n",
       "L 63.919927 218.843582 \n",
       "L 63.987462 211.213473 \n",
       "L 64.054996 222.606553 \n",
       "L 64.12253 217.063393 \n",
       "L 64.190064 211.373902 \n",
       "L 64.257598 221.302911 \n",
       "L 64.325132 213.260065 \n",
       "L 64.392666 215.30067 \n",
       "L 64.460201 221.756005 \n",
       "L 64.527735 215.149597 \n",
       "L 64.595269 216.591837 \n",
       "L 64.730337 213.683219 \n",
       "L 64.797871 217.62346 \n",
       "L 64.932939 213.404901 \n",
       "L 65.000474 226.84395 \n",
       "L 65.068008 216.338469 \n",
       "L 65.135542 232.685013 \n",
       "L 65.203076 224.39747 \n",
       "L 65.27061 219.525218 \n",
       "L 65.338144 225.825051 \n",
       "L 65.405678 221.495962 \n",
       "L 65.473212 221.518667 \n",
       "L 65.540747 226.012863 \n",
       "L 65.608281 220.666185 \n",
       "L 65.743349 224.733048 \n",
       "L 65.810883 215.492162 \n",
       "L 65.878417 224.006065 \n",
       "L 65.945951 218.527901 \n",
       "L 66.013485 223.256565 \n",
       "L 66.08102 218.251953 \n",
       "L 66.148554 221.935209 \n",
       "L 66.216088 221.383936 \n",
       "L 66.283622 214.326182 \n",
       "L 66.486224 234.093258 \n",
       "L 66.553759 227.806274 \n",
       "L 66.621293 226.006563 \n",
       "L 66.688827 218.335785 \n",
       "L 66.756361 220.993218 \n",
       "L 66.958963 228.364345 \n",
       "L 67.094032 221.884622 \n",
       "L 67.161566 221.955792 \n",
       "L 67.296634 225.7192 \n",
       "L 67.499236 214.167562 \n",
       "L 67.634305 219.460472 \n",
       "L 67.701839 236.999319 \n",
       "L 67.769373 233.485973 \n",
       "L 67.836907 237.437754 \n",
       "L 67.971975 228.717202 \n",
       "L 68.107044 239.092069 \n",
       "L 68.242112 230.738346 \n",
       "L 68.309646 233.432393 \n",
       "L 68.37718 229.750946 \n",
       "L 68.579782 238.258425 \n",
       "L 68.647317 238.179707 \n",
       "L 68.782385 225.190446 \n",
       "L 68.917453 242.194924 \n",
       "L 68.984987 229.929214 \n",
       "L 69.052521 233.477428 \n",
       "L 69.120055 237.668605 \n",
       "L 69.18759 233.362221 \n",
       "L 69.255124 238.150079 \n",
       "L 69.322658 228.516915 \n",
       "L 69.390192 243.313436 \n",
       "L 69.457726 237.398021 \n",
       "L 69.52526 226.708783 \n",
       "L 69.592794 242.701785 \n",
       "L 69.660328 228.371394 \n",
       "L 69.727863 231.739904 \n",
       "L 69.795397 245.127182 \n",
       "L 69.997999 224.436142 \n",
       "L 70.133067 238.642781 \n",
       "L 70.200602 230.461899 \n",
       "L 70.268136 232.905821 \n",
       "L 70.33567 251.166848 \n",
       "L 70.403204 235.771775 \n",
       "L 70.470738 260.765893 \n",
       "L 70.538272 247.511412 \n",
       "L 70.605806 244.870321 \n",
       "L 70.67334 246.96145 \n",
       "L 70.740875 245.756735 \n",
       "L 70.808409 236.423908 \n",
       "L 70.875943 249.779499 \n",
       "L 70.943477 246.476046 \n",
       "L 71.011011 249.411922 \n",
       "L 71.078545 249.062932 \n",
       "L 71.146079 232.948361 \n",
       "L 71.213613 249.717747 \n",
       "L 71.281148 237.372198 \n",
       "L 71.348682 245.764532 \n",
       "L 71.416216 238.624318 \n",
       "L 71.551284 244.986339 \n",
       "L 71.618818 230.979426 \n",
       "L 71.821421 261.197217 \n",
       "L 72.024023 241.644026 \n",
       "L 72.159091 250.166974 \n",
       "L 72.226625 248.395705 \n",
       "L 72.29416 255.293467 \n",
       "L 72.496762 244.536177 \n",
       "L 72.564296 245.781435 \n",
       "L 72.63183 250.094182 \n",
       "L 72.834433 233.543608 \n",
       "L 72.969501 244.286801 \n",
       "L 73.037035 265.211685 \n",
       "L 73.104569 255.743628 \n",
       "L 73.172103 258.964622 \n",
       "L 73.239637 257.292467 \n",
       "L 73.307172 250.109276 \n",
       "L 73.44224 269.485322 \n",
       "L 73.644842 252.830332 \n",
       "L 73.712376 252.180133 \n",
       "L 73.982513 261.949961 \n",
       "L 74.050047 259.254417 \n",
       "L 74.117581 247.015841 \n",
       "L 74.185115 253.667969 \n",
       "L 74.252649 268.128849 \n",
       "L 74.320183 252.120191 \n",
       "L 74.387718 258.985892 \n",
       "L 74.522786 258.637276 \n",
       "L 74.59032 262.301071 \n",
       "L 74.657854 256.08794 \n",
       "L 74.725388 267.665838 \n",
       "L 74.792922 265.018197 \n",
       "L 74.860456 248.529375 \n",
       "L 74.927991 267.964739 \n",
       "L 74.995525 248.413981 \n",
       "L 75.063059 249.119756 \n",
       "L 75.130593 272.421572 \n",
       "L 75.198127 259.345297 \n",
       "L 75.333195 241.249004 \n",
       "L 75.468264 263.52718 \n",
       "L 75.535798 253.443418 \n",
       "L 75.603332 257.608834 \n",
       "L 75.670866 279.23441 \n",
       "L 75.7384 261.005413 \n",
       "L 75.805934 294.107216 \n",
       "L 75.873468 277.767314 \n",
       "L 76.008537 271.630592 \n",
       "L 76.076071 275.407848 \n",
       "L 76.143605 255.625427 \n",
       "L 76.211139 281.327285 \n",
       "L 76.278673 280.883205 \n",
       "L 76.346207 285.541229 \n",
       "L 76.481276 256.03255 \n",
       "L 76.54881 280.582089 \n",
       "L 76.616344 263.312859 \n",
       "L 76.683878 274.737033 \n",
       "L 76.751412 270.224905 \n",
       "L 76.88648 274.67681 \n",
       "L 76.954015 256.060183 \n",
       "L 77.156617 288.771922 \n",
       "L 77.291685 276.902638 \n",
       "L 77.359219 271.085995 \n",
       "L 77.426753 284.960859 \n",
       "L 77.494288 281.009546 \n",
       "L 77.561822 278.767438 \n",
       "L 77.629356 291.338225 \n",
       "L 77.69689 285.56986 \n",
       "L 77.831958 275.081189 \n",
       "L 77.899492 274.834432 \n",
       "L 77.967026 280.123974 \n",
       "L 78.169629 258.71406 \n",
       "L 78.237163 264.305436 \n",
       "L 78.304697 275.842353 \n",
       "L 78.304697 275.842353 \n",
       "\" clip-path=\"url(#p202136b4d5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_34\">\n",
       "    <path d=\"M 51.69625 194.832185 \n",
       "L 51.831318 194.890194 \n",
       "L 51.966387 195.471406 \n",
       "L 52.168989 194.689034 \n",
       "L 52.236523 194.812537 \n",
       "L 52.439125 195.766441 \n",
       "L 52.50666 195.703816 \n",
       "L 52.641728 195.104079 \n",
       "L 52.979398 195.517876 \n",
       "L 53.114467 194.999164 \n",
       "L 53.182001 195.33443 \n",
       "L 53.249535 194.731699 \n",
       "L 53.317069 195.565468 \n",
       "L 53.384603 194.928742 \n",
       "L 53.452137 195.305176 \n",
       "L 53.519672 195.38932 \n",
       "L 53.65474 195.072143 \n",
       "L 53.857342 195.734442 \n",
       "L 53.924876 195.12067 \n",
       "L 53.99241 195.386389 \n",
       "L 54.127479 195.404041 \n",
       "L 54.330081 195.051871 \n",
       "L 54.397615 195.136264 \n",
       "L 54.465149 195.482821 \n",
       "L 54.532683 195.119049 \n",
       "L 54.600218 195.267377 \n",
       "L 54.667752 195.192152 \n",
       "L 54.735286 195.277981 \n",
       "L 54.80282 195.553367 \n",
       "L 54.937888 194.860379 \n",
       "L 55.005422 195.517003 \n",
       "L 55.072956 195.27717 \n",
       "L 55.140491 195.516628 \n",
       "L 55.208025 194.888947 \n",
       "L 55.343093 195.41689 \n",
       "L 55.478161 194.9686 \n",
       "L 55.61323 195.430114 \n",
       "L 55.680764 195.385017 \n",
       "L 55.815832 195.444086 \n",
       "L 55.883366 195.104578 \n",
       "L 55.9509 195.647616 \n",
       "L 56.018434 195.130463 \n",
       "L 56.085968 195.407409 \n",
       "L 56.153503 195.224588 \n",
       "L 56.221037 195.45837 \n",
       "L 56.288571 195.23569 \n",
       "L 56.356105 195.339296 \n",
       "L 56.558707 195.763821 \n",
       "L 56.76131 195.308545 \n",
       "L 56.828844 195.406287 \n",
       "L 56.896378 194.977644 \n",
       "L 57.031446 195.911151 \n",
       "L 57.09898 195.486127 \n",
       "L 57.166515 195.835927 \n",
       "L 57.369117 195.227083 \n",
       "L 57.571719 195.54819 \n",
       "L 57.639253 195.300872 \n",
       "L 57.774322 195.648988 \n",
       "L 57.841856 195.621793 \n",
       "L 57.90939 195.47571 \n",
       "L 57.976924 195.750972 \n",
       "L 58.044458 195.364433 \n",
       "L 58.111992 195.722654 \n",
       "L 58.179526 195.670009 \n",
       "L 58.247061 195.470782 \n",
       "L 58.314595 195.827382 \n",
       "L 58.382129 195.634081 \n",
       "L 58.449663 195.431736 \n",
       "L 58.584731 195.682172 \n",
       "L 58.787334 195.161963 \n",
       "L 58.854868 195.903355 \n",
       "L 58.922402 195.881523 \n",
       "L 59.125004 195.241741 \n",
       "L 59.192538 195.648552 \n",
       "L 59.260073 195.558108 \n",
       "L 59.462675 195.509018 \n",
       "L 59.665277 196.019061 \n",
       "L 59.800346 195.508644 \n",
       "L 59.86788 195.547442 \n",
       "L 59.935414 196.036775 \n",
       "L 60.070482 195.602893 \n",
       "L 60.138016 195.815842 \n",
       "L 60.273084 195.53347 \n",
       "L 60.408153 195.937848 \n",
       "L 60.610755 195.773676 \n",
       "L 60.948426 196.16664 \n",
       "L 61.01596 196.043013 \n",
       "L 61.083494 195.744734 \n",
       "L 61.151028 195.793699 \n",
       "L 61.218562 195.879964 \n",
       "L 61.286096 195.764819 \n",
       "L 61.421165 196.047753 \n",
       "L 61.488699 195.332809 \n",
       "L 61.556233 196.252406 \n",
       "L 61.623767 195.895433 \n",
       "L 61.758835 196.164894 \n",
       "L 61.826369 195.87672 \n",
       "L 61.893904 196.099649 \n",
       "L 62.028972 195.910278 \n",
       "L 62.096506 196.108818 \n",
       "L 62.16404 196.055363 \n",
       "L 62.231574 196.016566 \n",
       "L 62.299108 196.186351 \n",
       "L 62.366642 196.017564 \n",
       "L 62.434177 195.485441 \n",
       "L 62.569245 196.312848 \n",
       "L 62.636779 195.76931 \n",
       "L 62.704313 196.227331 \n",
       "L 62.771847 195.652544 \n",
       "L 62.906916 196.136887 \n",
       "L 62.97445 195.993799 \n",
       "L 63.041984 196.051246 \n",
       "L 63.109518 196.417825 \n",
       "L 63.177052 195.674063 \n",
       "L 63.244586 196.107197 \n",
       "L 63.379654 195.662649 \n",
       "L 63.447189 196.023863 \n",
       "L 63.514723 195.739245 \n",
       "L 63.582257 195.924624 \n",
       "L 63.717325 195.955064 \n",
       "L 63.784859 196.145433 \n",
       "L 63.852393 196.068836 \n",
       "L 63.919927 195.990493 \n",
       "L 63.987462 196.125722 \n",
       "L 64.12253 195.829003 \n",
       "L 64.257598 196.454502 \n",
       "L 64.325132 196.132708 \n",
       "L 64.392666 196.153042 \n",
       "L 64.460201 195.920757 \n",
       "L 64.527735 196.097466 \n",
       "L 64.595269 195.944522 \n",
       "L 64.662803 195.988746 \n",
       "L 64.797871 196.275298 \n",
       "L 64.865405 195.855138 \n",
       "L 64.932939 196.26775 \n",
       "L 65.000474 196.163584 \n",
       "L 65.068008 195.838859 \n",
       "L 65.27061 196.607633 \n",
       "L 65.338144 196.171942 \n",
       "L 65.405678 196.380151 \n",
       "L 65.743349 196.058856 \n",
       "L 65.878417 196.797877 \n",
       "L 66.013485 196.296505 \n",
       "L 66.08102 196.319522 \n",
       "L 66.216088 196.063222 \n",
       "L 66.351156 196.50908 \n",
       "L 66.41869 195.960677 \n",
       "L 66.486224 196.915766 \n",
       "L 66.553759 196.592975 \n",
       "L 66.756361 196.198888 \n",
       "L 66.891429 196.59678 \n",
       "L 66.958963 196.402231 \n",
       "L 67.026497 196.180051 \n",
       "L 67.094032 196.658406 \n",
       "L 67.161566 195.732446 \n",
       "L 67.2291 196.202943 \n",
       "L 67.364168 196.924 \n",
       "L 67.634305 195.995982 \n",
       "L 67.701839 196.5868 \n",
       "L 67.769373 196.294073 \n",
       "L 67.836907 196.580001 \n",
       "L 67.904441 196.552556 \n",
       "L 67.971975 196.361064 \n",
       "L 68.174578 196.877842 \n",
       "L 68.242112 195.97309 \n",
       "L 68.309646 196.486625 \n",
       "L 68.444714 196.116865 \n",
       "L 68.647317 196.944833 \n",
       "L 68.714851 196.21286 \n",
       "L 68.782385 196.610939 \n",
       "L 68.849919 196.359629 \n",
       "L 68.984987 197.158156 \n",
       "L 69.18759 196.221406 \n",
       "L 69.322658 196.60807 \n",
       "L 69.390192 196.453379 \n",
       "L 69.457726 196.67057 \n",
       "L 69.52526 196.612997 \n",
       "L 69.592794 196.238559 \n",
       "L 69.660328 196.919135 \n",
       "L 69.727863 196.879339 \n",
       "L 69.795397 197.067463 \n",
       "L 69.930465 196.262449 \n",
       "L 70.133067 196.455001 \n",
       "L 70.200602 196.919821 \n",
       "L 70.268136 196.444335 \n",
       "L 70.33567 196.535215 \n",
       "L 70.403204 196.563035 \n",
       "L 70.470738 196.716977 \n",
       "L 70.538272 196.515255 \n",
       "L 70.605806 196.88645 \n",
       "L 70.67334 196.647678 \n",
       "L 70.740875 196.476084 \n",
       "L 70.875943 197.041951 \n",
       "L 70.943477 197.04245 \n",
       "L 71.011011 196.314407 \n",
       "L 71.078545 196.893311 \n",
       "L 71.146079 196.32526 \n",
       "L 71.213613 196.357009 \n",
       "L 71.48375 196.979327 \n",
       "L 71.551284 196.39562 \n",
       "L 71.618818 197.041702 \n",
       "L 71.686352 196.933356 \n",
       "L 71.753887 196.897054 \n",
       "L 71.821421 197.031847 \n",
       "L 72.091557 196.330562 \n",
       "L 72.29416 196.904726 \n",
       "L 72.496762 196.548439 \n",
       "L 72.63183 196.622478 \n",
       "L 72.699364 197.184604 \n",
       "L 72.766898 196.661338 \n",
       "L 72.901967 197.181173 \n",
       "L 73.239637 196.930549 \n",
       "L 73.307172 197.071704 \n",
       "L 73.374706 196.995856 \n",
       "L 73.44224 197.020183 \n",
       "L 73.509774 197.269808 \n",
       "L 73.712376 196.676807 \n",
       "L 73.847445 197.253029 \n",
       "L 73.914979 197.16377 \n",
       "L 73.982513 196.416016 \n",
       "L 74.050047 197.397677 \n",
       "L 74.117581 196.878466 \n",
       "L 74.185115 196.290892 \n",
       "L 74.252649 197.253528 \n",
       "L 74.320183 196.963483 \n",
       "L 74.387718 197.254277 \n",
       "L 74.522786 196.593786 \n",
       "L 74.59032 196.691839 \n",
       "L 74.657854 196.453878 \n",
       "L 74.792922 197.226021 \n",
       "L 75.063059 196.764444 \n",
       "L 75.198127 197.549062 \n",
       "L 75.265661 197.205374 \n",
       "L 75.40073 197.374848 \n",
       "L 75.603332 196.744235 \n",
       "L 75.7384 197.427493 \n",
       "L 75.805934 196.941839 \n",
       "L 75.873468 197.615928 \n",
       "L 75.941003 197.487498 \n",
       "L 76.008537 197.09996 \n",
       "L 76.076071 197.696392 \n",
       "L 76.143605 197.558792 \n",
       "L 76.278673 197.062473 \n",
       "L 76.346207 197.559104 \n",
       "L 76.413741 197.248164 \n",
       "L 76.481276 196.738059 \n",
       "L 76.616344 197.177368 \n",
       "L 76.751412 196.809978 \n",
       "L 76.818946 197.004713 \n",
       "L 76.954015 197.60021 \n",
       "L 77.021549 197.275484 \n",
       "L 77.089083 197.374723 \n",
       "L 77.156617 197.51382 \n",
       "L 77.224151 197.468036 \n",
       "L 77.291685 197.054738 \n",
       "L 77.359219 197.5049 \n",
       "L 77.426753 197.454626 \n",
       "L 77.494288 197.596779 \n",
       "L 77.629356 197.179364 \n",
       "L 77.69689 197.821205 \n",
       "L 77.764424 197.434042 \n",
       "L 77.831958 197.157845 \n",
       "L 77.967026 197.642375 \n",
       "L 78.034561 197.255774 \n",
       "L 78.102095 197.470095 \n",
       "L 78.169629 197.843099 \n",
       "L 78.237163 197.025484 \n",
       "L 78.372231 197.734004 \n",
       "L 78.507299 197.301994 \n",
       "L 78.574834 197.945082 \n",
       "L 78.642368 197.552804 \n",
       "L 78.777436 197.91115 \n",
       "L 78.84497 197.172129 \n",
       "L 78.912504 197.190404 \n",
       "L 78.980038 197.930923 \n",
       "L 79.047573 197.5989 \n",
       "L 79.115107 197.618548 \n",
       "L 79.182641 197.789269 \n",
       "L 79.250175 197.429551 \n",
       "L 79.317709 197.576569 \n",
       "L 79.385243 197.687223 \n",
       "L 79.452777 197.331684 \n",
       "L 79.520311 197.361437 \n",
       "L 79.587846 197.241802 \n",
       "L 79.65538 198.063844 \n",
       "L 79.722914 197.367113 \n",
       "L 79.790448 197.502842 \n",
       "L 79.857982 197.891938 \n",
       "L 80.060584 197.410402 \n",
       "L 80.128119 197.861624 \n",
       "L 80.195653 197.806796 \n",
       "L 80.263187 197.536275 \n",
       "L 80.330721 197.762822 \n",
       "L 80.398255 197.412834 \n",
       "L 80.600858 198.270431 \n",
       "L 80.668392 196.9114 \n",
       "L 80.735926 197.943897 \n",
       "L 80.80346 197.215542 \n",
       "L 80.870994 197.473276 \n",
       "L 80.938528 198.039019 \n",
       "L 81.006062 197.551619 \n",
       "L 81.141131 198.210489 \n",
       "L 81.208665 197.38277 \n",
       "L 81.276199 197.857632 \n",
       "L 81.478801 197.541577 \n",
       "L 81.546335 198.302617 \n",
       "L 81.613869 197.648238 \n",
       "L 81.681404 197.875409 \n",
       "L 81.748938 198.135202 \n",
       "L 81.884006 197.391065 \n",
       "L 81.95154 197.777667 \n",
       "L 82.019074 197.589107 \n",
       "L 82.086608 196.984317 \n",
       "L 82.289211 197.817899 \n",
       "L 82.356745 198.196579 \n",
       "L 82.491813 197.58867 \n",
       "L 82.626881 198.221404 \n",
       "L 82.694416 197.311225 \n",
       "L 82.76195 197.69845 \n",
       "L 82.829484 198.72209 \n",
       "L 82.897018 197.879089 \n",
       "L 82.964552 198.112559 \n",
       "L 83.032086 198.330686 \n",
       "L 83.167154 197.619109 \n",
       "L 83.234689 197.857133 \n",
       "L 83.302223 198.755523 \n",
       "L 83.369757 197.781409 \n",
       "L 83.437291 198.02985 \n",
       "L 83.504825 197.994795 \n",
       "L 83.572359 198.189905 \n",
       "L 83.707427 197.710115 \n",
       "L 83.774962 198.925558 \n",
       "L 83.842496 197.579002 \n",
       "L 83.91003 198.126095 \n",
       "L 83.977564 197.734129 \n",
       "L 84.045098 198.200883 \n",
       "L 84.112632 198.059291 \n",
       "L 84.247701 197.35152 \n",
       "L 84.382769 198.706122 \n",
       "L 84.450303 197.589481 \n",
       "L 84.517837 197.648862 \n",
       "L 84.652905 198.442711 \n",
       "L 84.720439 198.433916 \n",
       "L 84.855508 197.952068 \n",
       "L 84.923042 198.768684 \n",
       "L 85.05811 197.721467 \n",
       "L 85.125644 198.688969 \n",
       "L 85.193178 198.403103 \n",
       "L 85.260712 198.237684 \n",
       "L 85.395781 198.552305 \n",
       "L 85.463315 197.957183 \n",
       "L 85.530849 198.191526 \n",
       "L 85.598383 198.338108 \n",
       "L 85.665917 197.942525 \n",
       "L 85.733451 198.733941 \n",
       "L 85.800985 198.511449 \n",
       "L 85.86852 198.109503 \n",
       "L 85.936054 198.526606 \n",
       "L 86.071122 198.071953 \n",
       "L 86.138656 198.819333 \n",
       "L 86.20619 198.402355 \n",
       "L 86.341259 198.887135 \n",
       "L 86.408793 197.751033 \n",
       "L 86.476327 198.307981 \n",
       "L 86.678929 198.064531 \n",
       "L 86.746463 198.155099 \n",
       "L 86.813997 198.100084 \n",
       "L 86.881532 198.615241 \n",
       "L 86.949066 197.711674 \n",
       "L 87.084134 198.993796 \n",
       "L 87.151668 198.838794 \n",
       "L 87.286736 197.871916 \n",
       "L 87.35427 198.308355 \n",
       "L 87.421805 198.350646 \n",
       "L 87.489339 198.7234 \n",
       "L 87.556873 198.63938 \n",
       "L 87.691941 198.366302 \n",
       "L 87.759475 198.566838 \n",
       "L 87.827009 198.526294 \n",
       "L 87.894544 198.207619 \n",
       "L 87.962078 198.620481 \n",
       "L 88.029612 198.411711 \n",
       "L 88.097146 198.161337 \n",
       "L 88.16468 198.762384 \n",
       "L 88.232214 198.426743 \n",
       "L 88.299748 198.132457 \n",
       "L 88.367282 198.329937 \n",
       "L 88.502351 198.208867 \n",
       "L 88.569885 198.661461 \n",
       "L 88.637419 197.94608 \n",
       "L 88.704953 199.205311 \n",
       "L 88.772487 199.154537 \n",
       "L 88.840021 198.708492 \n",
       "L 88.97509 199.651668 \n",
       "L 89.042624 198.063969 \n",
       "L 89.110158 198.900233 \n",
       "L 89.177692 199.407406 \n",
       "L 89.380294 197.801931 \n",
       "L 89.582897 198.735625 \n",
       "L 89.717965 198.619233 \n",
       "L 89.785499 198.744794 \n",
       "L 89.920567 198.017562 \n",
       "L 89.988102 199.979013 \n",
       "L 90.055636 198.754837 \n",
       "L 90.12317 198.977766 \n",
       "L 90.258238 198.220094 \n",
       "L 90.325772 199.222713 \n",
       "L 90.393306 198.48862 \n",
       "L 90.595909 199.86025 \n",
       "L 90.730977 198.860875 \n",
       "L 90.798511 199.573137 \n",
       "L 90.866045 198.491988 \n",
       "L 90.933579 198.946641 \n",
       "L 91.001113 198.757207 \n",
       "L 91.203716 199.435288 \n",
       "L 91.27125 198.996915 \n",
       "L 91.406318 199.56821 \n",
       "L 91.473852 198.687971 \n",
       "L 91.541387 199.499472 \n",
       "L 91.676455 198.965228 \n",
       "L 91.743989 199.79538 \n",
       "L 91.811523 199.618733 \n",
       "L 91.946591 198.579874 \n",
       "L 92.08166 199.630086 \n",
       "L 92.149194 198.893185 \n",
       "L 92.216728 199.178115 \n",
       "L 92.284262 199.755086 \n",
       "L 92.351796 199.497102 \n",
       "L 92.41933 198.110563 \n",
       "L 92.554398 199.358442 \n",
       "L 92.621933 199.119607 \n",
       "L 92.689467 199.667636 \n",
       "L 92.757001 198.836361 \n",
       "L 92.824535 199.501468 \n",
       "L 92.892069 199.459926 \n",
       "L 93.027137 199.1231 \n",
       "L 93.094672 200.593034 \n",
       "L 93.162206 199.139817 \n",
       "L 93.22974 199.918509 \n",
       "L 93.297274 200.140814 \n",
       "L 93.432342 198.849959 \n",
       "L 93.634945 200.555297 \n",
       "L 93.702479 199.654163 \n",
       "L 93.770013 199.717848 \n",
       "L 93.837547 199.776418 \n",
       "L 93.905081 199.558479 \n",
       "L 93.972615 200.01388 \n",
       "L 94.107683 198.369982 \n",
       "L 94.175218 199.466288 \n",
       "L 94.242752 199.031346 \n",
       "L 94.310286 198.991925 \n",
       "L 94.37782 199.137821 \n",
       "L 94.512888 200.670691 \n",
       "L 94.715491 199.006895 \n",
       "L 94.783025 199.379649 \n",
       "L 94.850559 199.346528 \n",
       "L 94.985627 199.192898 \n",
       "L 95.053161 199.808416 \n",
       "L 95.120695 199.04064 \n",
       "L 95.18823 199.431046 \n",
       "L 95.255764 199.072576 \n",
       "L 95.323298 199.306358 \n",
       "L 95.390832 199.18822 \n",
       "L 95.458366 200.404162 \n",
       "L 95.5259 199.448013 \n",
       "L 95.593434 199.922002 \n",
       "L 95.660968 199.518247 \n",
       "L 95.728503 199.606508 \n",
       "L 95.796037 199.692087 \n",
       "L 95.863571 199.130772 \n",
       "L 95.998639 199.75783 \n",
       "L 96.066173 198.267188 \n",
       "L 96.201241 200.408903 \n",
       "L 96.268776 199.411149 \n",
       "L 96.403844 200.884638 \n",
       "L 96.606446 199.067586 \n",
       "L 96.809049 199.943209 \n",
       "L 96.944117 198.883267 \n",
       "L 97.011651 200.336922 \n",
       "L 97.079185 199.632955 \n",
       "L 97.146719 199.094532 \n",
       "L 97.214253 200.545255 \n",
       "L 97.281788 199.219782 \n",
       "L 97.349322 199.658342 \n",
       "L 97.416856 199.523736 \n",
       "L 97.551924 200.152665 \n",
       "L 97.619458 199.074759 \n",
       "L 97.686992 200.600769 \n",
       "L 97.754526 200.24891 \n",
       "L 97.822061 199.964791 \n",
       "L 97.889595 200.070392 \n",
       "L 97.957129 200.537395 \n",
       "L 98.024663 199.615303 \n",
       "L 98.092197 201.339478 \n",
       "L 98.159731 200.521053 \n",
       "L 98.227265 200.015627 \n",
       "L 98.294799 200.344344 \n",
       "L 98.497402 200.995853 \n",
       "L 98.564936 199.614055 \n",
       "L 98.63247 200.28409 \n",
       "L 98.700004 202.042447 \n",
       "L 98.767538 199.82638 \n",
       "L 98.835073 200.343471 \n",
       "L 98.902607 199.874721 \n",
       "L 99.105209 201.072575 \n",
       "L 99.172743 199.59316 \n",
       "L 99.240277 201.338355 \n",
       "L 99.307811 199.777416 \n",
       "L 99.375346 200.61393 \n",
       "L 99.44288 199.826318 \n",
       "L 99.510414 201.391998 \n",
       "L 99.577948 200.777727 \n",
       "L 99.713016 199.856882 \n",
       "L 99.915619 200.983503 \n",
       "L 100.050687 199.587234 \n",
       "L 100.118221 201.540763 \n",
       "L 100.185755 200.725956 \n",
       "L 100.253289 200.274484 \n",
       "L 100.320823 201.369917 \n",
       "L 100.455892 199.747788 \n",
       "L 100.523426 200.378962 \n",
       "L 100.59096 200.418072 \n",
       "L 100.658494 201.015127 \n",
       "L 100.726028 199.829874 \n",
       "L 100.793562 201.846339 \n",
       "L 100.861096 199.995168 \n",
       "L 100.928631 200.43616 \n",
       "L 100.996165 200.2103 \n",
       "L 101.131233 202.016499 \n",
       "L 101.266301 201.159838 \n",
       "L 101.333835 201.777913 \n",
       "L 101.401369 200.089541 \n",
       "L 101.468904 200.201193 \n",
       "L 101.603972 201.050494 \n",
       "L 101.73904 199.585363 \n",
       "L 101.941642 202.564714 \n",
       "L 102.009177 201.029473 \n",
       "L 102.076711 201.163081 \n",
       "L 102.144245 202.026603 \n",
       "L 102.211779 201.410336 \n",
       "L 102.279313 201.898921 \n",
       "L 102.346847 200.303613 \n",
       "L 102.414381 201.361933 \n",
       "L 102.481916 200.338793 \n",
       "L 102.616984 201.904972 \n",
       "L 102.684518 200.405846 \n",
       "L 102.752052 201.680671 \n",
       "L 102.819586 200.965726 \n",
       "L 102.88712 199.848461 \n",
       "L 102.954654 201.186597 \n",
       "L 103.022189 200.924558 \n",
       "L 103.089723 200.83006 \n",
       "L 103.157257 201.290888 \n",
       "L 103.359859 199.692211 \n",
       "L 103.494927 201.849146 \n",
       "L 103.562462 201.605508 \n",
       "L 103.629996 200.550744 \n",
       "L 103.69753 200.831682 \n",
       "L 103.765064 200.329312 \n",
       "L 103.832598 201.513193 \n",
       "L 103.900132 201.078563 \n",
       "L 103.967666 200.944581 \n",
       "L 104.035201 202.087544 \n",
       "L 104.102735 201.756706 \n",
       "L 104.372871 200.903039 \n",
       "L 104.507939 202.308664 \n",
       "L 104.643008 201.290015 \n",
       "L 104.710542 201.956431 \n",
       "L 104.778076 200.98369 \n",
       "L 104.913144 202.221838 \n",
       "L 105.048212 200.445517 \n",
       "L 105.250815 201.418882 \n",
       "L 105.385883 201.383266 \n",
       "L 105.453417 202.206743 \n",
       "L 105.520951 202.029161 \n",
       "L 105.588485 201.875219 \n",
       "L 105.65602 200.771739 \n",
       "L 105.723554 201.022986 \n",
       "L 105.858622 202.706805 \n",
       "L 106.061224 200.328813 \n",
       "L 106.128759 202.370727 \n",
       "L 106.196293 201.867484 \n",
       "L 106.331361 200.915077 \n",
       "L 106.533963 202.38944 \n",
       "L 106.601497 200.403039 \n",
       "L 106.669032 201.464291 \n",
       "L 106.736566 201.710611 \n",
       "L 106.8041 201.623285 \n",
       "L 106.871634 201.276729 \n",
       "L 106.939168 202.772736 \n",
       "L 107.074236 200.697014 \n",
       "L 107.14177 202.533402 \n",
       "L 107.209305 202.217097 \n",
       "L 107.276839 200.488181 \n",
       "L 107.344373 202.362556 \n",
       "L 107.411907 201.935224 \n",
       "L 107.479441 200.935786 \n",
       "L 107.546975 201.010698 \n",
       "L 107.614509 201.301804 \n",
       "L 107.682044 200.846964 \n",
       "L 107.749578 202.853948 \n",
       "L 107.817112 200.90229 \n",
       "L 107.884646 201.926803 \n",
       "L 107.95218 201.331556 \n",
       "L 108.019714 201.37079 \n",
       "L 108.087248 201.20113 \n",
       "L 108.289851 204.28733 \n",
       "L 108.424919 202.664826 \n",
       "L 108.492453 203.219467 \n",
       "L 108.627521 201.375032 \n",
       "L 108.76259 203.268743 \n",
       "L 108.897658 203.026852 \n",
       "L 109.032726 202.294692 \n",
       "L 109.10026 203.05436 \n",
       "L 109.167794 201.197886 \n",
       "L 109.235328 204.092782 \n",
       "L 109.370397 201.488493 \n",
       "L 109.437931 202.753274 \n",
       "L 109.505465 201.871913 \n",
       "L 109.572999 204.217096 \n",
       "L 109.640533 202.97271 \n",
       "L 109.708067 201.864303 \n",
       "L 109.775602 203.096151 \n",
       "L 109.91067 201.096714 \n",
       "L 109.978204 202.22689 \n",
       "L 110.045738 201.261946 \n",
       "L 110.180806 204.240923 \n",
       "L 110.383409 201.967659 \n",
       "L 110.450943 202.157155 \n",
       "L 110.518477 201.194206 \n",
       "L 110.586011 203.738927 \n",
       "L 110.653545 201.671314 \n",
       "L 110.721079 203.216847 \n",
       "L 110.788613 202.985061 \n",
       "L 110.856148 202.341661 \n",
       "L 110.923682 202.463666 \n",
       "L 110.991216 202.326004 \n",
       "L 111.05875 204.665262 \n",
       "L 111.126284 202.495727 \n",
       "L 111.193818 202.702563 \n",
       "L 111.328887 203.32513 \n",
       "L 111.396421 201.402914 \n",
       "L 111.599023 204.207116 \n",
       "L 111.666557 202.697885 \n",
       "L 111.734091 203.246039 \n",
       "L 111.801625 204.295564 \n",
       "L 111.86916 202.295939 \n",
       "L 111.936694 203.811844 \n",
       "L 112.004228 203.732939 \n",
       "L 112.139296 200.757767 \n",
       "L 112.341898 203.549369 \n",
       "L 112.409433 201.652539 \n",
       "L 112.476967 204.023046 \n",
       "L 112.544501 201.649732 \n",
       "L 112.612035 204.105881 \n",
       "L 112.679569 202.019929 \n",
       "L 112.747103 204.466035 \n",
       "L 112.814637 204.332926 \n",
       "L 112.949706 202.030595 \n",
       "L 113.084774 204.841347 \n",
       "L 113.152308 203.647298 \n",
       "L 113.219842 206.709983 \n",
       "L 113.35491 202.010698 \n",
       "L 113.489979 204.711107 \n",
       "L 113.557513 202.132017 \n",
       "L 113.625047 205.148295 \n",
       "L 113.692581 203.345589 \n",
       "L 113.760115 204.376527 \n",
       "L 113.827649 201.069394 \n",
       "L 113.962718 205.710732 \n",
       "L 114.030252 204.824381 \n",
       "L 114.097786 205.20094 \n",
       "L 114.16532 202.333801 \n",
       "L 114.232854 203.485435 \n",
       "L 114.300388 203.75003 \n",
       "L 114.367922 203.369105 \n",
       "L 114.435456 202.239739 \n",
       "L 114.570525 204.431604 \n",
       "L 114.638059 204.289139 \n",
       "L 114.705593 204.11636 \n",
       "L 114.773127 205.915136 \n",
       "L 114.840661 204.783837 \n",
       "L 114.908195 203.918319 \n",
       "L 114.97573 206.28421 \n",
       "L 115.110798 203.626278 \n",
       "L 115.178332 204.491983 \n",
       "L 115.245866 201.368108 \n",
       "L 115.3134 202.731443 \n",
       "L 115.380934 205.408337 \n",
       "L 115.448468 204.307602 \n",
       "L 115.583537 206.155592 \n",
       "L 115.718605 202.092347 \n",
       "L 115.786139 205.500902 \n",
       "L 115.853673 205.438153 \n",
       "L 115.921207 205.130019 \n",
       "L 115.988741 202.748721 \n",
       "L 116.12381 206.578683 \n",
       "L 116.191344 205.161581 \n",
       "L 116.258878 204.66626 \n",
       "L 116.326412 205.298682 \n",
       "L 116.393946 204.953809 \n",
       "L 116.46148 203.392371 \n",
       "L 116.529015 203.889751 \n",
       "L 116.596549 205.493729 \n",
       "L 116.664083 204.532402 \n",
       "L 116.799151 206.680667 \n",
       "L 116.866685 205.679358 \n",
       "L 116.934219 203.653286 \n",
       "L 117.069288 207.360432 \n",
       "L 117.204356 203.438466 \n",
       "L 117.27189 205.523856 \n",
       "L 117.339424 202.973147 \n",
       "L 117.406958 203.180607 \n",
       "L 117.474492 204.101764 \n",
       "L 117.609561 202.444704 \n",
       "L 117.744629 207.43366 \n",
       "L 117.812163 205.957738 \n",
       "L 117.879697 201.92593 \n",
       "L 118.014765 206.39792 \n",
       "L 118.082299 204.799555 \n",
       "L 118.149834 206.279719 \n",
       "L 118.217368 202.804672 \n",
       "L 118.284902 204.502649 \n",
       "L 118.352436 205.043068 \n",
       "L 118.41997 206.984372 \n",
       "L 118.487504 206.287142 \n",
       "L 118.555038 206.475328 \n",
       "L 118.622573 201.596651 \n",
       "L 118.690107 210.778094 \n",
       "L 118.757641 203.196949 \n",
       "L 118.825175 207.909146 \n",
       "L 118.892709 207.113925 \n",
       "L 118.960243 204.357502 \n",
       "L 119.027777 205.940772 \n",
       "L 119.095311 205.744852 \n",
       "L 119.162846 205.087854 \n",
       "L 119.23038 205.487803 \n",
       "L 119.297914 209.445073 \n",
       "L 119.432982 205.274854 \n",
       "L 119.500516 208.826686 \n",
       "L 119.703119 203.536458 \n",
       "L 119.838187 208.084234 \n",
       "L 119.905721 206.322945 \n",
       "L 120.040789 203.972523 \n",
       "L 120.175858 208.118602 \n",
       "L 120.243392 207.495536 \n",
       "L 120.310926 204.506641 \n",
       "L 120.37846 206.76793 \n",
       "L 120.445994 205.785458 \n",
       "L 120.513528 204.331617 \n",
       "L 120.581062 206.022546 \n",
       "L 120.648596 203.672435 \n",
       "L 120.783665 208.063213 \n",
       "L 120.851199 207.732874 \n",
       "L 120.986267 206.5329 \n",
       "L 121.053801 211.400099 \n",
       "L 121.188869 203.929546 \n",
       "L 121.323938 206.714037 \n",
       "L 121.391472 206.622346 \n",
       "L 121.459006 205.430044 \n",
       "L 121.52654 207.445886 \n",
       "L 121.594074 206.672932 \n",
       "L 121.661608 206.121473 \n",
       "L 121.729142 206.816458 \n",
       "L 121.864211 204.147235 \n",
       "L 121.931745 208.520174 \n",
       "L 121.999279 204.963228 \n",
       "L 122.066813 209.323379 \n",
       "L 122.134347 206.793192 \n",
       "L 122.201881 205.985433 \n",
       "L 122.269416 207.304294 \n",
       "L 122.33695 206.546872 \n",
       "L 122.404484 206.76269 \n",
       "L 122.472018 208.24198 \n",
       "L 122.539552 205.573195 \n",
       "L 122.607086 207.732749 \n",
       "L 122.67462 207.50271 \n",
       "L 122.809689 203.37503 \n",
       "L 123.079825 209.021858 \n",
       "L 123.147359 205.341845 \n",
       "L 123.214893 208.31446 \n",
       "L 123.282427 206.366358 \n",
       "L 123.349962 209.254455 \n",
       "L 123.417496 206.1705 \n",
       "L 123.48503 209.691143 \n",
       "L 123.552564 205.426738 \n",
       "L 123.620098 207.255267 \n",
       "L 123.687632 207.967717 \n",
       "L 123.755166 214.053291 \n",
       "L 123.957769 205.899105 \n",
       "L 124.025303 210.573565 \n",
       "L 124.160371 207.190023 \n",
       "L 124.295439 206.866046 \n",
       "L 124.430508 213.509317 \n",
       "L 124.498042 202.459924 \n",
       "L 124.565576 209.501835 \n",
       "L 124.63311 205.827873 \n",
       "L 124.700644 209.745909 \n",
       "L 124.768178 207.515185 \n",
       "L 124.835712 210.40902 \n",
       "L 124.903247 207.742792 \n",
       "L 124.970781 208.62852 \n",
       "L 125.105849 209.499589 \n",
       "L 125.173383 209.126398 \n",
       "L 125.240917 205.0397 \n",
       "L 125.308451 209.667815 \n",
       "L 125.375985 207.8197 \n",
       "L 125.44352 205.741546 \n",
       "L 125.578588 213.369347 \n",
       "L 125.713656 206.044128 \n",
       "L 125.78119 210.981811 \n",
       "L 125.848724 207.501961 \n",
       "L 125.916259 207.656839 \n",
       "L 125.983793 206.698506 \n",
       "L 126.186395 210.89723 \n",
       "L 126.253929 208.983809 \n",
       "L 126.321463 209.517429 \n",
       "L 126.388997 211.425361 \n",
       "L 126.524066 206.137067 \n",
       "L 126.5916 207.129332 \n",
       "L 126.659134 211.740294 \n",
       "L 126.726668 209.284582 \n",
       "L 126.794202 208.227696 \n",
       "L 126.861736 210.619411 \n",
       "L 126.92927 205.749343 \n",
       "L 126.996805 207.271547 \n",
       "L 127.064339 209.919437 \n",
       "L 127.131873 206.569265 \n",
       "L 127.266941 213.939706 \n",
       "L 127.334475 208.433909 \n",
       "L 127.402009 208.842841 \n",
       "L 127.469544 210.60756 \n",
       "L 127.604612 208.051487 \n",
       "L 127.672146 209.051486 \n",
       "L 127.73968 204.395925 \n",
       "L 127.874748 210.122967 \n",
       "L 127.942282 208.291881 \n",
       "L 128.009817 213.487111 \n",
       "L 128.077351 211.887873 \n",
       "L 128.144885 209.292815 \n",
       "L 128.212419 210.262376 \n",
       "L 128.279953 208.661516 \n",
       "L 128.347487 211.156337 \n",
       "L 128.415021 209.190333 \n",
       "L 128.482555 211.677045 \n",
       "L 128.55009 206.878209 \n",
       "L 128.617624 207.347645 \n",
       "L 128.685158 214.921242 \n",
       "L 128.752692 211.591716 \n",
       "L 128.820226 210.873715 \n",
       "L 128.88776 212.495657 \n",
       "L 128.955294 212.491541 \n",
       "L 129.022828 211.930663 \n",
       "L 129.090363 212.33791 \n",
       "L 129.157897 209.688648 \n",
       "L 129.225431 210.858308 \n",
       "L 129.292965 211.225823 \n",
       "L 129.360499 210.816267 \n",
       "L 129.428033 207.237178 \n",
       "L 129.630636 214.794495 \n",
       "L 129.833238 211.325499 \n",
       "L 129.900772 212.077993 \n",
       "L 129.968306 209.670435 \n",
       "L 130.03584 210.228818 \n",
       "L 130.103375 208.825812 \n",
       "L 130.238443 212.746094 \n",
       "L 130.305977 210.364609 \n",
       "L 130.373511 203.197573 \n",
       "L 130.441045 210.007323 \n",
       "L 130.508579 206.004831 \n",
       "L 130.778716 212.245034 \n",
       "L 130.84625 208.891493 \n",
       "L 130.913784 209.568701 \n",
       "L 130.981318 211.245596 \n",
       "L 131.048852 209.824315 \n",
       "L 131.116387 212.926545 \n",
       "L 131.183921 209.930352 \n",
       "L 131.251455 211.451933 \n",
       "L 131.318989 210.935466 \n",
       "L 131.386523 212.701433 \n",
       "L 131.521591 208.114797 \n",
       "L 131.589125 216.192448 \n",
       "L 131.65666 212.337037 \n",
       "L 131.724194 209.226511 \n",
       "L 131.791728 210.384381 \n",
       "L 131.859262 208.817142 \n",
       "L 131.926796 216.192947 \n",
       "L 132.061864 209.221708 \n",
       "L 132.129398 207.383261 \n",
       "L 132.196933 212.159081 \n",
       "L 132.264467 208.111554 \n",
       "L 132.399535 218.576304 \n",
       "L 132.467069 216.383753 \n",
       "L 132.602137 211.77722 \n",
       "L 132.669672 217.473635 \n",
       "L 132.737206 215.603315 \n",
       "L 132.80474 212.705862 \n",
       "L 132.872274 214.207981 \n",
       "L 132.939808 223.699865 \n",
       "L 133.007342 211.965905 \n",
       "L 133.074876 214.024162 \n",
       "L 133.14241 215.478564 \n",
       "L 133.209945 211.56321 \n",
       "L 133.277479 212.621905 \n",
       "L 133.345013 215.958417 \n",
       "L 133.480081 207.949129 \n",
       "L 133.547615 208.417005 \n",
       "L 133.615149 215.419869 \n",
       "L 133.682683 213.049611 \n",
       "L 133.750218 206.259073 \n",
       "L 133.817752 216.035201 \n",
       "L 133.885286 215.100633 \n",
       "L 134.020354 210.458358 \n",
       "L 134.087888 214.481996 \n",
       "L 134.222956 208.207861 \n",
       "L 134.290491 222.09701 \n",
       "L 134.358025 210.707735 \n",
       "L 134.425559 212.125086 \n",
       "L 134.493093 211.995595 \n",
       "L 134.560627 212.075311 \n",
       "L 134.628161 217.108054 \n",
       "L 134.695695 213.430724 \n",
       "L 134.76323 214.300234 \n",
       "L 134.830764 217.218458 \n",
       "L 134.898298 215.939205 \n",
       "L 135.033366 213.042937 \n",
       "L 135.1009 218.620029 \n",
       "L 135.168434 215.413819 \n",
       "L 135.235968 216.024098 \n",
       "L 135.438571 211.087537 \n",
       "L 135.641173 213.817575 \n",
       "L 135.708707 209.608247 \n",
       "L 135.843776 220.130819 \n",
       "L 136.046378 209.200625 \n",
       "L 136.181446 216.908017 \n",
       "L 136.24898 218.01343 \n",
       "L 136.316515 217.480746 \n",
       "L 136.519117 209.809781 \n",
       "L 136.586651 211.575561 \n",
       "L 136.654185 217.291562 \n",
       "L 136.721719 215.532207 \n",
       "L 136.789253 219.2406 \n",
       "L 136.991856 210.538448 \n",
       "L 137.329526 225.394101 \n",
       "L 137.397061 211.862923 \n",
       "L 137.464595 218.440388 \n",
       "L 137.532129 219.477002 \n",
       "L 137.599663 214.399598 \n",
       "L 137.667197 217.022226 \n",
       "L 137.734731 220.556904 \n",
       "L 137.937334 211.955862 \n",
       "L 138.004868 217.700556 \n",
       "L 138.072402 212.182409 \n",
       "L 138.139936 213.234928 \n",
       "L 138.20747 214.931596 \n",
       "L 138.275004 212.236738 \n",
       "L 138.342538 213.816016 \n",
       "L 138.410073 214.561399 \n",
       "L 138.477607 217.765489 \n",
       "L 138.612675 212.671431 \n",
       "L 138.815277 218.578612 \n",
       "L 138.950346 214.512123 \n",
       "L 139.01788 214.835289 \n",
       "L 139.085414 215.968646 \n",
       "L 139.152948 211.473078 \n",
       "L 139.220482 211.808844 \n",
       "L 139.288016 218.304099 \n",
       "L 139.35555 211.480064 \n",
       "L 139.423084 213.631323 \n",
       "L 139.490619 215.49079 \n",
       "L 139.558153 214.144858 \n",
       "L 139.625687 216.896727 \n",
       "L 139.693221 212.790505 \n",
       "L 139.760755 216.106184 \n",
       "L 139.828289 213.408706 \n",
       "L 139.895823 222.312329 \n",
       "L 139.963358 220.816322 \n",
       "L 140.030892 212.451683 \n",
       "L 140.098426 213.613421 \n",
       "L 140.16596 216.085413 \n",
       "L 140.368562 210.882822 \n",
       "L 140.436096 218.982366 \n",
       "L 140.503631 215.030398 \n",
       "L 140.571165 214.23842 \n",
       "L 140.638699 221.512055 \n",
       "L 140.706233 216.728626 \n",
       "L 140.773767 217.388056 \n",
       "L 140.841301 221.652524 \n",
       "L 140.908835 219.205483 \n",
       "L 140.976369 215.200433 \n",
       "L 141.043904 221.020414 \n",
       "L 141.111438 216.711597 \n",
       "L 141.178972 220.778273 \n",
       "L 141.246506 219.113479 \n",
       "L 141.449108 208.221147 \n",
       "L 141.651711 219.84776 \n",
       "L 141.719245 209.157274 \n",
       "L 141.786779 218.849383 \n",
       "L 141.854313 215.065952 \n",
       "L 141.921847 218.157766 \n",
       "L 141.989381 209.741917 \n",
       "L 142.056916 213.118349 \n",
       "L 142.12445 212.807284 \n",
       "L 142.259518 218.066449 \n",
       "L 142.327052 214.194384 \n",
       "L 142.394586 216.754511 \n",
       "L 142.46212 214.490042 \n",
       "L 142.529654 220.696998 \n",
       "L 142.597189 219.803349 \n",
       "L 142.664723 213.722079 \n",
       "L 142.732257 220.694316 \n",
       "L 142.799791 218.805034 \n",
       "L 142.934859 211.203243 \n",
       "L 143.069927 218.012931 \n",
       "L 143.204996 214.572502 \n",
       "L 143.27253 209.273541 \n",
       "L 143.340064 216.265739 \n",
       "L 143.407598 214.209291 \n",
       "L 143.677735 219.508876 \n",
       "L 143.745269 218.592771 \n",
       "L 143.812803 218.662195 \n",
       "L 143.880337 221.033824 \n",
       "L 143.947871 215.047053 \n",
       "L 144.015405 218.239353 \n",
       "L 144.082939 216.17149 \n",
       "L 144.150474 216.507506 \n",
       "L 144.218008 221.065449 \n",
       "L 144.285542 212.44295 \n",
       "L 144.353076 216.212783 \n",
       "L 144.42061 223.006191 \n",
       "L 144.623212 215.321254 \n",
       "L 144.690747 210.725137 \n",
       "L 144.758281 229.585901 \n",
       "L 144.825815 221.677225 \n",
       "L 144.893349 217.772911 \n",
       "L 144.960883 221.997147 \n",
       "L 145.095951 211.244473 \n",
       "L 145.23102 223.41163 \n",
       "L 145.366088 211.140369 \n",
       "L 145.433622 212.0682 \n",
       "L 145.636224 220.246649 \n",
       "L 145.703759 219.592271 \n",
       "L 145.771293 213.622465 \n",
       "L 145.906361 220.539626 \n",
       "L 146.108963 213.427231 \n",
       "L 146.176497 213.140367 \n",
       "L 146.3791 225.78407 \n",
       "L 146.446634 214.093273 \n",
       "L 146.514168 223.69868 \n",
       "L 146.581702 217.967771 \n",
       "L 146.649236 222.515859 \n",
       "L 146.71677 221.562829 \n",
       "L 146.851839 213.235427 \n",
       "L 146.986907 221.82468 \n",
       "L 147.054441 215.495842 \n",
       "L 147.189509 223.807899 \n",
       "L 147.257044 216.065141 \n",
       "L 147.324578 224.807337 \n",
       "L 147.392112 213.4238 \n",
       "L 147.459646 217.378575 \n",
       "L 147.52718 223.534446 \n",
       "L 147.594714 220.958039 \n",
       "L 147.662248 223.841395 \n",
       "L 147.729782 217.71222 \n",
       "L 147.797317 219.112107 \n",
       "L 147.864851 226.571308 \n",
       "L 147.999919 219.864913 \n",
       "L 148.202521 222.12801 \n",
       "L 148.270055 222.1519 \n",
       "L 148.33759 223.566133 \n",
       "L 148.472658 217.895853 \n",
       "L 148.742794 225.042492 \n",
       "L 148.810328 210.198191 \n",
       "L 148.945397 226.483047 \n",
       "L 149.080465 209.586416 \n",
       "L 149.147999 224.565197 \n",
       "L 149.215533 215.832793 \n",
       "L 149.350602 223.233611 \n",
       "L 149.418136 222.840397 \n",
       "L 149.48567 222.192631 \n",
       "L 149.620738 215.114231 \n",
       "L 149.82334 232.264729 \n",
       "L 150.025943 215.038133 \n",
       "L 150.093477 227.149214 \n",
       "L 150.161011 212.325061 \n",
       "L 150.228545 217.248398 \n",
       "L 150.296079 222.199991 \n",
       "L 150.363613 218.649345 \n",
       "L 150.431148 220.460846 \n",
       "L 150.498682 224.009683 \n",
       "L 150.566216 221.005132 \n",
       "L 150.63375 222.648656 \n",
       "L 150.768818 216.37764 \n",
       "L 150.836352 223.279457 \n",
       "L 150.903887 216.643858 \n",
       "L 150.971421 224.328483 \n",
       "L 151.038955 217.964029 \n",
       "L 151.106489 221.257065 \n",
       "L 151.174023 219.702488 \n",
       "L 151.309091 222.551288 \n",
       "L 151.511694 219.085722 \n",
       "L 151.579228 226.818625 \n",
       "L 151.646762 224.699615 \n",
       "L 151.78183 215.789068 \n",
       "L 151.916898 221.749455 \n",
       "L 151.984433 216.038569 \n",
       "L 152.051967 222.721323 \n",
       "L 152.119501 220.275841 \n",
       "L 152.187035 217.81751 \n",
       "L 152.254569 229.107733 \n",
       "L 152.322103 228.053343 \n",
       "L 152.389637 216.796864 \n",
       "L 152.457172 229.984666 \n",
       "L 152.524706 215.551294 \n",
       "L 152.59224 218.027277 \n",
       "L 152.659774 216.351505 \n",
       "L 152.794842 226.663561 \n",
       "L 152.862376 223.930966 \n",
       "L 152.92991 215.51574 \n",
       "L 152.997445 225.984981 \n",
       "L 153.064979 221.077986 \n",
       "L 153.132513 224.378071 \n",
       "L 153.200047 215.755947 \n",
       "L 153.267581 228.869397 \n",
       "L 153.335115 223.256128 \n",
       "L 153.402649 224.214461 \n",
       "L 153.470183 219.833663 \n",
       "L 153.537718 220.127076 \n",
       "L 153.605252 220.76349 \n",
       "L 153.74032 230.575733 \n",
       "L 153.807854 222.161506 \n",
       "L 153.875388 225.793614 \n",
       "L 153.942922 228.934268 \n",
       "L 154.077991 222.961218 \n",
       "L 154.213059 226.07424 \n",
       "L 154.280593 217.545304 \n",
       "L 154.415661 224.219638 \n",
       "L 154.483195 220.823059 \n",
       "L 154.55073 229.379814 \n",
       "L 154.618264 225.444313 \n",
       "L 154.685798 226.64516 \n",
       "L 154.753332 220.176165 \n",
       "L 154.820866 221.132814 \n",
       "L 154.8884 230.946866 \n",
       "L 154.955934 213.113484 \n",
       "L 155.023468 225.85287 \n",
       "L 155.091003 213.402905 \n",
       "L 155.158537 221.301851 \n",
       "L 155.226071 221.882689 \n",
       "L 155.361139 228.450673 \n",
       "L 155.428673 228.572679 \n",
       "L 155.496207 227.370085 \n",
       "L 155.631276 216.803039 \n",
       "L 155.69881 218.31464 \n",
       "L 155.766344 223.750452 \n",
       "L 155.833878 221.008999 \n",
       "L 155.901412 215.721765 \n",
       "L 155.968946 228.791553 \n",
       "L 156.03648 224.209034 \n",
       "L 156.104015 229.991964 \n",
       "L 156.306617 220.128698 \n",
       "L 156.374151 225.567192 \n",
       "L 156.441685 224.05241 \n",
       "L 156.509219 218.275094 \n",
       "L 156.576753 220.947747 \n",
       "L 156.644288 233.184826 \n",
       "L 156.711822 229.361538 \n",
       "L 156.779356 217.830608 \n",
       "L 156.84689 222.74172 \n",
       "L 156.914424 216.956233 \n",
       "L 156.981958 225.790121 \n",
       "L 157.049492 225.727933 \n",
       "L 157.117026 218.1786 \n",
       "L 157.184561 218.523098 \n",
       "L 157.319629 226.486415 \n",
       "L 157.387163 225.304904 \n",
       "L 157.454697 224.194314 \n",
       "L 157.522231 228.248078 \n",
       "L 157.589765 220.302163 \n",
       "L 157.657299 228.656885 \n",
       "L 157.724834 226.505252 \n",
       "L 157.792368 227.401272 \n",
       "L 157.859902 221.47725 \n",
       "L 157.99497 230.235664 \n",
       "L 158.062504 220.785322 \n",
       "L 158.130038 235.856293 \n",
       "L 158.197573 231.040491 \n",
       "L 158.400175 218.54967 \n",
       "L 158.602777 229.591515 \n",
       "L 158.737846 222.939512 \n",
       "L 158.872914 231.747951 \n",
       "L 158.940448 218.896226 \n",
       "L 159.007982 222.799854 \n",
       "L 159.14305 222.649405 \n",
       "L 159.278119 238.069428 \n",
       "L 159.345653 230.797789 \n",
       "L 159.548255 225.814073 \n",
       "L 159.615789 225.272032 \n",
       "L 159.750858 222.691134 \n",
       "L 159.818392 230.242151 \n",
       "L 159.885926 227.367714 \n",
       "L 159.95346 230.847066 \n",
       "L 160.020994 226.527146 \n",
       "L 160.088528 231.871828 \n",
       "L 160.223596 219.992782 \n",
       "L 160.358665 226.042927 \n",
       "L 160.493733 227.922604 \n",
       "L 160.561267 226.85237 \n",
       "L 160.628801 228.24184 \n",
       "L 160.696335 223.051288 \n",
       "L 160.763869 229.68907 \n",
       "L 160.831404 217.17018 \n",
       "L 160.898938 221.883 \n",
       "L 160.966472 219.798047 \n",
       "L 161.034006 228.607422 \n",
       "L 161.10154 222.673918 \n",
       "L 161.236608 237.63829 \n",
       "L 161.304142 225.420111 \n",
       "L 161.371677 230.577605 \n",
       "L 161.439211 233.219319 \n",
       "L 161.574279 226.5254 \n",
       "L 161.641813 227.029578 \n",
       "L 161.776881 230.087398 \n",
       "L 161.844416 227.129815 \n",
       "L 161.979484 230.312884 \n",
       "L 162.047018 222.310395 \n",
       "L 162.114552 222.452673 \n",
       "L 162.317154 227.920109 \n",
       "L 162.384689 219.962343 \n",
       "L 162.452223 224.582163 \n",
       "L 162.519757 225.459719 \n",
       "L 162.587291 220.215774 \n",
       "L 162.789893 241.570049 \n",
       "L 162.992496 225.12807 \n",
       "L 163.127564 230.944745 \n",
       "L 163.195098 222.596448 \n",
       "L 163.262632 234.456656 \n",
       "L 163.330166 226.906512 \n",
       "L 163.397701 222.990846 \n",
       "L 163.532769 232.259926 \n",
       "L 163.600303 228.362037 \n",
       "L 163.667837 226.485604 \n",
       "L 163.735371 229.171855 \n",
       "L 163.802905 226.227683 \n",
       "L 163.870439 240.525264 \n",
       "L 163.937974 225.595822 \n",
       "L 164.005508 236.675404 \n",
       "L 164.073042 233.371951 \n",
       "L 164.275644 219.623833 \n",
       "L 164.410712 232.768721 \n",
       "L 164.478247 229.230363 \n",
       "L 164.545781 227.021095 \n",
       "L 164.613315 227.739408 \n",
       "L 164.680849 237.3087 \n",
       "L 164.748383 228.243213 \n",
       "L 164.883451 239.075477 \n",
       "L 164.950985 226.063324 \n",
       "L 165.01852 232.529262 \n",
       "L 165.086054 227.791117 \n",
       "L 165.153588 235.396339 \n",
       "L 165.221122 233.026642 \n",
       "L 165.35619 225.221945 \n",
       "L 165.491259 236.345814 \n",
       "L 165.558793 226.561203 \n",
       "L 165.626327 235.328724 \n",
       "L 165.693861 230.32274 \n",
       "L 165.761395 230.788745 \n",
       "L 165.828929 229.95142 \n",
       "L 165.963997 233.911933 \n",
       "L 166.099066 230.980486 \n",
       "L 166.1666 222.004445 \n",
       "L 166.369202 235.886109 \n",
       "L 166.436736 236.457964 \n",
       "L 166.50427 228.805962 \n",
       "L 166.571805 229.063072 \n",
       "L 166.774407 234.84594 \n",
       "L 166.841941 227.356424 \n",
       "L 166.909475 229.15545 \n",
       "L 166.977009 231.845505 \n",
       "L 167.044544 228.934018 \n",
       "L 167.112078 235.923222 \n",
       "L 167.179612 227.781699 \n",
       "L 167.247146 233.53026 \n",
       "L 167.31468 230.687572 \n",
       "L 167.382214 227.790119 \n",
       "L 167.449748 228.326109 \n",
       "L 167.517282 232.173848 \n",
       "L 167.584817 221.497834 \n",
       "L 167.652351 239.521273 \n",
       "L 167.719885 229.316004 \n",
       "L 167.854953 231.781259 \n",
       "L 167.922487 238.878497 \n",
       "L 167.990021 233.65869 \n",
       "L 168.057555 234.191063 \n",
       "L 168.12509 231.807893 \n",
       "L 168.192624 233.723436 \n",
       "L 168.327692 227.946556 \n",
       "L 168.395226 236.516098 \n",
       "L 168.597828 224.19001 \n",
       "L 168.732897 241.521895 \n",
       "L 168.867965 227.817065 \n",
       "L 168.935499 226.572431 \n",
       "L 169.070567 235.53319 \n",
       "L 169.138102 235.532005 \n",
       "L 169.27317 229.675846 \n",
       "L 169.340704 234.018907 \n",
       "L 169.408238 229.265355 \n",
       "L 169.543306 233.35773 \n",
       "L 169.61084 230.705162 \n",
       "L 169.745909 241.021646 \n",
       "L 169.813443 231.934452 \n",
       "L 169.880977 234.734039 \n",
       "L 169.948511 236.219192 \n",
       "L 170.151113 227.196619 \n",
       "L 170.286182 246.409304 \n",
       "L 170.42125 229.59033 \n",
       "L 170.488784 231.320431 \n",
       "L 170.556318 222.648095 \n",
       "L 170.758921 236.915299 \n",
       "L 170.826455 227.095447 \n",
       "L 170.893989 240.614586 \n",
       "L 170.961523 235.242085 \n",
       "L 171.096591 237.46969 \n",
       "L 171.299194 230.141103 \n",
       "L 171.434262 245.428579 \n",
       "L 171.501796 234.808951 \n",
       "L 171.56933 248.58414 \n",
       "L 171.636864 228.882559 \n",
       "L 171.704398 238.776014 \n",
       "L 171.771933 243.638473 \n",
       "L 171.839467 237.514413 \n",
       "L 171.907001 244.729166 \n",
       "L 172.042069 227.885491 \n",
       "L 172.109603 230.951544 \n",
       "L 172.177137 238.885483 \n",
       "L 172.244672 228.306399 \n",
       "L 172.312206 233.38636 \n",
       "L 172.447274 231.427654 \n",
       "L 172.582342 237.014289 \n",
       "L 172.649876 227.620895 \n",
       "L 172.71741 236.036994 \n",
       "L 172.784945 235.549095 \n",
       "L 172.852479 232.096004 \n",
       "L 172.920013 235.989464 \n",
       "L 172.987547 235.6091 \n",
       "L 173.055081 235.48672 \n",
       "L 173.122615 233.029324 \n",
       "L 173.190149 235.609412 \n",
       "L 173.257683 234.292422 \n",
       "L 173.325218 228.380376 \n",
       "L 173.460286 243.636103 \n",
       "L 173.595354 236.463703 \n",
       "L 173.662888 242.925899 \n",
       "L 173.730422 239.396211 \n",
       "L 173.797956 241.839884 \n",
       "L 173.865491 240.231228 \n",
       "L 173.933025 228.225249 \n",
       "L 174.000559 231.820243 \n",
       "L 174.068093 232.786435 \n",
       "L 174.135627 231.995954 \n",
       "L 174.203161 229.467763 \n",
       "L 174.270695 237.256679 \n",
       "L 174.33823 230.716327 \n",
       "L 174.405764 231.732731 \n",
       "L 174.473298 230.287685 \n",
       "L 174.540832 231.593884 \n",
       "L 174.608366 230.280761 \n",
       "L 174.743434 237.660247 \n",
       "L 174.878503 233.720504 \n",
       "L 174.946037 242.719936 \n",
       "L 175.013571 238.211706 \n",
       "L 175.081105 224.681214 \n",
       "L 175.148639 241.490146 \n",
       "L 175.216173 234.86353 \n",
       "L 175.283707 230.394533 \n",
       "L 175.351241 237.53849 \n",
       "L 175.418776 232.178527 \n",
       "L 175.48631 237.275891 \n",
       "L 175.553844 236.881118 \n",
       "L 175.621378 230.914057 \n",
       "L 175.756446 241.11864 \n",
       "L 175.891515 236.009612 \n",
       "L 175.959049 244.807384 \n",
       "L 176.026583 236.286994 \n",
       "L 176.094117 241.969874 \n",
       "L 176.161651 227.538435 \n",
       "L 176.229185 233.641662 \n",
       "L 176.296719 239.629182 \n",
       "L 176.364253 238.496324 \n",
       "L 176.431788 226.513049 \n",
       "L 176.499322 239.584397 \n",
       "L 176.566856 233.793795 \n",
       "L 176.63439 235.87906 \n",
       "L 176.701924 233.821178 \n",
       "L 176.769458 234.977988 \n",
       "L 176.836992 247.678577 \n",
       "L 176.904526 236.248321 \n",
       "L 176.972061 239.358599 \n",
       "L 177.039595 239.337266 \n",
       "L 177.107129 233.880185 \n",
       "L 177.174663 244.046157 \n",
       "L 177.242197 241.143652 \n",
       "L 177.377265 233.372575 \n",
       "L 177.444799 245.954464 \n",
       "L 177.512334 241.424964 \n",
       "L 177.647402 228.439757 \n",
       "L 177.714936 242.992204 \n",
       "L 177.78247 226.996707 \n",
       "L 177.850004 230.058082 \n",
       "L 177.985073 241.694924 \n",
       "L 178.052607 235.982665 \n",
       "L 178.120141 237.984785 \n",
       "L 178.187675 235.384175 \n",
       "L 178.255209 244.675024 \n",
       "L 178.322743 242.600425 \n",
       "L 178.390277 243.281936 \n",
       "L 178.457811 236.17659 \n",
       "L 178.59288 247.143024 \n",
       "L 178.727948 237.33733 \n",
       "L 178.863016 241.901511 \n",
       "L 178.93055 243.004117 \n",
       "L 178.998084 230.090517 \n",
       "L 179.065619 237.874505 \n",
       "L 179.133153 232.370705 \n",
       "L 179.268221 241.619637 \n",
       "L 179.335755 238.885982 \n",
       "L 179.403289 240.601549 \n",
       "L 179.538358 252.571787 \n",
       "L 179.605892 230.267974 \n",
       "L 179.673426 234.540738 \n",
       "L 179.74096 244.491891 \n",
       "L 179.943562 230.425721 \n",
       "L 180.078631 240.566869 \n",
       "L 180.146165 242.527321 \n",
       "L 180.348767 235.649894 \n",
       "L 180.416301 244.358719 \n",
       "L 180.483835 233.277702 \n",
       "L 180.551369 236.053836 \n",
       "L 180.753972 247.059503 \n",
       "L 180.956574 232.798037 \n",
       "L 181.024108 235.309013 \n",
       "L 181.091642 244.087886 \n",
       "L 181.159177 232.993833 \n",
       "L 181.226711 245.645395 \n",
       "L 181.294245 238.89434 \n",
       "L 181.429313 251.373435 \n",
       "L 181.564381 238.523582 \n",
       "L 181.69945 238.536057 \n",
       "L 181.834518 247.116452 \n",
       "L 181.902052 248.28449 \n",
       "L 181.969586 245.150386 \n",
       "L 182.03712 246.387972 \n",
       "L 182.104654 248.254862 \n",
       "L 182.172189 243.475112 \n",
       "L 182.239723 244.13211 \n",
       "L 182.307257 242.048404 \n",
       "L 182.374791 234.349121 \n",
       "L 182.442325 248.504862 \n",
       "L 182.509859 240.058511 \n",
       "L 182.577393 244.440306 \n",
       "L 182.644927 242.877308 \n",
       "L 182.712462 243.261103 \n",
       "L 182.779996 235.8465 \n",
       "L 182.84753 236.889726 \n",
       "L 182.915064 241.06743 \n",
       "L 182.982598 240.698168 \n",
       "L 183.050132 239.704531 \n",
       "L 183.117666 241.791668 \n",
       "L 183.185201 249.478476 \n",
       "L 183.252735 236.92503 \n",
       "L 183.387803 256.52257 \n",
       "L 183.455337 238.203971 \n",
       "L 183.522871 243.335704 \n",
       "L 183.657939 253.493256 \n",
       "L 183.725474 232.846191 \n",
       "L 183.793008 246.430138 \n",
       "L 183.928076 231.136798 \n",
       "L 183.99561 250.225294 \n",
       "L 184.063144 237.419353 \n",
       "L 184.130678 239.583773 \n",
       "L 184.265747 246.878615 \n",
       "L 184.468349 231.06226 \n",
       "L 184.670951 250.612956 \n",
       "L 184.80602 238.284373 \n",
       "L 184.941088 251.329772 \n",
       "L 185.008622 240.959209 \n",
       "L 185.14369 253.029372 \n",
       "L 185.346293 236.569242 \n",
       "L 185.481361 248.853601 \n",
       "L 185.616429 231.036811 \n",
       "L 185.683963 245.93675 \n",
       "L 185.751497 239.099617 \n",
       "L 185.819032 240.285369 \n",
       "L 185.886566 249.944169 \n",
       "L 185.9541 248.327965 \n",
       "L 186.021634 245.331773 \n",
       "L 186.089168 253.821973 \n",
       "L 186.224236 234.968507 \n",
       "L 186.29177 257.54702 \n",
       "L 186.359305 242.684819 \n",
       "L 186.426839 252.801515 \n",
       "L 186.494373 238.980979 \n",
       "L 186.561907 250.250494 \n",
       "L 186.629441 234.248448 \n",
       "L 186.696975 242.552583 \n",
       "L 186.764509 239.873007 \n",
       "L 186.899578 250.813804 \n",
       "L 186.967112 238.169727 \n",
       "L 187.034646 244.502869 \n",
       "L 187.237248 249.413107 \n",
       "L 187.304782 246.385539 \n",
       "L 187.372317 234.274396 \n",
       "L 187.574919 250.106906 \n",
       "L 187.642453 245.4932 \n",
       "L 187.777521 264.170269 \n",
       "L 187.845055 253.31505 \n",
       "L 187.91259 251.523073 \n",
       "L 188.115192 243.202034 \n",
       "L 188.182726 260.723228 \n",
       "L 188.25026 240.521896 \n",
       "L 188.317794 243.269586 \n",
       "L 188.385328 250.843245 \n",
       "L 188.587931 240.933635 \n",
       "L 188.790533 250.744568 \n",
       "L 188.925602 238.638477 \n",
       "L 188.993136 253.283426 \n",
       "L 189.06067 245.932882 \n",
       "L 189.128204 240.752061 \n",
       "L 189.195738 248.856096 \n",
       "L 189.263272 248.152442 \n",
       "L 189.39834 241.828532 \n",
       "L 189.465875 249.300582 \n",
       "L 189.533409 246.384978 \n",
       "L 189.668477 251.129735 \n",
       "L 189.803545 249.255859 \n",
       "L 189.871079 247.460077 \n",
       "L 189.938613 251.110773 \n",
       "L 190.073682 245.481598 \n",
       "L 190.141216 262.549387 \n",
       "L 190.20875 255.226413 \n",
       "L 190.276284 255.348232 \n",
       "L 190.411352 238.071923 \n",
       "L 190.478887 248.825595 \n",
       "L 190.546421 247.842437 \n",
       "L 190.613955 232.393035 \n",
       "L 190.816557 261.864133 \n",
       "L 190.951625 245.721119 \n",
       "L 191.01916 261.72934 \n",
       "L 191.086694 246.941988 \n",
       "L 191.154228 251.227477 \n",
       "L 191.221762 255.769389 \n",
       "L 191.35683 249.485025 \n",
       "L 191.424364 251.038043 \n",
       "L 191.491898 261.273502 \n",
       "L 191.694501 241.933883 \n",
       "L 191.829569 252.699719 \n",
       "L 191.897103 258.903182 \n",
       "L 191.964637 244.205089 \n",
       "L 192.032172 252.406867 \n",
       "L 192.099706 255.17932 \n",
       "L 192.16724 238.883424 \n",
       "L 192.234774 246.907058 \n",
       "L 192.302308 255.113826 \n",
       "L 192.369842 248.211386 \n",
       "L 192.437376 250.538231 \n",
       "L 192.50491 251.855346 \n",
       "L 192.572445 262.256036 \n",
       "L 192.639979 255.705455 \n",
       "L 192.775047 256.911479 \n",
       "L 192.842581 254.281304 \n",
       "L 192.910115 265.999921 \n",
       "L 192.977649 260.866005 \n",
       "L 193.112718 244.806262 \n",
       "L 193.180252 260.154054 \n",
       "L 193.247786 253.851976 \n",
       "L 193.31532 251.238018 \n",
       "L 193.382854 258.332137 \n",
       "L 193.450388 247.930698 \n",
       "L 193.517922 256.088938 \n",
       "L 193.720525 244.267153 \n",
       "L 193.855593 249.741138 \n",
       "L 193.923127 253.965187 \n",
       "L 193.990661 252.565051 \n",
       "L 194.058195 249.377116 \n",
       "L 194.260798 255.508661 \n",
       "L 194.328332 235.598684 \n",
       "L 194.395866 248.203028 \n",
       "L 194.530934 257.621683 \n",
       "L 194.598468 251.550331 \n",
       "L 194.666003 254.538851 \n",
       "L 194.733537 250.815177 \n",
       "L 194.801071 251.200281 \n",
       "L 194.868605 260.120933 \n",
       "L 194.936139 249.486273 \n",
       "L 195.003673 255.666283 \n",
       "L 195.206276 242.614646 \n",
       "L 195.341344 264.240316 \n",
       "L 195.476412 252.483526 \n",
       "L 195.543946 262.010902 \n",
       "L 195.61148 252.446039 \n",
       "L 195.679015 260.748927 \n",
       "L 195.746549 242.43039 \n",
       "L 195.814083 250.442111 \n",
       "L 195.881617 253.713752 \n",
       "L 195.949151 243.444923 \n",
       "L 196.151753 264.111012 \n",
       "L 196.286822 254.764899 \n",
       "L 196.42189 264.130286 \n",
       "L 196.489424 250.964378 \n",
       "L 196.556958 257.266332 \n",
       "L 196.624492 249.143771 \n",
       "L 196.692026 249.292286 \n",
       "L 196.759561 264.878165 \n",
       "L 196.827095 256.42907 \n",
       "L 196.962163 249.614579 \n",
       "L 197.164765 257.838001 \n",
       "L 197.232299 246.81624 \n",
       "L 197.299834 253.240699 \n",
       "L 197.367368 255.238764 \n",
       "L 197.502436 246.135727 \n",
       "L 197.56997 266.466861 \n",
       "L 197.637504 252.651565 \n",
       "L 197.705038 254.607464 \n",
       "L 197.772573 253.272011 \n",
       "L 197.840107 244.601546 \n",
       "L 197.907641 260.335566 \n",
       "L 198.042709 244.493138 \n",
       "L 198.110243 259.466181 \n",
       "L 198.177777 249.319295 \n",
       "L 198.245311 260.376048 \n",
       "L 198.312846 253.394828 \n",
       "L 198.38038 258.143577 \n",
       "L 198.515448 250.549708 \n",
       "L 198.582982 262.461563 \n",
       "L 198.71805 247.091128 \n",
       "L 198.785584 262.042464 \n",
       "L 198.853119 256.772071 \n",
       "L 198.920653 245.787735 \n",
       "L 198.988187 255.34923 \n",
       "L 199.055721 248.382544 \n",
       "L 199.123255 251.997748 \n",
       "L 199.190789 253.763652 \n",
       "L 199.258323 249.618134 \n",
       "L 199.325858 257.995061 \n",
       "L 199.393392 246.067862 \n",
       "L 199.460926 267.924445 \n",
       "L 199.52846 262.370308 \n",
       "L 199.595994 275.623073 \n",
       "L 199.663528 253.97498 \n",
       "L 199.731062 258.813923 \n",
       "L 199.798596 263.160352 \n",
       "L 199.866131 252.397012 \n",
       "L 199.933665 253.378298 \n",
       "L 200.001199 257.582449 \n",
       "L 200.068733 250.285674 \n",
       "L 200.136267 254.150004 \n",
       "L 200.203801 259.160792 \n",
       "L 200.271335 256.769513 \n",
       "L 200.338869 256.825776 \n",
       "L 200.406404 266.039092 \n",
       "L 200.609006 249.656744 \n",
       "L 200.67654 259.728344 \n",
       "L 200.744074 254.533924 \n",
       "L 200.811608 261.64426 \n",
       "L 200.879142 260.587125 \n",
       "L 200.946677 257.724977 \n",
       "L 201.014211 265.050757 \n",
       "L 201.081745 255.28199 \n",
       "L 201.149279 256.784359 \n",
       "L 201.216813 270.155918 \n",
       "L 201.284347 254.147946 \n",
       "L 201.351881 256.110208 \n",
       "L 201.419416 252.015026 \n",
       "L 201.48695 254.343305 \n",
       "L 201.554484 262.386151 \n",
       "L 201.622018 260.518138 \n",
       "L 201.757086 253.32397 \n",
       "L 201.82462 267.372799 \n",
       "L 201.892154 260.667652 \n",
       "L 201.959689 254.36576 \n",
       "L 202.027223 266.558303 \n",
       "L 202.094757 250.988081 \n",
       "L 202.162291 269.818156 \n",
       "L 202.229825 263.307745 \n",
       "L 202.297359 268.798509 \n",
       "L 202.432427 245.741453 \n",
       "L 202.499962 258.54178 \n",
       "L 202.567496 258.236079 \n",
       "L 202.63503 262.372366 \n",
       "L 202.702564 260.032111 \n",
       "L 202.770098 256.696098 \n",
       "L 202.837632 264.888332 \n",
       "L 202.905166 260.673453 \n",
       "L 202.972701 249.376742 \n",
       "L 203.040235 268.361134 \n",
       "L 203.107769 260.372866 \n",
       "L 203.175303 260.660104 \n",
       "L 203.242837 244.794161 \n",
       "L 203.377905 275.677932 \n",
       "L 203.512974 258.224228 \n",
       "L 203.580508 266.83974 \n",
       "L 203.78311 251.335947 \n",
       "L 203.850644 277.640287 \n",
       "L 203.918178 261.081574 \n",
       "L 203.985712 254.610832 \n",
       "L 204.120781 267.08999 \n",
       "L 204.188315 256.988076 \n",
       "L 204.255849 265.70551 \n",
       "L 204.323383 260.537475 \n",
       "L 204.390917 268.015575 \n",
       "L 204.525985 252.754172 \n",
       "L 204.661054 264.515578 \n",
       "L 204.728588 250.784675 \n",
       "L 204.796122 274.849621 \n",
       "L 204.93119 255.061868 \n",
       "L 204.998724 261.181623 \n",
       "L 205.066259 254.479595 \n",
       "L 205.133793 259.888211 \n",
       "L 205.201327 250.309189 \n",
       "L 205.268861 251.931069 \n",
       "L 205.336395 265.789092 \n",
       "L 205.403929 253.932253 \n",
       "L 205.471463 263.681871 \n",
       "L 205.538997 260.283795 \n",
       "L 205.606532 249.674958 \n",
       "L 205.674066 264.142325 \n",
       "L 205.7416 263.136649 \n",
       "L 205.876668 255.639836 \n",
       "L 205.944202 272.094352 \n",
       "L 206.011736 256.075465 \n",
       "L 206.07927 264.113258 \n",
       "L 206.214339 270.481766 \n",
       "L 206.349407 251.487207 \n",
       "L 206.416941 254.611706 \n",
       "L 206.484475 264.439355 \n",
       "L 206.552009 260.476721 \n",
       "L 206.619544 253.784174 \n",
       "L 206.822146 268.730208 \n",
       "L 206.957214 259.682747 \n",
       "L 207.024748 261.454515 \n",
       "L 207.092282 265.535849 \n",
       "L 207.159817 261.014645 \n",
       "L 207.227351 262.05918 \n",
       "L 207.294885 268.254784 \n",
       "L 207.362419 258.055253 \n",
       "L 207.429953 266.675756 \n",
       "L 207.497487 264.023001 \n",
       "L 207.565021 262.14426 \n",
       "L 207.70009 265.206758 \n",
       "L 207.767624 247.932132 \n",
       "L 207.835158 262.430063 \n",
       "L 207.902692 262.098414 \n",
       "L 207.970226 260.766205 \n",
       "L 208.03776 269.827513 \n",
       "L 208.105294 261.290094 \n",
       "L 208.172828 266.201081 \n",
       "L 208.240363 267.009151 \n",
       "L 208.307897 264.361511 \n",
       "L 208.375431 266.910349 \n",
       "L 208.442965 253.448283 \n",
       "L 208.510499 261.832945 \n",
       "L 208.578033 265.433554 \n",
       "L 208.713102 254.923145 \n",
       "L 208.780636 256.726412 \n",
       "L 208.84817 270.165212 \n",
       "L 208.915704 261.801383 \n",
       "L 208.983238 264.371616 \n",
       "L 209.050772 272.658723 \n",
       "L 209.18584 263.141078 \n",
       "L 209.253375 265.067037 \n",
       "L 209.320909 279.542669 \n",
       "L 209.455977 252.084637 \n",
       "L 209.523511 271.468729 \n",
       "L 209.591045 254.560496 \n",
       "L 209.658579 256.993004 \n",
       "L 209.726113 264.067973 \n",
       "L 209.793648 248.055511 \n",
       "L 209.861182 271.614936 \n",
       "L 209.928716 254.872684 \n",
       "L 209.99625 262.054627 \n",
       "\" clip-path=\"url(#p202136b4d5)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 43.78125 299.078125 \n",
       "L 43.78125 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 217.91125 299.078125 \n",
       "L 217.91125 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 43.78125 299.078125 \n",
       "L 217.91125 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 43.78125 189.718125 \n",
       "L 217.91125 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_37\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(100.53875 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_3\">\n",
       "    <g id=\"patch_19\">\n",
       "     <path d=\"M 91.050938 294.078125 \n",
       "L 170.641563 294.078125 \n",
       "Q 172.641563 294.078125 172.641563 292.078125 \n",
       "L 172.641563 263.165625 \n",
       "Q 172.641563 261.165625 170.641563 261.165625 \n",
       "L 91.050938 261.165625 \n",
       "Q 89.050938 261.165625 89.050938 263.165625 \n",
       "L 89.050938 292.078125 \n",
       "Q 89.050938 294.078125 91.050938 294.078125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_35\">\n",
       "     <path d=\"M 93.050938 269.264062 \n",
       "L 103.050938 269.264062 \n",
       "L 113.050938 269.264062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_38\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(121.050938 272.764062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_36\">\n",
       "     <path d=\"M 93.050938 284.220312 \n",
       "L 103.050938 284.220312 \n",
       "L 113.050938 284.220312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_39\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(121.050938 287.720312) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 265.63125 299.078125 \n",
       "L 439.76125 299.078125 \n",
       "L 439.76125 189.718125 \n",
       "L 265.63125 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_7\">\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_37\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"273.54625\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_40\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(270.365 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_38\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"341.08038\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_41\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(328.35538 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma3d0d8142f\" x=\"408.614509\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_42\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(395.889509 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_43\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(341.880625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_8\">\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_40\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"265.63125\" y=\"274.22358\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_44\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(242.728125 278.022798) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"265.63125\" y=\"245.301927\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_45\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(242.728125 249.101145) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_42\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80a1a8a7aa\" x=\"265.63125\" y=\"216.380274\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_46\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(242.728125 220.179493) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_47\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(236.648437 252.960625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_43\">\n",
       "    <path d=\"M 273.54625 244.398125 \n",
       "L 273.613784 242.138621 \n",
       "L 273.681318 246.657629 \n",
       "L 273.748852 243.268373 \n",
       "L 273.816387 248.917133 \n",
       "L 273.951455 238.749365 \n",
       "L 274.018989 242.138621 \n",
       "L 274.086523 238.749365 \n",
       "L 274.221591 250.046885 \n",
       "L 274.424194 237.619613 \n",
       "L 274.626796 252.306389 \n",
       "L 274.761864 237.619613 \n",
       "L 274.829398 247.787381 \n",
       "L 274.896933 246.657629 \n",
       "L 274.964467 247.787381 \n",
       "L 275.032001 239.879117 \n",
       "L 275.099535 251.176637 \n",
       "L 275.167069 250.046885 \n",
       "L 275.234603 234.230356 \n",
       "L 275.369672 254.565894 \n",
       "L 275.437206 236.489861 \n",
       "L 275.50474 251.176637 \n",
       "L 275.572274 245.527877 \n",
       "L 275.639808 235.360108 \n",
       "L 275.84241 254.565894 \n",
       "L 275.977479 246.657629 \n",
       "L 276.180081 233.100604 \n",
       "L 276.247615 238.749365 \n",
       "L 276.315149 236.489861 \n",
       "L 276.382683 234.230356 \n",
       "L 276.517752 242.138621 \n",
       "L 276.585286 233.100604 \n",
       "L 276.720354 244.398125 \n",
       "L 276.922956 227.451844 \n",
       "L 276.990491 245.527877 \n",
       "L 277.058025 233.100604 \n",
       "L 277.125559 239.879117 \n",
       "L 277.260627 247.787381 \n",
       "L 277.395695 235.360108 \n",
       "L 277.46323 261.344406 \n",
       "L 277.530764 236.489861 \n",
       "L 277.598298 238.749365 \n",
       "L 277.733366 236.489861 \n",
       "L 277.8009 237.619613 \n",
       "L 277.868434 229.711348 \n",
       "L 277.935968 233.100604 \n",
       "L 278.003503 241.008869 \n",
       "L 278.138571 231.970852 \n",
       "L 278.273639 241.008869 \n",
       "L 278.341173 257.95515 \n",
       "L 278.476241 228.581596 \n",
       "L 278.678844 260.214654 \n",
       "L 278.813912 230.8411 \n",
       "L 278.881446 246.657629 \n",
       "L 278.94898 242.138621 \n",
       "L 279.151583 254.565894 \n",
       "L 279.219117 236.489861 \n",
       "L 279.286651 250.046885 \n",
       "L 279.354185 245.527877 \n",
       "L 279.421719 252.306389 \n",
       "L 279.489253 245.527877 \n",
       "L 279.556788 247.787381 \n",
       "L 279.624322 253.436142 \n",
       "L 279.75939 231.970852 \n",
       "L 279.826924 241.008869 \n",
       "L 279.961992 255.695646 \n",
       "L 280.097061 244.398125 \n",
       "L 280.232129 245.527877 \n",
       "L 280.367197 244.398125 \n",
       "L 280.434731 245.527877 \n",
       "L 280.502265 253.436142 \n",
       "L 280.569799 244.398125 \n",
       "L 280.637334 250.046885 \n",
       "L 280.704868 259.084902 \n",
       "L 280.772402 241.008869 \n",
       "L 280.839936 255.695646 \n",
       "L 280.90747 251.176637 \n",
       "L 280.975004 234.230356 \n",
       "L 281.177607 257.95515 \n",
       "L 281.380209 235.360108 \n",
       "L 281.447743 243.268373 \n",
       "L 281.582811 235.360108 \n",
       "L 281.650346 238.749365 \n",
       "L 281.71788 236.489861 \n",
       "L 281.785414 246.657629 \n",
       "L 281.852948 242.138621 \n",
       "L 281.988016 242.138621 \n",
       "L 282.05555 250.046885 \n",
       "L 282.190619 236.489861 \n",
       "L 282.258153 237.619613 \n",
       "L 282.325687 238.749365 \n",
       "L 282.393221 252.306389 \n",
       "L 282.460755 234.230356 \n",
       "L 282.528289 241.008869 \n",
       "L 282.798426 261.344406 \n",
       "L 282.86596 229.711348 \n",
       "L 282.933494 250.046885 \n",
       "L 283.001028 251.176637 \n",
       "L 283.136096 246.657629 \n",
       "L 283.203631 254.565894 \n",
       "L 283.473767 227.451844 \n",
       "L 283.541301 246.657629 \n",
       "L 283.608835 244.398125 \n",
       "L 283.676369 250.046885 \n",
       "L 283.811438 227.451844 \n",
       "L 283.878972 246.657629 \n",
       "L 283.946506 245.527877 \n",
       "L 284.01404 248.917133 \n",
       "L 284.081574 247.787381 \n",
       "L 284.149108 248.917133 \n",
       "L 284.284177 243.268373 \n",
       "L 284.486779 259.084902 \n",
       "L 284.554313 238.749365 \n",
       "L 284.621847 259.084902 \n",
       "L 284.689381 251.176637 \n",
       "L 284.891984 247.787381 \n",
       "L 284.959518 254.565894 \n",
       "L 285.094586 244.398125 \n",
       "L 285.16212 244.398125 \n",
       "L 285.297189 263.60391 \n",
       "L 285.432257 244.398125 \n",
       "L 285.567325 246.657629 \n",
       "L 285.634859 245.527877 \n",
       "L 285.837462 255.695646 \n",
       "L 285.904996 243.268373 \n",
       "L 286.040064 260.214654 \n",
       "L 286.107598 231.970852 \n",
       "L 286.175132 259.084902 \n",
       "L 286.242666 255.695646 \n",
       "L 286.310201 233.100604 \n",
       "L 286.512803 257.95515 \n",
       "L 286.715405 245.527877 \n",
       "L 286.782939 243.268373 \n",
       "L 286.850474 235.360108 \n",
       "L 286.918008 244.398125 \n",
       "L 286.985542 241.008869 \n",
       "L 287.053076 242.138621 \n",
       "L 287.12061 239.879117 \n",
       "L 287.255678 251.176637 \n",
       "L 287.323212 236.489861 \n",
       "L 287.458281 247.787381 \n",
       "L 287.525815 242.138621 \n",
       "L 287.728417 254.565894 \n",
       "L 287.795951 238.749365 \n",
       "L 287.863485 242.138621 \n",
       "L 287.93102 259.084902 \n",
       "L 287.998554 251.176637 \n",
       "L 288.133622 264.733662 \n",
       "L 288.201156 233.100604 \n",
       "L 288.26869 250.046885 \n",
       "L 288.336224 250.046885 \n",
       "L 288.403759 253.436142 \n",
       "L 288.471293 251.176637 \n",
       "L 288.538827 260.214654 \n",
       "L 288.606361 254.565894 \n",
       "L 288.808963 235.360108 \n",
       "L 289.011566 253.436142 \n",
       "L 289.0791 248.917133 \n",
       "L 289.146634 231.970852 \n",
       "L 289.281702 259.084902 \n",
       "L 289.349236 254.565894 \n",
       "L 289.41677 251.176637 \n",
       "L 289.484305 266.993166 \n",
       "L 289.551839 216.154323 \n",
       "L 289.619373 233.100604 \n",
       "L 289.686907 231.970852 \n",
       "L 289.754441 239.879117 \n",
       "L 289.889509 221.803084 \n",
       "L 289.957044 227.451844 \n",
       "L 290.024578 228.581596 \n",
       "L 290.092112 239.879117 \n",
       "L 290.159646 236.489861 \n",
       "L 290.22718 229.711348 \n",
       "L 290.362248 236.489861 \n",
       "L 290.429782 228.581596 \n",
       "L 290.497317 230.8411 \n",
       "L 290.564851 234.230356 \n",
       "L 290.632385 245.527877 \n",
       "L 290.767453 227.451844 \n",
       "L 290.902521 238.749365 \n",
       "L 291.03759 228.581596 \n",
       "L 291.105124 234.230356 \n",
       "L 291.172658 230.8411 \n",
       "L 291.240192 229.711348 \n",
       "L 291.37526 237.619613 \n",
       "L 291.442794 226.322092 \n",
       "L 291.510328 251.176637 \n",
       "L 291.577863 244.398125 \n",
       "L 291.645397 216.154323 \n",
       "L 291.712931 227.451844 \n",
       "L 291.847999 243.268373 \n",
       "L 291.983067 228.581596 \n",
       "L 292.050602 233.100604 \n",
       "L 292.118136 235.360108 \n",
       "L 292.18567 222.932836 \n",
       "L 292.253204 236.489861 \n",
       "L 292.320738 215.024571 \n",
       "L 292.388272 228.581596 \n",
       "L 292.455806 222.932836 \n",
       "L 292.52334 229.711348 \n",
       "L 292.590875 217.284075 \n",
       "L 292.658409 225.19234 \n",
       "L 292.861011 211.635315 \n",
       "L 292.996079 238.749365 \n",
       "L 293.063613 219.54358 \n",
       "L 293.131148 234.230356 \n",
       "L 293.198682 226.322092 \n",
       "L 293.266216 235.360108 \n",
       "L 293.33375 234.230356 \n",
       "L 293.401284 226.322092 \n",
       "L 293.468818 239.879117 \n",
       "L 293.536352 218.413827 \n",
       "L 293.603887 225.19234 \n",
       "L 293.671421 225.19234 \n",
       "L 293.806489 231.970852 \n",
       "L 294.009091 216.154323 \n",
       "L 294.076625 219.54358 \n",
       "L 294.211694 213.894819 \n",
       "L 294.279228 228.581596 \n",
       "L 294.346762 226.322092 \n",
       "L 294.48183 217.284075 \n",
       "L 294.616898 242.138621 \n",
       "L 294.684433 235.360108 \n",
       "L 294.819501 230.8411 \n",
       "L 294.887035 211.635315 \n",
       "L 294.954569 219.54358 \n",
       "L 295.089637 226.322092 \n",
       "L 295.157172 225.19234 \n",
       "L 295.29224 207.116307 \n",
       "L 295.427308 228.581596 \n",
       "L 295.494842 224.062588 \n",
       "L 295.562376 229.711348 \n",
       "L 295.62991 227.451844 \n",
       "L 295.900047 209.375811 \n",
       "L 295.967581 231.970852 \n",
       "L 296.035115 227.451844 \n",
       "L 296.102649 217.284075 \n",
       "L 296.170183 226.322092 \n",
       "L 296.372786 213.894819 \n",
       "L 296.44032 212.765067 \n",
       "L 296.575388 217.284075 \n",
       "L 296.642922 209.375811 \n",
       "L 296.710456 228.581596 \n",
       "L 296.777991 216.154323 \n",
       "L 296.845525 239.879117 \n",
       "L 296.913059 236.489861 \n",
       "L 296.980593 207.116307 \n",
       "L 297.048127 225.19234 \n",
       "L 297.183195 238.749365 \n",
       "L 297.25073 217.284075 \n",
       "L 297.318264 221.803084 \n",
       "L 297.385798 230.8411 \n",
       "L 297.520866 215.024571 \n",
       "L 297.5884 219.54358 \n",
       "L 297.655934 202.597299 \n",
       "L 297.723468 211.635315 \n",
       "L 297.791003 218.413827 \n",
       "L 297.926071 208.246059 \n",
       "L 297.993605 216.154323 \n",
       "L 298.061139 213.894819 \n",
       "L 298.128673 210.505563 \n",
       "L 298.196207 196.948538 \n",
       "L 298.331276 211.635315 \n",
       "L 298.39881 207.116307 \n",
       "L 298.466344 225.19234 \n",
       "L 298.533878 205.986555 \n",
       "L 298.601412 211.635315 \n",
       "L 298.73648 202.597299 \n",
       "L 298.804015 219.54358 \n",
       "L 298.871549 213.894819 \n",
       "L 298.939083 207.116307 \n",
       "L 299.141685 219.54358 \n",
       "L 299.276753 202.597299 \n",
       "L 299.344288 205.986555 \n",
       "L 299.411822 205.986555 \n",
       "L 299.479356 194.689034 \n",
       "L 299.54689 200.337794 \n",
       "L 299.749492 208.246059 \n",
       "L 299.884561 210.505563 \n",
       "L 300.019629 227.451844 \n",
       "L 300.087163 219.54358 \n",
       "L 300.154697 221.803084 \n",
       "\" clip-path=\"url(#p1ce374a4b4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_44\">\n",
       "    <path d=\"M 273.54625 285.069199 \n",
       "L 273.681318 285.069199 \n",
       "L 273.816387 274.901431 \n",
       "L 274.018989 288.458456 \n",
       "L 274.35666 277.160935 \n",
       "L 274.424194 281.679943 \n",
       "L 274.491728 276.031183 \n",
       "L 274.559262 282.809695 \n",
       "L 274.626796 281.679943 \n",
       "L 274.69433 273.771679 \n",
       "L 274.896933 290.71796 \n",
       "L 275.167069 281.679943 \n",
       "L 275.234603 294.107216 \n",
       "L 275.302137 279.420439 \n",
       "L 275.369672 285.069199 \n",
       "L 275.437206 279.420439 \n",
       "L 275.50474 281.679943 \n",
       "L 275.572274 285.069199 \n",
       "L 275.84241 273.771679 \n",
       "L 275.909945 283.939447 \n",
       "L 275.977479 276.031183 \n",
       "L 276.045013 278.290687 \n",
       "L 276.112547 279.420439 \n",
       "L 276.180081 289.588208 \n",
       "L 276.247615 286.198951 \n",
       "L 276.315149 277.160935 \n",
       "L 276.382683 278.290687 \n",
       "L 276.450218 276.031183 \n",
       "L 276.517752 283.939447 \n",
       "L 276.585286 279.420439 \n",
       "L 276.65282 281.679943 \n",
       "L 276.720354 288.458456 \n",
       "L 276.855422 277.160935 \n",
       "L 276.922956 288.458456 \n",
       "L 276.990491 279.420439 \n",
       "L 277.058025 283.939447 \n",
       "L 277.193093 279.420439 \n",
       "L 277.328161 290.71796 \n",
       "L 277.46323 281.679943 \n",
       "L 277.530764 282.809695 \n",
       "L 277.598298 282.809695 \n",
       "L 277.665832 271.512175 \n",
       "L 277.733366 280.550191 \n",
       "L 277.8009 273.771679 \n",
       "L 277.868434 280.550191 \n",
       "L 277.935968 279.420439 \n",
       "L 278.003503 282.809695 \n",
       "L 278.138571 274.901431 \n",
       "L 278.206105 281.679943 \n",
       "L 278.273639 278.290687 \n",
       "L 278.341173 280.550191 \n",
       "L 278.408707 272.641927 \n",
       "L 278.543776 283.939447 \n",
       "L 278.61131 282.809695 \n",
       "L 278.678844 273.771679 \n",
       "L 278.813912 281.679943 \n",
       "L 278.881446 264.733662 \n",
       "L 278.94898 281.679943 \n",
       "L 279.016515 269.25267 \n",
       "L 279.084049 281.679943 \n",
       "L 279.151583 278.290687 \n",
       "L 279.219117 277.160935 \n",
       "L 279.286651 286.198951 \n",
       "L 279.354185 282.809695 \n",
       "L 279.421719 276.031183 \n",
       "L 279.489253 283.939447 \n",
       "L 279.556788 276.031183 \n",
       "L 279.691856 285.069199 \n",
       "L 279.75939 283.939447 \n",
       "L 279.826924 276.031183 \n",
       "L 279.894458 285.069199 \n",
       "L 280.029526 266.993166 \n",
       "L 280.097061 285.069199 \n",
       "L 280.164595 274.901431 \n",
       "L 280.232129 277.160935 \n",
       "L 280.299663 272.641927 \n",
       "L 280.367197 279.420439 \n",
       "L 280.434731 277.160935 \n",
       "L 280.637334 282.809695 \n",
       "L 280.704868 270.382423 \n",
       "L 280.772402 277.160935 \n",
       "L 280.90747 274.901431 \n",
       "L 280.975004 280.550191 \n",
       "L 281.042538 276.031183 \n",
       "L 281.110073 281.679943 \n",
       "L 281.177607 276.031183 \n",
       "L 281.245141 280.550191 \n",
       "L 281.380209 271.512175 \n",
       "L 281.447743 270.382423 \n",
       "L 281.515277 265.863414 \n",
       "L 281.650346 279.420439 \n",
       "L 281.71788 278.290687 \n",
       "L 281.785414 265.863414 \n",
       "L 281.920482 279.420439 \n",
       "L 281.988016 278.290687 \n",
       "L 282.05555 273.771679 \n",
       "L 282.123084 276.031183 \n",
       "L 282.190619 279.420439 \n",
       "L 282.325687 268.122918 \n",
       "L 282.460755 274.901431 \n",
       "L 282.528289 270.382423 \n",
       "L 282.595823 271.512175 \n",
       "L 282.730892 263.60391 \n",
       "L 282.798426 264.733662 \n",
       "L 282.86596 271.512175 \n",
       "L 282.933494 266.993166 \n",
       "L 283.001028 276.031183 \n",
       "L 283.068562 266.993166 \n",
       "L 283.136096 279.420439 \n",
       "L 283.203631 268.122918 \n",
       "L 283.271165 273.771679 \n",
       "L 283.338699 274.901431 \n",
       "L 283.406233 270.382423 \n",
       "L 283.541301 276.031183 \n",
       "L 283.608835 270.382423 \n",
       "L 283.676369 274.901431 \n",
       "L 283.743904 271.512175 \n",
       "L 283.878972 277.160935 \n",
       "L 284.081574 265.863414 \n",
       "L 284.351711 280.550191 \n",
       "L 284.419245 268.122918 \n",
       "L 284.486779 280.550191 \n",
       "L 284.554313 265.863414 \n",
       "L 284.621847 270.382423 \n",
       "L 284.689381 271.512175 \n",
       "L 284.756916 276.031183 \n",
       "L 284.82445 274.901431 \n",
       "L 284.959518 264.733662 \n",
       "L 285.027052 274.901431 \n",
       "L 285.094586 266.993166 \n",
       "L 285.16212 279.420439 \n",
       "L 285.229654 272.641927 \n",
       "L 285.432257 278.290687 \n",
       "L 285.499791 277.160935 \n",
       "L 285.634859 270.382423 \n",
       "L 285.702393 269.25267 \n",
       "L 285.769927 276.031183 \n",
       "L 285.837462 271.512175 \n",
       "L 285.904996 272.641927 \n",
       "L 285.97253 272.641927 \n",
       "L 286.107598 265.863414 \n",
       "L 286.175132 274.901431 \n",
       "L 286.242666 263.60391 \n",
       "L 286.310201 278.290687 \n",
       "L 286.377735 265.863414 \n",
       "L 286.445269 269.25267 \n",
       "L 286.512803 263.60391 \n",
       "L 286.580337 264.733662 \n",
       "L 286.647871 264.733662 \n",
       "L 286.715405 272.641927 \n",
       "L 286.782939 265.863414 \n",
       "L 286.850474 269.25267 \n",
       "L 286.918008 273.771679 \n",
       "L 286.985542 263.60391 \n",
       "L 287.053076 273.771679 \n",
       "L 287.12061 254.565894 \n",
       "L 287.188144 274.901431 \n",
       "L 287.255678 260.214654 \n",
       "L 287.323212 264.733662 \n",
       "L 287.390747 263.60391 \n",
       "L 287.458281 268.122918 \n",
       "L 287.525815 266.993166 \n",
       "L 287.593349 264.733662 \n",
       "L 287.660883 253.436142 \n",
       "L 287.728417 259.084902 \n",
       "L 287.795951 272.641927 \n",
       "L 287.863485 266.993166 \n",
       "L 287.93102 264.733662 \n",
       "L 287.998554 272.641927 \n",
       "L 288.066088 264.733662 \n",
       "L 288.133622 271.512175 \n",
       "L 288.201156 260.214654 \n",
       "L 288.26869 273.771679 \n",
       "L 288.336224 251.176637 \n",
       "L 288.403759 265.863414 \n",
       "L 288.471293 257.95515 \n",
       "L 288.606361 265.863414 \n",
       "L 288.673895 263.60391 \n",
       "L 288.741429 255.695646 \n",
       "L 288.808963 257.95515 \n",
       "L 288.876497 266.993166 \n",
       "L 288.944032 260.214654 \n",
       "L 289.011566 270.382423 \n",
       "L 289.0791 268.122918 \n",
       "L 289.214168 255.695646 \n",
       "L 289.281702 256.825398 \n",
       "L 289.41677 269.25267 \n",
       "L 289.484305 268.122918 \n",
       "L 289.551839 254.565894 \n",
       "L 289.619373 262.474158 \n",
       "L 289.754441 255.695646 \n",
       "L 289.821975 265.863414 \n",
       "L 289.889509 253.436142 \n",
       "L 289.957044 261.344406 \n",
       "L 290.024578 255.695646 \n",
       "L 290.092112 265.863414 \n",
       "L 290.159646 260.214654 \n",
       "L 290.294714 274.901431 \n",
       "L 290.497317 250.046885 \n",
       "L 290.564851 270.382423 \n",
       "L 290.632385 263.60391 \n",
       "L 290.767453 262.474158 \n",
       "L 290.834987 239.879117 \n",
       "L 290.902521 263.60391 \n",
       "L 290.970055 257.95515 \n",
       "L 291.03759 265.863414 \n",
       "L 291.105124 256.825398 \n",
       "L 291.172658 259.084902 \n",
       "L 291.307726 255.695646 \n",
       "L 291.37526 262.474158 \n",
       "L 291.442794 257.95515 \n",
       "L 291.510328 264.733662 \n",
       "L 291.645397 251.176637 \n",
       "L 291.712931 265.863414 \n",
       "L 291.780465 264.733662 \n",
       "L 291.915533 260.214654 \n",
       "L 291.983067 260.214654 \n",
       "L 292.050602 252.306389 \n",
       "L 292.118136 266.993166 \n",
       "L 292.18567 263.60391 \n",
       "L 292.455806 252.306389 \n",
       "L 292.658409 260.214654 \n",
       "L 292.793477 247.787381 \n",
       "L 292.861011 265.863414 \n",
       "L 292.928545 252.306389 \n",
       "L 293.063613 266.993166 \n",
       "L 293.131148 257.95515 \n",
       "L 293.198682 262.474158 \n",
       "L 293.266216 262.474158 \n",
       "L 293.33375 250.046885 \n",
       "L 293.401284 262.474158 \n",
       "L 293.468818 250.046885 \n",
       "L 293.536352 254.565894 \n",
       "L 293.603887 248.917133 \n",
       "L 293.671421 251.176637 \n",
       "L 293.738955 265.863414 \n",
       "L 293.806489 262.474158 \n",
       "L 293.874023 257.95515 \n",
       "L 293.941557 262.474158 \n",
       "L 294.009091 256.825398 \n",
       "L 294.076625 268.122918 \n",
       "L 294.14416 254.565894 \n",
       "L 294.211694 262.474158 \n",
       "L 294.346762 264.733662 \n",
       "L 294.414296 260.214654 \n",
       "L 294.48183 264.733662 \n",
       "L 294.549364 248.917133 \n",
       "L 294.616898 251.176637 \n",
       "L 294.684433 244.398125 \n",
       "L 294.819501 254.565894 \n",
       "L 294.887035 251.176637 \n",
       "L 294.954569 261.344406 \n",
       "L 295.022103 251.176637 \n",
       "L 295.089637 259.084902 \n",
       "L 295.359774 242.138621 \n",
       "L 295.562376 260.214654 \n",
       "L 295.62991 257.95515 \n",
       "L 295.764979 245.527877 \n",
       "L 295.832513 260.214654 \n",
       "L 295.967581 243.268373 \n",
       "L 296.035115 263.60391 \n",
       "L 296.102649 246.657629 \n",
       "L 296.170183 248.917133 \n",
       "L 296.305252 263.60391 \n",
       "L 296.372786 257.95515 \n",
       "L 296.44032 259.084902 \n",
       "L 296.507854 263.60391 \n",
       "L 296.642922 244.398125 \n",
       "L 296.845525 259.084902 \n",
       "L 297.115661 243.268373 \n",
       "L 297.318264 250.046885 \n",
       "L 297.385798 250.046885 \n",
       "L 297.453332 254.565894 \n",
       "L 297.520866 245.527877 \n",
       "L 297.5884 251.176637 \n",
       "L 297.655934 257.95515 \n",
       "L 297.791003 237.619613 \n",
       "L 297.858537 252.306389 \n",
       "L 297.993605 242.138621 \n",
       "L 298.128673 248.917133 \n",
       "L 298.263741 241.008869 \n",
       "L 298.331276 253.436142 \n",
       "L 298.39881 245.527877 \n",
       "L 298.466344 246.657629 \n",
       "L 298.668946 259.084902 \n",
       "L 298.804015 245.527877 \n",
       "L 298.871549 253.436142 \n",
       "L 298.939083 248.917133 \n",
       "L 299.006617 253.436142 \n",
       "L 299.074151 244.398125 \n",
       "L 299.141685 252.306389 \n",
       "L 299.209219 242.138621 \n",
       "L 299.276753 255.695646 \n",
       "L 299.344288 245.527877 \n",
       "L 299.411822 251.176637 \n",
       "L 299.479356 252.306389 \n",
       "L 299.54689 241.008869 \n",
       "L 299.614424 254.565894 \n",
       "L 299.681958 251.176637 \n",
       "L 299.749492 248.917133 \n",
       "L 299.817026 241.008869 \n",
       "L 299.884561 245.527877 \n",
       "L 299.952095 245.527877 \n",
       "L 300.087163 260.214654 \n",
       "L 300.222231 239.879117 \n",
       "L 300.289765 246.657629 \n",
       "L 300.357299 244.398125 \n",
       "L 300.424834 238.749365 \n",
       "L 300.559902 246.657629 \n",
       "L 300.627436 237.619613 \n",
       "L 300.762504 255.695646 \n",
       "L 300.897573 238.749365 \n",
       "L 301.100175 248.917133 \n",
       "L 301.167709 242.138621 \n",
       "L 301.302777 253.436142 \n",
       "L 301.370311 243.268373 \n",
       "L 301.437846 257.95515 \n",
       "L 301.50538 242.138621 \n",
       "L 301.572914 247.787381 \n",
       "L 301.707982 239.879117 \n",
       "L 301.775516 241.008869 \n",
       "L 301.84305 242.138621 \n",
       "L 301.910584 252.306389 \n",
       "L 301.978119 237.619613 \n",
       "L 302.045653 246.657629 \n",
       "L 302.180721 247.787381 \n",
       "L 302.248255 257.95515 \n",
       "L 302.315789 253.436142 \n",
       "L 302.383323 256.825398 \n",
       "L 302.450858 233.100604 \n",
       "L 302.518392 252.306389 \n",
       "L 302.585926 241.008869 \n",
       "L 302.65346 256.825398 \n",
       "L 302.720994 250.046885 \n",
       "L 302.788528 238.749365 \n",
       "L 302.856062 245.527877 \n",
       "L 302.923596 244.398125 \n",
       "L 302.991131 236.489861 \n",
       "L 303.058665 246.657629 \n",
       "L 303.126199 245.527877 \n",
       "L 303.261267 239.879117 \n",
       "L 303.328801 247.787381 \n",
       "L 303.396335 243.268373 \n",
       "L 303.598938 235.360108 \n",
       "L 303.734006 256.825398 \n",
       "L 303.80154 247.787381 \n",
       "L 303.936608 254.565894 \n",
       "L 304.071677 246.657629 \n",
       "L 304.139211 248.917133 \n",
       "L 304.206745 241.008869 \n",
       "L 304.274279 242.138621 \n",
       "L 304.341813 250.046885 \n",
       "L 304.409347 237.619613 \n",
       "L 304.476881 239.879117 \n",
       "L 304.544416 247.787381 \n",
       "L 304.61195 244.398125 \n",
       "L 304.679484 230.8411 \n",
       "L 304.747018 245.527877 \n",
       "L 304.814552 237.619613 \n",
       "L 305.017154 253.436142 \n",
       "L 305.152223 236.866443 \n",
       "L 305.354825 250.046885 \n",
       "L 305.422359 236.489861 \n",
       "L 305.489893 243.268373 \n",
       "L 305.557427 252.306389 \n",
       "L 305.624962 230.8411 \n",
       "L 305.692496 250.046885 \n",
       "L 305.76003 234.230356 \n",
       "L 305.827564 247.787381 \n",
       "L 305.895098 243.268373 \n",
       "L 306.097701 254.565894 \n",
       "L 306.232769 230.8411 \n",
       "L 306.300303 253.436142 \n",
       "L 306.367837 246.657629 \n",
       "L 306.435371 231.970852 \n",
       "L 306.502905 243.268373 \n",
       "L 306.570439 239.879117 \n",
       "L 306.705508 245.527877 \n",
       "L 306.773042 231.970852 \n",
       "L 306.90811 256.825398 \n",
       "L 306.975644 238.749365 \n",
       "L 307.043178 241.008869 \n",
       "L 307.178247 237.619613 \n",
       "L 307.245781 237.619613 \n",
       "L 307.313315 245.527877 \n",
       "L 307.380849 239.879117 \n",
       "L 307.448383 242.138621 \n",
       "L 307.515917 241.008869 \n",
       "L 307.583451 227.451844 \n",
       "L 307.650985 244.398125 \n",
       "L 307.71852 239.879117 \n",
       "L 307.786054 234.230356 \n",
       "L 307.921122 241.008869 \n",
       "L 307.988656 236.489861 \n",
       "L 308.123724 245.527877 \n",
       "L 308.191259 237.619613 \n",
       "L 308.258793 247.787381 \n",
       "L 308.326327 243.268373 \n",
       "L 308.393861 248.917133 \n",
       "L 308.461395 245.527877 \n",
       "L 308.528929 246.657629 \n",
       "L 308.596463 242.138621 \n",
       "L 308.663997 246.657629 \n",
       "L 308.731532 239.879117 \n",
       "L 308.799066 257.95515 \n",
       "L 308.934134 234.230356 \n",
       "L 309.001668 235.360108 \n",
       "L 309.136736 244.398125 \n",
       "L 309.20427 252.306389 \n",
       "L 309.271805 250.046885 \n",
       "L 309.406873 237.619613 \n",
       "L 309.474407 245.527877 \n",
       "L 309.541941 244.398125 \n",
       "L 309.609475 233.100604 \n",
       "L 309.677009 235.360108 \n",
       "L 309.744544 246.657629 \n",
       "L 309.812078 234.230356 \n",
       "L 309.947146 248.917133 \n",
       "L 310.01468 236.489861 \n",
       "L 310.082214 243.268373 \n",
       "L 310.149748 246.657629 \n",
       "L 310.217282 242.138621 \n",
       "L 310.284817 243.268373 \n",
       "L 310.487419 246.657629 \n",
       "L 310.554953 229.711348 \n",
       "L 310.622487 237.619613 \n",
       "L 310.690021 241.008869 \n",
       "L 310.82509 230.8411 \n",
       "L 310.892624 253.436142 \n",
       "L 310.960158 241.008869 \n",
       "L 311.095226 231.970852 \n",
       "L 311.230294 254.565894 \n",
       "L 311.432897 235.360108 \n",
       "L 311.500431 246.657629 \n",
       "L 311.567965 236.489861 \n",
       "L 311.635499 241.008869 \n",
       "L 311.703033 238.749365 \n",
       "L 311.770567 247.787381 \n",
       "L 311.838102 228.581596 \n",
       "L 311.905636 238.749365 \n",
       "L 311.97317 242.138621 \n",
       "L 312.040704 252.306389 \n",
       "L 312.108238 248.917133 \n",
       "L 312.175772 235.360108 \n",
       "L 312.243306 244.398125 \n",
       "L 312.31084 228.581596 \n",
       "L 312.378375 236.489861 \n",
       "L 312.445909 234.230356 \n",
       "L 312.513443 236.489861 \n",
       "L 312.580977 243.268373 \n",
       "L 312.648511 228.581596 \n",
       "L 312.783579 242.138621 \n",
       "L 312.851113 241.008869 \n",
       "L 312.986182 231.970852 \n",
       "L 313.053716 234.230356 \n",
       "L 313.12125 236.489861 \n",
       "L 313.256318 228.581596 \n",
       "L 313.323852 245.527877 \n",
       "L 313.391387 235.360108 \n",
       "L 313.526455 246.657629 \n",
       "L 313.593989 231.970852 \n",
       "L 313.661523 238.749365 \n",
       "L 313.729057 238.749365 \n",
       "L 313.796591 251.176637 \n",
       "L 313.93166 234.230356 \n",
       "L 313.999194 248.917133 \n",
       "L 314.066728 242.138621 \n",
       "L 314.134262 228.581596 \n",
       "L 314.201796 234.230356 \n",
       "L 314.26933 250.046885 \n",
       "L 314.336864 248.917133 \n",
       "L 314.539467 235.360108 \n",
       "L 314.607001 242.138621 \n",
       "L 314.674535 236.489861 \n",
       "L 314.742069 242.138621 \n",
       "L 314.877137 234.230356 \n",
       "L 315.012206 242.138621 \n",
       "L 315.07974 227.451844 \n",
       "L 315.147274 231.970852 \n",
       "L 315.214808 245.527877 \n",
       "L 315.282342 244.398125 \n",
       "L 315.349876 228.581596 \n",
       "L 315.41741 238.749365 \n",
       "L 315.484945 228.581596 \n",
       "L 315.552479 243.268373 \n",
       "L 315.620013 239.879117 \n",
       "L 315.755081 237.619613 \n",
       "L 315.957683 252.306389 \n",
       "L 316.025218 241.008869 \n",
       "L 316.092752 247.787381 \n",
       "L 316.22782 243.268373 \n",
       "L 316.295354 231.970852 \n",
       "L 316.362888 233.100604 \n",
       "L 316.565491 238.749365 \n",
       "L 316.633025 247.787381 \n",
       "L 316.700559 246.657629 \n",
       "L 316.835627 243.268373 \n",
       "L 316.903161 230.8411 \n",
       "L 316.970695 250.046885 \n",
       "L 317.03823 241.008869 \n",
       "L 317.173298 244.398125 \n",
       "L 317.308366 228.581596 \n",
       "L 317.3759 245.527877 \n",
       "L 317.443434 238.749365 \n",
       "L 317.510968 236.489861 \n",
       "L 317.578503 237.619613 \n",
       "L 317.646037 239.879117 \n",
       "L 317.781105 235.360108 \n",
       "L 317.916173 257.95515 \n",
       "L 317.983707 236.489861 \n",
       "L 318.051241 239.879117 \n",
       "L 318.118776 243.268373 \n",
       "L 318.18631 231.970852 \n",
       "L 318.253844 238.749365 \n",
       "L 318.321378 238.749365 \n",
       "L 318.456446 250.046885 \n",
       "L 318.591515 236.489861 \n",
       "L 318.659049 242.138621 \n",
       "L 318.726583 239.879117 \n",
       "L 318.794117 242.138621 \n",
       "L 318.861651 235.360108 \n",
       "L 318.996719 246.657629 \n",
       "L 319.064253 236.489861 \n",
       "L 319.131788 246.657629 \n",
       "L 319.199322 243.268373 \n",
       "L 319.266856 243.268373 \n",
       "L 319.401924 238.749365 \n",
       "L 319.469458 248.917133 \n",
       "L 319.536992 230.8411 \n",
       "L 319.604526 235.360108 \n",
       "L 319.739595 239.879117 \n",
       "L 319.807129 227.451844 \n",
       "L 319.874663 239.879117 \n",
       "L 319.942197 234.230356 \n",
       "L 320.009731 239.879117 \n",
       "L 320.077265 234.230356 \n",
       "L 320.144799 241.008869 \n",
       "L 320.212334 233.100604 \n",
       "L 320.279868 244.398125 \n",
       "L 320.347402 237.619613 \n",
       "L 320.414936 237.619613 \n",
       "L 320.48247 233.100604 \n",
       "L 320.550004 217.284075 \n",
       "L 320.685073 244.398125 \n",
       "L 320.752607 253.436142 \n",
       "L 320.820141 230.8411 \n",
       "L 320.887675 235.360108 \n",
       "L 320.955209 227.451844 \n",
       "L 321.022743 253.436142 \n",
       "L 321.090277 226.322092 \n",
       "L 321.157811 238.749365 \n",
       "L 321.225346 227.451844 \n",
       "L 321.29288 241.008869 \n",
       "L 321.360414 226.322092 \n",
       "L 321.427948 231.970852 \n",
       "L 321.563016 243.268373 \n",
       "L 321.698084 234.230356 \n",
       "L 321.833153 239.879117 \n",
       "L 321.900687 247.787381 \n",
       "L 321.968221 235.360108 \n",
       "L 322.035755 236.489861 \n",
       "L 322.103289 236.489861 \n",
       "L 322.170823 239.879117 \n",
       "L 322.238358 233.100604 \n",
       "L 322.305892 238.749365 \n",
       "L 322.373426 236.489861 \n",
       "L 322.44096 236.489861 \n",
       "L 322.508494 237.619613 \n",
       "L 322.576028 236.489861 \n",
       "L 322.643562 244.398125 \n",
       "L 322.711096 230.8411 \n",
       "L 322.778631 237.619613 \n",
       "L 322.846165 244.398125 \n",
       "L 322.913699 236.489861 \n",
       "L 322.981233 237.619613 \n",
       "L 323.048767 238.749365 \n",
       "L 323.116301 237.619613 \n",
       "L 323.183835 226.322092 \n",
       "L 323.251369 238.749365 \n",
       "L 323.318904 236.489861 \n",
       "L 323.386438 236.489861 \n",
       "L 323.724108 248.917133 \n",
       "L 323.791642 226.322092 \n",
       "L 323.859177 241.008869 \n",
       "L 323.926711 238.749365 \n",
       "L 323.994245 231.970852 \n",
       "L 324.196847 246.657629 \n",
       "L 324.331916 238.749365 \n",
       "L 324.39945 243.268373 \n",
       "L 324.466984 239.879117 \n",
       "L 324.534518 241.008869 \n",
       "L 324.602052 241.008869 \n",
       "L 324.669586 245.527877 \n",
       "L 324.73712 242.138621 \n",
       "L 324.804654 243.268373 \n",
       "L 324.872189 243.268373 \n",
       "L 324.939723 248.917133 \n",
       "L 325.007257 235.360108 \n",
       "L 325.074791 237.619613 \n",
       "L 325.142325 239.879117 \n",
       "L 325.209859 251.176637 \n",
       "L 325.277393 244.398125 \n",
       "L 325.412462 237.619613 \n",
       "L 325.479996 248.917133 \n",
       "L 325.54753 241.008869 \n",
       "L 325.615064 246.657629 \n",
       "L 325.682598 237.619613 \n",
       "L 325.750132 244.398125 \n",
       "L 325.817666 235.360108 \n",
       "L 325.885201 238.749365 \n",
       "L 325.952735 243.268373 \n",
       "L 326.020269 241.008869 \n",
       "L 326.087803 241.008869 \n",
       "L 326.222871 246.657629 \n",
       "L 326.357939 230.8411 \n",
       "L 326.493008 243.268373 \n",
       "L 326.560542 244.398125 \n",
       "L 326.69561 230.8411 \n",
       "L 326.830678 252.306389 \n",
       "L 326.898212 242.138621 \n",
       "L 326.965747 241.008869 \n",
       "L 327.100815 248.917133 \n",
       "L 327.303417 226.322092 \n",
       "L 327.370951 246.657629 \n",
       "L 327.438485 234.230356 \n",
       "L 327.573554 244.398125 \n",
       "L 327.708622 230.8411 \n",
       "L 327.776156 235.360108 \n",
       "L 327.84369 234.230356 \n",
       "L 327.911224 246.657629 \n",
       "L 327.978759 243.268373 \n",
       "L 328.046293 237.619613 \n",
       "L 328.113827 247.787381 \n",
       "L 328.181361 244.398125 \n",
       "L 328.248895 243.268373 \n",
       "L 328.383963 237.619613 \n",
       "L 328.519032 252.306389 \n",
       "L 328.6541 234.230356 \n",
       "L 328.721634 237.619613 \n",
       "L 328.924236 250.046885 \n",
       "L 328.99177 235.360108 \n",
       "L 329.059305 239.879117 \n",
       "L 329.126839 261.344406 \n",
       "L 329.194373 230.8411 \n",
       "L 329.261907 236.489861 \n",
       "L 329.329441 254.565894 \n",
       "L 329.396975 239.879117 \n",
       "L 329.464509 243.268373 \n",
       "L 329.532044 254.565894 \n",
       "L 329.599578 235.360108 \n",
       "L 329.667112 252.306389 \n",
       "L 329.80218 238.749365 \n",
       "L 329.937248 253.436142 \n",
       "L 330.139851 230.8411 \n",
       "L 330.342453 244.398125 \n",
       "L 330.409987 248.917133 \n",
       "L 330.61259 233.100604 \n",
       "L 330.815192 244.398125 \n",
       "L 330.882726 255.695646 \n",
       "L 330.95026 231.970852 \n",
       "L 331.017794 247.787381 \n",
       "L 331.152863 234.230356 \n",
       "L 331.355465 254.565894 \n",
       "L 331.422999 234.230356 \n",
       "L 331.490533 245.527877 \n",
       "L 331.558067 246.657629 \n",
       "L 331.625602 238.749365 \n",
       "L 331.76067 257.95515 \n",
       "L 331.828204 244.398125 \n",
       "L 331.895738 254.565894 \n",
       "L 331.963272 251.176637 \n",
       "L 332.030806 230.8411 \n",
       "L 332.09834 256.825398 \n",
       "L 332.165875 245.527877 \n",
       "L 332.300943 235.360108 \n",
       "L 332.368477 251.176637 \n",
       "L 332.436011 231.970852 \n",
       "L 332.503545 251.176637 \n",
       "L 332.571079 241.008869 \n",
       "L 332.706148 248.917133 \n",
       "L 332.90875 237.619613 \n",
       "L 332.976284 239.879117 \n",
       "L 333.043818 248.917133 \n",
       "L 333.111352 238.749365 \n",
       "L 333.246421 252.306389 \n",
       "L 333.313955 243.268373 \n",
       "L 333.381489 250.046885 \n",
       "L 333.449023 243.268373 \n",
       "L 333.516557 251.176637 \n",
       "L 333.584091 246.657629 \n",
       "L 333.651625 242.138621 \n",
       "L 333.71916 250.046885 \n",
       "L 333.786694 238.749365 \n",
       "L 333.854228 241.008869 \n",
       "L 333.989296 259.084902 \n",
       "L 334.124364 245.527877 \n",
       "L 334.191898 246.657629 \n",
       "L 334.259433 251.176637 \n",
       "L 334.326967 233.100604 \n",
       "L 334.394501 256.825398 \n",
       "L 334.462035 238.749365 \n",
       "L 334.529569 247.787381 \n",
       "L 334.597103 237.619613 \n",
       "L 334.664637 252.306389 \n",
       "L 334.732172 250.046885 \n",
       "L 334.799706 254.565894 \n",
       "L 334.86724 250.046885 \n",
       "L 334.934774 236.489861 \n",
       "L 335.002308 245.527877 \n",
       "L 335.069842 226.322092 \n",
       "L 335.137376 229.711348 \n",
       "L 335.20491 252.306389 \n",
       "L 335.272445 250.046885 \n",
       "L 335.475047 239.879117 \n",
       "L 335.677649 256.825398 \n",
       "L 335.812718 237.619613 \n",
       "L 335.880252 248.917133 \n",
       "L 335.947786 237.619613 \n",
       "L 336.01532 248.917133 \n",
       "L 336.082854 244.398125 \n",
       "L 336.150388 242.138621 \n",
       "L 336.217922 244.398125 \n",
       "L 336.285456 255.695646 \n",
       "L 336.420525 236.489861 \n",
       "L 336.488059 251.176637 \n",
       "L 336.555593 246.657629 \n",
       "L 336.623127 248.917133 \n",
       "L 336.690661 233.100604 \n",
       "L 336.758195 246.657629 \n",
       "L 336.82573 235.360108 \n",
       "L 336.893264 252.306389 \n",
       "L 336.960798 251.176637 \n",
       "L 337.028332 245.527877 \n",
       "L 337.1634 251.176637 \n",
       "L 337.298468 246.657629 \n",
       "L 337.366003 248.917133 \n",
       "L 337.433537 245.527877 \n",
       "L 337.568605 254.565894 \n",
       "L 337.703673 234.230356 \n",
       "L 337.838741 248.917133 \n",
       "L 337.906276 247.787381 \n",
       "L 337.97381 238.749365 \n",
       "L 338.041344 242.138621 \n",
       "L 338.108878 244.398125 \n",
       "L 338.176412 236.489861 \n",
       "L 338.379015 253.436142 \n",
       "L 338.446549 243.268373 \n",
       "L 338.514083 245.527877 \n",
       "L 338.581617 244.398125 \n",
       "L 338.649151 241.008869 \n",
       "L 338.716685 245.527877 \n",
       "L 338.784219 260.214654 \n",
       "L 338.851753 239.879117 \n",
       "L 338.919288 242.138621 \n",
       "L 339.054356 255.695646 \n",
       "L 339.324492 244.398125 \n",
       "L 339.459561 256.825398 \n",
       "L 339.527095 255.695646 \n",
       "L 339.594629 238.749365 \n",
       "L 339.662163 242.138621 \n",
       "L 339.729697 256.825398 \n",
       "L 339.797231 255.695646 \n",
       "L 339.864765 248.917133 \n",
       "L 340.067368 260.214654 \n",
       "L 340.202436 239.879117 \n",
       "L 340.337504 257.95515 \n",
       "L 340.405038 246.657629 \n",
       "L 340.472573 257.95515 \n",
       "L 340.540107 237.619613 \n",
       "L 340.607641 261.344406 \n",
       "L 340.675175 252.306389 \n",
       "L 340.742709 251.176637 \n",
       "L 340.877777 255.695646 \n",
       "L 341.012846 248.917133 \n",
       "L 341.08038 253.436142 \n",
       "L 341.147914 241.008869 \n",
       "L 341.215448 252.306389 \n",
       "L 341.282982 248.917133 \n",
       "L 341.350516 247.787381 \n",
       "L 341.41805 244.398125 \n",
       "L 341.620653 256.825398 \n",
       "L 341.688187 239.879117 \n",
       "L 341.823255 255.695646 \n",
       "L 341.890789 245.527877 \n",
       "L 341.958323 250.046885 \n",
       "L 342.025858 245.527877 \n",
       "L 342.093392 254.565894 \n",
       "L 342.160926 252.306389 \n",
       "L 342.22846 251.176637 \n",
       "L 342.295994 260.214654 \n",
       "L 342.363528 259.084902 \n",
       "L 342.566131 250.046885 \n",
       "L 342.633665 245.527877 \n",
       "L 342.701199 248.917133 \n",
       "L 342.768733 246.657629 \n",
       "L 342.836267 259.084902 \n",
       "L 342.903801 244.398125 \n",
       "L 342.971335 250.046885 \n",
       "L 343.038869 255.695646 \n",
       "L 343.106404 251.176637 \n",
       "L 343.173938 256.825398 \n",
       "L 343.241472 255.695646 \n",
       "L 343.37654 252.306389 \n",
       "L 343.444074 245.527877 \n",
       "L 343.511608 253.436142 \n",
       "L 343.579142 241.008869 \n",
       "L 343.646677 261.344406 \n",
       "L 343.714211 260.214654 \n",
       "L 343.916813 243.268373 \n",
       "L 343.984347 263.60391 \n",
       "L 344.051881 253.436142 \n",
       "L 344.119416 259.084902 \n",
       "L 344.18695 250.046885 \n",
       "L 344.254484 252.306389 \n",
       "L 344.322018 256.825398 \n",
       "L 344.389552 245.527877 \n",
       "L 344.457086 252.306389 \n",
       "L 344.52462 256.825398 \n",
       "L 344.592154 248.917133 \n",
       "L 344.659689 263.60391 \n",
       "L 344.727223 261.344406 \n",
       "L 344.862291 244.398125 \n",
       "L 344.997359 266.993166 \n",
       "L 345.064893 263.60391 \n",
       "L 345.33503 248.917133 \n",
       "L 345.402564 266.993166 \n",
       "L 345.470098 263.60391 \n",
       "L 345.605166 237.619613 \n",
       "L 345.672701 247.787381 \n",
       "L 345.807769 261.344406 \n",
       "L 345.875303 244.398125 \n",
       "L 345.942837 255.695646 \n",
       "L 346.010371 252.306389 \n",
       "L 346.077905 253.436142 \n",
       "L 346.145439 257.95515 \n",
       "L 346.280508 245.527877 \n",
       "L 346.348042 255.695646 \n",
       "L 346.415576 252.306389 \n",
       "L 346.48311 259.084902 \n",
       "L 346.550644 247.787381 \n",
       "L 346.618178 253.436142 \n",
       "L 346.685712 248.917133 \n",
       "L 346.820781 254.565894 \n",
       "L 346.888315 254.565894 \n",
       "L 346.955849 259.084902 \n",
       "L 347.090917 245.527877 \n",
       "L 347.225985 264.733662 \n",
       "L 347.29352 259.084902 \n",
       "L 347.428588 248.917133 \n",
       "L 347.496122 251.176637 \n",
       "L 347.563656 250.046885 \n",
       "L 347.63119 255.695646 \n",
       "L 347.698724 247.787381 \n",
       "L 347.833793 256.825398 \n",
       "L 347.968861 248.917133 \n",
       "L 348.036395 251.176637 \n",
       "L 348.103929 259.084902 \n",
       "L 348.171463 257.95515 \n",
       "L 348.238997 254.565894 \n",
       "L 348.374066 262.474158 \n",
       "L 348.4416 268.122918 \n",
       "L 348.576668 255.695646 \n",
       "L 348.644202 260.214654 \n",
       "L 348.711736 255.695646 \n",
       "L 348.77927 259.084902 \n",
       "L 348.846805 257.95515 \n",
       "L 348.914339 254.565894 \n",
       "L 348.981873 256.825398 \n",
       "L 349.049407 247.787381 \n",
       "L 349.252009 260.214654 \n",
       "L 349.319544 256.825398 \n",
       "L 349.454612 262.474158 \n",
       "L 349.657214 253.436142 \n",
       "L 349.792282 255.695646 \n",
       "L 349.859817 254.565894 \n",
       "L 350.062419 264.733662 \n",
       "L 350.197487 252.306389 \n",
       "L 350.40009 262.474158 \n",
       "L 350.535158 251.176637 \n",
       "L 350.670226 268.122918 \n",
       "L 350.872828 252.306389 \n",
       "L 351.007897 261.344406 \n",
       "L 351.075431 251.176637 \n",
       "L 351.210499 263.60391 \n",
       "L 351.345567 253.436142 \n",
       "L 351.480636 255.695646 \n",
       "L 351.54817 247.787381 \n",
       "L 351.615704 252.306389 \n",
       "L 351.750772 262.474158 \n",
       "L 351.818306 259.084902 \n",
       "L 351.88584 261.344406 \n",
       "L 351.953375 260.214654 \n",
       "L 352.088443 251.176637 \n",
       "L 352.223511 266.993166 \n",
       "L 352.291045 261.344406 \n",
       "L 352.358579 265.863414 \n",
       "L 352.493648 257.95515 \n",
       "L 352.561182 259.084902 \n",
       "L 352.628716 250.046885 \n",
       "L 352.763784 261.344406 \n",
       "L 352.831318 255.695646 \n",
       "L 352.898852 259.084902 \n",
       "L 352.966387 255.695646 \n",
       "L 353.033921 268.122918 \n",
       "L 353.168989 255.695646 \n",
       "L 353.236523 256.825398 \n",
       "L 353.304057 266.993166 \n",
       "L 353.371591 262.474158 \n",
       "L 353.439125 244.398125 \n",
       "L 353.50666 251.176637 \n",
       "L 353.641728 262.474158 \n",
       "L 353.709262 262.474158 \n",
       "L 353.776796 257.95515 \n",
       "L 353.84433 259.084902 \n",
       "L 353.911864 262.474158 \n",
       "L 353.979398 259.084902 \n",
       "L 354.046933 250.046885 \n",
       "L 354.114467 264.733662 \n",
       "L 354.182001 257.95515 \n",
       "L 354.317069 250.046885 \n",
       "L 354.452137 261.344406 \n",
       "L 354.519672 253.436142 \n",
       "L 354.587206 257.95515 \n",
       "L 354.789808 247.787381 \n",
       "L 354.857342 261.344406 \n",
       "L 354.924876 250.046885 \n",
       "L 354.99241 255.695646 \n",
       "L 355.059945 261.344406 \n",
       "L 355.195013 251.176637 \n",
       "L 355.397615 261.344406 \n",
       "L 355.465149 246.657629 \n",
       "L 355.532683 255.695646 \n",
       "L 355.600218 265.863414 \n",
       "L 355.735286 247.787381 \n",
       "L 355.870354 266.993166 \n",
       "L 355.937888 252.306389 \n",
       "L 356.072956 264.733662 \n",
       "L 356.140491 244.398125 \n",
       "L 356.208025 259.084902 \n",
       "L 356.275559 253.436142 \n",
       "L 356.410627 260.214654 \n",
       "L 356.478161 256.825398 \n",
       "L 356.545695 248.917133 \n",
       "L 356.61323 262.474158 \n",
       "L 356.680764 251.176637 \n",
       "L 356.748298 255.695646 \n",
       "L 356.815832 254.565894 \n",
       "L 356.883366 256.825398 \n",
       "L 356.9509 253.436142 \n",
       "L 357.018434 259.084902 \n",
       "L 357.085968 246.657629 \n",
       "L 357.221037 261.344406 \n",
       "L 357.288571 261.344406 \n",
       "L 357.356105 269.25267 \n",
       "L 357.491173 257.95515 \n",
       "L 357.558707 260.214654 \n",
       "L 357.626241 254.565894 \n",
       "L 357.693776 255.695646 \n",
       "L 357.896378 260.214654 \n",
       "L 358.031446 250.046885 \n",
       "L 358.09898 251.176637 \n",
       "L 358.166515 265.863414 \n",
       "L 358.234049 251.176637 \n",
       "L 358.301583 260.214654 \n",
       "L 358.369117 265.863414 \n",
       "L 358.571719 253.436142 \n",
       "L 358.639253 260.214654 \n",
       "L 358.706788 257.95515 \n",
       "L 358.774322 261.344406 \n",
       "L 358.841856 260.214654 \n",
       "L 358.90939 252.306389 \n",
       "L 358.976924 253.436142 \n",
       "L 359.044458 260.214654 \n",
       "L 359.111992 248.917133 \n",
       "L 359.179526 250.046885 \n",
       "L 359.247061 253.436142 \n",
       "L 359.314595 243.268373 \n",
       "L 359.382129 247.787381 \n",
       "L 359.449663 261.344406 \n",
       "L 359.517197 260.214654 \n",
       "L 359.652265 245.527877 \n",
       "L 359.719799 254.565894 \n",
       "L 359.787334 253.436142 \n",
       "L 359.854868 250.046885 \n",
       "L 360.05747 264.733662 \n",
       "L 360.125004 261.344406 \n",
       "L 360.192538 263.60391 \n",
       "L 360.260073 256.825398 \n",
       "L 360.327607 263.60391 \n",
       "L 360.462675 256.825398 \n",
       "L 360.530209 261.344406 \n",
       "L 360.665277 255.695646 \n",
       "L 360.732811 255.695646 \n",
       "L 360.800346 257.95515 \n",
       "L 360.935414 246.657629 \n",
       "L 361.070482 266.993166 \n",
       "L 361.138016 252.306389 \n",
       "L 361.20555 257.95515 \n",
       "L 361.273084 254.565894 \n",
       "L 361.340619 256.825398 \n",
       "L 361.408153 250.046885 \n",
       "L 361.543221 260.214654 \n",
       "L 361.745823 246.657629 \n",
       "L 361.813358 248.917133 \n",
       "L 361.948426 264.733662 \n",
       "L 362.01596 256.825398 \n",
       "L 362.083494 263.60391 \n",
       "L 362.151028 256.825398 \n",
       "L 362.218562 264.733662 \n",
       "L 362.286096 255.695646 \n",
       "L 362.353631 259.084902 \n",
       "L 362.421165 254.565894 \n",
       "L 362.488699 255.695646 \n",
       "L 362.556233 260.214654 \n",
       "L 362.691301 250.046885 \n",
       "L 362.826369 257.95515 \n",
       "L 362.893904 244.398125 \n",
       "L 362.961438 248.917133 \n",
       "L 363.028972 242.138621 \n",
       "L 363.16404 263.60391 \n",
       "L 363.231574 259.084902 \n",
       "L 363.299108 263.60391 \n",
       "L 363.366642 260.214654 \n",
       "L 363.434177 261.344406 \n",
       "L 363.501711 247.787381 \n",
       "L 363.569245 264.733662 \n",
       "L 363.636779 250.046885 \n",
       "L 363.704313 252.306389 \n",
       "L 363.839381 265.863414 \n",
       "L 364.041984 254.565894 \n",
       "L 364.109518 252.306389 \n",
       "L 364.177052 253.436142 \n",
       "L 364.244586 255.695646 \n",
       "L 364.31212 254.565894 \n",
       "L 364.379654 245.527877 \n",
       "L 364.514723 256.825398 \n",
       "L 364.582257 248.917133 \n",
       "L 364.649791 254.565894 \n",
       "L 364.717325 252.306389 \n",
       "L 364.784859 253.436142 \n",
       "L 364.852393 246.657629 \n",
       "L 364.987462 254.565894 \n",
       "L 365.054996 252.306389 \n",
       "L 365.12253 259.084902 \n",
       "L 365.257598 253.436142 \n",
       "L 365.325132 252.306389 \n",
       "L 365.392666 245.527877 \n",
       "L 365.460201 264.733662 \n",
       "L 365.527735 256.825398 \n",
       "L 365.595269 246.657629 \n",
       "L 365.662803 254.565894 \n",
       "L 365.730337 251.176637 \n",
       "L 365.797871 254.565894 \n",
       "L 365.865405 248.917133 \n",
       "L 365.932939 257.95515 \n",
       "L 366.000474 253.436142 \n",
       "L 366.068008 253.436142 \n",
       "L 366.135542 260.214654 \n",
       "L 366.405678 244.398125 \n",
       "L 366.473212 263.60391 \n",
       "L 366.540747 255.695646 \n",
       "L 366.608281 241.008869 \n",
       "L 366.743349 257.95515 \n",
       "L 366.810883 246.657629 \n",
       "L 366.945951 256.825398 \n",
       "L 367.08102 250.046885 \n",
       "L 367.216088 261.344406 \n",
       "L 367.41869 256.825398 \n",
       "L 367.486224 246.657629 \n",
       "L 367.553759 250.046885 \n",
       "L 367.621293 260.214654 \n",
       "L 367.688827 244.398125 \n",
       "L 367.756361 252.306389 \n",
       "L 367.823895 245.527877 \n",
       "L 367.958963 257.95515 \n",
       "L 368.026497 255.695646 \n",
       "L 368.2291 242.138621 \n",
       "L 368.296634 255.695646 \n",
       "L 368.364168 247.787381 \n",
       "L 368.431702 254.565894 \n",
       "L 368.499236 244.398125 \n",
       "L 368.634305 261.344406 \n",
       "L 368.701839 257.95515 \n",
       "L 368.769373 259.084902 \n",
       "L 368.904441 250.046885 \n",
       "L 368.971975 248.917133 \n",
       "L 369.039509 243.268373 \n",
       "L 369.107044 252.306389 \n",
       "L 369.174578 247.787381 \n",
       "L 369.242112 248.917133 \n",
       "L 369.309646 244.398125 \n",
       "L 369.37718 248.917133 \n",
       "L 369.444714 241.008869 \n",
       "L 369.512248 251.176637 \n",
       "L 369.647317 241.008869 \n",
       "L 369.714851 242.138621 \n",
       "L 369.782385 250.046885 \n",
       "L 369.849919 246.657629 \n",
       "L 369.984987 252.306389 \n",
       "L 370.052521 245.527877 \n",
       "L 370.120055 252.306389 \n",
       "L 370.18759 246.657629 \n",
       "L 370.255124 253.436142 \n",
       "L 370.322658 251.176637 \n",
       "L 370.390192 252.306389 \n",
       "L 370.52526 244.398125 \n",
       "L 370.592794 245.527877 \n",
       "L 370.660328 250.046885 \n",
       "L 370.727863 242.138621 \n",
       "L 370.795397 243.268373 \n",
       "L 370.930465 263.60391 \n",
       "L 370.997999 239.879117 \n",
       "L 371.065533 255.695646 \n",
       "L 371.133067 244.398125 \n",
       "L 371.200602 247.787381 \n",
       "L 371.268136 250.046885 \n",
       "L 371.33567 261.344406 \n",
       "L 371.470738 241.008869 \n",
       "L 371.538272 248.917133 \n",
       "L 371.605806 246.657629 \n",
       "L 371.67334 235.360108 \n",
       "L 371.740875 243.268373 \n",
       "L 371.808409 237.619613 \n",
       "L 371.875943 250.046885 \n",
       "L 371.943477 235.360108 \n",
       "L 372.011011 260.214654 \n",
       "L 372.078545 248.917133 \n",
       "L 372.213613 243.268373 \n",
       "L 372.281148 252.306389 \n",
       "L 372.348682 236.489861 \n",
       "L 372.416216 248.917133 \n",
       "L 372.48375 246.657629 \n",
       "L 372.551284 244.398125 \n",
       "L 372.618818 250.046885 \n",
       "L 372.821421 235.360108 \n",
       "L 372.888955 248.917133 \n",
       "L 372.956489 243.268373 \n",
       "L 373.024023 250.046885 \n",
       "L 373.091557 237.619613 \n",
       "L 373.159091 251.176637 \n",
       "L 373.226625 248.917133 \n",
       "L 373.29416 252.306389 \n",
       "L 373.496762 245.527877 \n",
       "L 373.564296 246.657629 \n",
       "L 373.63183 254.565894 \n",
       "L 373.699364 238.749365 \n",
       "L 373.766898 248.917133 \n",
       "L 373.834433 245.527877 \n",
       "L 374.037035 259.084902 \n",
       "L 374.172103 237.619613 \n",
       "L 374.239637 253.436142 \n",
       "L 374.307172 234.230356 \n",
       "L 374.44224 259.084902 \n",
       "L 374.577308 243.268373 \n",
       "L 374.644842 250.046885 \n",
       "L 374.712376 246.657629 \n",
       "L 374.77991 251.176637 \n",
       "L 374.847445 243.268373 \n",
       "L 374.982513 251.176637 \n",
       "L 375.050047 241.008869 \n",
       "L 375.117581 242.138621 \n",
       "L 375.185115 246.657629 \n",
       "L 375.252649 245.527877 \n",
       "L 375.320183 246.657629 \n",
       "L 375.387718 254.565894 \n",
       "L 375.59032 239.879117 \n",
       "L 375.725388 254.565894 \n",
       "L 375.860456 241.008869 \n",
       "L 375.927991 250.046885 \n",
       "L 375.995525 236.489861 \n",
       "L 376.063059 241.008869 \n",
       "L 376.130593 245.527877 \n",
       "L 376.198127 242.138621 \n",
       "L 376.333195 248.917133 \n",
       "L 376.40073 236.489861 \n",
       "L 376.468264 239.879117 \n",
       "L 376.535798 246.657629 \n",
       "L 376.603332 237.619613 \n",
       "L 376.670866 244.398125 \n",
       "L 376.7384 238.749365 \n",
       "L 376.873468 251.176637 \n",
       "L 376.941003 250.046885 \n",
       "L 377.008537 236.489861 \n",
       "L 377.076071 238.749365 \n",
       "L 377.143605 246.657629 \n",
       "L 377.211139 235.360108 \n",
       "L 377.278673 237.619613 \n",
       "L 377.346207 243.268373 \n",
       "L 377.413741 242.138621 \n",
       "L 377.481276 241.008869 \n",
       "L 377.54881 259.084902 \n",
       "L 377.616344 254.565894 \n",
       "L 377.683878 243.268373 \n",
       "L 377.751412 255.695646 \n",
       "L 377.818946 236.489861 \n",
       "L 377.88648 241.008869 \n",
       "L 378.156617 247.787381 \n",
       "L 378.291685 239.879117 \n",
       "L 378.359219 243.268373 \n",
       "L 378.561822 233.100604 \n",
       "L 378.69689 255.695646 \n",
       "L 378.764424 252.306389 \n",
       "L 378.831958 239.879117 \n",
       "L 378.899492 243.268373 \n",
       "L 378.967026 253.436142 \n",
       "L 379.034561 247.787381 \n",
       "L 379.169629 241.008869 \n",
       "L 379.237163 244.398125 \n",
       "L 379.439765 237.619613 \n",
       "L 379.574834 243.268373 \n",
       "L 379.642368 235.360108 \n",
       "L 379.709902 243.268373 \n",
       "L 379.777436 237.619613 \n",
       "L 379.84497 239.879117 \n",
       "L 379.912504 242.138621 \n",
       "L 379.980038 233.100604 \n",
       "L 380.115107 245.527877 \n",
       "L 380.182641 239.879117 \n",
       "L 380.250175 253.436142 \n",
       "L 380.317709 245.527877 \n",
       "L 380.385243 244.398125 \n",
       "L 380.452777 235.360108 \n",
       "L 380.520311 252.306389 \n",
       "L 380.587846 251.176637 \n",
       "L 380.722914 237.619613 \n",
       "L 380.857982 252.306389 \n",
       "L 380.925516 243.268373 \n",
       "L 380.99305 245.527877 \n",
       "L 381.060584 243.268373 \n",
       "L 381.195653 228.581596 \n",
       "L 381.330721 248.917133 \n",
       "L 381.398255 243.268373 \n",
       "L 381.465789 244.398125 \n",
       "L 381.600858 251.176637 \n",
       "L 381.80346 238.749365 \n",
       "L 381.870994 243.268373 \n",
       "L 381.938528 238.749365 \n",
       "L 382.006062 250.046885 \n",
       "L 382.073596 247.787381 \n",
       "L 382.141131 242.138621 \n",
       "L 382.208665 246.657629 \n",
       "L 382.276199 235.360108 \n",
       "L 382.343733 238.749365 \n",
       "L 382.546335 245.527877 \n",
       "L 382.613869 236.489861 \n",
       "L 382.748938 247.787381 \n",
       "L 382.884006 239.879117 \n",
       "L 382.95154 246.657629 \n",
       "L 383.019074 230.8411 \n",
       "L 383.086608 234.230356 \n",
       "L 383.154142 244.398125 \n",
       "L 383.356745 234.230356 \n",
       "L 383.424279 237.619613 \n",
       "L 383.491813 246.657629 \n",
       "L 383.559347 239.879117 \n",
       "L 383.626881 241.008869 \n",
       "L 383.694416 239.879117 \n",
       "L 383.76195 229.711348 \n",
       "L 383.964552 247.787381 \n",
       "L 384.032086 244.398125 \n",
       "L 384.09962 245.527877 \n",
       "L 384.167154 231.970852 \n",
       "L 384.302223 252.306389 \n",
       "L 384.369757 242.138621 \n",
       "L 384.437291 257.95515 \n",
       "L 384.639893 229.711348 \n",
       "L 384.707427 239.879117 \n",
       "L 384.774962 229.711348 \n",
       "L 384.842496 241.008869 \n",
       "L 384.91003 227.451844 \n",
       "L 385.045098 248.917133 \n",
       "L 385.112632 230.8411 \n",
       "L 385.180166 238.749365 \n",
       "L 385.247701 242.138621 \n",
       "L 385.382769 235.360108 \n",
       "L 385.517837 245.527877 \n",
       "L 385.585371 236.489861 \n",
       "L 385.652905 250.046885 \n",
       "L 385.855508 224.062588 \n",
       "L 385.990576 250.046885 \n",
       "L 386.05811 241.008869 \n",
       "L 386.193178 255.695646 \n",
       "L 386.260712 233.100604 \n",
       "L 386.328247 246.657629 \n",
       "L 386.395781 245.527877 \n",
       "L 386.530849 231.970852 \n",
       "L 386.598383 241.008869 \n",
       "L 386.733451 228.581596 \n",
       "L 386.800985 246.657629 \n",
       "L 386.86852 241.008869 \n",
       "L 386.936054 230.8411 \n",
       "L 387.003588 234.230356 \n",
       "L 387.071122 236.489861 \n",
       "L 387.138656 243.268373 \n",
       "L 387.273724 229.711348 \n",
       "L 387.341259 230.8411 \n",
       "L 387.408793 242.138621 \n",
       "L 387.476327 236.489861 \n",
       "L 387.611395 231.970852 \n",
       "L 387.678929 243.268373 \n",
       "L 387.813997 235.360108 \n",
       "L 388.0166 246.657629 \n",
       "L 388.084134 245.527877 \n",
       "L 388.151668 251.176637 \n",
       "L 388.35427 234.230356 \n",
       "L 388.489339 231.970852 \n",
       "L 388.556873 238.749365 \n",
       "L 388.624407 237.619613 \n",
       "L 388.691941 241.008869 \n",
       "L 388.827009 234.230356 \n",
       "L 388.894544 243.268373 \n",
       "L 388.962078 237.619613 \n",
       "L 389.029612 244.398125 \n",
       "L 389.097146 241.008869 \n",
       "L 389.16468 238.749365 \n",
       "L 389.232214 242.138621 \n",
       "L 389.299748 241.008869 \n",
       "L 389.367282 230.8411 \n",
       "L 389.434817 248.917133 \n",
       "L 389.502351 236.489861 \n",
       "L 389.569885 239.879117 \n",
       "L 389.772487 228.581596 \n",
       "L 389.840021 241.008869 \n",
       "L 389.907555 239.879117 \n",
       "L 389.97509 238.749365 \n",
       "L 390.042624 228.581596 \n",
       "L 390.110158 244.398125 \n",
       "L 390.177692 241.008869 \n",
       "L 390.245226 241.008869 \n",
       "L 390.31276 248.917133 \n",
       "L 390.515363 221.803084 \n",
       "L 390.785499 243.268373 \n",
       "L 390.853033 226.322092 \n",
       "L 390.920567 236.489861 \n",
       "L 390.988102 225.19234 \n",
       "L 391.055636 239.879117 \n",
       "L 391.12317 236.489861 \n",
       "L 391.190704 244.398125 \n",
       "L 391.258238 234.230356 \n",
       "L 391.325772 243.268373 \n",
       "L 391.393306 241.008869 \n",
       "L 391.46084 229.711348 \n",
       "L 391.528375 237.619613 \n",
       "L 391.595909 227.451844 \n",
       "L 391.663443 229.711348 \n",
       "L 391.730977 247.787381 \n",
       "L 391.798511 241.008869 \n",
       "L 391.933579 237.619613 \n",
       "L 392.001113 241.008869 \n",
       "L 392.068648 222.932836 \n",
       "L 392.136182 227.451844 \n",
       "L 392.203716 241.008869 \n",
       "L 392.27125 236.489861 \n",
       "L 392.338784 236.489861 \n",
       "L 392.473852 245.527877 \n",
       "L 392.608921 230.8411 \n",
       "L 392.676455 236.489861 \n",
       "L 392.879057 227.451844 \n",
       "L 393.08166 241.008869 \n",
       "L 393.284262 225.19234 \n",
       "L 393.351796 235.360108 \n",
       "L 393.41933 215.024571 \n",
       "L 393.486864 238.749365 \n",
       "L 393.554398 234.230356 \n",
       "L 393.621933 228.581596 \n",
       "L 393.689467 239.879117 \n",
       "L 393.757001 226.322092 \n",
       "L 393.824535 227.451844 \n",
       "L 393.892069 241.008869 \n",
       "L 394.027137 226.322092 \n",
       "L 394.094672 246.657629 \n",
       "L 394.162206 242.138621 \n",
       "L 394.297274 234.230356 \n",
       "L 394.432342 235.360108 \n",
       "L 394.499876 250.046885 \n",
       "L 394.634945 231.970852 \n",
       "L 394.702479 234.230356 \n",
       "L 394.770013 230.8411 \n",
       "L 394.837547 237.619613 \n",
       "L 394.905081 231.970852 \n",
       "L 395.040149 239.879117 \n",
       "L 395.107683 237.619613 \n",
       "L 395.175218 239.879117 \n",
       "L 395.310286 227.451844 \n",
       "L 395.37782 239.879117 \n",
       "L 395.445354 233.100604 \n",
       "L 395.580422 226.322092 \n",
       "L 395.783025 242.138621 \n",
       "L 395.850559 237.619613 \n",
       "L 395.918093 238.749365 \n",
       "L 395.985627 238.749365 \n",
       "L 396.120695 233.100604 \n",
       "L 396.18823 243.268373 \n",
       "L 396.255764 234.230356 \n",
       "L 396.323298 236.489861 \n",
       "L 396.390832 238.749365 \n",
       "L 396.660968 222.932836 \n",
       "L 396.728503 234.230356 \n",
       "L 396.796037 230.8411 \n",
       "L 396.863571 231.970852 \n",
       "L 396.931105 238.749365 \n",
       "L 396.998639 225.19234 \n",
       "L 397.066173 234.230356 \n",
       "L 397.133707 229.711348 \n",
       "L 397.268776 242.138621 \n",
       "L 397.471378 222.932836 \n",
       "L 397.67398 237.619613 \n",
       "L 397.809049 225.19234 \n",
       "L 397.944117 226.322092 \n",
       "L 398.011651 248.917133 \n",
       "L 398.079185 238.749365 \n",
       "L 398.214253 224.062588 \n",
       "L 398.281788 234.230356 \n",
       "L 398.349322 226.322092 \n",
       "L 398.416856 239.879117 \n",
       "L 398.48439 222.932836 \n",
       "L 398.551924 229.711348 \n",
       "L 398.619458 241.008869 \n",
       "L 398.686992 227.451844 \n",
       "L 398.754526 231.970852 \n",
       "L 398.822061 243.268373 \n",
       "L 398.889595 228.581596 \n",
       "L 398.957129 238.749365 \n",
       "L 399.024663 221.803084 \n",
       "L 399.092197 224.062588 \n",
       "L 399.159731 238.749365 \n",
       "L 399.227265 234.230356 \n",
       "L 399.362334 230.8411 \n",
       "L 399.429868 230.8411 \n",
       "L 399.63247 245.527877 \n",
       "L 399.902607 225.19234 \n",
       "L 400.037675 230.8411 \n",
       "L 400.172743 223.309414 \n",
       "L 400.307811 237.619613 \n",
       "L 400.375346 230.8411 \n",
       "L 400.44288 228.581596 \n",
       "L 400.510414 231.970852 \n",
       "L 400.577948 229.711348 \n",
       "L 400.645482 220.673332 \n",
       "L 400.713016 227.451844 \n",
       "L 400.78055 222.932836 \n",
       "L 400.848084 245.527877 \n",
       "L 400.915619 229.711348 \n",
       "L 400.983153 238.749365 \n",
       "L 401.050687 227.451844 \n",
       "L 401.185755 237.619613 \n",
       "L 401.388358 218.413827 \n",
       "L 401.523426 235.360108 \n",
       "L 401.59096 230.8411 \n",
       "L 401.658494 242.138621 \n",
       "L 401.726028 236.489861 \n",
       "L 401.793562 244.398125 \n",
       "L 401.861096 226.322092 \n",
       "L 401.928631 231.970852 \n",
       "L 401.996165 227.451844 \n",
       "L 402.063699 231.970852 \n",
       "L 402.131233 222.932836 \n",
       "L 402.198767 231.970852 \n",
       "L 402.266301 226.322092 \n",
       "L 402.333835 225.19234 \n",
       "L 402.401369 235.360108 \n",
       "L 402.468904 231.970852 \n",
       "L 402.536438 235.360108 \n",
       "L 402.671506 222.932836 \n",
       "L 402.73904 225.19234 \n",
       "L 402.806574 241.008869 \n",
       "L 402.874108 225.19234 \n",
       "L 402.941642 228.581596 \n",
       "L 403.009177 231.970852 \n",
       "L 403.144245 221.803084 \n",
       "L 403.346847 225.19234 \n",
       "L 403.414381 237.619613 \n",
       "L 403.54945 227.451844 \n",
       "L 403.616984 227.451844 \n",
       "L 403.684518 229.711348 \n",
       "L 403.752052 221.803084 \n",
       "L 403.819586 228.581596 \n",
       "L 403.88712 220.673332 \n",
       "L 403.954654 222.932836 \n",
       "L 404.224791 235.360108 \n",
       "L 404.292325 231.970852 \n",
       "L 404.494927 222.932836 \n",
       "L 404.629996 234.230356 \n",
       "L 404.69753 231.970852 \n",
       "L 404.765064 224.062588 \n",
       "L 404.832598 243.268373 \n",
       "L 404.900132 228.581596 \n",
       "L 404.967666 229.711348 \n",
       "L 405.035201 224.062588 \n",
       "L 405.102735 234.230356 \n",
       "L 405.237803 212.765067 \n",
       "L 405.305337 236.489861 \n",
       "L 405.372871 222.932836 \n",
       "L 405.440405 224.062588 \n",
       "L 405.507939 220.673332 \n",
       "L 405.575474 243.268373 \n",
       "L 405.643008 222.932836 \n",
       "L 405.778076 251.176637 \n",
       "L 405.84561 220.673332 \n",
       "L 405.913144 229.711348 \n",
       "L 405.980678 230.8411 \n",
       "L 406.115747 224.062588 \n",
       "L 406.183281 234.230356 \n",
       "L 406.250815 224.062588 \n",
       "L 406.318349 239.879117 \n",
       "L 406.520951 216.154323 \n",
       "L 406.588485 237.619613 \n",
       "L 406.65602 227.451844 \n",
       "L 406.723554 221.803084 \n",
       "L 406.791088 224.062588 \n",
       "L 406.858622 227.451844 \n",
       "L 406.99369 221.803084 \n",
       "L 407.128759 233.100604 \n",
       "L 407.196293 230.8411 \n",
       "L 407.263827 218.413827 \n",
       "L 407.466429 248.917133 \n",
       "L 407.533963 228.581596 \n",
       "L 407.601497 233.100604 \n",
       "L 407.669032 237.619613 \n",
       "L 407.736566 222.932836 \n",
       "L 407.8041 227.451844 \n",
       "L 407.871634 234.230356 \n",
       "L 407.939168 219.54358 \n",
       "L 408.006702 228.581596 \n",
       "L 408.074236 236.489861 \n",
       "L 408.14177 218.413827 \n",
       "L 408.209305 236.489861 \n",
       "L 408.276839 225.19234 \n",
       "L 408.344373 227.451844 \n",
       "L 408.411907 224.062588 \n",
       "L 408.546975 237.619613 \n",
       "L 408.749578 221.803084 \n",
       "L 408.817112 233.100604 \n",
       "L 409.019714 220.673332 \n",
       "L 409.222317 234.230356 \n",
       "L 409.357385 226.322092 \n",
       "L 409.424919 231.970852 \n",
       "L 409.627521 213.894819 \n",
       "L 409.695055 228.581596 \n",
       "L 409.76259 227.451844 \n",
       "L 409.830124 219.54358 \n",
       "L 409.965192 230.8411 \n",
       "L 410.032726 221.803084 \n",
       "L 410.10026 230.8411 \n",
       "L 410.167794 229.711348 \n",
       "L 410.302863 221.803084 \n",
       "L 410.370397 229.711348 \n",
       "L 410.437931 228.581596 \n",
       "L 410.572999 218.413827 \n",
       "L 410.708067 230.8411 \n",
       "L 410.775602 242.138621 \n",
       "L 410.843136 220.673332 \n",
       "L 410.91067 229.711348 \n",
       "L 410.978204 237.619613 \n",
       "L 411.045738 226.322092 \n",
       "L 411.113272 230.8411 \n",
       "L 411.180806 234.230356 \n",
       "L 411.24834 230.8411 \n",
       "L 411.315875 217.284075 \n",
       "L 411.383409 228.581596 \n",
       "L 411.450943 216.154323 \n",
       "L 411.518477 222.932836 \n",
       "L 411.586011 224.062588 \n",
       "L 411.653545 221.803084 \n",
       "L 411.721079 230.8411 \n",
       "L 411.788613 224.062588 \n",
       "L 411.856148 231.970852 \n",
       "L 411.991216 215.024571 \n",
       "L 412.05875 226.322092 \n",
       "L 412.126284 211.635315 \n",
       "L 412.261352 227.451844 \n",
       "L 412.328887 226.322092 \n",
       "L 412.396421 228.581596 \n",
       "L 412.463955 241.008869 \n",
       "L 412.666557 212.765067 \n",
       "L 412.801625 235.360108 \n",
       "L 412.86916 216.154323 \n",
       "L 412.936694 225.19234 \n",
       "L 413.004228 227.451844 \n",
       "L 413.071762 221.803084 \n",
       "L 413.139296 224.062588 \n",
       "L 413.20683 222.932836 \n",
       "L 413.274364 224.062588 \n",
       "L 413.341898 221.803084 \n",
       "L 413.409433 222.932836 \n",
       "L 413.476967 225.19234 \n",
       "L 413.544501 233.100604 \n",
       "L 413.747103 219.54358 \n",
       "L 413.814637 228.581596 \n",
       "L 413.882172 218.413827 \n",
       "L 414.01724 235.360108 \n",
       "L 414.152308 221.803084 \n",
       "L 414.219842 234.230356 \n",
       "L 414.287376 226.322092 \n",
       "L 414.35491 226.322092 \n",
       "L 414.422445 207.116307 \n",
       "L 414.557513 222.932836 \n",
       "L 414.692581 228.581596 \n",
       "L 414.760115 213.894819 \n",
       "L 414.827649 222.932836 \n",
       "L 414.895183 231.970852 \n",
       "L 414.962718 230.8411 \n",
       "L 415.030252 212.765067 \n",
       "L 415.097786 227.451844 \n",
       "L 415.16532 225.19234 \n",
       "L 415.232854 220.673332 \n",
       "L 415.300388 231.970852 \n",
       "L 415.367922 218.413827 \n",
       "L 415.435456 228.581596 \n",
       "L 415.502991 218.413827 \n",
       "L 415.570525 234.230356 \n",
       "L 415.705593 221.803084 \n",
       "L 415.773127 233.100604 \n",
       "L 415.840661 218.413827 \n",
       "L 415.908195 230.8411 \n",
       "L 415.97573 228.581596 \n",
       "L 416.043264 219.54358 \n",
       "L 416.245866 234.230356 \n",
       "L 416.3134 218.413827 \n",
       "L 416.380934 220.673332 \n",
       "L 416.583537 228.581596 \n",
       "L 416.651071 228.581596 \n",
       "L 416.786139 215.024571 \n",
       "L 416.853673 225.19234 \n",
       "L 416.921207 224.062588 \n",
       "L 416.988741 220.673332 \n",
       "L 417.056276 234.230356 \n",
       "L 417.12381 226.322092 \n",
       "L 417.191344 213.894819 \n",
       "L 417.258878 231.970852 \n",
       "L 417.326412 225.19234 \n",
       "L 417.393946 219.54358 \n",
       "L 417.596549 236.489861 \n",
       "L 417.731617 221.803084 \n",
       "L 417.799151 234.230356 \n",
       "L 417.866685 220.673332 \n",
       "L 417.934219 228.581596 \n",
       "L 418.001753 219.54358 \n",
       "L 418.069288 225.19234 \n",
       "L 418.27189 221.803084 \n",
       "L 418.339424 224.062588 \n",
       "L 418.406958 219.54358 \n",
       "L 418.542026 225.19234 \n",
       "L 418.609561 212.765067 \n",
       "L 418.744629 229.711348 \n",
       "L 418.879697 224.062588 \n",
       "L 418.947231 226.322092 \n",
       "L 419.014765 218.413827 \n",
       "L 419.149834 228.581596 \n",
       "L 419.217368 225.19234 \n",
       "L 419.284902 230.8411 \n",
       "L 419.41997 210.505563 \n",
       "L 419.487504 217.284075 \n",
       "L 419.555038 226.322092 \n",
       "L 419.622573 217.284075 \n",
       "L 419.690107 227.451844 \n",
       "L 419.757641 224.062588 \n",
       "L 419.892709 233.100604 \n",
       "L 420.095311 213.894819 \n",
       "L 420.297914 226.322092 \n",
       "L 420.365448 212.765067 \n",
       "L 420.56805 229.711348 \n",
       "L 420.703119 224.062588 \n",
       "L 420.770653 234.230356 \n",
       "L 420.838187 224.062588 \n",
       "L 420.905721 228.581596 \n",
       "L 420.973255 230.8411 \n",
       "L 421.040789 218.413827 \n",
       "L 421.108323 221.803084 \n",
       "L 421.175858 215.024571 \n",
       "L 421.243392 230.8411 \n",
       "L 421.445994 204.856803 \n",
       "L 421.513528 225.19234 \n",
       "L 421.581062 219.54358 \n",
       "L 421.648596 217.284075 \n",
       "L 421.783665 227.451844 \n",
       "L 421.851199 224.062588 \n",
       "L 421.918733 231.970852 \n",
       "L 421.986267 218.413827 \n",
       "L 422.053801 226.322092 \n",
       "L 422.121335 226.322092 \n",
       "L 422.256404 212.765067 \n",
       "L 422.459006 236.489861 \n",
       "L 422.52654 215.024571 \n",
       "L 422.594074 228.581596 \n",
       "L 422.729142 219.54358 \n",
       "L 422.796677 225.19234 \n",
       "L 422.864211 218.413827 \n",
       "L 422.931745 221.803084 \n",
       "L 423.066813 216.154323 \n",
       "L 423.134347 229.711348 \n",
       "L 423.33695 216.154323 \n",
       "L 423.404484 215.024571 \n",
       "L 423.539552 228.581596 \n",
       "L 423.607086 226.322092 \n",
       "L 423.67462 211.635315 \n",
       "L 423.742154 220.673332 \n",
       "L 423.809689 227.451844 \n",
       "L 423.944757 210.505563 \n",
       "L 424.012291 217.284075 \n",
       "L 424.079825 215.024571 \n",
       "L 424.147359 207.116307 \n",
       "L 424.282427 231.970852 \n",
       "L 424.349962 217.284075 \n",
       "L 424.417496 227.451844 \n",
       "L 424.48503 217.284075 \n",
       "L 424.552564 218.413827 \n",
       "L 424.687632 224.062588 \n",
       "L 424.755166 221.803084 \n",
       "L 424.822701 224.062588 \n",
       "L 424.890235 217.284075 \n",
       "L 425.092837 230.8411 \n",
       "L 425.227905 210.505563 \n",
       "L 425.362974 230.8411 \n",
       "L 425.430508 211.635315 \n",
       "L 425.498042 213.894819 \n",
       "L 425.565576 212.765067 \n",
       "L 425.63311 226.322092 \n",
       "L 425.700644 209.375811 \n",
       "L 425.768178 219.54358 \n",
       "L 425.835712 215.024571 \n",
       "L 425.903247 219.54358 \n",
       "L 425.970781 208.246059 \n",
       "L 426.038315 224.062588 \n",
       "L 426.105849 219.54358 \n",
       "L 426.173383 224.062588 \n",
       "L 426.240917 207.116307 \n",
       "L 426.375985 221.803084 \n",
       "L 426.44352 209.375811 \n",
       "L 426.578588 226.322092 \n",
       "L 426.646122 213.894819 \n",
       "L 426.713656 218.413827 \n",
       "L 426.78119 224.062588 \n",
       "L 426.848724 217.284075 \n",
       "L 426.916259 228.581596 \n",
       "L 426.983793 222.932836 \n",
       "L 427.051327 228.581596 \n",
       "L 427.186395 221.803084 \n",
       "L 427.253929 229.711348 \n",
       "L 427.388997 220.673332 \n",
       "L 427.456532 227.451844 \n",
       "L 427.524066 216.154323 \n",
       "L 427.5916 218.413827 \n",
       "L 427.726668 231.970852 \n",
       "L 427.92927 215.024571 \n",
       "L 427.996805 218.413827 \n",
       "L 428.064339 216.154323 \n",
       "L 428.131873 217.284075 \n",
       "L 428.199407 230.8411 \n",
       "L 428.266941 225.19234 \n",
       "L 428.334475 216.154323 \n",
       "L 428.402009 217.284075 \n",
       "L 428.469544 227.451844 \n",
       "L 428.537078 226.322092 \n",
       "L 428.604612 222.932836 \n",
       "L 428.672146 213.894819 \n",
       "L 428.73968 215.024571 \n",
       "L 428.807214 220.673332 \n",
       "L 428.874748 218.413827 \n",
       "L 428.942282 221.803084 \n",
       "L 429.009817 215.024571 \n",
       "L 429.077351 222.932836 \n",
       "L 429.144885 209.375811 \n",
       "L 429.212419 225.19234 \n",
       "L 429.279953 217.284075 \n",
       "L 429.347487 213.894819 \n",
       "L 429.415021 215.024571 \n",
       "L 429.482555 217.284075 \n",
       "L 429.55009 226.322092 \n",
       "L 429.617624 222.932836 \n",
       "L 429.685158 216.154323 \n",
       "L 429.752692 226.322092 \n",
       "L 429.820226 222.932836 \n",
       "L 429.88776 218.413827 \n",
       "L 429.955294 220.673332 \n",
       "L 430.022828 221.803084 \n",
       "L 430.090363 216.154323 \n",
       "L 430.157897 220.673332 \n",
       "L 430.225431 216.154323 \n",
       "L 430.292965 226.322092 \n",
       "L 430.428033 211.635315 \n",
       "L 430.495567 220.673332 \n",
       "L 430.563102 218.413827 \n",
       "L 430.630636 222.932836 \n",
       "L 430.69817 217.284075 \n",
       "L 430.765704 220.673332 \n",
       "L 430.833238 220.673332 \n",
       "L 430.968306 216.154323 \n",
       "L 431.03584 220.673332 \n",
       "L 431.170909 210.505563 \n",
       "L 431.305977 228.581596 \n",
       "L 431.373511 213.894819 \n",
       "L 431.441045 229.711348 \n",
       "L 431.508579 222.932836 \n",
       "L 431.576113 213.894819 \n",
       "L 431.643648 229.711348 \n",
       "L 431.711182 217.284075 \n",
       "L 431.778716 226.322092 \n",
       "L 431.84625 223.309414 \n",
       "\" clip-path=\"url(#p1ce374a4b4)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 265.63125 299.078125 \n",
       "L 265.63125 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 439.76125 299.078125 \n",
       "L 439.76125 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 265.63125 299.078125 \n",
       "L 439.76125 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 265.63125 189.718125 \n",
       "L 439.76125 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_48\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(323.703125 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_4\">\n",
       "    <g id=\"patch_25\">\n",
       "     <path d=\"M 313.99625 227.630625 \n",
       "L 391.39625 227.630625 \n",
       "Q 393.39625 227.630625 393.39625 225.630625 \n",
       "L 393.39625 196.718125 \n",
       "Q 393.39625 194.718125 391.39625 194.718125 \n",
       "L 313.99625 194.718125 \n",
       "Q 311.99625 194.718125 311.99625 196.718125 \n",
       "L 311.99625 225.630625 \n",
       "Q 311.99625 227.630625 313.99625 227.630625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_45\">\n",
       "     <path d=\"M 315.99625 202.816562 \n",
       "L 325.99625 202.816562 \n",
       "L 335.99625 202.816562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_49\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(343.99625 206.316562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_46\">\n",
       "     <path d=\"M 315.99625 217.772812 \n",
       "L 325.99625 217.772812 \n",
       "L 335.99625 217.772812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_50\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(343.99625 221.272812) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p0e6f75976b\">\n",
       "   <rect x=\"43.78125\" y=\"22.318125\" width=\"174.13\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pf0a217bc11\">\n",
       "   <rect x=\"265.63125\" y=\"22.318125\" width=\"174.13\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p202136b4d5\">\n",
       "   <rect x=\"43.78125\" y=\"189.718125\" width=\"174.13\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p1ce374a4b4\">\n",
       "   <rect x=\"265.63125\" y=\"189.718125\" width=\"174.13\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils import data \n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    DemoCallback(), \n",
    "]\n",
    "\n",
    "\n",
    "net = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params= net.parameters(), lr= 0.01)\n",
    "\n",
    "trainer= Trainer(\n",
    "    device= 'auto', \n",
    "    train_dataloader= data.DataLoader(train_dataset, batch_size= 128, shuffle= True),\n",
    "    val_dataloader= data.DataLoader(test_dataset, batch_size= 128, shuffle= False), \n",
    "    model= net, \n",
    "    loss_fn= loss_fn, \n",
    "    optimizer= opt, \n",
    "    is_tqdm= False, \n",
    "    callbacks= callbacks,\n",
    ")\n",
    "\n",
    "trainer.train(epochs= 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.2. <a id='toc8_8_2_'></a>[自己探索](#toc0_)\n",
    "#### 8.8.2.1. <a id='toc8_8_2_1_'></a>[lr的影响](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"453.23375pt\" height=\"336.634375pt\" viewBox=\"0 0 453.23375 336.634375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-28T11:02:44.868057</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 336.634375 \n",
       "L 453.23375 336.634375 \n",
       "L 453.23375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 131.678125 \n",
       "L 221.07875 131.678125 \n",
       "L 221.07875 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m4a0ac97568\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"57.913523\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(54.732273 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"96.762386\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(93.581136 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"135.61125\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(132.43 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"174.460114\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(171.278864 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"213.308977\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(210.127727 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(120.383125 159.954687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mf841f9edc8\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"113.282761\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1.55 -->\n",
       "      <g transform=\"translate(20.878125 117.08198) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"89.735233\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1.60 -->\n",
       "      <g transform=\"translate(20.878125 93.534451) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"66.187705\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1.65 -->\n",
       "      <g transform=\"translate(20.878125 69.986923) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"42.640177\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1.70 -->\n",
       "      <g transform=\"translate(20.878125 46.439395) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 86.655937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_10\">\n",
       "    <path d=\"M 57.913523 78.329912 \n",
       "L 96.762386 113.159399 \n",
       "L 135.61125 120.551971 \n",
       "L 174.460114 123.735193 \n",
       "L 213.308977 126.707216 \n",
       "\" clip-path=\"url(#p5ea1853c1b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_11\">\n",
       "    <path d=\"M 57.913523 27.289034 \n",
       "L 96.762386 97.803383 \n",
       "L 135.61125 115.609494 \n",
       "L 174.460114 121.251244 \n",
       "L 213.308977 125.193175 \n",
       "\" clip-path=\"url(#p5ea1853c1b)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 131.678125 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 221.07875 131.678125 \n",
       "L 221.07875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 131.678125 \n",
       "L 221.07875 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 221.07875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_12\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(105.30375 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 134.488125 60.230625 \n",
       "L 214.07875 60.230625 \n",
       "Q 216.07875 60.230625 216.07875 58.230625 \n",
       "L 216.07875 29.318125 \n",
       "Q 216.07875 27.318125 214.07875 27.318125 \n",
       "L 134.488125 27.318125 \n",
       "Q 132.488125 27.318125 132.488125 29.318125 \n",
       "L 132.488125 58.230625 \n",
       "Q 132.488125 60.230625 134.488125 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_12\">\n",
       "     <path d=\"M 136.488125 35.416562 \n",
       "L 146.488125 35.416562 \n",
       "L 156.488125 35.416562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(164.488125 38.916562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_13\">\n",
       "     <path d=\"M 136.488125 50.372812 \n",
       "L 146.488125 50.372812 \n",
       "L 156.488125 50.372812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(164.488125 53.872812) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 275.09875 131.678125 \n",
       "L 446.03375 131.678125 \n",
       "L 446.03375 22.318125 \n",
       "L 275.09875 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"282.868523\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(279.687273 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"321.717386\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(318.536136 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"360.56625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(357.385 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"399.415114\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(396.233864 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"438.263977\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(435.082727 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(345.338125 159.954687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"109.244159\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 0.80 -->\n",
       "      <g transform=\"translate(245.833125 113.043378) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"80.909565\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.85 -->\n",
       "      <g transform=\"translate(245.833125 84.708783) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"52.57497\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.90 -->\n",
       "      <g transform=\"translate(245.833125 56.374189) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"24.240375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.95 -->\n",
       "      <g transform=\"translate(245.833125 28.039594) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_25\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(239.753437 85.560625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 282.868523 82.209728 \n",
       "L 321.717386 40.01046 \n",
       "L 360.56625 33.39756 \n",
       "L 399.415114 30.315276 \n",
       "L 438.263977 27.289034 \n",
       "\" clip-path=\"url(#pcd6f6a5529)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_24\">\n",
       "    <path d=\"M 282.868523 126.707216 \n",
       "L 321.717386 57.230065 \n",
       "L 360.56625 38.315791 \n",
       "L 399.415114 32.428484 \n",
       "L 438.263977 28.051549 \n",
       "\" clip-path=\"url(#pcd6f6a5529)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 275.09875 131.678125 \n",
       "L 275.09875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 446.03375 131.678125 \n",
       "L 446.03375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 275.09875 131.678125 \n",
       "L 446.03375 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 275.09875 22.318125 \n",
       "L 446.03375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_26\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(331.573125 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_2\">\n",
       "    <g id=\"patch_13\">\n",
       "     <path d=\"M 361.63375 126.678125 \n",
       "L 439.03375 126.678125 \n",
       "Q 441.03375 126.678125 441.03375 124.678125 \n",
       "L 441.03375 95.765625 \n",
       "Q 441.03375 93.765625 439.03375 93.765625 \n",
       "L 361.63375 93.765625 \n",
       "Q 359.63375 93.765625 359.63375 95.765625 \n",
       "L 359.63375 124.678125 \n",
       "Q 359.63375 126.678125 361.63375 126.678125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_25\">\n",
       "     <path d=\"M 363.63375 101.864062 \n",
       "L 373.63375 101.864062 \n",
       "L 383.63375 101.864062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_27\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(391.63375 105.364062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_26\">\n",
       "     <path d=\"M 363.63375 116.820312 \n",
       "L 373.63375 116.820312 \n",
       "L 383.63375 116.820312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_28\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(391.63375 120.320312) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 50.14375 299.078125 \n",
       "L 221.07875 299.078125 \n",
       "L 221.07875 189.718125 \n",
       "L 50.14375 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_5\">\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"57.913523\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(54.732273 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"124.208512\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(111.483512 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"190.503501\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(177.778501 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_32\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(124.795625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"277.757097\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_33\">\n",
       "      <!-- 1.6 -->\n",
       "      <g transform=\"translate(27.240625 281.556316) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"254.113252\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_34\">\n",
       "      <!-- 1.8 -->\n",
       "      <g transform=\"translate(27.240625 257.91247) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"230.469406\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_35\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(27.240625 234.268624) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"50.14375\" y=\"206.82556\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_36\">\n",
       "      <!-- 2.2 -->\n",
       "      <g transform=\"translate(27.240625 210.624778) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_37\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(21.160938 254.055937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_34\">\n",
       "    <path d=\"M 57.913523 277.476126 \n",
       "L 58.046113 270.616624 \n",
       "L 58.112408 272.243416 \n",
       "L 58.178703 270.071556 \n",
       "L 58.244998 272.966505 \n",
       "L 58.311293 280.778781 \n",
       "L 58.510178 266.498981 \n",
       "L 58.709063 276.340272 \n",
       "L 58.907948 267.200099 \n",
       "L 58.974243 268.691755 \n",
       "L 59.106833 273.876718 \n",
       "L 59.173128 272.439827 \n",
       "L 59.239423 268.360432 \n",
       "L 59.305717 270.479064 \n",
       "L 59.438307 276.425731 \n",
       "L 59.504602 269.969862 \n",
       "L 59.570897 279.092968 \n",
       "L 59.637192 276.451831 \n",
       "L 59.703487 273.503723 \n",
       "L 59.769782 274.58742 \n",
       "L 59.836077 261.387892 \n",
       "L 59.902372 265.176721 \n",
       "L 60.034962 273.452101 \n",
       "L 60.101257 265.888874 \n",
       "L 60.167552 269.719164 \n",
       "L 60.300142 277.15674 \n",
       "L 60.432732 270.097655 \n",
       "L 60.499027 278.50541 \n",
       "L 60.565322 274.070368 \n",
       "L 60.697912 282.077083 \n",
       "L 60.830502 273.447338 \n",
       "L 60.896797 278.254022 \n",
       "L 61.029387 272.615058 \n",
       "L 61.161977 280.649578 \n",
       "L 61.294567 271.396155 \n",
       "L 61.360862 282.777708 \n",
       "L 61.427157 274.815442 \n",
       "L 61.559747 281.102267 \n",
       "L 61.626042 280.907941 \n",
       "L 61.692337 281.799299 \n",
       "L 61.758632 274.878197 \n",
       "L 61.824927 277.404506 \n",
       "L 61.891222 283.580985 \n",
       "L 61.957517 272.779296 \n",
       "L 62.023812 277.702541 \n",
       "L 62.288992 281.385872 \n",
       "L 62.355287 279.758079 \n",
       "L 62.487877 284.636002 \n",
       "L 62.554172 276.228657 \n",
       "L 62.620467 279.389834 \n",
       "L 62.753057 281.107003 \n",
       "L 62.951942 267.757499 \n",
       "L 63.018237 270.870774 \n",
       "L 63.084532 274.63394 \n",
       "L 63.150827 286.217528 \n",
       "L 63.217122 283.955375 \n",
       "L 63.416007 278.153089 \n",
       "L 63.548597 288.092903 \n",
       "L 63.747482 274.285735 \n",
       "L 63.946367 283.783006 \n",
       "L 64.145252 276.024353 \n",
       "L 64.277842 282.073348 \n",
       "L 64.344137 279.020601 \n",
       "L 64.410432 283.226973 \n",
       "L 64.476727 279.594912 \n",
       "L 64.675612 285.080941 \n",
       "L 64.741907 279.910141 \n",
       "L 64.808202 286.046709 \n",
       "L 64.874497 284.682748 \n",
       "L 64.940792 281.990496 \n",
       "L 65.007087 284.086424 \n",
       "L 65.073382 270.336571 \n",
       "L 65.139677 278.159205 \n",
       "L 65.272267 279.792818 \n",
       "L 65.338562 275.80955 \n",
       "L 65.537446 284.789079 \n",
       "L 65.670036 278.303488 \n",
       "L 65.935216 292.226739 \n",
       "L 66.266691 278.886142 \n",
       "L 66.399281 289.104613 \n",
       "L 66.465576 286.709974 \n",
       "L 66.531871 278.88321 \n",
       "L 66.730756 289.924703 \n",
       "L 66.863346 288.503089 \n",
       "L 66.929641 291.444587 \n",
       "L 66.995936 281.349484 \n",
       "L 67.062231 289.509049 \n",
       "L 67.128526 289.018633 \n",
       "L 67.194821 270.856132 \n",
       "L 67.261116 289.051385 \n",
       "L 67.327411 288.044283 \n",
       "L 67.393706 285.341785 \n",
       "L 67.460001 286.936643 \n",
       "L 67.526296 288.105643 \n",
       "L 67.658886 293.944176 \n",
       "L 67.725181 293.309534 \n",
       "L 67.791476 282.357206 \n",
       "L 67.857771 288.446253 \n",
       "L 67.924066 291.232279 \n",
       "L 68.122951 282.045994 \n",
       "L 68.189246 274.699683 \n",
       "L 68.255541 276.218707 \n",
       "L 68.388131 287.958951 \n",
       "L 68.454426 286.020919 \n",
       "L 68.653311 280.864099 \n",
       "L 68.785901 288.573032 \n",
       "L 68.984786 276.587756 \n",
       "L 69.183671 285.058308 \n",
       "L 69.382556 278.779374 \n",
       "L 69.515146 284.464464 \n",
       "L 69.581441 280.894074 \n",
       "L 69.647736 286.733579 \n",
       "L 69.714031 280.62772 \n",
       "L 69.780326 284.414731 \n",
       "L 69.846621 282.862716 \n",
       "L 69.912916 287.188707 \n",
       "L 69.979211 281.605959 \n",
       "L 70.045506 287.374606 \n",
       "L 70.111801 287.226124 \n",
       "L 70.178096 283.734611 \n",
       "L 70.244391 287.346561 \n",
       "L 70.310686 272.998552 \n",
       "L 70.376981 279.473446 \n",
       "L 70.509571 284.119613 \n",
       "L 70.575866 279.232741 \n",
       "L 70.642161 281.723113 \n",
       "L 70.774751 286.854609 \n",
       "L 70.907341 281.775313 \n",
       "L 71.172521 291.941289 \n",
       "L 71.238816 290.544309 \n",
       "L 71.305111 286.398607 \n",
       "L 71.371406 289.283311 \n",
       "L 71.503996 281.13912 \n",
       "L 71.636585 288.795473 \n",
       "L 71.70288 287.951214 \n",
       "L 71.769175 280.438636 \n",
       "L 71.96806 291.625187 \n",
       "L 72.034355 291.756926 \n",
       "L 72.10065 290.298784 \n",
       "L 72.166945 292.311803 \n",
       "L 72.23324 285.054912 \n",
       "L 72.299535 292.037627 \n",
       "L 72.36583 290.931918 \n",
       "L 72.432125 276.202529 \n",
       "L 72.49842 289.397871 \n",
       "L 72.564715 288.542211 \n",
       "L 72.697305 287.027429 \n",
       "L 72.89619 293.974673 \n",
       "L 72.962485 293.605666 \n",
       "L 73.02878 283.46844 \n",
       "L 73.095075 289.424929 \n",
       "L 73.16137 292.501845 \n",
       "L 73.227665 291.260366 \n",
       "L 73.29396 289.21303 \n",
       "L 73.42655 276.266073 \n",
       "L 73.492845 278.154301 \n",
       "L 73.625435 288.946731 \n",
       "L 73.890615 282.868818 \n",
       "L 74.023205 288.918461 \n",
       "L 74.22209 276.528566 \n",
       "L 74.420975 285.715992 \n",
       "L 74.48727 280.84241 \n",
       "L 74.553565 282.424668 \n",
       "L 74.61986 280.095053 \n",
       "L 74.75245 284.762542 \n",
       "L 74.818745 282.409124 \n",
       "L 74.88504 288.667116 \n",
       "L 74.951335 284.888363 \n",
       "L 75.01763 286.999074 \n",
       "L 75.083925 284.303665 \n",
       "L 75.15022 287.312569 \n",
       "L 75.216515 282.922483 \n",
       "L 75.349105 288.29708 \n",
       "L 75.4154 284.047838 \n",
       "L 75.481695 287.275928 \n",
       "L 75.54799 273.633237 \n",
       "L 75.614285 279.650283 \n",
       "L 75.68058 282.539919 \n",
       "L 75.746875 282.127605 \n",
       "L 75.94576 284.806187 \n",
       "L 76.012055 285.99138 \n",
       "L 76.144645 282.287954 \n",
       "L 76.34353 291.947476 \n",
       "L 76.409825 292.061881 \n",
       "L 76.542415 287.992266 \n",
       "L 76.60871 291.184447 \n",
       "L 76.7413 282.862194 \n",
       "L 76.87389 290.565392 \n",
       "L 76.940185 288.847053 \n",
       "L 77.00648 280.199143 \n",
       "L 77.205365 291.520759 \n",
       "L 77.27166 291.571211 \n",
       "L 77.337955 289.910766 \n",
       "L 77.40425 292.852179 \n",
       "L 77.470545 286.18386 \n",
       "L 77.53684 292.985723 \n",
       "L 77.603135 291.485809 \n",
       "L 77.669429 278.54697 \n",
       "L 77.735724 290.380874 \n",
       "L 77.802019 288.94893 \n",
       "L 77.868314 286.150615 \n",
       "L 77.934609 287.650839 \n",
       "L 78.133494 293.998617 \n",
       "L 78.199789 293.750681 \n",
       "L 78.266084 285.331286 \n",
       "L 78.332379 290.821078 \n",
       "L 78.398674 292.227035 \n",
       "L 78.531264 290.34319 \n",
       "L 78.663854 277.595351 \n",
       "L 78.730149 279.66828 \n",
       "L 78.796444 293.28046 \n",
       "L 78.862739 289.550088 \n",
       "L 79.127919 283.406361 \n",
       "L 79.260509 288.587364 \n",
       "L 79.459394 278.517036 \n",
       "L 79.525689 284.394184 \n",
       "L 79.591984 282.890591 \n",
       "L 79.658279 286.078277 \n",
       "L 79.724574 282.208526 \n",
       "L 79.790869 283.954769 \n",
       "L 79.857164 281.2669 \n",
       "L 79.989754 285.188667 \n",
       "L 80.056049 283.185399 \n",
       "L 80.122344 289.248135 \n",
       "L 80.188639 285.197545 \n",
       "L 80.254934 288.232521 \n",
       "L 80.321229 286.094639 \n",
       "L 80.387524 288.222261 \n",
       "L 80.453819 283.392042 \n",
       "L 80.586409 289.207632 \n",
       "L 80.652704 284.842237 \n",
       "L 80.718999 288.06996 \n",
       "L 80.785294 275.685378 \n",
       "L 80.851589 280.299272 \n",
       "L 80.917884 283.576377 \n",
       "L 80.984179 283.349919 \n",
       "L 81.249359 287.460783 \n",
       "L 81.381949 282.907728 \n",
       "L 81.580834 292.69348 \n",
       "L 81.713424 290.747415 \n",
       "L 81.779719 288.436303 \n",
       "L 81.846014 291.19668 \n",
       "L 81.978604 283.671757 \n",
       "L 82.111194 291.042674 \n",
       "L 82.177489 288.989912 \n",
       "L 82.243784 281.399725 \n",
       "L 82.442669 292.149665 \n",
       "L 82.508964 291.955438 \n",
       "L 82.575259 290.88978 \n",
       "L 82.641554 293.075874 \n",
       "L 82.707849 286.980598 \n",
       "L 82.774144 293.275006 \n",
       "L 82.840439 291.788551 \n",
       "L 82.906734 279.624056 \n",
       "L 82.973029 290.185167 \n",
       "L 83.039324 289.44838 \n",
       "L 83.105619 287.874986 \n",
       "L 83.171914 288.293811 \n",
       "L 83.370799 294.107216 \n",
       "L 83.437094 293.422488 \n",
       "L 83.503389 285.219558 \n",
       "L 83.635979 292.715944 \n",
       "L 83.768568 290.954565 \n",
       "L 83.901158 278.033103 \n",
       "L 83.967453 281.938804 \n",
       "L 84.033748 292.976788 \n",
       "L 84.033748 292.976788 \n",
       "\" clip-path=\"url(#p3c3172a232)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_35\">\n",
       "    <path d=\"M 57.913523 194.689034 \n",
       "L 58.244998 195.559069 \n",
       "L 58.311293 195.575614 \n",
       "L 58.775358 197.064973 \n",
       "L 58.841653 196.815952 \n",
       "L 58.974243 198.078698 \n",
       "L 59.106833 199.870827 \n",
       "L 59.173128 199.812257 \n",
       "L 59.239423 201.34364 \n",
       "L 59.305717 199.493731 \n",
       "L 59.372012 199.766455 \n",
       "L 59.438307 200.146144 \n",
       "L 59.570897 206.315591 \n",
       "L 59.637192 206.152368 \n",
       "L 59.703487 206.061074 \n",
       "L 59.769782 211.1914 \n",
       "L 59.902372 204.58268 \n",
       "L 59.968667 204.935875 \n",
       "L 60.167552 210.154647 \n",
       "L 60.366437 213.15797 \n",
       "L 60.432732 211.634479 \n",
       "L 60.631617 217.994404 \n",
       "L 60.697912 216.901843 \n",
       "L 60.764207 221.636358 \n",
       "L 60.830502 220.418568 \n",
       "L 60.896797 221.853444 \n",
       "L 60.963092 228.940503 \n",
       "L 61.029387 223.318055 \n",
       "L 61.228272 230.743159 \n",
       "L 61.294567 228.842924 \n",
       "L 61.360862 234.699608 \n",
       "L 61.427157 232.363848 \n",
       "L 61.493452 233.041065 \n",
       "L 61.559747 231.06821 \n",
       "L 61.758632 242.680392 \n",
       "L 61.824927 241.655449 \n",
       "L 61.891222 246.145551 \n",
       "L 61.957517 242.988828 \n",
       "L 62.023812 248.283152 \n",
       "L 62.156402 244.178249 \n",
       "L 62.288992 248.703076 \n",
       "L 62.355287 244.427368 \n",
       "L 62.421582 247.128836 \n",
       "L 62.487877 248.063417 \n",
       "L 62.554172 253.219138 \n",
       "L 62.620467 245.803941 \n",
       "L 62.686762 250.643926 \n",
       "L 62.753057 247.924785 \n",
       "L 62.819352 254.212259 \n",
       "L 62.885647 249.699552 \n",
       "L 62.951942 256.16826 \n",
       "L 63.018237 253.671842 \n",
       "L 63.217122 257.835202 \n",
       "L 63.283417 250.845003 \n",
       "L 63.349712 259.723218 \n",
       "L 63.416007 258.823518 \n",
       "L 63.482302 257.335343 \n",
       "L 63.548597 261.040334 \n",
       "L 63.614892 259.715115 \n",
       "L 63.747482 257.835963 \n",
       "L 63.813777 258.193949 \n",
       "L 63.946367 260.254589 \n",
       "L 64.078957 254.611241 \n",
       "L 64.145252 265.851401 \n",
       "L 64.211547 257.415095 \n",
       "L 64.277842 258.668652 \n",
       "L 64.476727 265.163333 \n",
       "L 64.543022 259.574356 \n",
       "L 64.609317 262.105795 \n",
       "L 64.675612 267.318831 \n",
       "L 64.741907 258.850802 \n",
       "L 64.808202 261.793596 \n",
       "L 64.874497 259.798009 \n",
       "L 64.940792 265.807291 \n",
       "L 65.007087 265.4747 \n",
       "L 65.073382 263.427293 \n",
       "L 65.139677 265.26559 \n",
       "L 65.205972 260.805125 \n",
       "L 65.272267 262.665618 \n",
       "L 65.338562 263.418767 \n",
       "L 65.404857 267.846861 \n",
       "L 65.471151 261.355281 \n",
       "L 65.603741 267.227284 \n",
       "L 65.670036 259.540477 \n",
       "L 65.736331 271.797519 \n",
       "L 65.802626 268.304878 \n",
       "L 65.868921 266.428291 \n",
       "L 65.935216 266.891283 \n",
       "L 66.001511 267.004195 \n",
       "L 66.067806 268.834261 \n",
       "L 66.134101 267.488495 \n",
       "L 66.200396 260.617986 \n",
       "L 66.399281 270.246335 \n",
       "L 66.598166 264.974404 \n",
       "L 66.664461 264.903785 \n",
       "L 66.730756 270.781651 \n",
       "L 66.797051 262.979578 \n",
       "L 66.863346 269.23151 \n",
       "L 66.929641 256.534659 \n",
       "L 67.062231 275.209562 \n",
       "L 67.128526 271.255339 \n",
       "L 67.194821 271.420704 \n",
       "L 67.261116 264.135176 \n",
       "L 67.327411 264.792832 \n",
       "L 67.460001 274.914923 \n",
       "L 67.592591 263.172495 \n",
       "L 67.791476 274.746697 \n",
       "L 67.990361 264.538217 \n",
       "L 68.056656 271.09182 \n",
       "L 68.122951 270.28106 \n",
       "L 68.189246 264.891679 \n",
       "L 68.388131 274.45265 \n",
       "L 68.454426 275.666874 \n",
       "L 68.520721 267.067274 \n",
       "L 68.587016 267.467595 \n",
       "L 68.653311 265.580114 \n",
       "L 68.719606 274.786312 \n",
       "L 68.785901 269.079603 \n",
       "L 68.852196 261.985512 \n",
       "L 68.918491 275.132812 \n",
       "L 68.984786 273.341501 \n",
       "L 69.051081 266.347469 \n",
       "L 69.117376 273.941038 \n",
       "L 69.183671 266.661119 \n",
       "L 69.249966 277.48888 \n",
       "L 69.316261 264.910972 \n",
       "L 69.382556 272.032686 \n",
       "L 69.581441 263.514852 \n",
       "L 69.647736 273.587717 \n",
       "L 69.714031 271.223983 \n",
       "L 69.780326 269.398186 \n",
       "L 69.979211 274.596312 \n",
       "L 70.045506 267.265504 \n",
       "L 70.111801 268.597234 \n",
       "L 70.178096 269.612763 \n",
       "L 70.310686 263.92866 \n",
       "L 70.376981 268.15944 \n",
       "L 70.443276 264.130075 \n",
       "L 70.509571 264.332574 \n",
       "L 70.575866 266.123379 \n",
       "L 70.642161 271.90864 \n",
       "L 70.708456 266.792901 \n",
       "L 70.841046 274.086814 \n",
       "L 70.973636 266.583665 \n",
       "L 71.106226 267.612343 \n",
       "L 71.172521 275.266765 \n",
       "L 71.305111 265.528929 \n",
       "L 71.371406 268.63365 \n",
       "L 71.437701 263.08037 \n",
       "L 71.503996 270.142795 \n",
       "L 71.57029 269.001402 \n",
       "L 71.636585 268.564003 \n",
       "L 71.70288 273.933625 \n",
       "L 71.769175 268.650956 \n",
       "L 71.83547 272.066269 \n",
       "L 71.901765 273.25087 \n",
       "L 71.96806 272.507769 \n",
       "L 72.034355 273.34222 \n",
       "L 72.10065 277.012458 \n",
       "L 72.23324 263.543263 \n",
       "L 72.36583 277.129259 \n",
       "L 72.432125 273.960557 \n",
       "L 72.564715 263.970797 \n",
       "L 72.63101 266.489778 \n",
       "L 72.697305 268.12931 \n",
       "L 72.7636 264.561724 \n",
       "L 72.829895 274.346122 \n",
       "L 72.89619 267.615246 \n",
       "L 72.962485 269.680395 \n",
       "L 73.02878 270.350241 \n",
       "L 73.16137 274.811186 \n",
       "L 73.227665 273.87056 \n",
       "L 73.29396 264.358533 \n",
       "L 73.360255 265.223115 \n",
       "L 73.42655 275.887004 \n",
       "L 73.492845 264.215181 \n",
       "L 73.55914 277.637348 \n",
       "L 73.625435 271.762653 \n",
       "L 73.69173 273.782325 \n",
       "L 73.758025 272.175587 \n",
       "L 73.82432 267.361363 \n",
       "L 73.890615 271.938221 \n",
       "L 73.95691 269.160722 \n",
       "L 74.023205 268.137766 \n",
       "L 74.22209 272.524525 \n",
       "L 74.288385 270.069568 \n",
       "L 74.48727 275.812538 \n",
       "L 74.553565 265.295791 \n",
       "L 74.61986 271.953272 \n",
       "L 74.75245 265.75557 \n",
       "L 74.88504 275.771767 \n",
       "L 75.01763 266.800257 \n",
       "L 75.15022 274.90479 \n",
       "L 75.349105 270.182297 \n",
       "L 75.4154 274.432568 \n",
       "L 75.481695 268.05559 \n",
       "L 75.54799 269.490663 \n",
       "L 75.614285 270.700039 \n",
       "L 75.68058 274.414867 \n",
       "L 75.746875 272.021284 \n",
       "L 75.879465 274.101682 \n",
       "L 75.94576 265.968639 \n",
       "L 76.144645 277.328222 \n",
       "L 76.21094 273.61772 \n",
       "L 76.277235 273.674881 \n",
       "L 76.34353 274.085771 \n",
       "L 76.409825 278.237026 \n",
       "L 76.47612 268.926203 \n",
       "L 76.542415 272.989194 \n",
       "L 76.60871 267.282598 \n",
       "L 76.7413 273.914501 \n",
       "L 76.807595 264.974658 \n",
       "L 76.87389 276.415542 \n",
       "L 76.940185 268.819323 \n",
       "L 77.00648 271.250717 \n",
       "L 77.072775 279.936551 \n",
       "L 77.13907 270.237513 \n",
       "L 77.205365 272.706577 \n",
       "L 77.27166 273.030472 \n",
       "L 77.337955 262.190507 \n",
       "L 77.470545 274.375985 \n",
       "L 77.53684 275.455792 \n",
       "L 77.603135 272.279352 \n",
       "L 77.669429 279.239942 \n",
       "L 77.735724 274.823658 \n",
       "L 77.802019 269.957192 \n",
       "L 77.868314 271.791797 \n",
       "L 77.934609 274.208139 \n",
       "L 78.000904 268.01878 \n",
       "L 78.067199 275.895911 \n",
       "L 78.133494 269.825791 \n",
       "L 78.199789 272.691384 \n",
       "L 78.266084 273.163833 \n",
       "L 78.332379 281.057509 \n",
       "L 78.398674 270.985659 \n",
       "L 78.464969 280.470246 \n",
       "L 78.531264 266.070375 \n",
       "L 78.597559 271.500977 \n",
       "L 78.663854 268.519921 \n",
       "L 78.730149 278.018319 \n",
       "L 78.796444 272.325267 \n",
       "L 78.862739 267.269647 \n",
       "L 78.995329 278.518262 \n",
       "L 79.061624 276.82484 \n",
       "L 79.127919 269.200224 \n",
       "L 79.194214 272.534137 \n",
       "L 79.260509 267.12349 \n",
       "L 79.326804 270.762907 \n",
       "L 79.393099 274.000651 \n",
       "L 79.459394 268.303286 \n",
       "L 79.525689 277.14672 \n",
       "L 79.591984 266.524545 \n",
       "L 79.724574 276.921841 \n",
       "L 79.857164 268.979685 \n",
       "L 79.923459 274.887372 \n",
       "L 79.989754 265.163615 \n",
       "L 80.056049 275.994335 \n",
       "L 80.122344 267.963987 \n",
       "L 80.188639 275.97352 \n",
       "L 80.254934 274.355846 \n",
       "L 80.387524 277.971883 \n",
       "L 80.520114 268.266828 \n",
       "L 80.586409 278.939948 \n",
       "L 80.652704 276.044351 \n",
       "L 80.718999 266.661852 \n",
       "L 80.785294 272.406582 \n",
       "L 80.851589 275.910159 \n",
       "L 80.917884 274.204123 \n",
       "L 80.984179 276.712154 \n",
       "L 81.050474 276.032034 \n",
       "L 81.249359 264.233685 \n",
       "L 81.315654 275.250854 \n",
       "L 81.381949 266.040442 \n",
       "L 81.448244 276.420672 \n",
       "L 81.514539 271.881371 \n",
       "L 81.580834 274.100949 \n",
       "L 81.647129 266.831417 \n",
       "L 81.713424 267.621291 \n",
       "L 81.779719 273.529527 \n",
       "L 81.846014 271.871633 \n",
       "L 81.912309 270.126334 \n",
       "L 82.044899 274.151599 \n",
       "L 82.111194 273.031853 \n",
       "L 82.177489 272.041085 \n",
       "L 82.243784 276.734787 \n",
       "L 82.310079 268.367676 \n",
       "L 82.376374 272.393293 \n",
       "L 82.508964 279.967809 \n",
       "L 82.641554 268.474598 \n",
       "L 82.840439 275.346361 \n",
       "L 82.906734 270.472609 \n",
       "L 82.973029 277.936186 \n",
       "L 83.039324 269.256511 \n",
       "L 83.105619 270.649474 \n",
       "L 83.171914 275.595367 \n",
       "L 83.238209 269.446679 \n",
       "L 83.304504 274.436035 \n",
       "L 83.370799 273.201305 \n",
       "L 83.437094 268.793026 \n",
       "L 83.635979 282.839801 \n",
       "L 83.768568 271.207353 \n",
       "L 83.834863 279.419076 \n",
       "L 83.901158 275.574538 \n",
       "L 83.967453 275.152021 \n",
       "L 84.033748 267.062595 \n",
       "L 84.100043 269.144487 \n",
       "L 84.232633 271.957233 \n",
       "L 84.298928 271.876523 \n",
       "L 84.365223 269.954106 \n",
       "L 84.431518 272.652263 \n",
       "L 84.497813 267.190925 \n",
       "L 84.564108 268.999204 \n",
       "L 84.630403 269.161666 \n",
       "L 84.696698 276.493095 \n",
       "L 84.829288 265.074745 \n",
       "L 84.895583 276.938583 \n",
       "L 84.961878 267.659441 \n",
       "L 85.094468 280.167786 \n",
       "L 85.160763 277.357309 \n",
       "L 85.227058 273.01466 \n",
       "L 85.293353 275.575581 \n",
       "L 85.359648 280.605707 \n",
       "L 85.558533 270.818306 \n",
       "L 85.823713 280.762532 \n",
       "L 85.890008 269.970228 \n",
       "L 85.956303 272.750278 \n",
       "L 86.022598 273.346955 \n",
       "L 86.088893 272.117468 \n",
       "L 86.287778 276.265904 \n",
       "L 86.486663 266.956716 \n",
       "L 86.552958 276.903731 \n",
       "L 86.619253 276.846345 \n",
       "L 86.685548 271.925087 \n",
       "L 86.884433 278.684022 \n",
       "L 87.017023 267.931432 \n",
       "L 87.083318 275.012826 \n",
       "L 87.149613 269.381091 \n",
       "L 87.348498 279.489611 \n",
       "L 87.414793 275.74888 \n",
       "L 87.481088 277.41671 \n",
       "L 87.547383 276.778982 \n",
       "L 87.679973 277.885833 \n",
       "L 87.812563 274.793316 \n",
       "L 87.878858 269.824325 \n",
       "L 87.945153 270.545371 \n",
       "L 88.011448 278.544785 \n",
       "L 88.077743 274.101696 \n",
       "L 88.210333 278.537006 \n",
       "L 88.342923 271.391462 \n",
       "L 88.409218 276.499845 \n",
       "L 88.475513 273.624076 \n",
       "L 88.674398 279.466809 \n",
       "L 88.740693 271.634422 \n",
       "L 88.806988 277.882338 \n",
       "L 88.873283 273.345362 \n",
       "L 88.939578 276.75749 \n",
       "L 89.005873 274.137351 \n",
       "L 89.072168 274.191933 \n",
       "L 89.138463 276.528158 \n",
       "L 89.271053 262.574452 \n",
       "L 89.337348 274.917629 \n",
       "L 89.403643 267.751594 \n",
       "L 89.536233 281.23722 \n",
       "L 89.602528 271.488012 \n",
       "L 89.668823 275.452367 \n",
       "L 89.735118 279.769551 \n",
       "L 89.867707 273.427622 \n",
       "L 89.934002 273.819713 \n",
       "L 90.000297 281.633779 \n",
       "L 90.066592 273.181829 \n",
       "L 90.132887 276.524479 \n",
       "L 90.331772 268.671348 \n",
       "L 90.464362 280.282981 \n",
       "L 90.530657 274.568211 \n",
       "L 90.596952 281.346539 \n",
       "L 90.663247 279.135402 \n",
       "L 90.862132 268.276636 \n",
       "L 90.928427 278.267213 \n",
       "L 90.994722 274.755716 \n",
       "L 91.061017 272.896083 \n",
       "L 91.127312 274.475635 \n",
       "L 91.193607 270.516748 \n",
       "L 91.326197 273.653516 \n",
       "L 91.392492 270.243516 \n",
       "L 91.591377 279.897359 \n",
       "L 91.657672 279.861591 \n",
       "L 91.723967 275.312637 \n",
       "L 91.790262 275.640633 \n",
       "L 91.856557 275.599764 \n",
       "L 91.922852 279.743874 \n",
       "L 91.989147 274.620002 \n",
       "L 92.055442 276.72984 \n",
       "L 92.121737 280.377811 \n",
       "L 92.188032 280.209064 \n",
       "L 92.254327 273.582601 \n",
       "L 92.320622 278.002803 \n",
       "L 92.386917 275.739706 \n",
       "L 92.453212 276.168945 \n",
       "L 92.519507 275.883946 \n",
       "L 92.585802 280.88039 \n",
       "L 92.718392 271.849563 \n",
       "L 92.784687 281.38931 \n",
       "L 92.917277 272.286596 \n",
       "L 92.983572 273.269515 \n",
       "L 93.049867 270.49847 \n",
       "L 93.116162 271.173897 \n",
       "L 93.315047 277.931071 \n",
       "L 93.381342 274.314709 \n",
       "L 93.447637 279.055171 \n",
       "L 93.580227 268.498218 \n",
       "L 93.712817 275.615788 \n",
       "L 93.779112 274.248572 \n",
       "L 93.845407 266.866818 \n",
       "L 93.911702 270.384839 \n",
       "L 93.977997 279.342848 \n",
       "L 94.044292 268.866506 \n",
       "L 94.110587 275.544126 \n",
       "L 94.176882 273.134139 \n",
       "L 94.243177 275.190804 \n",
       "L 94.375767 267.199197 \n",
       "L 94.442062 272.144794 \n",
       "L 94.508357 267.620023 \n",
       "L 94.574652 281.525334 \n",
       "L 94.640947 273.992857 \n",
       "L 94.707242 265.402305 \n",
       "L 94.839832 274.346926 \n",
       "L 94.972422 269.888757 \n",
       "L 95.171307 280.018557 \n",
       "L 95.303897 269.825889 \n",
       "L 95.436487 278.297667 \n",
       "L 95.502782 277.129287 \n",
       "L 95.569077 262.713942 \n",
       "L 95.635372 283.32 \n",
       "L 95.701667 275.555189 \n",
       "L 95.767962 273.607912 \n",
       "L 95.900552 277.138433 \n",
       "L 95.966846 273.415432 \n",
       "L 96.099436 280.850119 \n",
       "L 96.232026 266.93404 \n",
       "L 96.364616 278.335986 \n",
       "L 96.430911 276.200034 \n",
       "L 96.497206 275.464262 \n",
       "L 96.563501 279.643631 \n",
       "L 96.629796 277.555187 \n",
       "L 96.762386 280.395611 \n",
       "L 96.828681 276.540742 \n",
       "L 96.894976 279.279318 \n",
       "L 96.961271 277.59421 \n",
       "L 97.027566 275.385398 \n",
       "L 97.160156 281.557705 \n",
       "L 97.226451 280.58158 \n",
       "L 97.292746 278.864312 \n",
       "L 97.359041 281.129002 \n",
       "L 97.425336 280.405152 \n",
       "L 97.491631 279.671634 \n",
       "L 97.557926 274.666833 \n",
       "L 97.624221 282.356727 \n",
       "L 97.690516 275.86626 \n",
       "L 97.756811 278.233785 \n",
       "L 97.823106 276.593548 \n",
       "L 97.889401 283.756624 \n",
       "L 97.955696 282.29679 \n",
       "L 98.021991 283.235386 \n",
       "L 98.220876 276.481764 \n",
       "L 98.353466 281.754216 \n",
       "L 98.419761 281.339929 \n",
       "L 98.552351 272.714201 \n",
       "L 98.817531 283.371551 \n",
       "L 98.950121 278.937778 \n",
       "L 99.082711 282.071615 \n",
       "L 99.149006 278.870654 \n",
       "L 99.281596 282.165402 \n",
       "L 99.347891 279.252753 \n",
       "L 99.414186 281.592768 \n",
       "L 99.480481 274.453059 \n",
       "L 99.546776 282.577547 \n",
       "L 99.613071 282.276271 \n",
       "L 99.679366 283.738867 \n",
       "L 99.745661 279.62827 \n",
       "L 99.811956 280.526519 \n",
       "L 99.944546 282.443919 \n",
       "L 100.010841 278.954746 \n",
       "L 100.077136 282.550193 \n",
       "L 100.143431 279.449784 \n",
       "L 100.209726 282.9933 \n",
       "L 100.276021 279.872837 \n",
       "L 100.342316 284.688681 \n",
       "L 100.408611 276.209603 \n",
       "L 100.474906 281.331036 \n",
       "L 100.541201 280.506972 \n",
       "L 100.740086 277.026353 \n",
       "L 100.872676 280.364973 \n",
       "L 100.938971 275.164268 \n",
       "L 101.005266 278.336972 \n",
       "L 101.137856 280.402418 \n",
       "L 101.270446 276.48929 \n",
       "L 101.336741 282.873412 \n",
       "L 101.403036 281.196154 \n",
       "L 101.469331 277.620648 \n",
       "L 101.535626 279.441117 \n",
       "L 101.601921 278.723327 \n",
       "L 101.668216 282.377824 \n",
       "L 101.734511 280.303162 \n",
       "L 101.800806 281.380742 \n",
       "L 101.867101 273.337146 \n",
       "L 101.933396 282.093233 \n",
       "L 101.999691 278.157599 \n",
       "L 102.065985 284.291884 \n",
       "L 102.13228 279.760856 \n",
       "L 102.198575 281.23708 \n",
       "L 102.26487 281.371567 \n",
       "L 102.331165 279.895273 \n",
       "L 102.39746 281.124844 \n",
       "L 102.463755 278.157754 \n",
       "L 102.66264 282.64749 \n",
       "L 102.728935 280.082961 \n",
       "L 102.79523 283.359192 \n",
       "L 102.861525 282.978587 \n",
       "L 103.06041 276.494391 \n",
       "L 103.45818 285.161989 \n",
       "L 103.657065 279.972826 \n",
       "L 103.72336 281.325287 \n",
       "L 103.789655 277.666971 \n",
       "L 103.85595 282.362237 \n",
       "L 103.922245 280.20423 \n",
       "L 103.98854 283.20248 \n",
       "L 104.054835 282.814208 \n",
       "L 104.12113 281.737629 \n",
       "L 104.187425 278.126369 \n",
       "L 104.25372 282.655664 \n",
       "L 104.320015 279.494078 \n",
       "L 104.38631 282.128226 \n",
       "L 104.452605 281.264434 \n",
       "L 104.5189 278.949672 \n",
       "L 104.65149 281.580832 \n",
       "L 104.717785 277.761999 \n",
       "L 104.78408 280.98758 \n",
       "L 104.91667 275.854365 \n",
       "L 104.982965 284.011042 \n",
       "L 105.04926 281.23102 \n",
       "L 105.18185 284.775127 \n",
       "L 105.31444 278.808702 \n",
       "L 105.380735 281.654749 \n",
       "L 105.44703 281.077746 \n",
       "L 105.645915 278.130329 \n",
       "L 105.778505 282.915648 \n",
       "L 105.8448 282.509662 \n",
       "L 105.97739 278.423263 \n",
       "L 106.043685 281.839224 \n",
       "L 106.10998 279.635993 \n",
       "L 106.176275 275.5335 \n",
       "L 106.24257 286.768219 \n",
       "L 106.308865 284.939576 \n",
       "L 106.37516 281.299948 \n",
       "L 106.441455 284.480206 \n",
       "L 106.50775 283.708004 \n",
       "L 106.574045 279.784997 \n",
       "L 106.64034 283.698068 \n",
       "L 106.706635 278.808589 \n",
       "L 106.77293 284.101997 \n",
       "L 106.839225 283.478981 \n",
       "L 106.90552 283.598982 \n",
       "L 106.971815 283.348341 \n",
       "L 107.03811 277.596972 \n",
       "L 107.104405 284.390519 \n",
       "L 107.1707 281.720083 \n",
       "L 107.236995 281.13244 \n",
       "L 107.369585 283.678141 \n",
       "L 107.502175 277.489613 \n",
       "L 107.56847 282.940057 \n",
       "L 107.634765 280.901839 \n",
       "L 107.83365 285.935333 \n",
       "L 107.899945 286.355384 \n",
       "L 107.96624 280.015147 \n",
       "L 108.032535 285.928442 \n",
       "L 108.231419 274.86798 \n",
       "L 108.364009 282.80564 \n",
       "L 108.562894 276.12506 \n",
       "L 108.629189 283.103421 \n",
       "L 108.695484 279.076395 \n",
       "L 108.761779 276.395459 \n",
       "L 108.828074 285.408247 \n",
       "L 108.894369 284.77889 \n",
       "L 108.960664 284.952105 \n",
       "L 109.159549 281.161767 \n",
       "L 109.225844 282.177212 \n",
       "L 109.292139 285.387714 \n",
       "L 109.358434 281.50311 \n",
       "L 109.424729 282.698802 \n",
       "L 109.491024 284.623925 \n",
       "L 109.623614 281.156088 \n",
       "L 109.756204 284.082887 \n",
       "L 109.822499 284.233046 \n",
       "L 110.021384 278.620083 \n",
       "L 110.153974 284.429345 \n",
       "L 110.220269 282.911181 \n",
       "L 110.286564 285.748547 \n",
       "L 110.419154 276.601567 \n",
       "L 110.485449 279.558229 \n",
       "L 110.551744 280.078719 \n",
       "L 110.618039 285.079574 \n",
       "L 110.684334 279.513935 \n",
       "L 110.750629 283.5465 \n",
       "L 110.816924 277.409044 \n",
       "L 110.883219 280.754865 \n",
       "L 111.015809 284.030701 \n",
       "L 111.082104 279.286801 \n",
       "L 111.148399 282.117402 \n",
       "L 111.214694 277.757306 \n",
       "L 111.280989 285.317165 \n",
       "L 111.347284 283.30881 \n",
       "L 111.479874 279.953631 \n",
       "L 111.612464 282.389535 \n",
       "L 111.678759 280.829628 \n",
       "L 111.811349 288.159886 \n",
       "L 111.877644 287.881313 \n",
       "L 111.943939 285.373988 \n",
       "L 112.010234 277.032977 \n",
       "L 112.076529 286.405047 \n",
       "L 112.142824 278.696861 \n",
       "L 112.209119 284.294646 \n",
       "L 112.275414 277.889046 \n",
       "L 112.341709 284.007547 \n",
       "L 112.408004 282.256428 \n",
       "L 112.474299 284.871268 \n",
       "L 112.540594 283.301947 \n",
       "L 112.606889 279.767409 \n",
       "L 112.673184 280.521446 \n",
       "L 112.739479 284.415901 \n",
       "L 112.805774 280.397386 \n",
       "L 112.872069 285.833414 \n",
       "L 112.938364 283.146728 \n",
       "L 113.004659 285.523696 \n",
       "L 113.070954 283.661948 \n",
       "L 113.137249 278.165871 \n",
       "L 113.336134 284.151618 \n",
       "L 113.402429 278.65182 \n",
       "L 113.601314 286.531136 \n",
       "L 113.667609 286.169457 \n",
       "L 113.932789 280.40432 \n",
       "L 113.999084 283.567991 \n",
       "L 114.065379 279.168351 \n",
       "L 114.131674 281.692912 \n",
       "L 114.197969 285.743051 \n",
       "L 114.264263 281.229399 \n",
       "L 114.330558 285.406345 \n",
       "L 114.396853 278.821244 \n",
       "L 114.463148 279.535455 \n",
       "L 114.595738 285.069808 \n",
       "L 114.662033 284.387828 \n",
       "L 114.728328 277.478775 \n",
       "L 114.794623 285.182381 \n",
       "L 114.860918 283.011776 \n",
       "L 114.927213 280.719337 \n",
       "L 114.993508 280.873752 \n",
       "L 115.059803 284.29249 \n",
       "L 115.126098 281.350583 \n",
       "L 115.192393 285.795025 \n",
       "L 115.258688 283.3409 \n",
       "L 115.324983 280.030198 \n",
       "L 115.391278 286.188906 \n",
       "L 115.457573 285.718008 \n",
       "L 115.590163 278.100086 \n",
       "L 115.722753 283.516905 \n",
       "L 115.855343 283.314349 \n",
       "L 115.921638 284.641231 \n",
       "L 115.987933 279.451095 \n",
       "L 116.054228 282.460647 \n",
       "L 116.120523 281.044473 \n",
       "L 116.253113 283.706623 \n",
       "L 116.319408 283.111947 \n",
       "L 116.451998 279.65771 \n",
       "L 116.518293 284.115357 \n",
       "L 116.584588 281.049025 \n",
       "L 116.650883 282.764503 \n",
       "L 116.717178 283.619247 \n",
       "L 116.849768 274.388965 \n",
       "L 116.916063 286.065438 \n",
       "L 116.982358 278.24706 \n",
       "L 117.114948 285.222927 \n",
       "L 117.247538 279.199524 \n",
       "L 117.313833 285.4002 \n",
       "L 117.380128 279.126171 \n",
       "L 117.446423 284.266319 \n",
       "L 117.512718 282.80509 \n",
       "L 117.579013 282.83842 \n",
       "L 117.645308 281.052097 \n",
       "L 117.711603 284.696869 \n",
       "L 117.910488 279.844849 \n",
       "L 117.976783 283.759414 \n",
       "L 118.043078 282.165459 \n",
       "L 118.109373 279.329488 \n",
       "L 118.175668 285.541749 \n",
       "L 118.241963 285.2645 \n",
       "L 118.308258 286.043087 \n",
       "L 118.374553 289.07196 \n",
       "L 118.440848 279.275668 \n",
       "L 118.507143 285.546639 \n",
       "L 118.573438 285.51772 \n",
       "L 118.706028 279.101818 \n",
       "L 118.904913 284.200478 \n",
       "L 118.971208 281.399302 \n",
       "L 119.170093 289.003497 \n",
       "L 119.368978 277.990585 \n",
       "L 119.501568 284.790629 \n",
       "L 119.567863 281.123689 \n",
       "L 119.634158 281.95122 \n",
       "L 119.700453 283.943227 \n",
       "L 119.766748 283.356331 \n",
       "L 119.833043 283.861869 \n",
       "L 119.899338 286.540775 \n",
       "L 119.965633 281.712262 \n",
       "L 120.031928 287.515111 \n",
       "L 120.098223 282.929586 \n",
       "L 120.164518 283.209512 \n",
       "L 120.230813 287.34969 \n",
       "L 120.429697 281.904403 \n",
       "L 120.495992 285.882795 \n",
       "L 120.562287 281.179158 \n",
       "L 120.694877 288.550159 \n",
       "L 120.761172 282.923033 \n",
       "L 120.827467 285.466789 \n",
       "L 120.893762 284.527023 \n",
       "L 120.960057 280.992372 \n",
       "L 121.026352 287.990744 \n",
       "L 121.092647 280.952362 \n",
       "L 121.158942 283.637864 \n",
       "L 121.225237 283.342393 \n",
       "L 121.291532 286.86429 \n",
       "L 121.357827 285.785625 \n",
       "L 121.490417 284.834514 \n",
       "L 121.556712 282.485366 \n",
       "L 121.623007 288.380256 \n",
       "L 121.689302 282.346552 \n",
       "L 121.755597 285.869576 \n",
       "L 121.888187 282.42588 \n",
       "L 121.954482 288.247248 \n",
       "L 122.087072 280.491357 \n",
       "L 122.153367 279.594659 \n",
       "L 122.219662 284.590356 \n",
       "L 122.285957 281.15217 \n",
       "L 122.352252 285.665737 \n",
       "L 122.484842 278.26586 \n",
       "L 122.551137 285.455923 \n",
       "L 122.617432 282.798946 \n",
       "L 122.683727 286.319574 \n",
       "L 122.750022 281.161697 \n",
       "L 122.816317 283.427585 \n",
       "L 122.882612 284.873622 \n",
       "L 122.948907 279.660782 \n",
       "L 123.015202 281.73729 \n",
       "L 123.081497 281.731301 \n",
       "L 123.147792 281.153735 \n",
       "L 123.214087 289.010107 \n",
       "L 123.280382 286.650742 \n",
       "L 123.545562 282.433814 \n",
       "L 123.611857 286.728915 \n",
       "L 123.810742 280.692349 \n",
       "L 123.943332 285.842166 \n",
       "L 124.009627 282.489904 \n",
       "L 124.142217 286.111339 \n",
       "L 124.274807 278.103976 \n",
       "L 124.407397 288.573539 \n",
       "L 124.539987 280.065261 \n",
       "L 124.606282 281.903656 \n",
       "L 124.672577 286.197643 \n",
       "L 124.738872 281.098026 \n",
       "L 124.805167 282.196956 \n",
       "L 124.871462 282.020979 \n",
       "L 125.004052 284.784259 \n",
       "L 125.070347 284.98115 \n",
       "L 125.202937 282.013172 \n",
       "L 125.269232 283.193178 \n",
       "L 125.335527 288.134393 \n",
       "L 125.401822 285.922072 \n",
       "L 125.468117 286.89645 \n",
       "L 125.534412 281.276934 \n",
       "L 125.600707 285.820773 \n",
       "L 125.667002 284.444875 \n",
       "L 125.733297 283.199858 \n",
       "L 125.799592 278.539078 \n",
       "L 125.865887 287.267965 \n",
       "L 125.932182 285.098092 \n",
       "L 125.998477 287.335329 \n",
       "L 126.064772 282.004899 \n",
       "L 126.131067 282.01289 \n",
       "L 126.197362 283.881458 \n",
       "L 126.263657 281.141403 \n",
       "L 126.329952 283.620135 \n",
       "L 126.396247 280.45039 \n",
       "L 126.462541 285.141836 \n",
       "L 126.661426 279.62734 \n",
       "L 126.727721 280.838844 \n",
       "L 126.794016 284.968466 \n",
       "L 126.860311 281.692771 \n",
       "L 126.926606 281.791308 \n",
       "L 126.992901 283.079167 \n",
       "L 127.125491 280.611485 \n",
       "L 127.191786 288.521847 \n",
       "L 127.258081 286.02409 \n",
       "L 127.390671 281.425627 \n",
       "L 127.456966 282.420342 \n",
       "L 127.523261 285.859443 \n",
       "L 127.655851 279.716886 \n",
       "L 127.788441 287.39504 \n",
       "L 127.921031 283.59123 \n",
       "L 127.987326 283.485281 \n",
       "L 128.053621 281.715968 \n",
       "L 128.119916 282.747507 \n",
       "L 128.186211 286.164539 \n",
       "L 128.252506 284.705861 \n",
       "L 128.318801 282.636498 \n",
       "L 128.385096 286.959713 \n",
       "L 128.451391 282.078661 \n",
       "L 128.517686 282.702424 \n",
       "L 128.650276 287.021172 \n",
       "L 128.981751 283.067076 \n",
       "L 129.048046 287.753322 \n",
       "L 129.180636 281.868861 \n",
       "L 129.246931 287.771122 \n",
       "L 129.313226 281.176509 \n",
       "L 129.379521 288.427552 \n",
       "L 129.445816 280.563541 \n",
       "L 129.512111 282.356064 \n",
       "L 129.644701 287.821884 \n",
       "L 129.710996 279.917821 \n",
       "L 129.777291 283.158848 \n",
       "L 129.843586 285.218008 \n",
       "L 129.909881 282.129226 \n",
       "L 129.976176 284.631394 \n",
       "L 130.042471 284.371114 \n",
       "L 130.108766 281.494048 \n",
       "L 130.175061 283.559042 \n",
       "L 130.241356 282.790687 \n",
       "L 130.307651 281.898935 \n",
       "L 130.373946 284.862798 \n",
       "L 130.440241 281.112936 \n",
       "L 130.506536 285.818151 \n",
       "L 130.572831 285.313262 \n",
       "L 130.639126 283.157453 \n",
       "L 130.705421 283.434138 \n",
       "L 130.771716 283.94544 \n",
       "L 130.838011 283.033225 \n",
       "L 130.904306 279.822286 \n",
       "L 130.970601 284.466029 \n",
       "L 131.036896 283.147208 \n",
       "L 131.103191 285.376538 \n",
       "L 131.169486 282.538707 \n",
       "L 131.235781 283.62697 \n",
       "L 131.302076 282.262572 \n",
       "L 131.434666 286.656012 \n",
       "L 131.567256 280.611738 \n",
       "L 131.633551 280.639332 \n",
       "L 131.699846 285.155591 \n",
       "L 131.766141 284.304708 \n",
       "L 131.898731 281.210303 \n",
       "L 132.097616 287.927312 \n",
       "L 132.163911 283.223408 \n",
       "L 132.230206 284.491029 \n",
       "L 132.296501 288.266062 \n",
       "L 132.495386 281.0101 \n",
       "L 132.627975 287.178081 \n",
       "L 132.69427 281.760882 \n",
       "L 132.760565 284.312375 \n",
       "L 132.82686 287.515661 \n",
       "L 132.893155 284.889547 \n",
       "L 132.95945 287.752547 \n",
       "L 133.09204 283.43363 \n",
       "L 133.158335 284.175237 \n",
       "L 133.35722 279.1303 \n",
       "L 133.556105 286.490154 \n",
       "L 133.6224 285.919549 \n",
       "L 133.688695 284.166796 \n",
       "L 133.75499 284.946749 \n",
       "L 133.821285 286.072837 \n",
       "L 133.88758 283.682848 \n",
       "L 133.953875 285.28299 \n",
       "L 134.02017 285.695191 \n",
       "L 134.086465 288.567832 \n",
       "L 134.15276 282.814715 \n",
       "L 134.219055 284.189302 \n",
       "L 134.28535 286.587098 \n",
       "L 134.484235 281.344072 \n",
       "L 134.616825 285.67549 \n",
       "L 134.68312 283.653817 \n",
       "L 134.81571 287.884273 \n",
       "L 134.882005 283.570641 \n",
       "L 134.9483 285.140794 \n",
       "L 135.014595 286.088367 \n",
       "L 135.08089 282.318422 \n",
       "L 135.21348 288.137253 \n",
       "L 135.279775 281.022883 \n",
       "L 135.34607 283.707088 \n",
       "L 135.412365 283.97891 \n",
       "L 135.47866 282.993807 \n",
       "L 135.61125 285.480825 \n",
       "L 135.74384 283.340181 \n",
       "L 135.810135 287.936191 \n",
       "L 136.00902 281.118883 \n",
       "L 136.075315 286.966604 \n",
       "L 136.14161 281.828344 \n",
       "L 136.2742 286.46256 \n",
       "L 136.340495 281.050871 \n",
       "L 136.40679 283.141979 \n",
       "L 136.473085 282.35116 \n",
       "L 136.605675 284.899158 \n",
       "L 136.67197 282.196576 \n",
       "L 136.738265 282.262291 \n",
       "L 136.80456 286.628376 \n",
       "L 136.870855 283.549544 \n",
       "L 136.93715 286.513858 \n",
       "L 137.06974 282.008098 \n",
       "L 137.20233 286.823308 \n",
       "L 137.268625 286.667188 \n",
       "L 137.33492 284.883458 \n",
       "L 137.401215 286.447298 \n",
       "L 137.46751 286.384176 \n",
       "L 137.6001 284.796717 \n",
       "L 137.666395 285.204972 \n",
       "L 137.73269 280.526167 \n",
       "L 137.798985 283.671024 \n",
       "L 137.99787 287.565 \n",
       "L 138.064165 281.928262 \n",
       "L 138.13046 283.920157 \n",
       "L 138.196755 286.738258 \n",
       "L 138.26305 283.605309 \n",
       "L 138.329345 285.232242 \n",
       "L 138.39564 288.349365 \n",
       "L 138.461935 281.277329 \n",
       "L 138.52823 281.332911 \n",
       "L 138.66082 285.462871 \n",
       "L 138.727114 279.026436 \n",
       "L 138.859704 289.639704 \n",
       "L 139.058589 280.744394 \n",
       "L 139.124884 284.638553 \n",
       "L 139.191179 283.024375 \n",
       "L 139.257474 278.681796 \n",
       "L 139.323769 285.190612 \n",
       "L 139.390064 284.035902 \n",
       "L 139.456359 283.505278 \n",
       "L 139.522654 286.425144 \n",
       "L 139.588949 280.481112 \n",
       "L 139.655244 284.241953 \n",
       "L 139.721539 283.37444 \n",
       "L 139.787834 286.582645 \n",
       "L 139.854129 284.708087 \n",
       "L 139.920424 280.218788 \n",
       "L 139.986719 284.959362 \n",
       "L 140.053014 280.871822 \n",
       "L 140.119309 283.605225 \n",
       "L 140.251899 286.33161 \n",
       "L 140.318194 284.361192 \n",
       "L 140.384489 286.193528 \n",
       "L 140.450784 280.788279 \n",
       "L 140.583374 285.990873 \n",
       "L 140.715964 280.023081 \n",
       "L 140.782259 288.096737 \n",
       "L 140.848554 282.20396 \n",
       "L 140.914849 283.729636 \n",
       "L 140.981144 287.539041 \n",
       "L 141.047439 281.88449 \n",
       "L 141.113734 283.543033 \n",
       "L 141.180029 286.298435 \n",
       "L 141.312619 281.996218 \n",
       "L 141.378914 284.308739 \n",
       "L 141.445209 280.7878 \n",
       "L 141.710389 289.815259 \n",
       "L 141.842979 282.627013 \n",
       "L 141.909274 285.966422 \n",
       "L 142.041864 282.203749 \n",
       "L 142.174454 286.137847 \n",
       "L 142.240749 285.856907 \n",
       "L 142.307044 283.39131 \n",
       "L 142.373339 283.478882 \n",
       "L 142.439634 286.170317 \n",
       "L 142.505929 286.118949 \n",
       "L 142.572224 285.266107 \n",
       "L 142.638519 289.835158 \n",
       "L 142.771109 283.318393 \n",
       "L 142.837404 283.83832 \n",
       "L 142.903699 288.585687 \n",
       "L 142.969994 284.830737 \n",
       "L 143.036289 287.960882 \n",
       "L 143.102584 285.930161 \n",
       "L 143.168879 281.592825 \n",
       "L 143.301469 286.802521 \n",
       "L 143.367764 286.501752 \n",
       "L 143.434059 286.598795 \n",
       "L 143.566649 283.153296 \n",
       "L 143.632944 283.449442 \n",
       "L 143.699239 282.563905 \n",
       "L 143.898124 288.043099 \n",
       "L 144.030714 280.968202 \n",
       "L 144.097009 287.646076 \n",
       "L 144.163304 287.600866 \n",
       "L 144.229599 286.904187 \n",
       "L 144.295894 277.533864 \n",
       "L 144.362189 281.028999 \n",
       "L 144.428484 286.261075 \n",
       "L 144.494779 284.885262 \n",
       "L 144.693664 279.992964 \n",
       "L 144.759959 285.974666 \n",
       "L 144.826253 282.842366 \n",
       "L 144.892548 281.896878 \n",
       "L 145.025138 285.369675 \n",
       "L 145.091433 281.46279 \n",
       "L 145.157728 281.584411 \n",
       "L 145.224023 286.589776 \n",
       "L 145.290318 286.426905 \n",
       "L 145.356613 284.199646 \n",
       "L 145.422908 289.292457 \n",
       "L 145.489203 284.413026 \n",
       "L 145.555498 287.565606 \n",
       "L 145.621793 285.952075 \n",
       "L 145.688088 281.448387 \n",
       "L 145.820678 285.358485 \n",
       "L 145.886973 285.276423 \n",
       "L 145.953268 281.450248 \n",
       "L 146.019563 283.815955 \n",
       "L 146.152153 287.762962 \n",
       "L 146.218448 286.966604 \n",
       "L 146.351038 284.089384 \n",
       "L 146.616218 285.927004 \n",
       "L 146.748808 283.423018 \n",
       "L 146.815103 287.426932 \n",
       "L 146.947693 283.827666 \n",
       "L 147.013988 285.648347 \n",
       "L 147.146578 281.890959 \n",
       "L 147.279168 288.557375 \n",
       "L 147.345463 282.957349 \n",
       "L 147.411758 286.06169 \n",
       "L 147.478053 285.741078 \n",
       "L 147.610643 282.294098 \n",
       "L 147.743233 289.824842 \n",
       "L 147.875823 286.283933 \n",
       "L 147.942118 286.677532 \n",
       "L 148.008413 283.659087 \n",
       "L 148.074708 290.052737 \n",
       "L 148.141003 281.656243 \n",
       "L 148.207298 283.106705 \n",
       "L 148.273593 286.119118 \n",
       "L 148.339888 279.892623 \n",
       "L 148.406183 286.852058 \n",
       "L 148.472478 283.057098 \n",
       "L 148.538773 284.746166 \n",
       "L 148.605068 284.561635 \n",
       "L 148.671363 283.99289 \n",
       "L 148.737658 280.014512 \n",
       "L 148.803953 280.391566 \n",
       "L 148.870248 290.611701 \n",
       "L 148.936543 283.218419 \n",
       "L 149.002838 283.767785 \n",
       "L 149.069133 291.1733 \n",
       "L 149.135428 283.092274 \n",
       "L 149.201723 285.236554 \n",
       "L 149.268018 282.402275 \n",
       "L 149.400608 290.528652 \n",
       "L 149.599493 282.977445 \n",
       "L 149.732083 290.344472 \n",
       "L 149.864673 279.955647 \n",
       "L 149.997263 283.683736 \n",
       "L 150.063558 282.358728 \n",
       "L 150.196148 287.497143 \n",
       "L 150.262443 285.94245 \n",
       "L 150.328738 284.014142 \n",
       "L 150.395033 284.092837 \n",
       "L 150.593918 289.655178 \n",
       "L 150.859098 279.244156 \n",
       "L 150.925392 287.276421 \n",
       "L 150.991687 284.75582 \n",
       "L 151.057982 284.301495 \n",
       "L 151.124277 280.691335 \n",
       "L 151.190572 287.883371 \n",
       "L 151.256867 283.506251 \n",
       "L 151.323162 289.683279 \n",
       "L 151.455752 282.975162 \n",
       "L 151.522047 285.771434 \n",
       "L 151.588342 283.845536 \n",
       "L 151.654637 286.711919 \n",
       "L 151.720932 281.428601 \n",
       "L 151.919817 288.746346 \n",
       "L 152.052407 285.202844 \n",
       "L 152.118702 287.34122 \n",
       "L 152.184997 284.848593 \n",
       "L 152.251292 286.858879 \n",
       "L 152.383882 281.773706 \n",
       "L 152.450177 283.90233 \n",
       "L 152.516472 284.947186 \n",
       "L 152.582767 281.179313 \n",
       "L 152.649062 285.366067 \n",
       "L 152.715357 283.179382 \n",
       "L 152.980537 286.748081 \n",
       "L 153.113127 283.0298 \n",
       "L 153.179422 285.289276 \n",
       "L 153.245717 284.95512 \n",
       "L 153.312012 284.372241 \n",
       "L 153.378307 284.443706 \n",
       "L 153.444602 288.115565 \n",
       "L 153.510897 285.853017 \n",
       "L 153.643487 282.563652 \n",
       "L 153.776077 287.559461 \n",
       "L 153.908667 283.019287 \n",
       "L 154.041257 288.61066 \n",
       "L 154.173847 285.609394 \n",
       "L 154.240142 287.194499 \n",
       "L 154.306437 287.007586 \n",
       "L 154.439027 285.855145 \n",
       "L 154.571617 287.527696 \n",
       "L 154.770502 285.005108 \n",
       "L 154.836797 289.523044 \n",
       "L 155.035682 281.367635 \n",
       "L 155.101977 288.567846 \n",
       "L 155.168272 287.469507 \n",
       "L 155.234567 288.885371 \n",
       "L 155.300862 282.086144 \n",
       "L 155.367157 283.901174 \n",
       "L 155.433452 284.422017 \n",
       "L 155.566042 280.364029 \n",
       "L 155.698632 287.123796 \n",
       "L 155.764927 288.133251 \n",
       "L 155.963812 282.283768 \n",
       "L 156.030107 283.042737 \n",
       "L 156.096402 278.85277 \n",
       "L 156.162697 281.108313 \n",
       "L 156.228992 282.299679 \n",
       "L 156.295287 286.799351 \n",
       "L 156.361582 284.393549 \n",
       "L 156.560467 286.932345 \n",
       "L 156.626762 286.906653 \n",
       "L 156.693057 287.029092 \n",
       "L 156.759352 283.985153 \n",
       "L 156.825647 289.621863 \n",
       "L 156.891942 281.385533 \n",
       "L 156.958237 282.950105 \n",
       "L 157.157121 285.466324 \n",
       "L 157.223416 282.477347 \n",
       "L 157.289711 288.52716 \n",
       "L 157.356006 286.603122 \n",
       "L 157.488596 284.537198 \n",
       "L 157.554891 288.335103 \n",
       "L 157.621186 285.927892 \n",
       "L 157.687481 283.814052 \n",
       "L 157.753776 288.902057 \n",
       "L 157.820071 286.712722 \n",
       "L 157.952661 280.864676 \n",
       "L 158.018956 284.574149 \n",
       "L 158.085251 284.166852 \n",
       "L 158.151546 284.109762 \n",
       "L 158.217841 285.218671 \n",
       "L 158.284136 288.615649 \n",
       "L 158.350431 286.767642 \n",
       "L 158.416726 284.740022 \n",
       "L 158.483021 288.907779 \n",
       "L 158.549316 286.71461 \n",
       "L 158.615611 286.234383 \n",
       "L 158.681906 286.338557 \n",
       "L 158.748201 287.897027 \n",
       "L 158.814496 286.151249 \n",
       "L 158.880791 286.481092 \n",
       "L 159.013381 287.812893 \n",
       "L 159.145971 282.073616 \n",
       "L 159.212266 288.167694 \n",
       "L 159.278561 280.950234 \n",
       "L 159.477446 289.595763 \n",
       "L 159.543741 281.737135 \n",
       "L 159.610036 285.268771 \n",
       "L 159.742626 285.328721 \n",
       "L 159.808921 286.986827 \n",
       "L 159.875216 283.71498 \n",
       "L 159.941511 285.808103 \n",
       "L 160.007806 284.276382 \n",
       "L 160.074101 285.556433 \n",
       "L 160.206691 282.718786 \n",
       "L 160.339281 286.530417 \n",
       "L 160.405576 286.510391 \n",
       "L 160.471871 290.189564 \n",
       "L 160.604461 285.313064 \n",
       "L 160.670756 286.126136 \n",
       "L 160.737051 286.415166 \n",
       "L 160.803346 282.824073 \n",
       "L 160.869641 287.406526 \n",
       "L 160.935936 286.604644 \n",
       "L 161.002231 284.111016 \n",
       "L 161.068526 277.319836 \n",
       "L 161.201116 289.419335 \n",
       "L 161.267411 282.394383 \n",
       "L 161.333706 282.892832 \n",
       "L 161.466296 287.217894 \n",
       "L 161.598886 281.490666 \n",
       "L 161.797771 288.817402 \n",
       "L 161.996656 284.948243 \n",
       "L 162.062951 285.824098 \n",
       "L 162.261836 282.419285 \n",
       "L 162.328131 288.904594 \n",
       "L 162.394426 284.642203 \n",
       "L 162.460721 287.353114 \n",
       "L 162.527016 284.312445 \n",
       "L 162.593311 289.836017 \n",
       "L 162.659606 285.735116 \n",
       "L 162.725901 285.769306 \n",
       "L 162.792196 288.822574 \n",
       "L 162.858491 285.900284 \n",
       "L 162.924786 289.262735 \n",
       "L 162.991081 280.278725 \n",
       "L 163.057376 285.770024 \n",
       "L 163.12367 277.961765 \n",
       "L 163.25626 286.699827 \n",
       "L 163.38885 284.652421 \n",
       "L 163.455145 287.059673 \n",
       "L 163.52144 278.256587 \n",
       "L 163.587735 281.983478 \n",
       "L 163.65403 281.806458 \n",
       "L 163.720325 281.132525 \n",
       "L 163.852915 289.319769 \n",
       "L 164.0518 280.743041 \n",
       "L 164.18439 289.356805 \n",
       "L 164.250685 282.759359 \n",
       "L 164.31698 285.994833 \n",
       "L 164.383275 285.728408 \n",
       "L 164.44957 291.264086 \n",
       "L 164.515865 289.20122 \n",
       "L 164.58216 286.252619 \n",
       "L 164.648455 289.51067 \n",
       "L 164.71475 287.843798 \n",
       "L 164.781045 285.674038 \n",
       "L 164.913635 287.637099 \n",
       "L 165.046225 285.818856 \n",
       "L 165.11252 288.464108 \n",
       "L 165.178815 285.113143 \n",
       "L 165.24511 287.547398 \n",
       "L 165.311405 286.499286 \n",
       "L 165.443995 284.453303 \n",
       "L 165.51029 286.827212 \n",
       "L 165.576585 286.500597 \n",
       "L 165.64288 284.147291 \n",
       "L 165.709175 288.93371 \n",
       "L 165.77547 287.55498 \n",
       "L 165.90806 285.126236 \n",
       "L 166.04065 286.88767 \n",
       "L 166.239535 283.778031 \n",
       "L 166.43842 289.725332 \n",
       "L 166.504715 286.284399 \n",
       "L 166.57101 288.582404 \n",
       "L 166.83619 282.602449 \n",
       "L 167.035075 286.889404 \n",
       "L 167.10137 285.491085 \n",
       "L 167.167665 286.713229 \n",
       "L 167.300255 283.756229 \n",
       "L 167.36655 286.674939 \n",
       "L 167.432845 286.399537 \n",
       "L 167.63173 283.656974 \n",
       "L 167.698025 288.313104 \n",
       "L 167.76432 285.086691 \n",
       "L 167.830615 289.262425 \n",
       "L 167.89691 282.797762 \n",
       "L 167.963205 283.88718 \n",
       "L 168.0295 282.860179 \n",
       "L 168.095795 287.21912 \n",
       "L 168.16209 284.298211 \n",
       "L 168.228385 282.68819 \n",
       "L 168.360975 286.40065 \n",
       "L 168.42727 283.11626 \n",
       "L 168.493565 285.010463 \n",
       "L 168.55986 288.650853 \n",
       "L 168.626155 285.032265 \n",
       "L 168.69245 289.262848 \n",
       "L 168.82504 284.702732 \n",
       "L 168.891335 286.358837 \n",
       "L 168.95763 280.956829 \n",
       "L 169.09022 288.557065 \n",
       "L 169.289104 283.498612 \n",
       "L 169.355399 283.614047 \n",
       "L 169.421694 285.007856 \n",
       "L 169.487989 283.901245 \n",
       "L 169.686874 288.781408 \n",
       "L 169.753169 286.672431 \n",
       "L 169.885759 286.040733 \n",
       "L 169.952054 286.001795 \n",
       "L 170.018349 282.886124 \n",
       "L 170.084644 284.558985 \n",
       "L 170.150939 284.298324 \n",
       "L 170.217234 281.48827 \n",
       "L 170.283529 283.123024 \n",
       "L 170.548709 287.36188 \n",
       "L 170.615004 282.09756 \n",
       "L 170.681299 290.400858 \n",
       "L 170.747594 287.554924 \n",
       "L 170.880184 284.375792 \n",
       "L 170.946479 287.163622 \n",
       "L 171.012774 284.659819 \n",
       "L 171.079069 285.979937 \n",
       "L 171.145364 286.403934 \n",
       "L 171.211659 284.764078 \n",
       "L 171.277954 286.761751 \n",
       "L 171.344249 282.742842 \n",
       "L 171.410544 285.196502 \n",
       "L 171.476839 287.050245 \n",
       "L 171.609429 284.124151 \n",
       "L 171.675724 286.717063 \n",
       "L 171.742019 284.314136 \n",
       "L 171.808314 286.950242 \n",
       "L 171.874609 285.540762 \n",
       "L 171.940904 283.557661 \n",
       "L 172.007199 289.920208 \n",
       "L 172.139789 283.226001 \n",
       "L 172.272379 289.897279 \n",
       "L 172.338674 281.758359 \n",
       "L 172.404969 282.71801 \n",
       "L 172.537559 289.786664 \n",
       "L 172.603854 284.471567 \n",
       "L 172.670149 284.54423 \n",
       "L 172.802739 288.422957 \n",
       "L 172.935329 283.630056 \n",
       "L 173.001624 284.832499 \n",
       "L 173.067919 285.422509 \n",
       "L 173.134214 283.842224 \n",
       "L 173.266804 288.870898 \n",
       "L 173.333099 288.587562 \n",
       "L 173.399394 285.069286 \n",
       "L 173.465689 287.922577 \n",
       "L 173.531984 286.347534 \n",
       "L 173.598279 287.013533 \n",
       "L 173.664574 286.456599 \n",
       "L 173.730869 284.882246 \n",
       "L 173.797164 287.677686 \n",
       "L 173.863459 285.424341 \n",
       "L 173.929754 287.693893 \n",
       "L 173.996049 286.637339 \n",
       "L 174.128639 287.851451 \n",
       "L 174.194934 286.702871 \n",
       "L 174.261229 287.924973 \n",
       "L 174.327524 284.624066 \n",
       "L 174.393819 285.573486 \n",
       "L 174.460114 286.737328 \n",
       "L 174.526409 283.871001 \n",
       "L 174.592704 284.705212 \n",
       "L 174.725294 286.447114 \n",
       "L 174.791589 285.211892 \n",
       "L 174.857884 279.880532 \n",
       "L 174.990474 287.522581 \n",
       "L 175.056769 281.345862 \n",
       "L 175.123064 285.63479 \n",
       "L 175.189359 283.158454 \n",
       "L 175.321948 287.258072 \n",
       "L 175.388243 285.503895 \n",
       "L 175.454538 283.692614 \n",
       "L 175.587128 289.046509 \n",
       "L 175.653423 289.278336 \n",
       "L 175.852308 282.36731 \n",
       "L 175.984898 291.118239 \n",
       "L 176.117488 283.712767 \n",
       "L 176.183783 286.444465 \n",
       "L 176.250078 285.97468 \n",
       "L 176.316373 285.208298 \n",
       "L 176.382668 287.379721 \n",
       "L 176.515258 283.556985 \n",
       "L 176.647848 285.669303 \n",
       "L 176.714143 290.09455 \n",
       "L 176.780438 287.785031 \n",
       "L 176.913028 288.325308 \n",
       "L 176.979323 283.517539 \n",
       "L 177.111913 287.639311 \n",
       "L 177.178208 288.327704 \n",
       "L 177.443388 281.247508 \n",
       "L 177.509683 288.696682 \n",
       "L 177.575978 282.933349 \n",
       "L 177.642273 288.603684 \n",
       "L 177.708568 283.773606 \n",
       "L 177.774863 283.79645 \n",
       "L 177.841158 288.557459 \n",
       "L 177.907453 285.90564 \n",
       "L 177.973748 288.516745 \n",
       "L 178.040043 285.921283 \n",
       "L 178.106338 289.172555 \n",
       "L 178.172633 283.24084 \n",
       "L 178.238928 283.394692 \n",
       "L 178.371518 289.661999 \n",
       "L 178.437813 289.421646 \n",
       "L 178.636698 281.066472 \n",
       "L 178.835583 290.840611 \n",
       "L 178.901878 284.524077 \n",
       "L 178.968173 287.24219 \n",
       "L 179.034468 286.544031 \n",
       "L 179.233353 288.229956 \n",
       "L 179.299648 287.285624 \n",
       "L 179.365943 284.616399 \n",
       "L 179.432238 285.729536 \n",
       "L 179.498533 285.207058 \n",
       "L 179.564828 286.898465 \n",
       "L 179.631123 286.589804 \n",
       "L 179.697418 286.630969 \n",
       "L 179.763713 282.928656 \n",
       "L 179.830008 287.198629 \n",
       "L 179.962598 284.213373 \n",
       "L 180.028893 283.934518 \n",
       "L 180.161483 289.870432 \n",
       "L 180.227778 289.13621 \n",
       "L 180.294073 287.212454 \n",
       "L 180.360368 288.87724 \n",
       "L 180.426663 286.195571 \n",
       "L 180.492958 291.433017 \n",
       "L 180.559253 285.247476 \n",
       "L 180.625548 287.901368 \n",
       "L 180.758138 279.735432 \n",
       "L 180.890728 287.371364 \n",
       "L 181.023318 283.208469 \n",
       "L 181.288498 287.30262 \n",
       "L 181.421087 285.015142 \n",
       "L 181.487382 288.870644 \n",
       "L 181.553677 286.621837 \n",
       "L 181.619972 284.688738 \n",
       "L 181.686267 289.040421 \n",
       "L 181.752562 288.950593 \n",
       "L 181.818857 287.391179 \n",
       "L 181.951447 290.894558 \n",
       "L 182.084037 283.487705 \n",
       "L 182.150332 285.216641 \n",
       "L 182.216627 278.854785 \n",
       "L 182.415512 287.797095 \n",
       "L 182.481807 290.440346 \n",
       "L 182.614397 284.375609 \n",
       "L 182.680692 287.808003 \n",
       "L 182.746987 283.524332 \n",
       "L 182.813282 289.52892 \n",
       "L 182.879577 288.049836 \n",
       "L 182.945872 284.257046 \n",
       "L 183.012167 288.610956 \n",
       "L 183.078462 286.545891 \n",
       "L 183.211052 289.372814 \n",
       "L 183.277347 288.589295 \n",
       "L 183.343642 288.677122 \n",
       "L 183.476232 285.248167 \n",
       "L 183.542527 287.164453 \n",
       "L 183.608822 284.067357 \n",
       "L 183.675117 289.593705 \n",
       "L 183.741412 282.610694 \n",
       "L 183.807707 284.013889 \n",
       "L 183.874002 286.842756 \n",
       "L 183.940297 283.665443 \n",
       "L 184.006592 287.559715 \n",
       "L 184.072887 281.490328 \n",
       "L 184.139182 286.18472 \n",
       "L 184.205477 284.954317 \n",
       "L 184.271772 282.028011 \n",
       "L 184.404362 287.268881 \n",
       "L 184.536952 284.338559 \n",
       "L 184.603247 287.036293 \n",
       "L 184.669542 281.072912 \n",
       "L 184.735837 288.396561 \n",
       "L 184.802132 284.580575 \n",
       "L 184.868427 287.206887 \n",
       "L 184.934722 285.716824 \n",
       "L 185.001017 287.793261 \n",
       "L 185.133607 285.641963 \n",
       "L 185.199902 285.902835 \n",
       "L 185.332492 280.113092 \n",
       "L 185.398787 288.107518 \n",
       "L 185.465082 287.885725 \n",
       "L 185.531377 288.571707 \n",
       "L 185.597672 287.732338 \n",
       "L 185.663967 290.271923 \n",
       "L 185.796557 284.61413 \n",
       "L 185.862852 286.457726 \n",
       "L 185.929147 285.832047 \n",
       "L 185.995442 289.158716 \n",
       "L 186.061737 286.413841 \n",
       "L 186.128032 287.533376 \n",
       "L 186.260622 284.480319 \n",
       "L 186.326917 287.408978 \n",
       "L 186.393212 283.641147 \n",
       "L 186.525802 289.716299 \n",
       "L 186.658392 283.505574 \n",
       "L 186.790982 286.556869 \n",
       "L 186.857277 284.646262 \n",
       "L 187.056162 287.224052 \n",
       "L 187.188752 288.566944 \n",
       "L 187.255047 286.033645 \n",
       "L 187.321342 289.083629 \n",
       "L 187.387637 287.423508 \n",
       "L 187.453932 284.931811 \n",
       "L 187.520226 285.750224 \n",
       "L 187.586521 289.031782 \n",
       "L 187.652816 287.182253 \n",
       "L 187.851701 286.390673 \n",
       "L 187.917996 284.578306 \n",
       "L 188.050586 288.466575 \n",
       "L 188.249471 284.251733 \n",
       "L 188.315766 289.364542 \n",
       "L 188.382061 287.474665 \n",
       "L 188.448356 290.024114 \n",
       "L 188.514651 281.470725 \n",
       "L 188.580946 285.547217 \n",
       "L 188.647241 283.938182 \n",
       "L 188.713536 286.760384 \n",
       "L 188.846126 281.260051 \n",
       "L 189.111306 290.052399 \n",
       "L 189.177601 282.836207 \n",
       "L 189.243896 284.088397 \n",
       "L 189.310191 284.033421 \n",
       "L 189.376486 286.737088 \n",
       "L 189.442781 284.724012 \n",
       "L 189.509076 285.487308 \n",
       "L 189.575371 288.355706 \n",
       "L 189.641666 284.936969 \n",
       "L 189.707961 287.539858 \n",
       "L 189.774256 286.785962 \n",
       "L 189.840551 287.252886 \n",
       "L 189.906846 289.633686 \n",
       "L 189.973141 283.74053 \n",
       "L 190.039436 286.906259 \n",
       "L 190.105731 286.427483 \n",
       "L 190.238321 282.816844 \n",
       "L 190.370911 287.719486 \n",
       "L 190.437206 287.393969 \n",
       "L 190.702386 280.327444 \n",
       "L 190.768681 281.80495 \n",
       "L 190.834976 288.930468 \n",
       "L 190.901271 281.625943 \n",
       "L 190.967566 282.684498 \n",
       "L 191.166451 288.63583 \n",
       "L 191.232746 284.024965 \n",
       "L 191.299041 290.26327 \n",
       "L 191.365336 288.998748 \n",
       "L 191.431631 289.361695 \n",
       "L 191.497926 290.504032 \n",
       "L 191.696811 284.807653 \n",
       "L 191.763106 283.327413 \n",
       "L 191.829401 289.01876 \n",
       "L 191.895696 288.527667 \n",
       "L 192.028286 284.905077 \n",
       "L 192.094581 288.986135 \n",
       "L 192.160876 287.106405 \n",
       "L 192.227171 283.244617 \n",
       "L 192.293466 285.303002 \n",
       "L 192.359761 287.123753 \n",
       "L 192.426056 285.752775 \n",
       "L 192.492351 288.678334 \n",
       "L 192.558646 284.513226 \n",
       "L 192.624941 285.886671 \n",
       "L 192.757531 283.546514 \n",
       "L 192.890121 291.075764 \n",
       "L 193.089006 281.591951 \n",
       "L 193.155301 288.156293 \n",
       "L 193.221596 284.726915 \n",
       "L 193.287891 287.595342 \n",
       "L 193.354186 286.786075 \n",
       "L 193.486776 286.07092 \n",
       "L 193.619365 289.670765 \n",
       "L 193.68566 285.777381 \n",
       "L 193.751955 287.894518 \n",
       "L 193.884545 284.614666 \n",
       "L 193.95084 289.058403 \n",
       "L 194.08343 285.042595 \n",
       "L 194.282315 289.10102 \n",
       "L 194.34861 285.836726 \n",
       "L 194.414905 289.741821 \n",
       "L 194.4812 284.961871 \n",
       "L 194.547495 286.804833 \n",
       "L 194.61379 287.646034 \n",
       "L 194.680085 282.585284 \n",
       "L 194.812675 291.222935 \n",
       "L 195.01156 282.245069 \n",
       "L 195.077855 283.931868 \n",
       "L 195.14415 289.07406 \n",
       "L 195.210445 286.275238 \n",
       "L 195.27674 283.506659 \n",
       "L 195.343035 285.219587 \n",
       "L 195.40933 287.469831 \n",
       "L 195.475625 286.627207 \n",
       "L 195.54192 286.676264 \n",
       "L 195.608215 289.913584 \n",
       "L 195.67451 283.371185 \n",
       "L 195.740805 286.053177 \n",
       "L 195.8071 282.651098 \n",
       "L 195.873395 287.34098 \n",
       "L 195.93969 282.165543 \n",
       "L 196.005985 288.821629 \n",
       "L 196.07228 287.661536 \n",
       "L 196.20487 284.290263 \n",
       "L 196.33746 289.067183 \n",
       "L 196.403755 287.633266 \n",
       "L 196.47005 286.900551 \n",
       "L 196.536345 282.916142 \n",
       "L 196.60264 283.749831 \n",
       "L 196.801525 289.5012 \n",
       "L 196.934115 284.179169 \n",
       "L 197.066705 288.386668 \n",
       "L 197.133 284.603744 \n",
       "L 197.26559 288.063985 \n",
       "L 197.331885 288.427918 \n",
       "L 197.464475 284.22669 \n",
       "L 197.597065 290.117352 \n",
       "L 197.79595 285.718924 \n",
       "L 197.862245 284.582717 \n",
       "L 197.92854 288.920533 \n",
       "L 197.994835 288.066014 \n",
       "L 198.06113 289.462628 \n",
       "L 198.19372 286.918646 \n",
       "L 198.260015 281.575096 \n",
       "L 198.4589 291.082289 \n",
       "L 198.59149 285.822999 \n",
       "L 198.72408 288.870898 \n",
       "L 198.85667 283.667346 \n",
       "L 198.922965 287.146626 \n",
       "L 198.98926 283.232046 \n",
       "L 199.12185 289.157856 \n",
       "L 199.188145 286.548555 \n",
       "L 199.25444 288.003851 \n",
       "L 199.320735 286.762625 \n",
       "L 199.38703 282.419679 \n",
       "L 199.453325 290.875772 \n",
       "L 199.51962 284.89648 \n",
       "L 199.585915 287.195852 \n",
       "L 199.65221 287.458585 \n",
       "L 199.718504 285.265501 \n",
       "L 199.784799 289.079317 \n",
       "L 199.851094 284.456375 \n",
       "L 199.917389 287.679673 \n",
       "L 199.983684 285.60301 \n",
       "L 200.049979 286.492916 \n",
       "L 200.182569 289.343487 \n",
       "L 200.248864 288.36753 \n",
       "L 200.315159 282.489777 \n",
       "L 200.381454 290.550594 \n",
       "L 200.447749 285.480586 \n",
       "L 200.514044 286.751252 \n",
       "L 200.580339 286.584548 \n",
       "L 200.712929 285.459897 \n",
       "L 200.845519 289.382341 \n",
       "L 200.978109 286.23358 \n",
       "L 201.044404 290.290553 \n",
       "L 201.110699 287.222925 \n",
       "L 201.176994 289.523678 \n",
       "L 201.243289 286.779141 \n",
       "L 201.309584 287.890728 \n",
       "L 201.375879 289.663549 \n",
       "L 201.442174 285.773956 \n",
       "L 201.508469 287.064944 \n",
       "L 201.574764 288.537447 \n",
       "L 201.641059 288.138353 \n",
       "L 201.707354 285.7582 \n",
       "L 201.773649 288.916291 \n",
       "L 201.839944 287.206662 \n",
       "L 201.972534 286.703759 \n",
       "L 202.038829 285.04375 \n",
       "L 202.105124 288.526483 \n",
       "L 202.171419 282.342267 \n",
       "L 202.237714 285.373297 \n",
       "L 202.304009 287.668089 \n",
       "L 202.370304 286.280932 \n",
       "L 202.436599 288.024356 \n",
       "L 202.502894 286.871971 \n",
       "L 202.569189 287.934782 \n",
       "L 202.635484 282.715812 \n",
       "L 202.834369 289.749191 \n",
       "L 202.900664 288.440616 \n",
       "L 202.966959 289.935893 \n",
       "L 203.099549 283.807837 \n",
       "L 203.232139 291.589419 \n",
       "L 203.364729 286.832736 \n",
       "L 203.563614 284.343492 \n",
       "L 203.629909 287.377298 \n",
       "L 203.696204 285.39011 \n",
       "L 203.828794 291.868316 \n",
       "L 203.961384 285.175265 \n",
       "L 204.027679 286.183677 \n",
       "L 204.093974 286.680463 \n",
       "L 204.160269 288.132575 \n",
       "L 204.226564 283.540228 \n",
       "L 204.292859 287.00395 \n",
       "L 204.425449 283.897975 \n",
       "L 204.491744 288.331523 \n",
       "L 204.558039 287.790978 \n",
       "L 204.690629 283.768885 \n",
       "L 204.756924 287.877607 \n",
       "L 204.823219 285.562254 \n",
       "L 204.889514 282.140092 \n",
       "L 204.955809 285.726012 \n",
       "L 205.022104 283.732652 \n",
       "L 205.088399 284.272985 \n",
       "L 205.154694 289.060263 \n",
       "L 205.220989 283.174985 \n",
       "L 205.287284 289.921955 \n",
       "L 205.353579 285.703379 \n",
       "L 205.419874 290.56277 \n",
       "L 205.486169 284.346846 \n",
       "L 205.552464 288.025666 \n",
       "L 205.685054 283.51558 \n",
       "L 205.751349 286.715118 \n",
       "L 205.817643 284.569766 \n",
       "L 205.950233 290.442206 \n",
       "L 206.082823 286.891771 \n",
       "L 206.149118 288.812103 \n",
       "L 206.215413 288.173289 \n",
       "L 206.281708 283.214726 \n",
       "L 206.348003 284.400314 \n",
       "L 206.414298 287.67856 \n",
       "L 206.480593 285.959305 \n",
       "L 206.546888 286.160114 \n",
       "L 206.613183 283.979742 \n",
       "L 206.812068 289.760987 \n",
       "L 206.944658 283.112314 \n",
       "L 207.010953 287.593947 \n",
       "L 207.077248 286.61582 \n",
       "L 207.143543 286.605067 \n",
       "L 207.209838 284.566652 \n",
       "L 207.276133 285.508941 \n",
       "L 207.342428 290.32408 \n",
       "L 207.408723 285.887051 \n",
       "L 207.475018 286.898381 \n",
       "L 207.607608 287.015281 \n",
       "L 207.673903 289.011432 \n",
       "L 207.806493 283.58393 \n",
       "L 207.872788 288.013349 \n",
       "L 207.939083 286.526457 \n",
       "L 208.005378 286.8375 \n",
       "L 208.071673 288.939826 \n",
       "L 208.270558 285.63493 \n",
       "L 208.336853 285.976653 \n",
       "L 208.403148 287.602529 \n",
       "L 208.469443 283.530575 \n",
       "L 208.535738 287.954371 \n",
       "L 208.602033 287.577726 \n",
       "L 208.668328 287.954498 \n",
       "L 208.734623 287.053627 \n",
       "L 208.800918 287.460276 \n",
       "L 208.867213 287.75693 \n",
       "L 208.999803 285.392181 \n",
       "L 209.198688 288.718724 \n",
       "L 209.264983 283.328005 \n",
       "L 209.331278 289.348983 \n",
       "L 209.397573 285.10015 \n",
       "L 209.463868 287.820531 \n",
       "L 209.596458 286.327579 \n",
       "L 209.662753 286.504881 \n",
       "L 209.729048 284.1602 \n",
       "L 209.795343 287.43629 \n",
       "L 209.927933 284.334698 \n",
       "L 210.060523 288.692173 \n",
       "L 210.126818 287.054769 \n",
       "L 210.193113 290.827828 \n",
       "L 210.259408 283.59512 \n",
       "L 210.325703 289.184999 \n",
       "L 210.391998 288.670568 \n",
       "L 210.458293 287.071286 \n",
       "L 210.524588 290.875152 \n",
       "L 210.657178 284.884685 \n",
       "L 210.789768 288.508867 \n",
       "L 210.856063 288.864598 \n",
       "L 210.922358 288.193146 \n",
       "L 210.988653 290.259972 \n",
       "L 211.253833 284.045372 \n",
       "L 211.386423 289.804055 \n",
       "L 211.519013 287.473988 \n",
       "L 211.585308 290.048551 \n",
       "L 211.651603 285.309006 \n",
       "L 211.717898 286.272053 \n",
       "L 211.850488 289.535431 \n",
       "L 211.916783 285.797759 \n",
       "L 212.049372 289.880128 \n",
       "L 212.115667 284.268419 \n",
       "L 212.181962 288.899478 \n",
       "L 212.248257 286.698375 \n",
       "L 212.314552 285.576769 \n",
       "L 212.380847 288.06872 \n",
       "L 212.447142 285.150842 \n",
       "L 212.513437 286.564381 \n",
       "L 212.579732 286.95192 \n",
       "L 212.646027 286.883922 \n",
       "L 212.712322 286.940363 \n",
       "L 212.778617 285.13059 \n",
       "L 212.844912 290.160209 \n",
       "L 212.911207 287.12288 \n",
       "L 212.977502 287.080559 \n",
       "L 213.043797 288.619017 \n",
       "L 213.110092 285.291065 \n",
       "L 213.176387 286.794742 \n",
       "L 213.242682 286.245488 \n",
       "L 213.308977 289.279365 \n",
       "L 213.308977 289.279365 \n",
       "\" clip-path=\"url(#p3c3172a232)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 50.14375 299.078125 \n",
       "L 50.14375 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 221.07875 299.078125 \n",
       "L 221.07875 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 50.14375 299.078125 \n",
       "L 221.07875 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 50.14375 189.718125 \n",
       "L 221.07875 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_38\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(105.30375 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_3\">\n",
       "    <g id=\"patch_19\">\n",
       "     <path d=\"M 134.488125 227.630625 \n",
       "L 214.07875 227.630625 \n",
       "Q 216.07875 227.630625 216.07875 225.630625 \n",
       "L 216.07875 196.718125 \n",
       "Q 216.07875 194.718125 214.07875 194.718125 \n",
       "L 134.488125 194.718125 \n",
       "Q 132.488125 194.718125 132.488125 196.718125 \n",
       "L 132.488125 225.630625 \n",
       "Q 132.488125 227.630625 134.488125 227.630625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_36\">\n",
       "     <path d=\"M 136.488125 202.816562 \n",
       "L 146.488125 202.816562 \n",
       "L 156.488125 202.816562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_39\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(164.488125 206.316562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_37\">\n",
       "     <path d=\"M 136.488125 217.772812 \n",
       "L 146.488125 217.772812 \n",
       "L 156.488125 217.772812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_40\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(164.488125 221.272812) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 275.09875 299.078125 \n",
       "L 446.03375 299.078125 \n",
       "L 446.03375 189.718125 \n",
       "L 275.09875 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_7\">\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_38\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"282.868523\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_41\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(279.687273 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"349.163512\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_42\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(336.438512 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_40\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4a0ac97568\" x=\"415.458501\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_43\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(402.733501 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_44\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(349.750625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_8\">\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"276.262927\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_45\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(245.833125 280.062146) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_42\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"249.071629\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_46\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(245.833125 252.870848) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_43\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"221.880332\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_47\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(245.833125 225.67955) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_44\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf841f9edc8\" x=\"275.09875\" y=\"194.689034\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_48\">\n",
       "      <!-- 1.00 -->\n",
       "      <g transform=\"translate(245.833125 198.488253) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_49\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(239.753437 252.960625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_45\">\n",
       "    <path d=\"M 282.868523 208.284683 \n",
       "L 283.001113 215.932235 \n",
       "L 283.067408 213.383051 \n",
       "L 283.133703 216.781963 \n",
       "L 283.199998 214.232779 \n",
       "L 283.266293 204.885771 \n",
       "L 283.465178 218.481419 \n",
       "L 283.664063 208.284683 \n",
       "L 283.862948 218.481419 \n",
       "L 283.995538 210.833867 \n",
       "L 284.128128 211.683595 \n",
       "L 284.194423 217.631691 \n",
       "L 284.260717 215.082507 \n",
       "L 284.393307 209.984139 \n",
       "L 284.459602 213.383051 \n",
       "L 284.525897 207.434955 \n",
       "L 284.592192 208.284683 \n",
       "L 284.791077 222.73006 \n",
       "L 284.857372 221.880332 \n",
       "L 284.989962 211.683595 \n",
       "L 285.056257 219.331148 \n",
       "L 285.122552 216.781963 \n",
       "L 285.255142 208.284683 \n",
       "L 285.387732 215.082507 \n",
       "L 285.454027 209.984139 \n",
       "L 285.520322 214.232779 \n",
       "L 285.652912 204.885771 \n",
       "L 285.785502 211.683595 \n",
       "L 285.851797 208.284683 \n",
       "L 285.984387 213.383051 \n",
       "L 286.050682 211.683595 \n",
       "L 286.116977 206.585227 \n",
       "L 286.183272 209.984139 \n",
       "L 286.249567 213.383051 \n",
       "L 286.315862 203.186315 \n",
       "L 286.382157 211.683595 \n",
       "L 286.448452 206.585227 \n",
       "L 286.514747 204.885771 \n",
       "L 286.581042 205.735499 \n",
       "L 286.647337 205.735499 \n",
       "L 286.713632 210.833867 \n",
       "L 286.779927 209.134411 \n",
       "L 286.846222 204.036043 \n",
       "L 286.912517 213.383051 \n",
       "L 286.978812 209.134411 \n",
       "L 287.045107 209.134411 \n",
       "L 287.243992 205.735499 \n",
       "L 287.310287 207.434955 \n",
       "L 287.442877 203.186315 \n",
       "L 287.509172 209.984139 \n",
       "L 287.575467 207.434955 \n",
       "L 287.708057 205.735499 \n",
       "L 287.906942 216.781963 \n",
       "L 287.973237 215.082507 \n",
       "L 288.105827 199.787402 \n",
       "L 288.172122 204.036043 \n",
       "L 288.304712 204.036043 \n",
       "L 288.371007 209.134411 \n",
       "L 288.503597 198.937674 \n",
       "L 288.702482 211.683595 \n",
       "L 288.901367 202.336587 \n",
       "L 289.100252 210.833867 \n",
       "L 289.232842 203.186315 \n",
       "L 289.299137 206.585227 \n",
       "L 289.365432 203.186315 \n",
       "L 289.431727 206.585227 \n",
       "L 289.630612 200.63713 \n",
       "L 289.696907 205.735499 \n",
       "L 289.763202 199.787402 \n",
       "L 289.829497 202.336587 \n",
       "L 289.895792 203.186315 \n",
       "L 289.962087 202.336587 \n",
       "L 290.028382 215.082507 \n",
       "L 290.094677 209.134411 \n",
       "L 290.160972 205.735499 \n",
       "L 290.227267 206.585227 \n",
       "L 290.293562 211.683595 \n",
       "L 290.359857 209.134411 \n",
       "L 290.492446 202.336587 \n",
       "L 290.558741 203.186315 \n",
       "L 290.625036 208.284683 \n",
       "L 290.823921 195.538762 \n",
       "L 290.890216 195.538762 \n",
       "L 291.221691 207.434955 \n",
       "L 291.354281 199.787402 \n",
       "L 291.420576 200.63713 \n",
       "L 291.486871 206.585227 \n",
       "L 291.553166 199.787402 \n",
       "L 291.619461 202.336587 \n",
       "L 291.752051 198.087946 \n",
       "L 291.884641 197.238218 \n",
       "L 291.950936 206.585227 \n",
       "L 292.017231 197.238218 \n",
       "L 292.083526 198.937674 \n",
       "L 292.149821 215.932235 \n",
       "L 292.216116 198.937674 \n",
       "L 292.282411 199.787402 \n",
       "L 292.415001 201.486858 \n",
       "L 292.613886 194.689034 \n",
       "L 292.680181 195.538762 \n",
       "L 292.746476 203.186315 \n",
       "L 292.812771 198.937674 \n",
       "L 292.879066 195.538762 \n",
       "L 293.210541 211.683595 \n",
       "L 293.343131 198.937674 \n",
       "L 293.542016 205.735499 \n",
       "L 293.608311 206.585227 \n",
       "L 293.740901 199.787402 \n",
       "L 293.939786 209.134411 \n",
       "L 294.138671 200.63713 \n",
       "L 294.337556 209.134411 \n",
       "L 294.470146 202.336587 \n",
       "L 294.536441 205.735499 \n",
       "L 294.602736 199.787402 \n",
       "L 294.669031 205.735499 \n",
       "L 294.735326 202.336587 \n",
       "L 294.801621 204.036043 \n",
       "L 294.867916 198.937674 \n",
       "L 294.934211 203.186315 \n",
       "L 295.000506 199.787402 \n",
       "L 295.066801 200.63713 \n",
       "L 295.133096 204.036043 \n",
       "L 295.199391 198.937674 \n",
       "L 295.265686 213.383051 \n",
       "L 295.331981 208.284683 \n",
       "L 295.464571 204.036043 \n",
       "L 295.530866 208.284683 \n",
       "L 295.597161 206.585227 \n",
       "L 295.729751 198.937674 \n",
       "L 295.862341 204.885771 \n",
       "L 296.127521 196.38849 \n",
       "L 296.193816 197.238218 \n",
       "L 296.458996 205.735499 \n",
       "L 296.591585 199.787402 \n",
       "L 296.65788 199.787402 \n",
       "L 296.724175 206.585227 \n",
       "L 296.92306 196.38849 \n",
       "L 296.989355 195.538762 \n",
       "L 297.18824 199.787402 \n",
       "L 297.254535 195.538762 \n",
       "L 297.32083 197.238218 \n",
       "L 297.387125 211.683595 \n",
       "L 297.45342 198.937674 \n",
       "L 297.519715 199.787402 \n",
       "L 297.58601 199.787402 \n",
       "L 297.652305 202.336587 \n",
       "L 297.85119 194.689034 \n",
       "L 297.917485 194.689034 \n",
       "L 297.98378 203.186315 \n",
       "L 298.050075 199.787402 \n",
       "L 298.11637 195.538762 \n",
       "L 298.182665 197.238218 \n",
       "L 298.24896 198.087946 \n",
       "L 298.38155 209.984139 \n",
       "L 298.447845 209.134411 \n",
       "L 298.580435 198.087946 \n",
       "L 298.845615 204.885771 \n",
       "L 298.978205 199.787402 \n",
       "L 299.17709 209.134411 \n",
       "L 299.375975 202.336587 \n",
       "L 299.44227 206.585227 \n",
       "L 299.508565 203.186315 \n",
       "L 299.57486 206.585227 \n",
       "L 299.641155 204.885771 \n",
       "L 299.84004 198.937674 \n",
       "L 299.906335 202.336587 \n",
       "L 299.97263 198.087946 \n",
       "L 300.038925 202.336587 \n",
       "L 300.10522 199.787402 \n",
       "L 300.171515 203.186315 \n",
       "L 300.23781 201.486858 \n",
       "L 300.304105 198.937674 \n",
       "L 300.50299 215.082507 \n",
       "L 300.701875 204.885771 \n",
       "L 300.76817 204.885771 \n",
       "L 300.967055 201.486858 \n",
       "L 301.099645 204.036043 \n",
       "L 301.29853 196.38849 \n",
       "L 301.364825 196.38849 \n",
       "L 301.497415 198.937674 \n",
       "L 301.56371 196.38849 \n",
       "L 301.6963 204.885771 \n",
       "L 301.82889 198.087946 \n",
       "L 301.895185 198.937674 \n",
       "L 301.96148 206.585227 \n",
       "L 302.027775 201.486858 \n",
       "L 302.22666 196.38849 \n",
       "L 302.292955 197.238218 \n",
       "L 302.35925 195.538762 \n",
       "L 302.425545 200.63713 \n",
       "L 302.49184 194.689034 \n",
       "L 302.558135 196.38849 \n",
       "L 302.624429 208.284683 \n",
       "L 302.690724 198.087946 \n",
       "L 302.757019 199.787402 \n",
       "L 302.823314 201.486858 \n",
       "L 302.889609 200.63713 \n",
       "L 303.088494 194.689034 \n",
       "L 303.154789 194.689034 \n",
       "L 303.221084 203.186315 \n",
       "L 303.353674 196.38849 \n",
       "L 303.486264 198.087946 \n",
       "L 303.618854 208.284683 \n",
       "L 303.685149 205.735499 \n",
       "L 303.751444 194.689034 \n",
       "L 303.817739 198.087946 \n",
       "L 304.082919 204.885771 \n",
       "L 304.215509 199.787402 \n",
       "L 304.414394 208.284683 \n",
       "L 304.480689 202.336587 \n",
       "L 304.546984 203.186315 \n",
       "L 304.613279 200.63713 \n",
       "L 304.679574 204.885771 \n",
       "L 304.745869 202.336587 \n",
       "L 304.812164 204.036043 \n",
       "L 304.878459 203.186315 \n",
       "L 304.944754 202.336587 \n",
       "L 305.011049 203.186315 \n",
       "L 305.077344 198.087946 \n",
       "L 305.143639 203.186315 \n",
       "L 305.209934 198.937674 \n",
       "L 305.276229 202.336587 \n",
       "L 305.342524 198.937674 \n",
       "L 305.408819 203.186315 \n",
       "L 305.475114 201.486858 \n",
       "L 305.541409 198.937674 \n",
       "L 305.607704 203.186315 \n",
       "L 305.673999 198.937674 \n",
       "L 305.740294 211.683595 \n",
       "L 305.806589 207.434955 \n",
       "L 306.005474 201.486858 \n",
       "L 306.071769 204.885771 \n",
       "L 306.138064 203.186315 \n",
       "L 306.204359 199.787402 \n",
       "L 306.336949 204.036043 \n",
       "L 306.535834 195.538762 \n",
       "L 306.734719 198.937674 \n",
       "L 306.801014 197.238218 \n",
       "L 306.933604 203.186315 \n",
       "L 307.066194 198.087946 \n",
       "L 307.132489 198.937674 \n",
       "L 307.198784 204.885771 \n",
       "L 307.397669 196.38849 \n",
       "L 307.530259 196.38849 \n",
       "L 307.596554 195.538762 \n",
       "L 307.662849 200.63713 \n",
       "L 307.729144 194.689034 \n",
       "L 307.795439 196.38849 \n",
       "L 307.861734 207.434955 \n",
       "L 307.928029 198.087946 \n",
       "L 307.994324 198.937674 \n",
       "L 308.060619 200.63713 \n",
       "L 308.126914 199.787402 \n",
       "L 308.325799 194.689034 \n",
       "L 308.392094 195.538762 \n",
       "L 308.458389 203.186315 \n",
       "L 308.590979 196.38849 \n",
       "L 308.657274 196.38849 \n",
       "L 308.723568 197.238218 \n",
       "L 308.856158 208.284683 \n",
       "L 308.988748 194.689034 \n",
       "L 308.988748 194.689034 \n",
       "\" clip-path=\"url(#pecab2c8bb3)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_46\">\n",
       "    <path d=\"M 282.868523 293.257488 \n",
       "L 282.934818 288.15912 \n",
       "L 283.001113 294.107216 \n",
       "L 283.067408 290.708304 \n",
       "L 283.199998 279.661839 \n",
       "L 283.266293 286.459663 \n",
       "L 283.332588 272.864015 \n",
       "L 283.398883 276.262927 \n",
       "L 283.597768 269.465102 \n",
       "L 283.664063 272.014287 \n",
       "L 283.730358 262.667278 \n",
       "L 283.796653 276.262927 \n",
       "L 283.862948 271.164559 \n",
       "L 283.929243 257.56891 \n",
       "L 284.128128 277.112655 \n",
       "L 284.194423 272.864015 \n",
       "L 284.260717 276.262927 \n",
       "L 284.327012 275.413199 \n",
       "L 284.525897 260.118094 \n",
       "L 284.592192 266.915918 \n",
       "L 284.724782 247.372173 \n",
       "L 284.791077 260.118094 \n",
       "L 284.857372 257.56891 \n",
       "L 284.989962 247.372173 \n",
       "L 285.056257 249.921357 \n",
       "L 285.122552 248.221901 \n",
       "L 285.188847 242.273805 \n",
       "L 285.255142 245.672717 \n",
       "L 285.387732 251.620813 \n",
       "L 285.520322 237.175437 \n",
       "L 285.652912 243.123533 \n",
       "L 285.851797 250.771085 \n",
       "L 285.918092 242.273805 \n",
       "L 285.984387 249.921357 \n",
       "L 286.116977 239.724621 \n",
       "L 286.183272 239.724621 \n",
       "L 286.249567 242.273805 \n",
       "L 286.315862 234.626252 \n",
       "L 286.382157 243.973261 \n",
       "L 286.448452 239.724621 \n",
       "L 286.514747 240.574349 \n",
       "L 286.713632 227.828428 \n",
       "L 286.779927 232.077068 \n",
       "L 286.978812 222.73006 \n",
       "L 287.045107 233.776524 \n",
       "L 287.111402 230.377612 \n",
       "L 287.243992 223.579788 \n",
       "L 287.310287 230.377612 \n",
       "L 287.376582 226.9787 \n",
       "L 287.442877 227.828428 \n",
       "L 287.509172 221.030604 \n",
       "L 287.575467 232.077068 \n",
       "L 287.641762 222.73006 \n",
       "L 287.708057 226.9787 \n",
       "L 287.774352 220.180876 \n",
       "L 287.840647 227.828428 \n",
       "L 287.906942 219.331148 \n",
       "L 287.973237 225.279244 \n",
       "L 288.105827 214.232779 \n",
       "L 288.172122 215.082507 \n",
       "L 288.238417 223.579788 \n",
       "L 288.371007 215.082507 \n",
       "L 288.437302 219.331148 \n",
       "L 288.503597 214.232779 \n",
       "L 288.569892 217.631691 \n",
       "L 288.636187 221.030604 \n",
       "L 288.702482 218.481419 \n",
       "L 288.768777 222.73006 \n",
       "L 288.901367 218.481419 \n",
       "L 288.967662 218.481419 \n",
       "L 289.033957 222.73006 \n",
       "L 289.100252 213.383051 \n",
       "L 289.166547 222.73006 \n",
       "L 289.232842 221.880332 \n",
       "L 289.431727 212.533323 \n",
       "L 289.498022 219.331148 \n",
       "L 289.564317 216.781963 \n",
       "L 289.630612 212.533323 \n",
       "L 289.696907 221.030604 \n",
       "L 289.763202 215.932235 \n",
       "L 289.829497 218.481419 \n",
       "L 289.962087 214.232779 \n",
       "L 290.028382 218.481419 \n",
       "L 290.094677 212.533323 \n",
       "L 290.227267 218.481419 \n",
       "L 290.359857 215.082507 \n",
       "L 290.426151 218.481419 \n",
       "L 290.558741 212.533323 \n",
       "L 290.625036 221.030604 \n",
       "L 290.691331 209.984139 \n",
       "L 290.757626 215.082507 \n",
       "L 290.823921 214.232779 \n",
       "L 290.890216 217.631691 \n",
       "L 291.022806 213.383051 \n",
       "L 291.089101 214.232779 \n",
       "L 291.155396 222.73006 \n",
       "L 291.221691 213.383051 \n",
       "L 291.287986 215.932235 \n",
       "L 291.354281 210.833867 \n",
       "L 291.553166 218.481419 \n",
       "L 291.619461 216.781963 \n",
       "L 291.685756 210.833867 \n",
       "L 291.752051 218.481419 \n",
       "L 291.818346 212.533323 \n",
       "L 291.884641 223.579788 \n",
       "L 291.950936 219.331148 \n",
       "L 292.017231 207.434955 \n",
       "L 292.083526 212.533323 \n",
       "L 292.149821 209.984139 \n",
       "L 292.216116 217.631691 \n",
       "L 292.282411 216.781963 \n",
       "L 292.415001 206.585227 \n",
       "L 292.547591 219.331148 \n",
       "L 292.613886 215.082507 \n",
       "L 292.680181 207.434955 \n",
       "L 292.746476 209.134411 \n",
       "L 292.945361 219.331148 \n",
       "L 293.077951 211.683595 \n",
       "L 293.144246 218.481419 \n",
       "L 293.343131 209.134411 \n",
       "L 293.409426 206.585227 \n",
       "L 293.608311 218.481419 \n",
       "L 293.674606 207.434955 \n",
       "L 293.807196 221.880332 \n",
       "L 293.873491 207.434955 \n",
       "L 293.939786 210.833867 \n",
       "L 294.006081 215.932235 \n",
       "L 294.072376 210.833867 \n",
       "L 294.138671 218.481419 \n",
       "L 294.204966 205.735499 \n",
       "L 294.271261 218.481419 \n",
       "L 294.337556 212.533323 \n",
       "L 294.403851 215.082507 \n",
       "L 294.470146 210.833867 \n",
       "L 294.536441 222.73006 \n",
       "L 294.602736 210.833867 \n",
       "L 294.669031 212.533323 \n",
       "L 294.735326 216.781963 \n",
       "L 294.934211 208.284683 \n",
       "L 295.000506 218.481419 \n",
       "L 295.066801 217.631691 \n",
       "L 295.199391 215.082507 \n",
       "L 295.265686 221.880332 \n",
       "L 295.331981 216.781963 \n",
       "L 295.398276 220.180876 \n",
       "L 295.464571 219.331148 \n",
       "L 295.597161 213.383051 \n",
       "L 295.663456 219.331148 \n",
       "L 295.796046 209.134411 \n",
       "L 295.928636 217.631691 \n",
       "L 296.061226 215.932235 \n",
       "L 296.127521 209.134411 \n",
       "L 296.260111 220.180876 \n",
       "L 296.326406 216.781963 \n",
       "L 296.392701 221.030604 \n",
       "L 296.458996 213.383051 \n",
       "L 296.52529 215.082507 \n",
       "L 296.591585 217.631691 \n",
       "L 296.65788 210.833867 \n",
       "L 296.724175 216.781963 \n",
       "L 296.79047 213.383051 \n",
       "L 297.05565 208.284683 \n",
       "L 297.18824 221.030604 \n",
       "L 297.32083 208.284683 \n",
       "L 297.387125 211.683595 \n",
       "L 297.45342 213.383051 \n",
       "L 297.519715 220.180876 \n",
       "L 297.58601 218.481419 \n",
       "L 297.784895 209.984139 \n",
       "L 297.917485 215.932235 \n",
       "L 298.182665 208.284683 \n",
       "L 298.24896 219.331148 \n",
       "L 298.315255 218.481419 \n",
       "L 298.38155 207.434955 \n",
       "L 298.447845 221.030604 \n",
       "L 298.51414 207.434955 \n",
       "L 298.580435 211.683595 \n",
       "L 298.64673 209.984139 \n",
       "L 298.91191 216.781963 \n",
       "L 298.978205 215.932235 \n",
       "L 299.110795 212.533323 \n",
       "L 299.17709 213.383051 \n",
       "L 299.243385 216.781963 \n",
       "L 299.44227 209.134411 \n",
       "L 299.508565 221.030604 \n",
       "L 299.57486 212.533323 \n",
       "L 299.70745 221.030604 \n",
       "L 299.84004 209.134411 \n",
       "L 299.97263 218.481419 \n",
       "L 300.10522 209.134411 \n",
       "L 300.171515 211.683595 \n",
       "L 300.23781 210.833867 \n",
       "L 300.304105 215.932235 \n",
       "L 300.3704 210.833867 \n",
       "L 300.436695 216.781963 \n",
       "L 300.50299 213.383051 \n",
       "L 300.569285 215.082507 \n",
       "L 300.63558 209.134411 \n",
       "L 300.701875 212.533323 \n",
       "L 300.76817 213.383051 \n",
       "L 300.834465 209.984139 \n",
       "L 300.90076 219.331148 \n",
       "L 300.967055 211.683595 \n",
       "L 301.03335 217.631691 \n",
       "L 301.099645 208.284683 \n",
       "L 301.16594 211.683595 \n",
       "L 301.29853 209.984139 \n",
       "L 301.364825 206.585227 \n",
       "L 301.43112 214.232779 \n",
       "L 301.497415 212.533323 \n",
       "L 301.56371 215.082507 \n",
       "L 301.630005 213.383051 \n",
       "L 301.6963 212.533323 \n",
       "L 301.762595 218.481419 \n",
       "L 301.82889 209.984139 \n",
       "L 301.895185 217.631691 \n",
       "L 301.96148 213.383051 \n",
       "L 302.027775 204.885771 \n",
       "L 302.09407 215.082507 \n",
       "L 302.160365 211.683595 \n",
       "L 302.22666 213.383051 \n",
       "L 302.292955 223.579788 \n",
       "L 302.425545 211.683595 \n",
       "L 302.49184 208.284683 \n",
       "L 302.558135 212.533323 \n",
       "L 302.624429 206.585227 \n",
       "L 302.690724 209.984139 \n",
       "L 302.757019 214.232779 \n",
       "L 302.823314 213.383051 \n",
       "L 302.889609 211.683595 \n",
       "L 302.955904 218.481419 \n",
       "L 303.022199 208.284683 \n",
       "L 303.088494 217.631691 \n",
       "L 303.154789 212.533323 \n",
       "L 303.221084 211.683595 \n",
       "L 303.287379 204.036043 \n",
       "L 303.353674 212.533323 \n",
       "L 303.419969 204.885771 \n",
       "L 303.486264 217.631691 \n",
       "L 303.552559 213.383051 \n",
       "L 303.618854 215.932235 \n",
       "L 303.685149 207.434955 \n",
       "L 303.817739 216.781963 \n",
       "L 303.950329 207.434955 \n",
       "L 304.016624 209.984139 \n",
       "L 304.082919 217.631691 \n",
       "L 304.149214 214.232779 \n",
       "L 304.215509 219.331148 \n",
       "L 304.281804 215.932235 \n",
       "L 304.348099 212.533323 \n",
       "L 304.414394 216.781963 \n",
       "L 304.480689 209.134411 \n",
       "L 304.546984 217.631691 \n",
       "L 304.679574 208.284683 \n",
       "L 304.812164 217.631691 \n",
       "L 304.878459 208.284683 \n",
       "L 304.944754 218.481419 \n",
       "L 305.011049 209.134411 \n",
       "L 305.077344 220.180876 \n",
       "L 305.209934 210.833867 \n",
       "L 305.276229 210.833867 \n",
       "L 305.342524 207.434955 \n",
       "L 305.475114 217.631691 \n",
       "L 305.541409 206.585227 \n",
       "L 305.607704 209.134411 \n",
       "L 305.673999 219.331148 \n",
       "L 305.740294 214.232779 \n",
       "L 305.939179 208.284683 \n",
       "L 306.204359 221.030604 \n",
       "L 306.270654 209.134411 \n",
       "L 306.336949 218.481419 \n",
       "L 306.403244 210.833867 \n",
       "L 306.469539 212.533323 \n",
       "L 306.535834 211.683595 \n",
       "L 306.602129 219.331148 \n",
       "L 306.668424 218.481419 \n",
       "L 306.734719 210.833867 \n",
       "L 306.801014 214.232779 \n",
       "L 306.867309 215.932235 \n",
       "L 306.999899 213.383051 \n",
       "L 307.066194 213.383051 \n",
       "L 307.132489 212.533323 \n",
       "L 307.198784 209.984139 \n",
       "L 307.265079 218.481419 \n",
       "L 307.463964 206.585227 \n",
       "L 307.596554 216.781963 \n",
       "L 307.662849 215.082507 \n",
       "L 307.795439 209.984139 \n",
       "L 307.861734 215.932235 \n",
       "L 307.928029 208.284683 \n",
       "L 308.060619 215.932235 \n",
       "L 308.126914 209.984139 \n",
       "L 308.193209 214.232779 \n",
       "L 308.259504 211.683595 \n",
       "L 308.325799 212.533323 \n",
       "L 308.392094 216.781963 \n",
       "L 308.590979 204.036043 \n",
       "L 308.723568 215.932235 \n",
       "L 308.789863 206.585227 \n",
       "L 308.856158 210.833867 \n",
       "L 308.922453 210.833867 \n",
       "L 308.988748 217.631691 \n",
       "L 309.055043 215.932235 \n",
       "L 309.187633 213.383051 \n",
       "L 309.320223 215.932235 \n",
       "L 309.386518 212.533323 \n",
       "L 309.452813 217.631691 \n",
       "L 309.519108 215.082507 \n",
       "L 309.585403 217.631691 \n",
       "L 309.651698 209.984139 \n",
       "L 309.717993 215.082507 \n",
       "L 309.784288 220.180876 \n",
       "L 309.850583 209.984139 \n",
       "L 309.916878 218.481419 \n",
       "L 310.049468 205.735499 \n",
       "L 310.115763 208.284683 \n",
       "L 310.182058 214.232779 \n",
       "L 310.248353 212.533323 \n",
       "L 310.314648 204.885771 \n",
       "L 310.513533 215.932235 \n",
       "L 310.778713 205.735499 \n",
       "L 310.845008 215.932235 \n",
       "L 310.911303 213.383051 \n",
       "L 310.977598 211.683595 \n",
       "L 311.043893 212.533323 \n",
       "L 311.110188 212.533323 \n",
       "L 311.242778 209.134411 \n",
       "L 311.441663 220.180876 \n",
       "L 311.574253 208.284683 \n",
       "L 311.640548 214.232779 \n",
       "L 311.773138 209.134411 \n",
       "L 311.839433 208.284683 \n",
       "L 311.905728 210.833867 \n",
       "L 311.972023 218.481419 \n",
       "L 312.038318 210.833867 \n",
       "L 312.104613 215.932235 \n",
       "L 312.303498 207.434955 \n",
       "L 312.369793 210.833867 \n",
       "L 312.436088 208.284683 \n",
       "L 312.502383 209.984139 \n",
       "L 312.568678 209.984139 \n",
       "L 312.634973 207.434955 \n",
       "L 312.900153 215.082507 \n",
       "L 312.966448 208.284683 \n",
       "L 313.032743 211.683595 \n",
       "L 313.099038 207.434955 \n",
       "L 313.165333 208.284683 \n",
       "L 313.297923 215.082507 \n",
       "L 313.364218 209.984139 \n",
       "L 313.430513 212.533323 \n",
       "L 313.629398 206.585227 \n",
       "L 313.695693 215.082507 \n",
       "L 313.761988 209.134411 \n",
       "L 313.828283 213.383051 \n",
       "L 313.894578 209.417649 \n",
       "L 314.027168 213.383051 \n",
       "L 314.093463 209.134411 \n",
       "L 314.226053 223.579788 \n",
       "L 314.292348 210.833867 \n",
       "L 314.358643 215.932235 \n",
       "L 314.424938 214.232779 \n",
       "L 314.491233 206.585227 \n",
       "L 314.557528 215.082507 \n",
       "L 314.623823 210.833867 \n",
       "L 314.690118 208.284683 \n",
       "L 314.756413 209.984139 \n",
       "L 314.889002 213.383051 \n",
       "L 314.955297 205.735499 \n",
       "L 315.021592 213.383051 \n",
       "L 315.087887 209.134411 \n",
       "L 315.286772 216.781963 \n",
       "L 315.419362 206.585227 \n",
       "L 315.485657 211.683595 \n",
       "L 315.551952 204.885771 \n",
       "L 315.618247 208.284683 \n",
       "L 315.817132 218.481419 \n",
       "L 315.883427 208.284683 \n",
       "L 315.949722 211.683595 \n",
       "L 316.148607 215.082507 \n",
       "L 316.214902 212.533323 \n",
       "L 316.281197 213.383051 \n",
       "L 316.347492 215.932235 \n",
       "L 316.413787 208.284683 \n",
       "L 316.480082 209.134411 \n",
       "L 316.546377 206.585227 \n",
       "L 316.612672 207.434955 \n",
       "L 316.745262 211.683595 \n",
       "L 316.811557 210.833867 \n",
       "L 316.877852 205.735499 \n",
       "L 316.944147 212.533323 \n",
       "L 317.010442 209.134411 \n",
       "L 317.076737 205.735499 \n",
       "L 317.143032 206.585227 \n",
       "L 317.209327 213.383051 \n",
       "L 317.275622 208.284683 \n",
       "L 317.341917 211.683595 \n",
       "L 317.474507 210.833867 \n",
       "L 317.540802 205.735499 \n",
       "L 317.607097 207.434955 \n",
       "L 317.673392 215.082507 \n",
       "L 317.739687 204.036043 \n",
       "L 317.938572 214.232779 \n",
       "L 318.004867 213.383051 \n",
       "L 318.071162 215.082507 \n",
       "L 318.270047 209.134411 \n",
       "L 318.336342 211.683595 \n",
       "L 318.402637 207.434955 \n",
       "L 318.468932 209.134411 \n",
       "L 318.535227 215.082507 \n",
       "L 318.601522 212.533323 \n",
       "L 318.734112 210.833867 \n",
       "L 318.800407 220.180876 \n",
       "L 318.866702 215.082507 \n",
       "L 318.932997 207.434955 \n",
       "L 318.999292 216.781963 \n",
       "L 319.065587 211.683595 \n",
       "L 319.131882 211.683595 \n",
       "L 319.198177 210.833867 \n",
       "L 319.264472 211.683595 \n",
       "L 319.330767 218.481419 \n",
       "L 319.397062 214.232779 \n",
       "L 319.463357 217.631691 \n",
       "L 319.529652 204.885771 \n",
       "L 319.595947 212.533323 \n",
       "L 319.662242 221.880332 \n",
       "L 319.794832 210.833867 \n",
       "L 319.927422 216.781963 \n",
       "L 320.126307 206.585227 \n",
       "L 320.258897 216.781963 \n",
       "L 320.391487 208.284683 \n",
       "L 320.457782 209.984139 \n",
       "L 320.524077 223.579788 \n",
       "L 320.590372 204.036043 \n",
       "L 320.656667 210.833867 \n",
       "L 320.722962 212.533323 \n",
       "L 320.855552 209.134411 \n",
       "L 320.921846 213.383051 \n",
       "L 321.054436 205.735499 \n",
       "L 321.187026 218.481419 \n",
       "L 321.319616 208.284683 \n",
       "L 321.385911 209.984139 \n",
       "L 321.584796 206.585227 \n",
       "L 321.717386 204.036043 \n",
       "L 321.783681 209.134411 \n",
       "L 321.849976 207.434955 \n",
       "L 321.916271 204.885771 \n",
       "L 321.982566 206.585227 \n",
       "L 322.048861 206.585227 \n",
       "L 322.115156 203.186315 \n",
       "L 322.181451 204.036043 \n",
       "L 322.247746 205.735499 \n",
       "L 322.314041 204.036043 \n",
       "L 322.380336 205.735499 \n",
       "L 322.446631 204.036043 \n",
       "L 322.512926 209.984139 \n",
       "L 322.579221 203.186315 \n",
       "L 322.778106 210.833867 \n",
       "L 322.844401 201.486858 \n",
       "L 322.910696 203.186315 \n",
       "L 322.976991 202.336587 \n",
       "L 323.175876 208.284683 \n",
       "L 323.308466 204.885771 \n",
       "L 323.374761 204.885771 \n",
       "L 323.441056 211.683595 \n",
       "L 323.507351 210.833867 \n",
       "L 323.639941 208.284683 \n",
       "L 323.772531 201.486858 \n",
       "L 323.905121 205.735499 \n",
       "L 324.037711 202.336587 \n",
       "L 324.104006 207.434955 \n",
       "L 324.236596 202.336587 \n",
       "L 324.435481 213.383051 \n",
       "L 324.568071 201.486858 \n",
       "L 324.700661 205.735499 \n",
       "L 324.766956 204.036043 \n",
       "L 324.833251 204.885771 \n",
       "L 324.899546 204.885771 \n",
       "L 324.965841 206.585227 \n",
       "L 325.032136 204.885771 \n",
       "L 325.098431 208.284683 \n",
       "L 325.297316 200.63713 \n",
       "L 325.363611 208.284683 \n",
       "L 325.429906 204.036043 \n",
       "L 325.628791 206.585227 \n",
       "L 325.695086 209.984139 \n",
       "L 325.827676 206.585227 \n",
       "L 325.893971 213.383051 \n",
       "L 325.960266 209.134411 \n",
       "L 326.092856 204.885771 \n",
       "L 326.225446 210.833867 \n",
       "L 326.291741 201.486858 \n",
       "L 326.358036 205.735499 \n",
       "L 326.424331 209.134411 \n",
       "L 326.689511 203.186315 \n",
       "L 326.755806 203.186315 \n",
       "L 326.822101 209.984139 \n",
       "L 326.888396 204.036043 \n",
       "L 326.954691 208.284683 \n",
       "L 327.020985 199.787402 \n",
       "L 327.08728 206.585227 \n",
       "L 327.153575 203.186315 \n",
       "L 327.418755 207.434955 \n",
       "L 327.48505 204.036043 \n",
       "L 327.551345 205.735499 \n",
       "L 327.61764 204.036043 \n",
       "L 327.683935 205.735499 \n",
       "L 327.816525 202.336587 \n",
       "L 328.01541 209.984139 \n",
       "L 328.41318 200.63713 \n",
       "L 328.612065 205.735499 \n",
       "L 328.67836 203.186315 \n",
       "L 328.744655 208.284683 \n",
       "L 328.81095 204.885771 \n",
       "L 328.877245 205.735499 \n",
       "L 329.009835 204.036043 \n",
       "L 329.07613 204.885771 \n",
       "L 329.142425 209.134411 \n",
       "L 329.20872 204.036043 \n",
       "L 329.275015 207.434955 \n",
       "L 329.34131 204.036043 \n",
       "L 329.407605 204.885771 \n",
       "L 329.4739 209.134411 \n",
       "L 329.60649 204.036043 \n",
       "L 329.672785 209.134411 \n",
       "L 329.73908 204.885771 \n",
       "L 329.805375 206.585227 \n",
       "L 329.87167 211.683595 \n",
       "L 329.937965 201.486858 \n",
       "L 330.00426 207.434955 \n",
       "L 330.13685 201.486858 \n",
       "L 330.26944 207.434955 \n",
       "L 330.335735 204.036043 \n",
       "L 330.40203 204.885771 \n",
       "L 330.468325 209.134411 \n",
       "L 330.53462 206.585227 \n",
       "L 330.66721 207.434955 \n",
       "L 330.733505 204.036043 \n",
       "L 330.7998 204.885771 \n",
       "L 330.866095 205.735499 \n",
       "L 330.93239 209.134411 \n",
       "L 330.998685 204.036043 \n",
       "L 331.06498 207.434955 \n",
       "L 331.131275 209.984139 \n",
       "L 331.19757 200.63713 \n",
       "L 331.263865 201.486858 \n",
       "L 331.33016 204.885771 \n",
       "L 331.396455 203.186315 \n",
       "L 331.46275 203.186315 \n",
       "L 331.529045 204.885771 \n",
       "L 331.59534 202.336587 \n",
       "L 331.661635 206.585227 \n",
       "L 331.794225 201.486858 \n",
       "L 331.99311 208.284683 \n",
       "L 332.059405 202.336587 \n",
       "L 332.1257 204.036043 \n",
       "L 332.191995 205.735499 \n",
       "L 332.324585 201.486858 \n",
       "L 332.457175 209.984139 \n",
       "L 332.52347 203.186315 \n",
       "L 332.589765 204.885771 \n",
       "L 332.78865 199.787402 \n",
       "L 332.854945 199.787402 \n",
       "L 332.92124 206.585227 \n",
       "L 332.987535 201.486858 \n",
       "L 333.186419 209.134411 \n",
       "L 333.319009 204.036043 \n",
       "L 333.451599 206.585227 \n",
       "L 333.517894 211.683595 \n",
       "L 333.584189 203.186315 \n",
       "L 333.650484 208.284683 \n",
       "L 333.716779 210.833867 \n",
       "L 333.849369 199.787402 \n",
       "L 334.114549 205.735499 \n",
       "L 334.180844 204.885771 \n",
       "L 334.247139 201.486858 \n",
       "L 334.379729 204.036043 \n",
       "L 334.446024 201.486858 \n",
       "L 334.512319 203.186315 \n",
       "L 334.578614 205.735499 \n",
       "L 334.711204 203.186315 \n",
       "L 334.976384 207.434955 \n",
       "L 335.108974 201.486858 \n",
       "L 335.175269 204.036043 \n",
       "L 335.241564 201.486858 \n",
       "L 335.374154 209.984139 \n",
       "L 335.440449 206.585227 \n",
       "L 335.573039 201.486858 \n",
       "L 335.639334 208.284683 \n",
       "L 335.705629 203.186315 \n",
       "L 335.771924 209.134411 \n",
       "L 335.904514 202.336587 \n",
       "L 335.970809 203.186315 \n",
       "L 336.037104 209.134411 \n",
       "L 336.103399 202.336587 \n",
       "L 336.169694 209.134411 \n",
       "L 336.302284 201.486858 \n",
       "L 336.434874 204.885771 \n",
       "L 336.501169 204.885771 \n",
       "L 336.567464 204.036043 \n",
       "L 336.633759 204.885771 \n",
       "L 336.766349 198.087946 \n",
       "L 336.898939 200.63713 \n",
       "L 336.965234 210.833867 \n",
       "L 337.031529 198.937674 \n",
       "L 337.097824 208.284683 \n",
       "L 337.164119 202.336587 \n",
       "L 337.230414 207.434955 \n",
       "L 337.429299 200.63713 \n",
       "L 337.561889 206.585227 \n",
       "L 337.694479 201.486858 \n",
       "L 337.760774 207.434955 \n",
       "L 337.827069 200.63713 \n",
       "L 337.893364 204.036043 \n",
       "L 337.959659 199.787402 \n",
       "L 338.025954 202.336587 \n",
       "L 338.092249 208.284683 \n",
       "L 338.291134 201.486858 \n",
       "L 338.357429 207.434955 \n",
       "L 338.423724 204.036043 \n",
       "L 338.622609 199.787402 \n",
       "L 338.887789 206.585227 \n",
       "L 338.954084 203.186315 \n",
       "L 339.086674 206.585227 \n",
       "L 339.152969 200.63713 \n",
       "L 339.219263 206.585227 \n",
       "L 339.285558 201.486858 \n",
       "L 339.351853 209.134411 \n",
       "L 339.418148 206.585227 \n",
       "L 339.550738 202.336587 \n",
       "L 339.617033 203.186315 \n",
       "L 339.683328 208.284683 \n",
       "L 339.749623 201.486858 \n",
       "L 339.815918 204.036043 \n",
       "L 339.882213 206.585227 \n",
       "L 339.948508 205.735499 \n",
       "L 340.014803 201.486858 \n",
       "L 340.081098 206.585227 \n",
       "L 340.147393 201.486858 \n",
       "L 340.213688 202.336587 \n",
       "L 340.279983 205.735499 \n",
       "L 340.346278 200.63713 \n",
       "L 340.412573 201.486858 \n",
       "L 340.545163 209.134411 \n",
       "L 340.611458 208.284683 \n",
       "L 340.677753 201.486858 \n",
       "L 340.744048 203.186315 \n",
       "L 340.810343 203.186315 \n",
       "L 340.876638 202.336587 \n",
       "L 340.942933 208.284683 \n",
       "L 341.009228 204.885771 \n",
       "L 341.075523 207.434955 \n",
       "L 341.208113 202.336587 \n",
       "L 341.274408 202.336587 \n",
       "L 341.406998 205.735499 \n",
       "L 341.473293 201.486858 \n",
       "L 341.539588 206.585227 \n",
       "L 341.605883 204.036043 \n",
       "L 341.672178 202.336587 \n",
       "L 341.804768 212.533323 \n",
       "L 341.871063 199.787402 \n",
       "L 341.937358 207.434955 \n",
       "L 342.069948 201.486858 \n",
       "L 342.136243 204.885771 \n",
       "L 342.202538 209.134411 \n",
       "L 342.268833 201.486858 \n",
       "L 342.335128 208.284683 \n",
       "L 342.401423 202.336587 \n",
       "L 342.467718 204.885771 \n",
       "L 342.534013 204.036043 \n",
       "L 342.600308 204.885771 \n",
       "L 342.666603 201.486858 \n",
       "L 342.799193 204.885771 \n",
       "L 342.865488 205.735499 \n",
       "L 342.931783 203.186315 \n",
       "L 342.998078 204.885771 \n",
       "L 343.064373 207.434955 \n",
       "L 343.130668 200.63713 \n",
       "L 343.196963 201.486858 \n",
       "L 343.263258 200.63713 \n",
       "L 343.329553 197.238218 \n",
       "L 343.395848 209.134411 \n",
       "L 343.462143 201.486858 \n",
       "L 343.528438 201.486858 \n",
       "L 343.661028 206.585227 \n",
       "L 343.793618 202.336587 \n",
       "L 343.859913 202.336587 \n",
       "L 343.926208 204.885771 \n",
       "L 344.125093 197.238218 \n",
       "L 344.323978 208.284683 \n",
       "L 344.456568 201.486858 \n",
       "L 344.589158 205.735499 \n",
       "L 344.721748 204.036043 \n",
       "L 344.788043 204.036043 \n",
       "L 344.854338 200.63713 \n",
       "L 344.920633 206.585227 \n",
       "L 344.986928 199.220913 \n",
       "L 345.053223 203.186315 \n",
       "L 345.119518 204.036043 \n",
       "L 345.185813 198.937674 \n",
       "L 345.384697 206.585227 \n",
       "L 345.450992 201.486858 \n",
       "L 345.517287 205.735499 \n",
       "L 345.649877 198.087946 \n",
       "L 345.716172 204.036043 \n",
       "L 345.782467 200.63713 \n",
       "L 345.915057 206.585227 \n",
       "L 345.981352 199.787402 \n",
       "L 346.047647 206.585227 \n",
       "L 346.113942 202.336587 \n",
       "L 346.180237 204.036043 \n",
       "L 346.379122 200.63713 \n",
       "L 346.511712 204.885771 \n",
       "L 346.578007 199.787402 \n",
       "L 346.644302 205.735499 \n",
       "L 346.710597 200.63713 \n",
       "L 346.776892 203.186315 \n",
       "L 346.843187 204.885771 \n",
       "L 346.909482 197.238218 \n",
       "L 347.042072 206.585227 \n",
       "L 347.108367 208.284683 \n",
       "L 347.307252 199.787402 \n",
       "L 347.439842 206.585227 \n",
       "L 347.506137 202.336587 \n",
       "L 347.572432 205.735499 \n",
       "L 347.638727 200.63713 \n",
       "L 347.705022 205.735499 \n",
       "L 347.771317 203.186315 \n",
       "L 347.837612 202.336587 \n",
       "L 347.903907 207.434955 \n",
       "L 347.970202 204.036043 \n",
       "L 348.102792 205.735499 \n",
       "L 348.169087 198.087946 \n",
       "L 348.235382 201.486858 \n",
       "L 348.301677 200.63713 \n",
       "L 348.500562 204.885771 \n",
       "L 348.566857 200.63713 \n",
       "L 348.699447 206.585227 \n",
       "L 348.765742 205.735499 \n",
       "L 348.898332 201.486858 \n",
       "L 348.964627 204.885771 \n",
       "L 349.030922 203.186315 \n",
       "L 349.097217 200.63713 \n",
       "L 349.229807 208.284683 \n",
       "L 349.362397 198.937674 \n",
       "L 349.428692 202.336587 \n",
       "L 349.494987 208.284683 \n",
       "L 349.561282 204.885771 \n",
       "L 349.627577 200.63713 \n",
       "L 349.693872 205.735499 \n",
       "L 349.760167 204.036043 \n",
       "L 349.826462 204.036043 \n",
       "L 349.959052 201.486858 \n",
       "L 350.157937 204.036043 \n",
       "L 350.224232 203.186315 \n",
       "L 350.290527 198.087946 \n",
       "L 350.356822 199.787402 \n",
       "L 350.423117 200.63713 \n",
       "L 350.489412 204.885771 \n",
       "L 350.555707 199.787402 \n",
       "L 350.622002 203.186315 \n",
       "L 350.688297 203.186315 \n",
       "L 350.754592 205.735499 \n",
       "L 350.820887 199.787402 \n",
       "L 350.887182 203.186315 \n",
       "L 350.953477 199.787402 \n",
       "L 351.086067 205.735499 \n",
       "L 351.152362 203.186315 \n",
       "L 351.218657 204.885771 \n",
       "L 351.284952 204.036043 \n",
       "L 351.351247 205.735499 \n",
       "L 351.417541 201.486858 \n",
       "L 351.483836 206.585227 \n",
       "L 351.550131 202.336587 \n",
       "L 351.616426 208.284683 \n",
       "L 351.682721 204.885771 \n",
       "L 351.749016 202.336587 \n",
       "L 351.881606 205.735499 \n",
       "L 351.947901 204.036043 \n",
       "L 352.080491 206.585227 \n",
       "L 352.146786 198.937674 \n",
       "L 352.213081 202.336587 \n",
       "L 352.411966 205.735499 \n",
       "L 352.478261 200.63713 \n",
       "L 352.610851 207.434955 \n",
       "L 352.743441 198.087946 \n",
       "L 352.876031 204.036043 \n",
       "L 352.942326 202.336587 \n",
       "L 353.074916 204.885771 \n",
       "L 353.207506 202.336587 \n",
       "L 353.273801 204.885771 \n",
       "L 353.340096 199.787402 \n",
       "L 353.472686 204.036043 \n",
       "L 353.538981 203.186315 \n",
       "L 353.605276 198.937674 \n",
       "L 353.671571 199.787402 \n",
       "L 353.737866 199.787402 \n",
       "L 353.936751 203.186315 \n",
       "L 354.003046 199.787402 \n",
       "L 354.135636 203.186315 \n",
       "L 354.201931 200.63713 \n",
       "L 354.268226 206.585227 \n",
       "L 354.334521 199.787402 \n",
       "L 354.400816 206.585227 \n",
       "L 354.467111 204.885771 \n",
       "L 354.599701 199.787402 \n",
       "L 354.665996 205.735499 \n",
       "L 354.732291 204.036043 \n",
       "L 354.798586 200.63713 \n",
       "L 354.931176 203.186315 \n",
       "L 354.997471 202.336587 \n",
       "L 355.063766 204.885771 \n",
       "L 355.130061 203.186315 \n",
       "L 355.196356 203.186315 \n",
       "L 355.262651 204.885771 \n",
       "L 355.328946 201.486858 \n",
       "L 355.395241 204.885771 \n",
       "L 355.461536 200.63713 \n",
       "L 355.527831 203.186315 \n",
       "L 355.660421 204.036043 \n",
       "L 355.726716 202.336587 \n",
       "L 355.793011 203.186315 \n",
       "L 355.859306 207.434955 \n",
       "L 355.925601 202.336587 \n",
       "L 355.991896 204.036043 \n",
       "L 356.058191 201.486858 \n",
       "L 356.124486 205.735499 \n",
       "L 356.190781 203.186315 \n",
       "L 356.257076 205.735499 \n",
       "L 356.389666 200.63713 \n",
       "L 356.588551 207.434955 \n",
       "L 356.654846 201.486858 \n",
       "L 356.721141 202.336587 \n",
       "L 356.853731 205.735499 \n",
       "L 356.986321 198.937674 \n",
       "L 357.052616 199.787402 \n",
       "L 357.118911 204.036043 \n",
       "L 357.251501 199.787402 \n",
       "L 357.450386 206.585227 \n",
       "L 357.582975 198.087946 \n",
       "L 357.64927 204.036043 \n",
       "L 357.715565 203.186315 \n",
       "L 357.91445 199.787402 \n",
       "L 358.04704 204.036043 \n",
       "L 358.113335 202.336587 \n",
       "L 358.17963 208.284683 \n",
       "L 358.245925 204.036043 \n",
       "L 358.31222 208.284683 \n",
       "L 358.511105 200.63713 \n",
       "L 358.5774 200.63713 \n",
       "L 358.643695 203.186315 \n",
       "L 358.70999 202.336587 \n",
       "L 358.776285 201.486858 \n",
       "L 358.84258 204.036043 \n",
       "L 359.041465 200.63713 \n",
       "L 359.10776 204.885771 \n",
       "L 359.174055 203.186315 \n",
       "L 359.24035 200.63713 \n",
       "L 359.306645 205.735499 \n",
       "L 359.37294 202.336587 \n",
       "L 359.439235 204.885771 \n",
       "L 359.63812 201.486858 \n",
       "L 359.77071 200.63713 \n",
       "L 359.837005 204.885771 \n",
       "L 359.9033 202.336587 \n",
       "L 359.969595 200.63713 \n",
       "L 360.03589 204.036043 \n",
       "L 360.102185 202.336587 \n",
       "L 360.16848 198.937674 \n",
       "L 360.234775 204.885771 \n",
       "L 360.30107 204.036043 \n",
       "L 360.367365 203.186315 \n",
       "L 360.43366 204.036043 \n",
       "L 360.499955 199.787402 \n",
       "L 360.56625 201.486858 \n",
       "L 360.632545 204.885771 \n",
       "L 360.69884 203.186315 \n",
       "L 360.765135 199.787402 \n",
       "L 360.96402 204.036043 \n",
       "L 361.030315 200.63713 \n",
       "L 361.09661 204.885771 \n",
       "L 361.162905 202.336587 \n",
       "L 361.2292 199.787402 \n",
       "L 361.295495 206.585227 \n",
       "L 361.36179 203.186315 \n",
       "L 361.428085 204.885771 \n",
       "L 361.49438 204.036043 \n",
       "L 361.560675 203.186315 \n",
       "L 361.62697 205.735499 \n",
       "L 361.693265 204.885771 \n",
       "L 361.75956 199.787402 \n",
       "L 361.825855 204.036043 \n",
       "L 361.89215 200.63713 \n",
       "L 362.02474 206.585227 \n",
       "L 362.223625 199.787402 \n",
       "L 362.28992 201.486858 \n",
       "L 362.356215 200.63713 \n",
       "L 362.42251 198.937674 \n",
       "L 362.488805 202.336587 \n",
       "L 362.5551 201.486858 \n",
       "L 362.621395 201.486858 \n",
       "L 362.68769 207.434955 \n",
       "L 362.886575 200.63713 \n",
       "L 362.95287 199.787402 \n",
       "L 363.019165 204.036043 \n",
       "L 363.08546 201.486858 \n",
       "L 363.151755 201.486858 \n",
       "L 363.21805 204.885771 \n",
       "L 363.35064 198.937674 \n",
       "L 363.416935 206.585227 \n",
       "L 363.48323 203.186315 \n",
       "L 363.549525 204.036043 \n",
       "L 363.61582 200.63713 \n",
       "L 363.682114 206.585227 \n",
       "L 363.814704 198.087946 \n",
       "L 363.880999 204.885771 \n",
       "L 363.947294 203.186315 \n",
       "L 364.013589 205.735499 \n",
       "L 364.146179 203.186315 \n",
       "L 364.212474 209.984139 \n",
       "L 364.278769 201.486858 \n",
       "L 364.345064 203.186315 \n",
       "L 364.411359 203.186315 \n",
       "L 364.477654 200.63713 \n",
       "L 364.543949 207.434955 \n",
       "L 364.610244 200.63713 \n",
       "L 364.676539 204.036043 \n",
       "L 364.742834 200.63713 \n",
       "L 364.875424 205.735499 \n",
       "L 364.941719 202.336587 \n",
       "L 365.008014 205.735499 \n",
       "L 365.074309 204.885771 \n",
       "L 365.206899 202.336587 \n",
       "L 365.273194 203.186315 \n",
       "L 365.339489 200.63713 \n",
       "L 365.405784 205.735499 \n",
       "L 365.472079 202.336587 \n",
       "L 365.604669 202.336587 \n",
       "L 365.670964 206.585227 \n",
       "L 365.737259 200.63713 \n",
       "L 365.803554 204.036043 \n",
       "L 365.869849 204.036043 \n",
       "L 365.936144 198.937674 \n",
       "L 366.002439 205.735499 \n",
       "L 366.068734 203.186315 \n",
       "L 366.135029 201.486858 \n",
       "L 366.267619 206.585227 \n",
       "L 366.333914 203.186315 \n",
       "L 366.400209 206.585227 \n",
       "L 366.466504 205.735499 \n",
       "L 366.665389 198.087946 \n",
       "L 366.797979 204.036043 \n",
       "L 366.864274 201.486858 \n",
       "L 366.930569 203.186315 \n",
       "L 366.996864 205.735499 \n",
       "L 367.195749 200.63713 \n",
       "L 367.328339 204.036043 \n",
       "L 367.460929 199.787402 \n",
       "L 367.527224 201.486858 \n",
       "L 367.593519 197.238218 \n",
       "L 367.792404 203.186315 \n",
       "L 367.858699 198.937674 \n",
       "L 367.924994 202.336587 \n",
       "L 367.991289 198.937674 \n",
       "L 368.057584 199.787402 \n",
       "L 368.123879 205.735499 \n",
       "L 368.190174 200.63713 \n",
       "L 368.256469 201.486858 \n",
       "L 368.521649 204.885771 \n",
       "L 368.587944 204.036043 \n",
       "L 368.654239 206.585227 \n",
       "L 368.853124 198.087946 \n",
       "L 368.985714 206.585227 \n",
       "L 369.052009 198.937674 \n",
       "L 369.118304 199.787402 \n",
       "L 369.184599 199.787402 \n",
       "L 369.250894 210.833867 \n",
       "L 369.317189 205.735499 \n",
       "L 369.383484 199.787402 \n",
       "L 369.449779 201.486858 \n",
       "L 369.648664 208.284683 \n",
       "L 369.714959 200.63713 \n",
       "L 369.781253 204.036043 \n",
       "L 369.847548 206.585227 \n",
       "L 369.980138 201.486858 \n",
       "L 370.046433 206.585227 \n",
       "L 370.112728 204.885771 \n",
       "L 370.245318 200.63713 \n",
       "L 370.311613 202.336587 \n",
       "L 370.377908 198.087946 \n",
       "L 370.444203 202.336587 \n",
       "L 370.510498 198.937674 \n",
       "L 370.643088 205.735499 \n",
       "L 370.775678 201.486858 \n",
       "L 370.841973 201.486858 \n",
       "L 370.908268 206.585227 \n",
       "L 370.974563 204.036043 \n",
       "L 371.107153 199.787402 \n",
       "L 371.173448 201.486858 \n",
       "L 371.372333 203.186315 \n",
       "L 371.504923 200.63713 \n",
       "L 371.703808 204.036043 \n",
       "L 371.770103 200.63713 \n",
       "L 371.836398 202.336587 \n",
       "L 371.968988 199.787402 \n",
       "L 372.101578 205.735499 \n",
       "L 372.234168 199.787402 \n",
       "L 372.300463 204.885771 \n",
       "L 372.366758 201.486858 \n",
       "L 372.433053 201.486858 \n",
       "L 372.565643 204.885771 \n",
       "L 372.698233 196.38849 \n",
       "L 372.764528 198.937674 \n",
       "L 372.963413 202.336587 \n",
       "L 373.029708 197.238218 \n",
       "L 373.096003 205.735499 \n",
       "L 373.162298 204.036043 \n",
       "L 373.228593 200.63713 \n",
       "L 373.294888 207.434955 \n",
       "L 373.361183 199.787402 \n",
       "L 373.427478 203.186315 \n",
       "L 373.560068 202.336587 \n",
       "L 373.626363 203.186315 \n",
       "L 373.758953 208.284683 \n",
       "L 373.825248 196.38849 \n",
       "L 373.891543 203.186315 \n",
       "L 373.957838 204.036043 \n",
       "L 374.024133 198.087946 \n",
       "L 374.090428 204.036043 \n",
       "L 374.156723 200.63713 \n",
       "L 374.223018 204.885771 \n",
       "L 374.355608 197.238218 \n",
       "L 374.554493 204.885771 \n",
       "L 374.687083 196.38849 \n",
       "L 374.819673 206.585227 \n",
       "L 374.952263 204.036043 \n",
       "L 375.018558 204.036043 \n",
       "L 375.151148 199.787402 \n",
       "L 375.217443 200.63713 \n",
       "L 375.283738 204.036043 \n",
       "L 375.350033 203.186315 \n",
       "L 375.416328 202.336587 \n",
       "L 375.548918 197.238218 \n",
       "L 375.747803 204.885771 \n",
       "L 375.814098 207.434955 \n",
       "L 375.880392 199.787402 \n",
       "L 375.946687 203.186315 \n",
       "L 376.012982 202.336587 \n",
       "L 376.079277 207.15171 \n",
       "L 376.145572 198.937674 \n",
       "L 376.211867 204.036043 \n",
       "L 376.278162 197.238218 \n",
       "L 376.410752 203.186315 \n",
       "L 376.477047 199.787402 \n",
       "L 376.675932 205.735499 \n",
       "L 376.874817 198.937674 \n",
       "L 376.941112 201.486858 \n",
       "L 377.007407 200.63713 \n",
       "L 377.073702 200.63713 \n",
       "L 377.139997 202.336587 \n",
       "L 377.206292 199.787402 \n",
       "L 377.272587 201.486858 \n",
       "L 377.338882 206.585227 \n",
       "L 377.471472 202.336587 \n",
       "L 377.537767 205.735499 \n",
       "L 377.670357 203.186315 \n",
       "L 377.736652 203.186315 \n",
       "L 377.935537 200.63713 \n",
       "L 378.001832 201.486858 \n",
       "L 378.068127 204.036043 \n",
       "L 378.134422 201.486858 \n",
       "L 378.200717 202.336587 \n",
       "L 378.333307 204.036043 \n",
       "L 378.399602 197.238218 \n",
       "L 378.465897 199.787402 \n",
       "L 378.598487 204.036043 \n",
       "L 378.731077 200.63713 \n",
       "L 378.797372 201.486858 \n",
       "L 378.863667 204.885771 \n",
       "L 378.996257 198.087946 \n",
       "L 379.128847 203.186315 \n",
       "L 379.195142 199.787402 \n",
       "L 379.261437 201.486858 \n",
       "L 379.327732 201.486858 \n",
       "L 379.526617 198.937674 \n",
       "L 379.659207 203.186315 \n",
       "L 379.791797 198.087946 \n",
       "L 379.990682 205.735499 \n",
       "L 380.056977 198.937674 \n",
       "L 380.123272 199.787402 \n",
       "L 380.189567 198.087946 \n",
       "L 380.322157 204.885771 \n",
       "L 380.388452 203.186315 \n",
       "L 380.454747 204.036043 \n",
       "L 380.521042 207.434955 \n",
       "L 380.719927 198.087946 \n",
       "L 380.786222 204.036043 \n",
       "L 380.852517 203.186315 \n",
       "L 380.918812 204.036043 \n",
       "L 380.985107 203.186315 \n",
       "L 381.051402 209.134411 \n",
       "L 381.117697 207.434955 \n",
       "L 381.183992 205.735499 \n",
       "L 381.250287 199.787402 \n",
       "L 381.316582 201.486858 \n",
       "L 381.382877 198.937674 \n",
       "L 381.449172 201.486858 \n",
       "L 381.515467 199.787402 \n",
       "L 381.714352 203.186315 \n",
       "L 381.780647 198.087946 \n",
       "L 381.846942 205.735499 \n",
       "L 381.913237 204.036043 \n",
       "L 381.979531 202.336587 \n",
       "L 382.045826 204.036043 \n",
       "L 382.112121 202.336587 \n",
       "L 382.178416 204.036043 \n",
       "L 382.244711 197.238218 \n",
       "L 382.311006 201.486858 \n",
       "L 382.443596 203.186315 \n",
       "L 382.509891 198.937674 \n",
       "L 382.576186 200.63713 \n",
       "L 382.642481 203.186315 \n",
       "L 382.708776 198.087946 \n",
       "L 382.775071 199.787402 \n",
       "L 382.907661 204.885771 \n",
       "L 383.040251 202.336587 \n",
       "L 383.106546 204.036043 \n",
       "L 383.239136 198.087946 \n",
       "L 383.305431 199.787402 \n",
       "L 383.371726 204.885771 \n",
       "L 383.438021 198.087946 \n",
       "L 383.504316 200.63713 \n",
       "L 383.636906 200.63713 \n",
       "L 383.769496 199.787402 \n",
       "L 383.835791 201.486858 \n",
       "L 383.968381 198.937674 \n",
       "L 384.100971 206.585227 \n",
       "L 384.167266 199.787402 \n",
       "L 384.233561 204.885771 \n",
       "L 384.299856 201.486858 \n",
       "L 384.366151 204.036043 \n",
       "L 384.432446 198.087946 \n",
       "L 384.498741 205.735499 \n",
       "L 384.565036 201.486858 \n",
       "L 384.631331 200.63713 \n",
       "L 384.697626 202.336587 \n",
       "L 384.763921 200.63713 \n",
       "L 384.830216 203.186315 \n",
       "L 384.896511 202.336587 \n",
       "L 384.962806 202.336587 \n",
       "L 385.029101 200.63713 \n",
       "L 385.161691 205.735499 \n",
       "L 385.294281 200.63713 \n",
       "L 385.360576 201.486858 \n",
       "L 385.426871 197.238218 \n",
       "L 385.559461 203.186315 \n",
       "L 385.625756 200.63713 \n",
       "L 385.692051 201.486858 \n",
       "L 385.758346 204.036043 \n",
       "L 385.824641 199.787402 \n",
       "L 385.890936 200.63713 \n",
       "L 386.023526 210.833867 \n",
       "L 386.156116 198.087946 \n",
       "L 386.222411 205.735499 \n",
       "L 386.288706 204.036043 \n",
       "L 386.355001 204.036043 \n",
       "L 386.421296 199.787402 \n",
       "L 386.553886 206.585227 \n",
       "L 386.752771 198.087946 \n",
       "L 386.885361 204.036043 \n",
       "L 386.951656 202.336587 \n",
       "L 387.017951 202.336587 \n",
       "L 387.216836 204.885771 \n",
       "L 387.283131 198.937674 \n",
       "L 387.349426 203.186315 \n",
       "L 387.548311 198.087946 \n",
       "L 387.680901 201.486858 \n",
       "L 387.747196 198.937674 \n",
       "L 387.813491 202.336587 \n",
       "L 387.879786 198.937674 \n",
       "L 388.07867 209.134411 \n",
       "L 388.21126 200.63713 \n",
       "L 388.34385 203.186315 \n",
       "L 388.410145 200.63713 \n",
       "L 388.47644 209.134411 \n",
       "L 388.542735 204.036043 \n",
       "L 388.675325 206.585227 \n",
       "L 388.807915 198.087946 \n",
       "L 388.87421 204.036043 \n",
       "L 388.940505 199.787402 \n",
       "L 389.0068 206.585227 \n",
       "L 389.13939 197.238218 \n",
       "L 389.205685 204.036043 \n",
       "L 389.27198 202.336587 \n",
       "L 389.40457 196.38849 \n",
       "L 389.470865 198.937674 \n",
       "L 389.53716 199.787402 \n",
       "L 389.603455 198.087946 \n",
       "L 389.736045 203.186315 \n",
       "L 389.80234 198.937674 \n",
       "L 389.868635 200.63713 \n",
       "L 390.001225 201.486858 \n",
       "L 390.06752 198.937674 \n",
       "L 390.133815 202.336587 \n",
       "L 390.20011 199.787402 \n",
       "L 390.266405 201.486858 \n",
       "L 390.398995 202.336587 \n",
       "L 390.46529 200.63713 \n",
       "L 390.59788 202.336587 \n",
       "L 390.664175 198.937674 \n",
       "L 390.73047 199.787402 \n",
       "L 390.929355 201.486858 \n",
       "L 391.061945 200.63713 \n",
       "L 391.12824 201.486858 \n",
       "L 391.194535 204.885771 \n",
       "L 391.39342 197.238218 \n",
       "L 391.459715 201.486858 \n",
       "L 391.52601 199.787402 \n",
       "L 391.592305 200.63713 \n",
       "L 391.6586 199.787402 \n",
       "L 391.79119 204.885771 \n",
       "L 391.857485 204.036043 \n",
       "L 391.92378 203.186315 \n",
       "L 392.122665 198.937674 \n",
       "L 392.255255 202.336587 \n",
       "L 392.45414 200.63713 \n",
       "L 392.520435 200.63713 \n",
       "L 392.58673 204.885771 \n",
       "L 392.653025 198.937674 \n",
       "L 392.71932 202.336587 \n",
       "L 392.785615 198.087946 \n",
       "L 392.918205 204.885771 \n",
       "L 392.9845 204.885771 \n",
       "L 393.050795 201.486858 \n",
       "L 393.11709 203.186315 \n",
       "L 393.183385 204.885771 \n",
       "L 393.315975 201.486858 \n",
       "L 393.38227 204.885771 \n",
       "L 393.64745 198.087946 \n",
       "L 393.713745 204.036043 \n",
       "L 393.78004 201.486858 \n",
       "L 393.846335 200.63713 \n",
       "L 393.91263 207.434955 \n",
       "L 394.04522 198.087946 \n",
       "L 394.244104 203.186315 \n",
       "L 394.376694 202.336587 \n",
       "L 394.442989 204.036043 \n",
       "L 394.641874 198.937674 \n",
       "L 394.840759 201.486858 \n",
       "L 394.907054 201.486858 \n",
       "L 394.973349 204.885771 \n",
       "L 395.039644 203.186315 \n",
       "L 395.238529 204.885771 \n",
       "L 395.437414 201.486858 \n",
       "L 395.503709 199.787402 \n",
       "L 395.570004 206.585227 \n",
       "L 395.636299 196.38849 \n",
       "L 395.702594 199.787402 \n",
       "L 395.835184 204.036043 \n",
       "L 395.901479 199.787402 \n",
       "L 395.967774 201.486858 \n",
       "L 396.100364 201.486858 \n",
       "L 396.166659 202.336587 \n",
       "L 396.232954 200.63713 \n",
       "L 396.299249 205.735499 \n",
       "L 396.431839 200.63713 \n",
       "L 396.564429 204.036043 \n",
       "L 396.763314 198.937674 \n",
       "L 396.895904 202.336587 \n",
       "L 396.962199 197.238218 \n",
       "L 397.094789 204.036043 \n",
       "L 397.227379 198.087946 \n",
       "L 397.293674 206.585227 \n",
       "L 397.359969 204.885771 \n",
       "L 397.492559 197.238218 \n",
       "L 397.625149 203.186315 \n",
       "L 397.757739 198.937674 \n",
       "L 397.890329 204.885771 \n",
       "L 397.956624 203.186315 \n",
       "L 398.022919 201.486858 \n",
       "L 398.089214 204.036043 \n",
       "L 398.288099 198.087946 \n",
       "L 398.354394 203.186315 \n",
       "L 398.420689 199.787402 \n",
       "L 398.553279 199.787402 \n",
       "L 398.685869 203.186315 \n",
       "L 398.752164 199.787402 \n",
       "L 398.818459 201.486858 \n",
       "L 398.884754 199.787402 \n",
       "L 398.951049 200.63713 \n",
       "L 399.017344 200.63713 \n",
       "L 399.083639 199.787402 \n",
       "L 399.282524 202.336587 \n",
       "L 399.415114 201.486858 \n",
       "L 399.481409 203.186315 \n",
       "L 399.547704 202.336587 \n",
       "L 399.680294 201.486858 \n",
       "L 399.746589 202.336587 \n",
       "L 399.812884 207.434955 \n",
       "L 399.879179 204.885771 \n",
       "L 399.945474 200.63713 \n",
       "L 400.011769 204.885771 \n",
       "L 400.078064 201.486858 \n",
       "L 400.144359 204.036043 \n",
       "L 400.210654 202.336587 \n",
       "L 400.276948 199.787402 \n",
       "L 400.409538 203.186315 \n",
       "L 400.475833 198.937674 \n",
       "L 400.542128 199.787402 \n",
       "L 400.608423 198.937674 \n",
       "L 400.807308 204.036043 \n",
       "L 400.939898 196.38849 \n",
       "L 401.072488 204.885771 \n",
       "L 401.138783 201.486858 \n",
       "L 401.205078 202.336587 \n",
       "L 401.271373 202.336587 \n",
       "L 401.337668 199.787402 \n",
       "L 401.470258 204.885771 \n",
       "L 401.669143 198.087946 \n",
       "L 401.735438 200.63713 \n",
       "L 401.801733 199.787402 \n",
       "L 401.868028 198.937674 \n",
       "L 401.934323 204.036043 \n",
       "L 402.000618 198.937674 \n",
       "L 402.066913 199.787402 \n",
       "L 402.133208 198.937674 \n",
       "L 402.199503 200.63713 \n",
       "L 402.265798 205.735499 \n",
       "L 402.332093 203.186315 \n",
       "L 402.398388 204.885771 \n",
       "L 402.464683 198.937674 \n",
       "L 402.530978 204.885771 \n",
       "L 402.597273 198.937674 \n",
       "L 402.729863 204.036043 \n",
       "L 402.796158 198.087946 \n",
       "L 402.862453 202.336587 \n",
       "L 402.928748 198.937674 \n",
       "L 402.995043 202.336587 \n",
       "L 403.061338 198.087946 \n",
       "L 403.127633 204.036043 \n",
       "L 403.193928 203.186315 \n",
       "L 403.392813 198.087946 \n",
       "L 403.591698 207.434955 \n",
       "L 403.790583 196.38849 \n",
       "L 403.856878 203.186315 \n",
       "L 403.923173 199.787402 \n",
       "L 403.989468 200.63713 \n",
       "L 404.188353 198.937674 \n",
       "L 404.320943 204.885771 \n",
       "L 404.453533 203.186315 \n",
       "L 404.586123 199.787402 \n",
       "L 404.718713 204.036043 \n",
       "L 404.785008 201.486858 \n",
       "L 404.851303 203.186315 \n",
       "L 404.917598 204.036043 \n",
       "L 405.182778 198.087946 \n",
       "L 405.249073 199.787402 \n",
       "L 405.315368 198.087946 \n",
       "L 405.381663 200.63713 \n",
       "L 405.447958 196.38849 \n",
       "L 405.514253 201.486858 \n",
       "L 405.580548 199.787402 \n",
       "L 405.713138 207.434955 \n",
       "L 405.845728 198.937674 \n",
       "L 405.978318 204.885771 \n",
       "L 406.110908 198.937674 \n",
       "L 406.177203 202.336587 \n",
       "L 406.243498 198.937674 \n",
       "L 406.309793 203.186315 \n",
       "L 406.376087 202.336587 \n",
       "L 406.442382 198.087946 \n",
       "L 406.508677 199.787402 \n",
       "L 406.574972 203.186315 \n",
       "L 406.641267 198.087946 \n",
       "L 406.707562 198.937674 \n",
       "L 406.773857 199.787402 \n",
       "L 406.906447 197.238218 \n",
       "L 407.039037 204.036043 \n",
       "L 407.105332 202.336587 \n",
       "L 407.171627 208.284683 \n",
       "L 407.370512 200.63713 \n",
       "L 407.436807 197.238218 \n",
       "L 407.569397 201.486858 \n",
       "L 407.635692 199.787402 \n",
       "L 407.701987 204.036043 \n",
       "L 407.768282 197.238218 \n",
       "L 407.834577 198.937674 \n",
       "L 407.900872 204.036043 \n",
       "L 407.967167 198.937674 \n",
       "L 408.033462 200.63713 \n",
       "L 408.166052 197.238218 \n",
       "L 408.232347 198.087946 \n",
       "L 408.298642 198.087946 \n",
       "L 408.431232 202.336587 \n",
       "L 408.497527 198.937674 \n",
       "L 408.563822 202.336587 \n",
       "L 408.630117 198.087946 \n",
       "L 408.696412 204.885771 \n",
       "L 408.762707 204.036043 \n",
       "L 408.829002 200.63713 \n",
       "L 408.895297 203.186315 \n",
       "L 408.961592 199.787402 \n",
       "L 409.027887 204.885771 \n",
       "L 409.094182 202.336587 \n",
       "L 409.160477 203.186315 \n",
       "L 409.226772 205.735499 \n",
       "L 409.359362 200.63713 \n",
       "L 409.491952 204.885771 \n",
       "L 409.558247 200.63713 \n",
       "L 409.624542 205.735499 \n",
       "L 409.690837 199.787402 \n",
       "L 409.757132 203.186315 \n",
       "L 409.823427 199.787402 \n",
       "L 409.889722 201.486858 \n",
       "L 409.956017 198.937674 \n",
       "L 410.088607 202.336587 \n",
       "L 410.154902 201.486858 \n",
       "L 410.287492 207.434955 \n",
       "L 410.353787 198.087946 \n",
       "L 410.420082 200.63713 \n",
       "L 410.486377 198.087946 \n",
       "L 410.552672 199.787402 \n",
       "L 410.618967 198.087946 \n",
       "L 410.685262 198.937674 \n",
       "L 410.817852 201.486858 \n",
       "L 410.884147 201.486858 \n",
       "L 410.950442 197.238218 \n",
       "L 411.016737 201.486858 \n",
       "L 411.083032 200.63713 \n",
       "L 411.149327 201.486858 \n",
       "L 411.215622 204.036043 \n",
       "L 411.281917 198.937674 \n",
       "L 411.348212 204.036043 \n",
       "L 411.480802 197.238218 \n",
       "L 411.679687 204.036043 \n",
       "L 411.745982 200.63713 \n",
       "L 411.812277 204.036043 \n",
       "L 411.944867 199.787402 \n",
       "L 412.077457 200.63713 \n",
       "L 412.143752 198.937674 \n",
       "L 412.210047 202.336587 \n",
       "L 412.276342 198.937674 \n",
       "L 412.342637 200.63713 \n",
       "L 412.408932 203.186315 \n",
       "L 412.475226 202.336587 \n",
       "L 412.541521 197.238218 \n",
       "L 412.607816 199.787402 \n",
       "L 412.674111 201.486858 \n",
       "L 412.740406 200.63713 \n",
       "L 412.806701 201.486858 \n",
       "L 412.872996 204.036043 \n",
       "L 413.005586 198.937674 \n",
       "L 413.071881 202.336587 \n",
       "L 413.138176 200.63713 \n",
       "L 413.204471 202.336587 \n",
       "L 413.270766 198.937674 \n",
       "L 413.337061 200.63713 \n",
       "L 413.403356 198.087946 \n",
       "L 413.469651 204.885771 \n",
       "L 413.535946 202.336587 \n",
       "L 413.602241 204.036043 \n",
       "L 413.668536 199.787402 \n",
       "L 413.801126 206.585227 \n",
       "L 414.000011 197.238218 \n",
       "L 414.066306 198.087946 \n",
       "L 414.198896 203.186315 \n",
       "L 414.265191 203.186315 \n",
       "L 414.331486 200.63713 \n",
       "L 414.397781 201.486858 \n",
       "L 414.464076 201.486858 \n",
       "L 414.530371 198.937674 \n",
       "L 414.596666 201.486858 \n",
       "L 414.662961 199.787402 \n",
       "L 414.795551 199.787402 \n",
       "L 414.861846 198.087946 \n",
       "L 414.928141 204.036043 \n",
       "L 414.994436 200.63713 \n",
       "L 415.193321 204.885771 \n",
       "L 415.392206 199.787402 \n",
       "L 415.657386 206.585227 \n",
       "L 415.723681 205.735499 \n",
       "L 415.789976 198.937674 \n",
       "L 415.856271 206.585227 \n",
       "L 415.922566 203.186315 \n",
       "L 416.055156 198.937674 \n",
       "L 416.121451 198.937674 \n",
       "L 416.187746 202.336587 \n",
       "L 416.254041 198.087946 \n",
       "L 416.320336 198.937674 \n",
       "L 416.452926 197.238218 \n",
       "L 416.651811 203.186315 \n",
       "L 416.718106 204.885771 \n",
       "L 416.784401 198.087946 \n",
       "L 416.850696 198.937674 \n",
       "L 416.916991 199.787402 \n",
       "L 416.983286 204.036043 \n",
       "L 417.049581 198.087946 \n",
       "L 417.115876 200.63713 \n",
       "L 417.182171 204.885771 \n",
       "L 417.248466 203.186315 \n",
       "L 417.447351 198.937674 \n",
       "L 417.513646 203.186315 \n",
       "L 417.579941 201.486858 \n",
       "L 417.712531 202.336587 \n",
       "L 417.845121 196.38849 \n",
       "L 417.911416 204.036043 \n",
       "L 417.977711 199.787402 \n",
       "L 418.044006 205.735499 \n",
       "L 418.110301 198.937674 \n",
       "L 418.176596 203.186315 \n",
       "L 418.242891 198.937674 \n",
       "L 418.309186 199.787402 \n",
       "L 418.441776 201.486858 \n",
       "L 418.574365 198.087946 \n",
       "L 418.64066 201.486858 \n",
       "L 418.706955 198.937674 \n",
       "L 418.77325 200.63713 \n",
       "L 418.839545 204.036043 \n",
       "L 418.90584 198.937674 \n",
       "L 419.03843 203.186315 \n",
       "L 419.237315 197.238218 \n",
       "L 419.30361 202.336587 \n",
       "L 419.369905 198.937674 \n",
       "L 419.502495 202.336587 \n",
       "L 419.56879 198.937674 \n",
       "L 419.635085 204.885771 \n",
       "L 419.767675 197.238218 \n",
       "L 419.96656 204.036043 \n",
       "L 420.032855 204.036043 \n",
       "L 420.09915 198.937674 \n",
       "L 420.165445 201.486858 \n",
       "L 420.23174 202.336587 \n",
       "L 420.430625 200.63713 \n",
       "L 420.49692 201.486858 \n",
       "L 420.563215 197.238218 \n",
       "L 420.62951 205.735499 \n",
       "L 420.695805 201.486858 \n",
       "L 420.7621 205.735499 \n",
       "L 420.828395 199.787402 \n",
       "L 420.89469 205.735499 \n",
       "L 420.960985 198.087946 \n",
       "L 421.02728 200.63713 \n",
       "L 421.15987 204.036043 \n",
       "L 421.29246 198.087946 \n",
       "L 421.55764 204.036043 \n",
       "L 421.756525 198.087946 \n",
       "L 421.889115 204.036043 \n",
       "L 422.021705 198.937674 \n",
       "L 422.088 202.336587 \n",
       "L 422.286885 198.087946 \n",
       "L 422.419475 204.036043 \n",
       "L 422.552065 198.087946 \n",
       "L 422.684655 199.787402 \n",
       "L 422.817245 203.186315 \n",
       "L 422.88354 198.937674 \n",
       "L 422.949835 199.787402 \n",
       "L 423.01613 198.087946 \n",
       "L 423.082425 198.937674 \n",
       "L 423.14872 199.787402 \n",
       "L 423.215015 205.735499 \n",
       "L 423.4139 197.238218 \n",
       "L 423.54649 201.486858 \n",
       "L 423.67908 198.937674 \n",
       "L 423.81167 204.036043 \n",
       "L 423.877965 200.63713 \n",
       "L 423.94426 204.036043 \n",
       "L 424.07685 198.087946 \n",
       "L 424.143145 200.63713 \n",
       "L 424.20944 199.787402 \n",
       "L 424.275735 200.63713 \n",
       "L 424.34203 204.036043 \n",
       "L 424.408325 197.238218 \n",
       "L 424.47462 204.036043 \n",
       "L 424.540915 200.63713 \n",
       "L 424.60721 199.787402 \n",
       "L 424.673504 201.486858 \n",
       "L 424.739799 198.087946 \n",
       "L 424.806094 202.336587 \n",
       "L 424.872389 199.787402 \n",
       "L 424.938684 201.486858 \n",
       "L 425.004979 200.63713 \n",
       "L 425.203864 198.087946 \n",
       "L 425.270159 204.885771 \n",
       "L 425.336454 196.38849 \n",
       "L 425.402749 202.336587 \n",
       "L 425.469044 199.787402 \n",
       "L 425.667929 202.336587 \n",
       "L 425.800519 198.937674 \n",
       "L 425.933109 201.486858 \n",
       "L 425.999404 197.238218 \n",
       "L 426.065699 200.63713 \n",
       "L 426.131994 198.937674 \n",
       "L 426.198289 201.486858 \n",
       "L 426.264584 200.63713 \n",
       "L 426.330879 198.087946 \n",
       "L 426.397174 201.486858 \n",
       "L 426.529764 198.937674 \n",
       "L 426.662354 201.486858 \n",
       "L 426.728649 198.087946 \n",
       "L 426.794944 201.486858 \n",
       "L 426.861239 199.787402 \n",
       "L 426.927534 200.63713 \n",
       "L 426.993829 203.186315 \n",
       "L 427.060124 198.937674 \n",
       "L 427.126419 204.885771 \n",
       "L 427.192714 202.336587 \n",
       "L 427.391599 199.787402 \n",
       "L 427.457894 199.787402 \n",
       "L 427.524189 200.63713 \n",
       "L 427.590484 205.735499 \n",
       "L 427.789369 197.238218 \n",
       "L 427.855664 198.937674 \n",
       "L 427.921959 197.238218 \n",
       "L 428.054549 204.036043 \n",
       "L 428.187139 196.38849 \n",
       "L 428.253434 200.63713 \n",
       "L 428.319729 199.787402 \n",
       "L 428.386024 200.63713 \n",
       "L 428.518614 204.036043 \n",
       "L 428.584909 200.63713 \n",
       "L 428.651204 202.336587 \n",
       "L 428.717499 200.63713 \n",
       "L 428.783794 195.538762 \n",
       "L 428.850089 198.087946 \n",
       "L 428.916384 202.336587 \n",
       "L 428.982679 201.486858 \n",
       "L 429.115269 198.937674 \n",
       "L 429.181564 204.885771 \n",
       "L 429.247859 200.63713 \n",
       "L 429.314154 204.885771 \n",
       "L 429.380449 203.186315 \n",
       "L 429.513039 198.937674 \n",
       "L 429.645629 204.036043 \n",
       "L 429.711924 199.787402 \n",
       "L 429.844514 205.735499 \n",
       "L 429.910809 200.63713 \n",
       "L 429.977104 203.186315 \n",
       "L 430.043399 203.186315 \n",
       "L 430.109694 198.937674 \n",
       "L 430.175989 204.885771 \n",
       "L 430.242284 198.087946 \n",
       "L 430.308579 201.486858 \n",
       "L 430.374874 197.238218 \n",
       "L 430.441169 203.186315 \n",
       "L 430.507464 199.787402 \n",
       "L 430.640054 204.036043 \n",
       "L 430.706349 200.63713 \n",
       "L 430.772643 204.036043 \n",
       "L 430.905233 197.238218 \n",
       "L 431.037823 199.787402 \n",
       "L 431.104118 198.937674 \n",
       "L 431.170413 199.787402 \n",
       "L 431.236708 204.885771 \n",
       "L 431.303003 203.186315 \n",
       "L 431.369298 199.787402 \n",
       "L 431.568183 203.186315 \n",
       "L 431.767068 197.238218 \n",
       "L 431.899658 204.885771 \n",
       "L 431.965953 200.63713 \n",
       "L 432.032248 201.486858 \n",
       "L 432.098543 200.63713 \n",
       "L 432.164838 204.036043 \n",
       "L 432.297428 196.38849 \n",
       "L 432.363723 199.787402 \n",
       "L 432.430018 201.486858 \n",
       "L 432.496313 200.63713 \n",
       "L 432.562608 200.63713 \n",
       "L 432.628903 198.087946 \n",
       "L 432.695198 199.787402 \n",
       "L 432.761493 204.036043 \n",
       "L 432.827788 198.937674 \n",
       "L 432.894083 199.787402 \n",
       "L 432.960378 201.486858 \n",
       "L 433.026673 198.937674 \n",
       "L 433.092968 199.787402 \n",
       "L 433.159263 199.787402 \n",
       "L 433.225558 202.336587 \n",
       "L 433.291853 201.486858 \n",
       "L 433.358148 200.63713 \n",
       "L 433.424443 204.036043 \n",
       "L 433.557033 199.787402 \n",
       "L 433.822213 199.787402 \n",
       "L 433.954803 202.336587 \n",
       "L 434.153688 198.937674 \n",
       "L 434.219983 204.036043 \n",
       "L 434.418868 198.087946 \n",
       "L 434.485163 200.63713 \n",
       "L 434.551458 199.787402 \n",
       "L 434.617753 199.787402 \n",
       "L 434.684048 203.186315 \n",
       "L 434.750343 201.486858 \n",
       "L 434.882933 203.186315 \n",
       "L 435.015523 198.937674 \n",
       "L 435.081818 199.787402 \n",
       "L 435.148113 197.238218 \n",
       "L 435.214408 203.186315 \n",
       "L 435.346998 198.087946 \n",
       "L 435.413293 200.63713 \n",
       "L 435.479588 197.238218 \n",
       "L 435.612178 204.036043 \n",
       "L 435.744768 198.087946 \n",
       "L 435.877358 199.787402 \n",
       "L 435.943653 198.087946 \n",
       "L 436.009948 200.63713 \n",
       "L 436.076243 199.787402 \n",
       "L 436.142538 200.63713 \n",
       "L 436.208833 203.186315 \n",
       "L 436.341423 198.937674 \n",
       "L 436.407718 198.937674 \n",
       "L 436.474013 199.787402 \n",
       "L 436.540308 197.238218 \n",
       "L 436.606603 203.186315 \n",
       "L 436.672898 199.787402 \n",
       "L 436.739193 199.787402 \n",
       "L 436.805488 198.087946 \n",
       "L 436.871783 201.486858 \n",
       "L 436.938077 199.787402 \n",
       "L 437.004372 197.238218 \n",
       "L 437.070667 201.486858 \n",
       "L 437.136962 198.937674 \n",
       "L 437.269552 201.486858 \n",
       "L 437.335847 199.787402 \n",
       "L 437.468437 201.486858 \n",
       "L 437.601027 200.63713 \n",
       "L 437.667322 200.63713 \n",
       "L 437.733617 202.336587 \n",
       "L 437.799912 196.38849 \n",
       "L 437.932502 200.63713 \n",
       "L 437.998797 199.787402 \n",
       "L 438.065092 202.336587 \n",
       "L 438.131387 200.63713 \n",
       "L 438.197682 200.63713 \n",
       "L 438.263977 199.220913 \n",
       "L 438.263977 199.220913 \n",
       "\" clip-path=\"url(#pecab2c8bb3)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 275.09875 299.078125 \n",
       "L 275.09875 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 446.03375 299.078125 \n",
       "L 446.03375 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 275.09875 299.078125 \n",
       "L 446.03375 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 275.09875 189.718125 \n",
       "L 446.03375 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_50\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(331.573125 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_4\">\n",
       "    <g id=\"patch_25\">\n",
       "     <path d=\"M 361.63375 294.078125 \n",
       "L 439.03375 294.078125 \n",
       "Q 441.03375 294.078125 441.03375 292.078125 \n",
       "L 441.03375 263.165625 \n",
       "Q 441.03375 261.165625 439.03375 261.165625 \n",
       "L 361.63375 261.165625 \n",
       "Q 359.63375 261.165625 359.63375 263.165625 \n",
       "L 359.63375 292.078125 \n",
       "Q 359.63375 294.078125 361.63375 294.078125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_47\">\n",
       "     <path d=\"M 363.63375 269.264062 \n",
       "L 373.63375 269.264062 \n",
       "L 383.63375 269.264062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_51\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(391.63375 272.764062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_48\">\n",
       "     <path d=\"M 363.63375 284.220312 \n",
       "L 373.63375 284.220312 \n",
       "L 383.63375 284.220312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_52\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(391.63375 287.720312) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5ea1853c1b\">\n",
       "   <rect x=\"50.14375\" y=\"22.318125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pcd6f6a5529\">\n",
       "   <rect x=\"275.09875\" y=\"22.318125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p3c3172a232\">\n",
       "   <rect x=\"50.14375\" y=\"189.718125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pecab2c8bb3\">\n",
       "   <rect x=\"275.09875\" y=\"189.718125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "net = Net()      \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params= net.parameters(), lr= 0.5)   \n",
    "   \n",
    "trainer= Trainer(\n",
    "    device= 'auto', \n",
    "    train_dataloader= data.DataLoader(train_dataset, batch_size= 128, shuffle= True),\n",
    "    val_dataloader= data.DataLoader(test_dataset, batch_size= 128), \n",
    "    model= net, \n",
    "    loss_fn= loss_fn, \n",
    "    optimizer= opt, \n",
    "    is_tqdm= False\n",
    ")\n",
    "\n",
    "trainer.train(epochs= 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.8.2.2. <a id='toc8_8_2_2_'></a>[不同模型的效率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"453.23375pt\" height=\"336.634375pt\" viewBox=\"0 0 453.23375 336.634375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-28T11:04:11.334275</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 336.634375 \n",
       "L 453.23375 336.634375 \n",
       "L 453.23375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 131.678125 \n",
       "L 221.07875 131.678125 \n",
       "L 221.07875 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mae681c3b4b\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"57.913523\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(54.732273 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"96.762386\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(93.581136 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"135.61125\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(132.43 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"174.460114\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(171.278864 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"213.308977\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(210.127727 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(120.383125 159.954687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m05c44a6906\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"110.563807\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1.55 -->\n",
       "      <g transform=\"translate(20.878125 114.363025) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"83.555781\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1.60 -->\n",
       "      <g transform=\"translate(20.878125 87.355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"56.547755\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1.65 -->\n",
       "      <g transform=\"translate(20.878125 60.346974) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"29.53973\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1.70 -->\n",
       "      <g transform=\"translate(20.878125 33.338948) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 86.655937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_10\">\n",
       "    <path d=\"M 57.913523 101.673185 \n",
       "L 96.762386 114.435985 \n",
       "L 135.61125 121.04545 \n",
       "L 174.460114 124.678856 \n",
       "L 213.308977 126.707216 \n",
       "\" clip-path=\"url(#pb435515c99)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_11\">\n",
       "    <path d=\"M 57.913523 27.289034 \n",
       "L 96.762386 107.160162 \n",
       "L 135.61125 116.557541 \n",
       "L 174.460114 122.364861 \n",
       "L 213.308977 126.532482 \n",
       "\" clip-path=\"url(#pb435515c99)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 131.678125 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 221.07875 131.678125 \n",
       "L 221.07875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 131.678125 \n",
       "L 221.07875 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 221.07875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_12\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(105.30375 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 134.488125 60.230625 \n",
       "L 214.07875 60.230625 \n",
       "Q 216.07875 60.230625 216.07875 58.230625 \n",
       "L 216.07875 29.318125 \n",
       "Q 216.07875 27.318125 214.07875 27.318125 \n",
       "L 134.488125 27.318125 \n",
       "Q 132.488125 27.318125 132.488125 29.318125 \n",
       "L 132.488125 58.230625 \n",
       "Q 132.488125 60.230625 134.488125 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_12\">\n",
       "     <path d=\"M 136.488125 35.416562 \n",
       "L 146.488125 35.416562 \n",
       "L 156.488125 35.416562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(164.488125 38.916562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_13\">\n",
       "     <path d=\"M 136.488125 50.372812 \n",
       "L 146.488125 50.372812 \n",
       "L 156.488125 50.372812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(164.488125 53.872812) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 275.09875 131.678125 \n",
       "L 446.03375 131.678125 \n",
       "L 446.03375 22.318125 \n",
       "L 275.09875 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"282.868523\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(279.687273 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"321.717386\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(318.536136 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"360.56625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(357.385 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"399.415114\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(396.233864 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"438.263977\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(435.082727 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(345.338125 159.954687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"127.306806\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 0.80 -->\n",
       "      <g transform=\"translate(245.833125 131.106025) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"93.392314\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.85 -->\n",
       "      <g transform=\"translate(245.833125 97.191533) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"59.477822\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.90 -->\n",
       "      <g transform=\"translate(245.833125 63.277041) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"25.56333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.95 -->\n",
       "      <g transform=\"translate(245.833125 29.362549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_25\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(239.753437 85.560625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 282.868523 50.744304 \n",
       "L 321.717386 40.749724 \n",
       "L 360.56625 34.176108 \n",
       "L 399.415114 29.883135 \n",
       "L 438.263977 27.602493 \n",
       "\" clip-path=\"url(#p9e8601982c)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_24\">\n",
       "    <path d=\"M 282.868523 126.707216 \n",
       "L 321.717386 47.796364 \n",
       "L 360.56625 38.38446 \n",
       "L 399.415114 32.075961 \n",
       "L 438.263977 27.289034 \n",
       "\" clip-path=\"url(#p9e8601982c)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 275.09875 131.678125 \n",
       "L 275.09875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 446.03375 131.678125 \n",
       "L 446.03375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 275.09875 131.678125 \n",
       "L 446.03375 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 275.09875 22.318125 \n",
       "L 446.03375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_26\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(331.573125 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_2\">\n",
       "    <g id=\"patch_13\">\n",
       "     <path d=\"M 361.63375 126.678125 \n",
       "L 439.03375 126.678125 \n",
       "Q 441.03375 126.678125 441.03375 124.678125 \n",
       "L 441.03375 95.765625 \n",
       "Q 441.03375 93.765625 439.03375 93.765625 \n",
       "L 361.63375 93.765625 \n",
       "Q 359.63375 93.765625 359.63375 95.765625 \n",
       "L 359.63375 124.678125 \n",
       "Q 359.63375 126.678125 361.63375 126.678125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_25\">\n",
       "     <path d=\"M 363.63375 101.864062 \n",
       "L 373.63375 101.864062 \n",
       "L 383.63375 101.864062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_27\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(391.63375 105.364062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_26\">\n",
       "     <path d=\"M 363.63375 116.820312 \n",
       "L 373.63375 116.820312 \n",
       "L 383.63375 116.820312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_28\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(391.63375 120.320312) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 50.14375 299.078125 \n",
       "L 221.07875 299.078125 \n",
       "L 221.07875 189.718125 \n",
       "L 50.14375 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_5\">\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"57.913523\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(54.732273 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"124.208512\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(111.483512 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"190.503501\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(177.778501 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_32\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(124.795625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"277.754834\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_33\">\n",
       "      <!-- 1.6 -->\n",
       "      <g transform=\"translate(27.240625 281.554053) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"254.059372\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_34\">\n",
       "      <!-- 1.8 -->\n",
       "      <g transform=\"translate(27.240625 257.858591) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"230.36391\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_35\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(27.240625 234.163129) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"50.14375\" y=\"206.668448\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_36\">\n",
       "      <!-- 2.2 -->\n",
       "      <g transform=\"translate(27.240625 210.467667) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_37\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(21.160938 254.055937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_34\">\n",
       "    <path d=\"M 57.913523 282.631187 \n",
       "L 57.979818 282.636624 \n",
       "L 58.178703 274.857757 \n",
       "L 58.311293 285.296366 \n",
       "L 58.510178 270.973161 \n",
       "L 58.709063 280.673289 \n",
       "L 58.775358 274.724134 \n",
       "L 58.841653 276.979079 \n",
       "L 58.907948 273.877919 \n",
       "L 59.040538 281.391121 \n",
       "L 59.106833 275.775805 \n",
       "L 59.173128 280.825598 \n",
       "L 59.239423 276.900269 \n",
       "L 59.438307 282.722679 \n",
       "L 59.504602 277.564996 \n",
       "L 59.570897 282.889055 \n",
       "L 59.637192 280.918644 \n",
       "L 59.703487 278.543818 \n",
       "L 59.769782 283.625869 \n",
       "L 59.836077 269.080105 \n",
       "L 59.902372 276.298195 \n",
       "L 60.034962 278.815584 \n",
       "L 60.101257 275.403267 \n",
       "L 60.300142 282.210035 \n",
       "L 60.432732 275.559093 \n",
       "L 60.631617 289.824151 \n",
       "L 60.697912 290.930975 \n",
       "L 60.830502 282.832264 \n",
       "L 60.896797 287.004657 \n",
       "L 61.029387 276.920805 \n",
       "L 61.161977 289.599021 \n",
       "L 61.294567 276.354505 \n",
       "L 61.360862 281.871521 \n",
       "L 61.493452 289.159763 \n",
       "L 61.559747 287.90498 \n",
       "L 61.626042 286.10449 \n",
       "L 61.692337 291.099469 \n",
       "L 61.758632 279.274983 \n",
       "L 61.824927 287.149848 \n",
       "L 61.891222 288.530486 \n",
       "L 61.957517 270.442453 \n",
       "L 62.023812 288.532802 \n",
       "L 62.090107 286.469599 \n",
       "L 62.156402 281.215904 \n",
       "L 62.421582 293.767176 \n",
       "L 62.487877 293.12886 \n",
       "L 62.554172 279.302538 \n",
       "L 62.620467 286.815104 \n",
       "L 62.753057 291.036125 \n",
       "L 62.951942 270.949236 \n",
       "L 63.018237 272.266897 \n",
       "L 63.150827 287.328031 \n",
       "L 63.217122 285.070007 \n",
       "L 63.349712 279.722927 \n",
       "L 63.416007 280.288647 \n",
       "L 63.548597 288.460842 \n",
       "L 63.747482 274.313307 \n",
       "L 63.946367 284.363672 \n",
       "L 64.145252 277.328638 \n",
       "L 64.277842 282.948219 \n",
       "L 64.344137 280.779866 \n",
       "L 64.410432 285.371814 \n",
       "L 64.476727 279.687914 \n",
       "L 64.675612 286.839835 \n",
       "L 64.741907 280.533649 \n",
       "L 64.808202 286.005455 \n",
       "L 64.874497 284.609917 \n",
       "L 64.940792 282.155546 \n",
       "L 65.007087 285.940289 \n",
       "L 65.073382 270.987624 \n",
       "L 65.139677 278.977356 \n",
       "L 65.272267 281.289431 \n",
       "L 65.338562 277.997814 \n",
       "L 65.537446 284.367641 \n",
       "L 65.670036 279.71941 \n",
       "L 65.868921 290.25492 \n",
       "L 65.935216 291.982109 \n",
       "L 66.266691 280.590313 \n",
       "L 66.399281 290.077796 \n",
       "L 66.465576 288.449247 \n",
       "L 66.531871 279.941687 \n",
       "L 66.598166 285.198207 \n",
       "L 66.730756 290.931257 \n",
       "L 66.863346 288.75676 \n",
       "L 66.929641 291.792288 \n",
       "L 66.995936 284.050636 \n",
       "L 67.062231 291.85922 \n",
       "L 67.128526 290.833564 \n",
       "L 67.194821 273.713464 \n",
       "L 67.261116 289.528643 \n",
       "L 67.327411 287.639965 \n",
       "L 67.393706 285.44924 \n",
       "L 67.658886 294.107216 \n",
       "L 67.725181 293.659893 \n",
       "L 67.791476 283.880376 \n",
       "L 67.857771 289.046039 \n",
       "L 67.924066 291.965344 \n",
       "L 67.990361 290.601316 \n",
       "L 68.056656 288.33212 \n",
       "L 68.189246 274.745362 \n",
       "L 68.255541 276.126692 \n",
       "L 68.388131 287.95348 \n",
       "L 68.454426 287.325927 \n",
       "L 68.653311 281.146486 \n",
       "L 68.785901 289.360544 \n",
       "L 68.984786 276.155292 \n",
       "L 69.183671 285.773645 \n",
       "L 69.249966 279.953296 \n",
       "L 69.316261 283.222457 \n",
       "L 69.382556 279.770989 \n",
       "L 69.448851 281.960457 \n",
       "L 69.515146 284.831572 \n",
       "L 69.581441 281.947689 \n",
       "L 69.647736 287.589798 \n",
       "L 69.714031 283.21918 \n",
       "L 69.912916 287.807457 \n",
       "L 69.979211 282.352712 \n",
       "L 70.111801 287.352027 \n",
       "L 70.178096 283.88806 \n",
       "L 70.244391 287.134354 \n",
       "L 70.310686 273.467572 \n",
       "L 70.376981 279.388776 \n",
       "L 70.575866 282.278535 \n",
       "L 70.642161 282.747593 \n",
       "L 70.774751 287.022043 \n",
       "L 70.907341 281.417942 \n",
       "L 71.106226 292.366469 \n",
       "L 71.172521 292.324762 \n",
       "L 71.305111 285.877495 \n",
       "L 71.371406 290.503312 \n",
       "L 71.503996 282.435547 \n",
       "L 71.636585 291.365261 \n",
       "L 71.769175 279.859798 \n",
       "L 71.96806 291.730638 \n",
       "L 72.10065 290.299593 \n",
       "L 72.166945 293.216059 \n",
       "L 72.23324 285.750326 \n",
       "L 72.299535 291.85905 \n",
       "L 72.36583 291.309699 \n",
       "L 72.432125 280.530429 \n",
       "L 72.49842 290.303576 \n",
       "L 72.564715 288.275527 \n",
       "L 72.63101 287.091687 \n",
       "L 72.7636 291.592284 \n",
       "L 72.89619 294.064676 \n",
       "L 72.962485 293.348044 \n",
       "L 73.02878 283.776667 \n",
       "L 73.095075 289.642055 \n",
       "L 73.16137 292.367824 \n",
       "L 73.227665 291.285929 \n",
       "L 73.29396 290.449332 \n",
       "L 73.42655 276.414884 \n",
       "L 73.492845 278.500487 \n",
       "L 73.55914 289.856296 \n",
       "L 73.625435 289.442785 \n",
       "L 73.82432 283.510169 \n",
       "L 73.890615 283.996317 \n",
       "L 74.023205 289.66139 \n",
       "L 74.22209 276.571712 \n",
       "L 74.288385 285.570801 \n",
       "L 74.35468 282.370889 \n",
       "L 74.420975 285.58597 \n",
       "L 74.48727 281.992631 \n",
       "L 74.553565 285.116841 \n",
       "L 74.61986 280.724967 \n",
       "L 74.75245 284.65007 \n",
       "L 74.818745 283.345078 \n",
       "L 74.88504 288.018068 \n",
       "L 74.951335 285.004332 \n",
       "L 75.15022 287.619938 \n",
       "L 75.216515 283.503644 \n",
       "L 75.349105 289.160907 \n",
       "L 75.54799 275.342945 \n",
       "L 75.68058 283.23966 \n",
       "L 75.746875 282.790204 \n",
       "L 75.81317 282.50786 \n",
       "L 76.012055 286.90349 \n",
       "L 76.144645 281.718407 \n",
       "L 76.34353 292.065466 \n",
       "L 76.409825 292.686523 \n",
       "L 76.542415 288.035171 \n",
       "L 76.60871 291.266918 \n",
       "L 76.7413 283.623426 \n",
       "L 76.87389 291.831622 \n",
       "L 76.940185 290.014621 \n",
       "L 77.00648 280.219427 \n",
       "L 77.205365 292.204768 \n",
       "L 77.337955 291.327381 \n",
       "L 77.40425 293.218051 \n",
       "L 77.470545 286.298661 \n",
       "L 77.53684 291.986304 \n",
       "L 77.603135 291.557074 \n",
       "L 77.669429 282.605595 \n",
       "L 77.735724 291.243855 \n",
       "L 77.802019 288.879805 \n",
       "L 77.868314 287.773885 \n",
       "L 77.934609 287.798516 \n",
       "L 78.133494 294.013859 \n",
       "L 78.199789 293.791794 \n",
       "L 78.266084 285.017623 \n",
       "L 78.398674 292.394871 \n",
       "L 78.531264 290.426706 \n",
       "L 78.663854 277.741245 \n",
       "L 78.730149 280.618984 \n",
       "L 78.796444 292.553183 \n",
       "L 78.862739 289.989919 \n",
       "L 78.929034 288.755828 \n",
       "L 79.061624 285.140667 \n",
       "L 79.127919 285.137433 \n",
       "L 79.260509 289.077111 \n",
       "L 79.459394 278.18207 \n",
       "L 79.525689 285.332522 \n",
       "L 79.591984 282.318575 \n",
       "L 79.658279 285.921575 \n",
       "L 79.724574 282.732312 \n",
       "L 79.790869 285.495692 \n",
       "L 79.857164 281.052663 \n",
       "L 79.923459 283.856536 \n",
       "L 80.056049 282.852461 \n",
       "L 80.122344 288.180164 \n",
       "L 80.188639 284.55612 \n",
       "L 80.254934 288.055071 \n",
       "L 80.321229 286.052388 \n",
       "L 80.387524 288.244751 \n",
       "L 80.453819 284.085705 \n",
       "L 80.586409 289.466089 \n",
       "L 80.652704 284.948728 \n",
       "L 80.718999 288.246333 \n",
       "L 80.785294 276.891908 \n",
       "L 80.851589 280.692088 \n",
       "L 80.917884 283.685725 \n",
       "L 80.984179 282.02609 \n",
       "L 81.249359 287.518488 \n",
       "L 81.381949 283.407109 \n",
       "L 81.580834 292.683261 \n",
       "L 81.713424 291.607707 \n",
       "L 81.912309 283.686092 \n",
       "L 82.111194 291.127829 \n",
       "L 82.177489 290.120195 \n",
       "L 82.243784 282.00972 \n",
       "L 82.442669 291.7688 \n",
       "L 82.575259 291.38567 \n",
       "L 82.641554 293.003881 \n",
       "L 82.707849 286.445786 \n",
       "L 82.774144 292.583887 \n",
       "L 82.840439 292.298718 \n",
       "L 82.906734 283.916928 \n",
       "L 82.973029 291.177092 \n",
       "L 83.039324 288.580568 \n",
       "L 83.370799 294.041951 \n",
       "L 83.437094 293.57696 \n",
       "L 83.503389 286.782818 \n",
       "L 83.569684 292.480573 \n",
       "L 83.635979 292.339549 \n",
       "L 83.901158 279.285688 \n",
       "L 83.967453 282.887798 \n",
       "L 84.033748 293.939258 \n",
       "L 84.033748 293.939258 \n",
       "\" clip-path=\"url(#pec08b2ec4f)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_35\">\n",
       "    <path d=\"M 57.913523 194.689034 \n",
       "L 58.046113 194.910916 \n",
       "L 58.775358 197.856392 \n",
       "L 58.841653 197.799671 \n",
       "L 59.173128 201.980595 \n",
       "L 59.239423 201.959777 \n",
       "L 59.305717 202.140898 \n",
       "L 59.372012 207.764462 \n",
       "L 59.438307 204.127099 \n",
       "L 59.637192 210.516896 \n",
       "L 59.703487 209.931953 \n",
       "L 59.968667 220.152663 \n",
       "L 60.034962 217.9802 \n",
       "L 60.101257 217.993024 \n",
       "L 60.167552 223.897337 \n",
       "L 60.233847 223.068875 \n",
       "L 60.300142 225.180381 \n",
       "L 60.366437 224.402114 \n",
       "L 60.432732 224.190317 \n",
       "L 60.565322 230.738383 \n",
       "L 60.697912 233.92735 \n",
       "L 60.764207 230.485288 \n",
       "L 60.830502 239.949425 \n",
       "L 60.896797 235.790337 \n",
       "L 60.963092 238.896907 \n",
       "L 61.029387 234.023871 \n",
       "L 61.095682 240.506502 \n",
       "L 61.161977 239.705963 \n",
       "L 61.228272 237.958775 \n",
       "L 61.294567 243.495705 \n",
       "L 61.360862 237.588765 \n",
       "L 61.427157 242.417213 \n",
       "L 61.493452 241.268767 \n",
       "L 61.559747 242.621285 \n",
       "L 61.626042 241.681727 \n",
       "L 61.692337 238.356015 \n",
       "L 61.891222 248.439952 \n",
       "L 61.957517 248.573717 \n",
       "L 62.023812 240.878942 \n",
       "L 62.156402 253.423265 \n",
       "L 62.222697 246.524863 \n",
       "L 62.288992 254.795316 \n",
       "L 62.355287 250.841119 \n",
       "L 62.487877 253.198573 \n",
       "L 62.554172 248.464612 \n",
       "L 62.686762 257.990808 \n",
       "L 62.753057 256.408118 \n",
       "L 62.819352 256.039845 \n",
       "L 62.885647 249.261268 \n",
       "L 62.951942 250.383246 \n",
       "L 63.018237 247.277707 \n",
       "L 63.084532 261.095245 \n",
       "L 63.150827 256.242646 \n",
       "L 63.217122 256.189216 \n",
       "L 63.283417 250.15419 \n",
       "L 63.548597 263.576577 \n",
       "L 63.681187 252.58116 \n",
       "L 63.747482 259.400428 \n",
       "L 63.813777 256.496517 \n",
       "L 63.880072 260.014889 \n",
       "L 63.946367 258.586259 \n",
       "L 64.012662 253.420426 \n",
       "L 64.211547 262.456463 \n",
       "L 64.277842 261.953239 \n",
       "L 64.344137 257.982645 \n",
       "L 64.410432 259.332747 \n",
       "L 64.476727 257.197825 \n",
       "L 64.543022 260.136154 \n",
       "L 64.609317 253.677575 \n",
       "L 64.675612 261.713463 \n",
       "L 64.741907 255.636814 \n",
       "L 64.808202 266.104108 \n",
       "L 64.874497 260.228438 \n",
       "L 64.940792 265.633637 \n",
       "L 65.007087 257.634173 \n",
       "L 65.073382 259.929639 \n",
       "L 65.139677 264.012092 \n",
       "L 65.205972 262.2852 \n",
       "L 65.272267 264.421013 \n",
       "L 65.338562 254.720475 \n",
       "L 65.404857 257.266705 \n",
       "L 65.603741 253.609146 \n",
       "L 65.736331 267.018893 \n",
       "L 65.802626 260.039083 \n",
       "L 65.868921 263.095697 \n",
       "L 65.935216 269.119947 \n",
       "L 66.001511 256.795387 \n",
       "L 66.067806 259.292495 \n",
       "L 66.134101 257.02426 \n",
       "L 66.266691 263.723223 \n",
       "L 66.332986 253.574374 \n",
       "L 66.399281 262.895326 \n",
       "L 66.465576 258.589536 \n",
       "L 66.598166 262.901314 \n",
       "L 66.664461 258.814948 \n",
       "L 66.797051 266.073544 \n",
       "L 66.929641 257.8509 \n",
       "L 66.995936 267.46849 \n",
       "L 67.062231 263.322296 \n",
       "L 67.194821 262.108627 \n",
       "L 67.261116 271.697772 \n",
       "L 67.327411 268.452509 \n",
       "L 67.393706 259.930218 \n",
       "L 67.460001 265.793177 \n",
       "L 67.526296 262.06096 \n",
       "L 67.658886 268.330156 \n",
       "L 67.725181 263.435694 \n",
       "L 67.791476 269.001606 \n",
       "L 67.990361 262.406141 \n",
       "L 68.056656 273.935557 \n",
       "L 68.122951 265.82278 \n",
       "L 68.189246 268.303519 \n",
       "L 68.255541 265.20959 \n",
       "L 68.321836 266.759344 \n",
       "L 68.388131 267.091432 \n",
       "L 68.520721 270.120096 \n",
       "L 68.587016 264.606541 \n",
       "L 68.653311 266.318801 \n",
       "L 68.719606 271.498516 \n",
       "L 68.918491 265.24802 \n",
       "L 68.984786 272.489343 \n",
       "L 69.051081 269.394651 \n",
       "L 69.117376 264.126988 \n",
       "L 69.183671 267.048101 \n",
       "L 69.249966 269.197585 \n",
       "L 69.316261 266.996479 \n",
       "L 69.515146 269.852623 \n",
       "L 69.581441 269.089596 \n",
       "L 69.647736 263.67968 \n",
       "L 69.714031 269.802371 \n",
       "L 69.780326 268.901512 \n",
       "L 69.846621 268.513749 \n",
       "L 69.912916 270.927627 \n",
       "L 70.045506 263.310785 \n",
       "L 70.111801 264.096156 \n",
       "L 70.310686 272.424135 \n",
       "L 70.376981 267.106502 \n",
       "L 70.443276 269.276366 \n",
       "L 70.509571 270.647188 \n",
       "L 70.575866 262.339901 \n",
       "L 70.642161 264.745559 \n",
       "L 70.774751 272.845371 \n",
       "L 70.907341 267.917507 \n",
       "L 70.973636 272.991027 \n",
       "L 71.039931 265.444169 \n",
       "L 71.106226 270.168921 \n",
       "L 71.172521 269.676671 \n",
       "L 71.305111 266.527731 \n",
       "L 71.371406 274.30931 \n",
       "L 71.437701 269.157078 \n",
       "L 71.503996 272.608942 \n",
       "L 71.57029 266.587502 \n",
       "L 71.636585 269.507584 \n",
       "L 71.70288 279.532131 \n",
       "L 71.901765 262.948981 \n",
       "L 71.96806 275.029373 \n",
       "L 72.034355 270.743299 \n",
       "L 72.10065 269.55994 \n",
       "L 72.166945 270.051653 \n",
       "L 72.23324 270.157608 \n",
       "L 72.299535 274.576147 \n",
       "L 72.36583 270.577319 \n",
       "L 72.49842 278.816757 \n",
       "L 72.63101 268.808748 \n",
       "L 72.829895 277.235041 \n",
       "L 72.89619 269.339993 \n",
       "L 72.962485 273.954285 \n",
       "L 73.02878 271.235196 \n",
       "L 73.095075 274.721888 \n",
       "L 73.16137 273.40516 \n",
       "L 73.227665 271.210127 \n",
       "L 73.29396 276.323956 \n",
       "L 73.42655 271.050897 \n",
       "L 73.625435 274.207027 \n",
       "L 73.758025 272.101467 \n",
       "L 73.890615 276.479104 \n",
       "L 73.95691 274.984743 \n",
       "L 74.023205 274.825315 \n",
       "L 74.0895 275.765933 \n",
       "L 74.155795 280.08257 \n",
       "L 74.35468 270.383204 \n",
       "L 74.48727 281.162658 \n",
       "L 74.61986 274.559749 \n",
       "L 74.686155 273.276988 \n",
       "L 74.818745 278.133132 \n",
       "L 74.88504 271.751428 \n",
       "L 74.951335 273.734381 \n",
       "L 75.01763 275.43239 \n",
       "L 75.083925 273.524052 \n",
       "L 75.15022 279.571267 \n",
       "L 75.216515 276.109277 \n",
       "L 75.28281 274.296288 \n",
       "L 75.349105 275.718209 \n",
       "L 75.4154 280.116537 \n",
       "L 75.54799 274.486646 \n",
       "L 75.614285 277.431472 \n",
       "L 75.68058 274.945648 \n",
       "L 75.81317 278.662838 \n",
       "L 75.879465 278.9337 \n",
       "L 76.012055 277.146091 \n",
       "L 76.07835 274.479174 \n",
       "L 76.144645 278.184895 \n",
       "L 76.277235 272.463073 \n",
       "L 76.409825 276.894902 \n",
       "L 76.47612 276.688006 \n",
       "L 76.542415 277.909132 \n",
       "L 76.60871 272.92411 \n",
       "L 76.675005 273.273937 \n",
       "L 76.87389 281.422715 \n",
       "L 77.00648 274.116212 \n",
       "L 77.27166 284.174458 \n",
       "L 77.337955 280.345495 \n",
       "L 77.40425 283.702292 \n",
       "L 77.53684 277.541537 \n",
       "L 77.603135 284.688232 \n",
       "L 77.735724 272.837151 \n",
       "L 77.802019 280.433894 \n",
       "L 77.868314 276.620142 \n",
       "L 77.934609 282.01424 \n",
       "L 78.000904 277.054909 \n",
       "L 78.067199 277.965711 \n",
       "L 78.133494 283.239151 \n",
       "L 78.199789 280.297319 \n",
       "L 78.266084 278.959589 \n",
       "L 78.332379 283.746725 \n",
       "L 78.398674 278.936115 \n",
       "L 78.464969 278.995886 \n",
       "L 78.531264 280.149459 \n",
       "L 78.663854 277.86799 \n",
       "L 78.730149 274.981706 \n",
       "L 78.796444 278.955592 \n",
       "L 78.862739 278.575695 \n",
       "L 78.929034 279.79257 \n",
       "L 78.995329 276.653643 \n",
       "L 79.061624 281.29751 \n",
       "L 79.127919 279.105372 \n",
       "L 79.194214 276.98124 \n",
       "L 79.260509 278.227845 \n",
       "L 79.326804 276.849961 \n",
       "L 79.393099 281.600419 \n",
       "L 79.459394 277.121332 \n",
       "L 79.525689 279.708845 \n",
       "L 79.591984 277.597509 \n",
       "L 79.658279 282.822194 \n",
       "L 79.724574 282.758059 \n",
       "L 79.790869 274.119531 \n",
       "L 79.857164 276.92387 \n",
       "L 79.923459 275.207796 \n",
       "L 79.989754 278.26999 \n",
       "L 80.056049 271.426486 \n",
       "L 80.122344 282.083276 \n",
       "L 80.188639 276.807152 \n",
       "L 80.321229 283.495593 \n",
       "L 80.387524 278.544779 \n",
       "L 80.453819 281.243162 \n",
       "L 80.520114 277.143096 \n",
       "L 80.586409 279.695061 \n",
       "L 80.785294 282.972766 \n",
       "L 80.984179 276.578986 \n",
       "L 81.116769 281.539673 \n",
       "L 81.183064 279.366306 \n",
       "L 81.249359 280.553733 \n",
       "L 81.315654 280.80485 \n",
       "L 81.381949 277.509293 \n",
       "L 81.448244 278.139205 \n",
       "L 81.514539 279.258952 \n",
       "L 81.580834 283.064287 \n",
       "L 81.713424 277.079342 \n",
       "L 81.779719 277.114397 \n",
       "L 81.846014 277.892424 \n",
       "L 81.912309 275.333708 \n",
       "L 81.978604 281.637973 \n",
       "L 82.044899 278.157608 \n",
       "L 82.111194 278.917726 \n",
       "L 82.177489 283.223432 \n",
       "L 82.243784 276.609253 \n",
       "L 82.310079 277.476964 \n",
       "L 82.376374 282.538056 \n",
       "L 82.442669 280.089745 \n",
       "L 82.508964 277.234165 \n",
       "L 82.575259 283.453364 \n",
       "L 82.641554 280.309494 \n",
       "L 82.707849 280.846656 \n",
       "L 82.774144 276.573124 \n",
       "L 82.840439 277.120456 \n",
       "L 82.906734 275.384666 \n",
       "L 83.039324 282.249242 \n",
       "L 83.105619 282.528212 \n",
       "L 83.171914 277.306111 \n",
       "L 83.238209 279.685344 \n",
       "L 83.304504 280.216744 \n",
       "L 83.370799 283.381658 \n",
       "L 83.437094 282.075819 \n",
       "L 83.569684 276.664391 \n",
       "L 83.635979 280.4695 \n",
       "L 83.702274 277.643043 \n",
       "L 83.834863 284.632317 \n",
       "L 83.901158 276.618009 \n",
       "L 84.033748 283.793982 \n",
       "L 84.232633 280.292446 \n",
       "L 84.431518 282.918094 \n",
       "L 84.564108 278.426324 \n",
       "L 84.630403 282.953275 \n",
       "L 84.696698 274.263366 \n",
       "L 84.762993 276.137736 \n",
       "L 84.829288 285.836537 \n",
       "L 84.895583 284.227492 \n",
       "L 85.028173 286.039903 \n",
       "L 85.094468 273.983252 \n",
       "L 85.160763 281.178307 \n",
       "L 85.425943 279.436048 \n",
       "L 85.492238 274.655507 \n",
       "L 85.624828 281.564206 \n",
       "L 85.691123 279.313356 \n",
       "L 85.823713 283.337169 \n",
       "L 85.890008 280.644152 \n",
       "L 85.956303 282.133754 \n",
       "L 86.022598 283.762826 \n",
       "L 86.088893 274.929773 \n",
       "L 86.155188 280.771928 \n",
       "L 86.221483 275.320658 \n",
       "L 86.354073 281.598187 \n",
       "L 86.420368 280.585553 \n",
       "L 86.486663 283.53754 \n",
       "L 86.552958 278.213001 \n",
       "L 86.619253 281.044401 \n",
       "L 86.685548 282.769485 \n",
       "L 86.751843 282.032728 \n",
       "L 86.818138 280.828056 \n",
       "L 86.884433 282.023759 \n",
       "L 87.017023 278.85852 \n",
       "L 87.149613 285.838147 \n",
       "L 87.215908 279.962039 \n",
       "L 87.282203 280.858266 \n",
       "L 87.414793 283.279333 \n",
       "L 87.481088 280.072104 \n",
       "L 87.613678 283.567045 \n",
       "L 87.746268 273.217669 \n",
       "L 87.812563 283.7489 \n",
       "L 87.878858 276.060622 \n",
       "L 87.945153 281.942619 \n",
       "L 88.011448 278.575102 \n",
       "L 88.077743 278.90212 \n",
       "L 88.144038 284.363065 \n",
       "L 88.210333 277.472967 \n",
       "L 88.276628 283.457558 \n",
       "L 88.342923 281.93841 \n",
       "L 88.475513 277.691742 \n",
       "L 88.541808 281.771145 \n",
       "L 88.608103 278.769513 \n",
       "L 88.674398 280.124503 \n",
       "L 88.740693 278.654702 \n",
       "L 88.806988 283.968183 \n",
       "L 88.873283 278.150222 \n",
       "L 88.939578 283.413281 \n",
       "L 89.005873 280.643587 \n",
       "L 89.138463 286.075197 \n",
       "L 89.204758 284.526079 \n",
       "L 89.271053 277.896096 \n",
       "L 89.403643 283.445073 \n",
       "L 89.469938 284.015116 \n",
       "L 89.668823 277.465764 \n",
       "L 89.735118 282.683811 \n",
       "L 89.801413 280.627924 \n",
       "L 89.867707 278.624139 \n",
       "L 90.000297 285.105768 \n",
       "L 90.199182 281.573005 \n",
       "L 90.398067 287.101432 \n",
       "L 90.596952 275.787895 \n",
       "L 90.795837 283.407278 \n",
       "L 90.928427 279.94091 \n",
       "L 90.994722 284.811333 \n",
       "L 91.193607 276.687709 \n",
       "L 91.392492 280.803269 \n",
       "L 91.458787 280.781871 \n",
       "L 91.591377 285.565392 \n",
       "L 91.790262 278.33719 \n",
       "L 91.922852 285.971318 \n",
       "L 91.989147 275.743533 \n",
       "L 92.055442 284.373601 \n",
       "L 92.121737 282.137016 \n",
       "L 92.254327 278.956312 \n",
       "L 92.519507 282.225769 \n",
       "L 92.585802 277.455157 \n",
       "L 92.652097 280.09788 \n",
       "L 92.718392 286.130562 \n",
       "L 92.784687 284.626399 \n",
       "L 93.049867 279.637126 \n",
       "L 93.116162 284.725886 \n",
       "L 93.182457 281.30758 \n",
       "L 93.248752 281.035122 \n",
       "L 93.315047 279.520069 \n",
       "L 93.381342 281.910318 \n",
       "L 93.513932 278.451831 \n",
       "L 93.580227 283.389638 \n",
       "L 93.646522 279.997023 \n",
       "L 93.712817 282.462014 \n",
       "L 93.845407 279.404722 \n",
       "L 93.911702 279.409891 \n",
       "L 93.977997 283.426797 \n",
       "L 94.110587 278.979432 \n",
       "L 94.176882 283.910361 \n",
       "L 94.243177 281.356928 \n",
       "L 94.309472 282.737495 \n",
       "L 94.375767 279.937549 \n",
       "L 94.442062 280.219385 \n",
       "L 94.508357 285.094088 \n",
       "L 94.574652 277.656955 \n",
       "L 94.640947 284.659067 \n",
       "L 94.707242 283.569022 \n",
       "L 94.773537 284.363997 \n",
       "L 94.839832 283.54182 \n",
       "L 94.906127 281.01001 \n",
       "L 95.038717 284.055495 \n",
       "L 95.105012 282.034013 \n",
       "L 95.171307 284.040072 \n",
       "L 95.237602 283.049598 \n",
       "L 95.303897 282.409305 \n",
       "L 95.370192 283.377054 \n",
       "L 95.436487 283.180213 \n",
       "L 95.569077 278.908518 \n",
       "L 95.635372 281.116756 \n",
       "L 95.701667 280.988514 \n",
       "L 95.767962 280.816121 \n",
       "L 95.834257 279.74736 \n",
       "L 95.900552 284.403557 \n",
       "L 95.966846 282.266162 \n",
       "L 96.099436 278.271501 \n",
       "L 96.165731 284.384645 \n",
       "L 96.232026 277.284474 \n",
       "L 96.298321 278.74313 \n",
       "L 96.430911 283.396163 \n",
       "L 96.497206 281.258416 \n",
       "L 96.696091 286.579071 \n",
       "L 96.762386 285.853598 \n",
       "L 96.828681 283.177939 \n",
       "L 96.961271 286.302404 \n",
       "L 97.093861 279.702758 \n",
       "L 97.160156 284.327586 \n",
       "L 97.226451 281.833359 \n",
       "L 97.292746 279.40242 \n",
       "L 97.425336 287.001832 \n",
       "L 97.557926 276.978585 \n",
       "L 97.624221 281.869812 \n",
       "L 97.690516 282.359435 \n",
       "L 97.756811 281.376277 \n",
       "L 97.823106 276.299607 \n",
       "L 97.889401 283.190453 \n",
       "L 97.955696 280.907416 \n",
       "L 98.021991 278.458964 \n",
       "L 98.088286 283.155977 \n",
       "L 98.154581 281.079272 \n",
       "L 98.220876 279.175016 \n",
       "L 98.287171 284.5056 \n",
       "L 98.353466 279.218799 \n",
       "L 98.419761 282.752847 \n",
       "L 98.486056 282.592502 \n",
       "L 98.552351 285.606223 \n",
       "L 98.618646 279.29912 \n",
       "L 98.684941 285.976572 \n",
       "L 98.751236 284.932669 \n",
       "L 98.817531 280.888321 \n",
       "L 98.883826 287.052635 \n",
       "L 98.950121 273.417645 \n",
       "L 99.016416 283.778884 \n",
       "L 99.082711 280.464599 \n",
       "L 99.281596 286.03681 \n",
       "L 99.347891 282.425222 \n",
       "L 99.480481 285.268994 \n",
       "L 99.546776 279.368791 \n",
       "L 99.613071 287.643651 \n",
       "L 99.679366 282.412398 \n",
       "L 99.745661 281.617367 \n",
       "L 99.811956 286.165249 \n",
       "L 99.878251 279.738844 \n",
       "L 99.944546 285.984496 \n",
       "L 100.010841 285.277059 \n",
       "L 100.143431 282.393402 \n",
       "L 100.209726 282.950253 \n",
       "L 100.276021 285.729664 \n",
       "L 100.408611 275.955118 \n",
       "L 100.474906 276.462579 \n",
       "L 100.541201 280.9872 \n",
       "L 100.607496 278.547504 \n",
       "L 100.806381 282.315242 \n",
       "L 101.005266 280.385549 \n",
       "L 101.204151 287.023908 \n",
       "L 101.270446 284.566218 \n",
       "L 101.336741 285.040729 \n",
       "L 101.403036 285.643863 \n",
       "L 101.469331 280.929971 \n",
       "L 101.668216 285.478645 \n",
       "L 101.734511 281.751075 \n",
       "L 101.800806 284.667569 \n",
       "L 101.867101 284.064167 \n",
       "L 102.065985 288.264581 \n",
       "L 102.198575 278.06935 \n",
       "L 102.26487 281.739536 \n",
       "L 102.331165 281.252074 \n",
       "L 102.39746 281.064569 \n",
       "L 102.53005 287.248021 \n",
       "L 102.596345 282.065819 \n",
       "L 102.66264 285.065021 \n",
       "L 102.728935 281.517682 \n",
       "L 102.79523 283.971657 \n",
       "L 102.861525 281.345007 \n",
       "L 102.994115 284.820033 \n",
       "L 103.32559 275.367012 \n",
       "L 103.391885 284.284961 \n",
       "L 103.45818 281.257498 \n",
       "L 103.657065 289.271692 \n",
       "L 103.72336 284.703923 \n",
       "L 103.789655 285.152051 \n",
       "L 103.98854 280.877177 \n",
       "L 104.054835 286.330001 \n",
       "L 104.12113 282.52043 \n",
       "L 104.187425 287.762148 \n",
       "L 104.38631 281.926914 \n",
       "L 104.452605 278.661778 \n",
       "L 104.5189 279.566423 \n",
       "L 104.65149 278.804074 \n",
       "L 104.850375 286.434473 \n",
       "L 105.04926 277.955457 \n",
       "L 105.115555 288.128712 \n",
       "L 105.18185 287.967279 \n",
       "L 105.31444 282.282292 \n",
       "L 105.380735 283.411501 \n",
       "L 105.44703 284.501631 \n",
       "L 105.513325 281.168109 \n",
       "L 105.57962 282.496292 \n",
       "L 105.645915 281.172092 \n",
       "L 105.71221 283.419213 \n",
       "L 105.8448 278.805726 \n",
       "L 105.911095 279.533783 \n",
       "L 105.97739 279.255676 \n",
       "L 106.043685 280.490954 \n",
       "L 106.10998 286.476336 \n",
       "L 106.176275 282.529864 \n",
       "L 106.24257 285.001917 \n",
       "L 106.308865 280.338532 \n",
       "L 106.37516 281.59073 \n",
       "L 106.441455 285.615051 \n",
       "L 106.574045 280.63178 \n",
       "L 106.64034 287.082422 \n",
       "L 106.706635 284.39456 \n",
       "L 106.77293 285.121078 \n",
       "L 106.839225 276.797492 \n",
       "L 106.971815 285.335065 \n",
       "L 107.104405 278.591937 \n",
       "L 107.1707 284.788142 \n",
       "L 107.236995 282.536079 \n",
       "L 107.30329 281.377435 \n",
       "L 107.502175 284.055156 \n",
       "L 107.634765 278.768129 \n",
       "L 107.70106 287.771568 \n",
       "L 107.767355 282.682639 \n",
       "L 107.83365 284.17587 \n",
       "L 107.899945 280.412794 \n",
       "L 108.032535 284.645113 \n",
       "L 108.09883 282.166761 \n",
       "L 108.165124 284.071921 \n",
       "L 108.231419 280.3562 \n",
       "L 108.364009 287.12427 \n",
       "L 108.430304 282.581655 \n",
       "L 108.562894 287.117801 \n",
       "L 108.629189 281.968833 \n",
       "L 108.695484 285.82665 \n",
       "L 108.828074 280.451761 \n",
       "L 108.894369 286.896795 \n",
       "L 108.960664 280.138457 \n",
       "L 109.026959 285.978578 \n",
       "L 109.093254 284.56345 \n",
       "L 109.159549 284.075508 \n",
       "L 109.225844 287.132998 \n",
       "L 109.424729 282.940338 \n",
       "L 109.491024 284.096143 \n",
       "L 109.557319 281.247724 \n",
       "L 109.756204 286.86972 \n",
       "L 109.822499 286.704531 \n",
       "L 109.888794 281.612014 \n",
       "L 109.955089 283.407561 \n",
       "L 110.021384 283.019501 \n",
       "L 110.153974 285.254828 \n",
       "L 110.352859 282.057218 \n",
       "L 110.419154 282.447128 \n",
       "L 110.485449 282.396184 \n",
       "L 110.551744 287.072013 \n",
       "L 110.618039 287.04611 \n",
       "L 110.684334 284.71357 \n",
       "L 110.750629 278.298209 \n",
       "L 110.816924 282.513594 \n",
       "L 110.949514 286.335665 \n",
       "L 111.015809 280.984714 \n",
       "L 111.082104 286.450984 \n",
       "L 111.148399 282.564199 \n",
       "L 111.214694 284.930494 \n",
       "L 111.347284 281.517527 \n",
       "L 111.546169 286.9175 \n",
       "L 111.745054 282.20118 \n",
       "L 111.877644 288.519653 \n",
       "L 112.076529 281.434141 \n",
       "L 112.142824 282.410859 \n",
       "L 112.209119 280.522308 \n",
       "L 112.341709 286.939449 \n",
       "L 112.474299 283.022495 \n",
       "L 112.540594 285.799025 \n",
       "L 112.606889 281.01967 \n",
       "L 112.673184 284.780417 \n",
       "L 112.739479 282.151564 \n",
       "L 112.805774 284.257985 \n",
       "L 112.872069 282.266219 \n",
       "L 112.938364 282.751633 \n",
       "L 113.004659 282.69213 \n",
       "L 113.070954 278.721055 \n",
       "L 113.137249 287.305476 \n",
       "L 113.203544 283.820083 \n",
       "L 113.336134 286.086468 \n",
       "L 113.402429 277.303569 \n",
       "L 113.468724 280.430208 \n",
       "L 113.667609 284.164416 \n",
       "L 113.800199 280.757706 \n",
       "L 113.932789 285.35214 \n",
       "L 113.999084 279.364893 \n",
       "L 114.065379 282.45532 \n",
       "L 114.330558 288.84242 \n",
       "L 114.396853 288.049267 \n",
       "L 114.529443 280.777634 \n",
       "L 114.595738 284.92798 \n",
       "L 114.662033 283.895941 \n",
       "L 114.728328 285.084427 \n",
       "L 114.794623 284.517859 \n",
       "L 114.860918 282.893448 \n",
       "L 114.927213 284.615863 \n",
       "L 114.993508 276.721874 \n",
       "L 115.059803 284.373784 \n",
       "L 115.126098 283.012835 \n",
       "L 115.192393 283.704863 \n",
       "L 115.258688 282.884649 \n",
       "L 115.457573 284.7246 \n",
       "L 115.523868 280.680083 \n",
       "L 115.590163 284.0227 \n",
       "L 115.656458 281.173321 \n",
       "L 115.722753 285.378283 \n",
       "L 115.789048 280.964207 \n",
       "L 115.921638 287.962279 \n",
       "L 116.054228 278.636328 \n",
       "L 116.253113 285.223742 \n",
       "L 116.319408 282.481152 \n",
       "L 116.451998 288.490092 \n",
       "L 116.518293 286.73412 \n",
       "L 116.584588 283.541664 \n",
       "L 116.650883 287.65067 \n",
       "L 116.717178 286.801871 \n",
       "L 116.783473 284.599084 \n",
       "L 116.849768 286.789611 \n",
       "L 116.982358 282.89349 \n",
       "L 117.048653 285.093141 \n",
       "L 117.114948 283.86816 \n",
       "L 117.181243 282.973331 \n",
       "L 117.313833 288.923432 \n",
       "L 117.380128 283.863852 \n",
       "L 117.446423 284.456111 \n",
       "L 117.711603 282.553253 \n",
       "L 117.777898 275.912931 \n",
       "L 117.844193 285.748773 \n",
       "L 117.910488 282.508862 \n",
       "L 117.976783 288.320313 \n",
       "L 118.043078 286.601584 \n",
       "L 118.109373 286.372697 \n",
       "L 118.175668 287.48267 \n",
       "L 118.308258 283.658806 \n",
       "L 118.374553 285.829235 \n",
       "L 118.440848 285.216073 \n",
       "L 118.573438 282.065975 \n",
       "L 118.639733 283.112519 \n",
       "L 118.706028 289.981134 \n",
       "L 118.772323 282.639308 \n",
       "L 118.838618 286.799258 \n",
       "L 118.904913 288.051484 \n",
       "L 118.971208 287.632903 \n",
       "L 119.037503 283.386969 \n",
       "L 119.103798 283.707716 \n",
       "L 119.170093 284.01996 \n",
       "L 119.302683 287.783559 \n",
       "L 119.501568 281.086404 \n",
       "L 119.567863 287.222005 \n",
       "L 119.700453 281.380415 \n",
       "L 119.766748 285.803841 \n",
       "L 119.833043 278.032374 \n",
       "L 119.899338 287.593668 \n",
       "L 119.965633 285.219011 \n",
       "L 120.031928 277.334697 \n",
       "L 120.230813 287.487628 \n",
       "L 120.297108 283.363679 \n",
       "L 120.363402 287.926391 \n",
       "L 120.429697 287.199351 \n",
       "L 120.495992 283.408465 \n",
       "L 120.562287 285.521623 \n",
       "L 120.628582 286.159233 \n",
       "L 120.694877 285.632875 \n",
       "L 120.761172 286.852772 \n",
       "L 120.893762 282.770643 \n",
       "L 120.960057 287.81973 \n",
       "L 121.026352 279.882608 \n",
       "L 121.158942 287.699355 \n",
       "L 121.225237 284.715886 \n",
       "L 121.291532 284.746732 \n",
       "L 121.357827 289.321167 \n",
       "L 121.424122 281.048172 \n",
       "L 121.490417 286.73549 \n",
       "L 121.556712 284.783877 \n",
       "L 121.623007 287.658142 \n",
       "L 121.821892 282.521249 \n",
       "L 121.954482 286.471124 \n",
       "L 122.020777 281.174663 \n",
       "L 122.087072 287.201653 \n",
       "L 122.153367 280.295794 \n",
       "L 122.219662 281.245916 \n",
       "L 122.285957 281.937916 \n",
       "L 122.352252 285.390824 \n",
       "L 122.418547 284.73967 \n",
       "L 122.484842 280.996748 \n",
       "L 122.617432 285.621406 \n",
       "L 122.750022 280.20916 \n",
       "L 122.948907 286.267857 \n",
       "L 123.015202 283.892353 \n",
       "L 123.081497 286.045849 \n",
       "L 123.147792 279.51829 \n",
       "L 123.214087 287.11807 \n",
       "L 123.280382 285.964214 \n",
       "L 123.346677 281.40999 \n",
       "L 123.412972 283.029741 \n",
       "L 123.545562 285.573471 \n",
       "L 123.611857 285.761668 \n",
       "L 123.678152 282.797097 \n",
       "L 123.744447 285.865278 \n",
       "L 123.810742 284.15364 \n",
       "L 124.009627 286.973402 \n",
       "L 124.075922 278.560837 \n",
       "L 124.208512 287.102124 \n",
       "L 124.274807 286.379533 \n",
       "L 124.341102 282.698457 \n",
       "L 124.407397 284.191745 \n",
       "L 124.473692 284.953318 \n",
       "L 124.539987 284.351808 \n",
       "L 124.606282 286.003068 \n",
       "L 124.672577 281.623496 \n",
       "L 124.738872 286.619648 \n",
       "L 124.805167 284.049549 \n",
       "L 124.937757 283.678451 \n",
       "L 125.004052 282.754768 \n",
       "L 125.070347 283.727686 \n",
       "L 125.136642 283.289615 \n",
       "L 125.202937 282.896583 \n",
       "L 125.401822 284.47885 \n",
       "L 125.468117 288.707313 \n",
       "L 125.667002 280.341427 \n",
       "L 125.799592 287.894359 \n",
       "L 125.865887 282.211688 \n",
       "L 125.932182 285.413592 \n",
       "L 125.998477 284.059902 \n",
       "L 126.064772 284.382569 \n",
       "L 126.131067 284.14131 \n",
       "L 126.263657 289.457742 \n",
       "L 126.396247 284.470305 \n",
       "L 126.528836 287.069414 \n",
       "L 126.595131 283.761569 \n",
       "L 126.661426 286.328292 \n",
       "L 126.727721 282.072034 \n",
       "L 126.794016 282.360904 \n",
       "L 126.926606 287.647041 \n",
       "L 127.125491 284.872827 \n",
       "L 127.191786 285.22418 \n",
       "L 127.258081 280.476646 \n",
       "L 127.324376 284.298054 \n",
       "L 127.390671 280.990096 \n",
       "L 127.456966 287.898977 \n",
       "L 127.523261 286.353277 \n",
       "L 127.589556 284.055255 \n",
       "L 127.655851 285.35584 \n",
       "L 127.788441 284.351992 \n",
       "L 127.854736 285.042042 \n",
       "L 127.921031 283.790649 \n",
       "L 128.053621 285.896619 \n",
       "L 128.186211 282.1559 \n",
       "L 128.318801 287.802993 \n",
       "L 128.451391 281.422235 \n",
       "L 128.517686 282.042798 \n",
       "L 128.583981 280.114814 \n",
       "L 128.650276 284.28441 \n",
       "L 128.716571 281.293993 \n",
       "L 128.782866 286.965973 \n",
       "L 128.849161 281.341364 \n",
       "L 128.915456 285.661334 \n",
       "L 128.981751 283.691643 \n",
       "L 129.048046 281.941786 \n",
       "L 129.114341 289.567214 \n",
       "L 129.180636 283.64817 \n",
       "L 129.246931 283.931405 \n",
       "L 129.379521 289.885235 \n",
       "L 129.445816 287.699934 \n",
       "L 129.512111 283.375274 \n",
       "L 129.578406 284.507464 \n",
       "L 129.777291 286.231828 \n",
       "L 129.843586 289.335898 \n",
       "L 129.909881 285.446613 \n",
       "L 129.976176 286.319762 \n",
       "L 130.042471 287.5348 \n",
       "L 130.108766 284.986141 \n",
       "L 130.175061 287.122716 \n",
       "L 130.241356 282.713697 \n",
       "L 130.307651 283.300335 \n",
       "L 130.440241 285.328525 \n",
       "L 130.572831 282.711381 \n",
       "L 130.639126 288.599141 \n",
       "L 130.705421 288.15984 \n",
       "L 130.771716 280.034013 \n",
       "L 130.838011 282.654335 \n",
       "L 130.904306 287.201978 \n",
       "L 130.970601 284.496052 \n",
       "L 131.036896 287.724438 \n",
       "L 131.103191 286.497394 \n",
       "L 131.302076 284.858887 \n",
       "L 131.500961 287.092972 \n",
       "L 131.567256 283.011013 \n",
       "L 131.633551 287.329655 \n",
       "L 131.699846 280.360268 \n",
       "L 131.766141 281.416572 \n",
       "L 131.898731 287.045079 \n",
       "L 131.965026 280.765657 \n",
       "L 132.031321 283.165567 \n",
       "L 132.097616 279.397194 \n",
       "L 132.230206 288.352232 \n",
       "L 132.362796 279.114256 \n",
       "L 132.429091 284.319295 \n",
       "L 132.495386 281.17325 \n",
       "L 132.56168 278.779936 \n",
       "L 132.627975 288.446041 \n",
       "L 132.69427 283.882396 \n",
       "L 132.760565 282.96456 \n",
       "L 132.82686 283.474238 \n",
       "L 132.893155 286.237788 \n",
       "L 132.95945 283.760919 \n",
       "L 133.025745 287.419905 \n",
       "L 133.09204 281.35899 \n",
       "L 133.158335 282.499258 \n",
       "L 133.22463 285.09365 \n",
       "L 133.290925 282.549694 \n",
       "L 133.35722 283.719283 \n",
       "L 133.48981 288.278662 \n",
       "L 133.556105 281.104016 \n",
       "L 133.6224 285.820351 \n",
       "L 133.688695 282.865992 \n",
       "L 133.75499 286.255372 \n",
       "L 133.821285 284.619944 \n",
       "L 133.88758 285.392802 \n",
       "L 133.953875 284.488736 \n",
       "L 134.02017 289.565675 \n",
       "L 134.086465 278.649378 \n",
       "L 134.15276 283.459409 \n",
       "L 134.219055 283.319543 \n",
       "L 134.28535 279.942252 \n",
       "L 134.351645 281.035333 \n",
       "L 134.484235 288.78673 \n",
       "L 134.68312 283.050135 \n",
       "L 134.749415 287.022114 \n",
       "L 134.81571 285.698888 \n",
       "L 134.9483 283.150215 \n",
       "L 135.014595 285.448124 \n",
       "L 135.08089 284.952541 \n",
       "L 135.147185 282.944024 \n",
       "L 135.21348 286.625043 \n",
       "L 135.34607 280.123189 \n",
       "L 135.544955 287.262017 \n",
       "L 135.61125 287.912352 \n",
       "L 135.677545 283.862736 \n",
       "L 135.74384 285.197246 \n",
       "L 135.810135 287.652973 \n",
       "L 135.87643 287.503277 \n",
       "L 135.942725 288.399927 \n",
       "L 136.14161 285.555548 \n",
       "L 136.207905 286.395182 \n",
       "L 136.2742 287.550167 \n",
       "L 136.53938 278.5753 \n",
       "L 136.605675 290.130958 \n",
       "L 136.67197 283.668508 \n",
       "L 136.738265 283.791567 \n",
       "L 136.80456 289.726331 \n",
       "L 136.870855 285.014162 \n",
       "L 136.93715 285.147913 \n",
       "L 137.003445 284.836021 \n",
       "L 137.136035 288.575808 \n",
       "L 137.268625 286.493002 \n",
       "L 137.33492 288.272278 \n",
       "L 137.6001 283.884218 \n",
       "L 137.666395 287.632733 \n",
       "L 137.73269 285.936137 \n",
       "L 137.798985 285.397604 \n",
       "L 137.931575 286.833027 \n",
       "L 137.99787 288.515289 \n",
       "L 138.064165 288.349238 \n",
       "L 138.196755 282.718668 \n",
       "L 138.329345 286.421098 \n",
       "L 138.461935 281.363453 \n",
       "L 138.594525 281.714438 \n",
       "L 138.66082 282.611329 \n",
       "L 138.727114 285.429749 \n",
       "L 138.793409 281.377704 \n",
       "L 138.859704 281.971911 \n",
       "L 138.992294 285.828727 \n",
       "L 139.058589 289.176556 \n",
       "L 139.191179 283.511398 \n",
       "L 139.323769 286.790162 \n",
       "L 139.456359 284.107385 \n",
       "L 139.588949 285.688154 \n",
       "L 139.721539 281.181866 \n",
       "L 139.854129 287.082718 \n",
       "L 139.920424 284.19988 \n",
       "L 139.986719 285.59056 \n",
       "L 140.119309 280.759062 \n",
       "L 140.185604 283.122278 \n",
       "L 140.251899 278.820584 \n",
       "L 140.318194 288.707115 \n",
       "L 140.384489 284.126664 \n",
       "L 140.450784 285.145187 \n",
       "L 140.517079 288.841275 \n",
       "L 140.649669 284.765714 \n",
       "L 140.715964 285.349456 \n",
       "L 140.782259 284.449303 \n",
       "L 140.848554 286.670422 \n",
       "L 140.914849 283.014657 \n",
       "L 141.047439 287.029557 \n",
       "L 141.113734 283.251523 \n",
       "L 141.246324 286.863859 \n",
       "L 141.312619 286.429375 \n",
       "L 141.378914 282.40915 \n",
       "L 141.445209 290.351173 \n",
       "L 141.511504 280.750418 \n",
       "L 141.577799 284.650748 \n",
       "L 141.644094 282.091312 \n",
       "L 141.710389 283.417885 \n",
       "L 141.776684 282.807181 \n",
       "L 141.909274 289.037961 \n",
       "L 141.975569 284.241898 \n",
       "L 142.041864 290.408175 \n",
       "L 142.108159 288.836968 \n",
       "L 142.174454 284.518198 \n",
       "L 142.240749 285.280505 \n",
       "L 142.373339 287.452813 \n",
       "L 142.439634 285.347479 \n",
       "L 142.505929 287.589219 \n",
       "L 142.572224 283.561706 \n",
       "L 142.638519 285.751301 \n",
       "L 142.704814 288.757155 \n",
       "L 142.771109 286.195446 \n",
       "L 142.837404 286.818226 \n",
       "L 142.903699 286.28828 \n",
       "L 142.969994 283.617508 \n",
       "L 143.036289 285.103664 \n",
       "L 143.102584 282.762282 \n",
       "L 143.301469 288.843634 \n",
       "L 143.434059 281.721218 \n",
       "L 143.566649 286.724855 \n",
       "L 143.632944 284.424573 \n",
       "L 143.699239 289.851565 \n",
       "L 143.765534 289.178533 \n",
       "L 143.964419 284.488722 \n",
       "L 144.030714 281.848401 \n",
       "L 144.295894 287.314148 \n",
       "L 144.362189 281.940373 \n",
       "L 144.428484 286.024903 \n",
       "L 144.494779 285.69403 \n",
       "L 144.561074 287.31443 \n",
       "L 144.627369 286.310228 \n",
       "L 144.693664 283.699411 \n",
       "L 144.826253 288.706268 \n",
       "L 144.958843 282.918644 \n",
       "L 145.025138 285.914993 \n",
       "L 145.157728 281.066787 \n",
       "L 145.356613 290.792479 \n",
       "L 145.422908 285.617113 \n",
       "L 145.489203 288.023929 \n",
       "L 145.688088 280.187282 \n",
       "L 145.754383 284.653996 \n",
       "L 145.820678 283.713492 \n",
       "L 145.953268 284.619577 \n",
       "L 146.019563 289.480552 \n",
       "L 146.085858 287.445285 \n",
       "L 146.152153 279.207048 \n",
       "L 146.218448 289.558867 \n",
       "L 146.284743 284.964673 \n",
       "L 146.351038 288.313505 \n",
       "L 146.417333 287.874402 \n",
       "L 146.549923 282.986593 \n",
       "L 146.682513 287.301987 \n",
       "L 146.748808 284.067684 \n",
       "L 146.815103 285.723859 \n",
       "L 146.947693 283.896943 \n",
       "L 147.146578 284.730617 \n",
       "L 147.212873 284.136847 \n",
       "L 147.279168 285.370402 \n",
       "L 147.345463 283.003216 \n",
       "L 147.411758 286.495897 \n",
       "L 147.478053 284.515133 \n",
       "L 147.544348 286.862673 \n",
       "L 147.610643 285.332522 \n",
       "L 147.676938 287.361716 \n",
       "L 147.743233 284.164162 \n",
       "L 147.809528 284.575413 \n",
       "L 147.875823 291.633016 \n",
       "L 147.942118 283.588004 \n",
       "L 148.008413 286.132624 \n",
       "L 148.074708 284.292122 \n",
       "L 148.141003 288.714827 \n",
       "L 148.273593 279.678056 \n",
       "L 148.339888 281.778051 \n",
       "L 148.406183 287.053624 \n",
       "L 148.472478 283.842949 \n",
       "L 148.538773 285.586874 \n",
       "L 148.605068 285.044585 \n",
       "L 148.671363 282.341187 \n",
       "L 148.737658 286.415491 \n",
       "L 148.870248 281.848542 \n",
       "L 149.069133 287.841749 \n",
       "L 149.135428 283.456118 \n",
       "L 149.201723 285.262286 \n",
       "L 149.334313 291.180355 \n",
       "L 149.400608 283.743717 \n",
       "L 149.466903 286.23437 \n",
       "L 149.533198 289.259489 \n",
       "L 149.599493 287.919005 \n",
       "L 149.732083 284.949038 \n",
       "L 149.798378 288.766548 \n",
       "L 149.864673 282.813381 \n",
       "L 149.930968 288.32342 \n",
       "L 149.997263 284.301062 \n",
       "L 150.063558 290.865808 \n",
       "L 150.129853 288.78745 \n",
       "L 150.262443 283.028187 \n",
       "L 150.461328 288.131198 \n",
       "L 150.527623 281.64399 \n",
       "L 150.593918 285.302834 \n",
       "L 150.660213 282.423768 \n",
       "L 150.726508 288.546855 \n",
       "L 150.792803 286.551615 \n",
       "L 150.925392 286.619719 \n",
       "L 150.991687 281.874035 \n",
       "L 151.124277 291.072196 \n",
       "L 151.190572 285.804434 \n",
       "L 151.256867 288.604366 \n",
       "L 151.323162 288.484711 \n",
       "L 151.389457 283.415244 \n",
       "L 151.522047 289.655247 \n",
       "L 151.588342 285.794166 \n",
       "L 151.654637 287.96437 \n",
       "L 151.853522 284.52865 \n",
       "L 151.919817 288.173681 \n",
       "L 152.052407 282.057981 \n",
       "L 152.118702 288.258409 \n",
       "L 152.184997 287.061886 \n",
       "L 152.251292 285.774322 \n",
       "L 152.450177 288.672428 \n",
       "L 152.649062 283.293456 \n",
       "L 152.715357 287.129213 \n",
       "L 152.781652 286.467777 \n",
       "L 152.847947 286.58143 \n",
       "L 152.914242 287.450045 \n",
       "L 153.046832 285.241411 \n",
       "L 153.113127 288.397964 \n",
       "L 153.245717 283.887142 \n",
       "L 153.312012 285.515861 \n",
       "L 153.378307 283.158138 \n",
       "L 153.510897 289.805296 \n",
       "L 153.643487 283.36495 \n",
       "L 153.709782 286.170645 \n",
       "L 153.776077 286.481547 \n",
       "L 153.842372 282.852136 \n",
       "L 153.908667 285.050163 \n",
       "L 153.974962 287.053553 \n",
       "L 154.041257 284.288789 \n",
       "L 154.107552 286.942485 \n",
       "L 154.173847 285.284417 \n",
       "L 154.240142 287.19123 \n",
       "L 154.372732 284.119913 \n",
       "L 154.505322 289.975344 \n",
       "L 154.571617 289.068976 \n",
       "L 154.637912 287.147602 \n",
       "L 154.704207 287.442023 \n",
       "L 154.770502 287.651574 \n",
       "L 154.836797 284.050368 \n",
       "L 154.903092 284.798071 \n",
       "L 154.969387 284.469147 \n",
       "L 155.035682 288.550711 \n",
       "L 155.101977 285.591323 \n",
       "L 155.168272 289.498009 \n",
       "L 155.234567 287.130244 \n",
       "L 155.300862 286.078926 \n",
       "L 155.367157 289.024402 \n",
       "L 155.433452 288.993415 \n",
       "L 155.566042 280.68158 \n",
       "L 155.632337 286.236715 \n",
       "L 155.698632 285.965782 \n",
       "L 155.764927 287.065558 \n",
       "L 155.831222 284.656412 \n",
       "L 155.963812 288.8205 \n",
       "L 156.030107 284.090705 \n",
       "L 156.096402 285.662633 \n",
       "L 156.295287 282.805147 \n",
       "L 156.361582 290.277631 \n",
       "L 156.427877 285.642606 \n",
       "L 156.560467 289.534942 \n",
       "L 156.626762 289.215099 \n",
       "L 156.693057 285.471315 \n",
       "L 156.759352 287.447842 \n",
       "L 156.825647 286.871048 \n",
       "L 156.891942 280.724402 \n",
       "L 156.958237 284.794258 \n",
       "L 157.024531 285.03361 \n",
       "L 157.289711 288.601118 \n",
       "L 157.356006 289.17085 \n",
       "L 157.488596 283.752629 \n",
       "L 157.621186 286.212564 \n",
       "L 157.687481 285.841155 \n",
       "L 157.753776 284.4882 \n",
       "L 157.820071 286.883533 \n",
       "L 157.886366 286.828381 \n",
       "L 157.952661 286.974447 \n",
       "L 158.018956 286.530415 \n",
       "L 158.151546 283.44109 \n",
       "L 158.284136 287.265096 \n",
       "L 158.350431 284.542434 \n",
       "L 158.416726 287.448604 \n",
       "L 158.549316 284.741492 \n",
       "L 158.615611 285.610856 \n",
       "L 158.681906 288.453597 \n",
       "L 158.748201 284.41004 \n",
       "L 158.814496 287.573033 \n",
       "L 158.880791 285.439396 \n",
       "L 158.947086 284.711367 \n",
       "L 159.013381 289.354739 \n",
       "L 159.212266 279.01299 \n",
       "L 159.278561 288.594183 \n",
       "L 159.344856 283.746259 \n",
       "L 159.477446 287.963522 \n",
       "L 159.543741 286.472409 \n",
       "L 159.610036 283.604613 \n",
       "L 159.676331 284.507845 \n",
       "L 159.742626 286.577207 \n",
       "L 159.875216 283.601831 \n",
       "L 159.941511 288.464627 \n",
       "L 160.007806 287.593159 \n",
       "L 160.074101 285.161712 \n",
       "L 160.140396 286.922754 \n",
       "L 160.272986 283.997023 \n",
       "L 160.405576 288.192409 \n",
       "L 160.471871 284.493552 \n",
       "L 160.604461 287.425738 \n",
       "L 160.670756 287.511073 \n",
       "L 160.737051 287.381263 \n",
       "L 160.803346 286.060198 \n",
       "L 160.869641 289.486413 \n",
       "L 161.002231 284.288082 \n",
       "L 161.068526 288.372866 \n",
       "L 161.134821 285.662478 \n",
       "L 161.201116 286.413147 \n",
       "L 161.267411 288.740405 \n",
       "L 161.400001 284.787479 \n",
       "L 161.466296 287.058129 \n",
       "L 161.532591 286.740518 \n",
       "L 161.598886 286.0675 \n",
       "L 161.731476 289.349852 \n",
       "L 161.797771 283.045065 \n",
       "L 161.864066 286.133048 \n",
       "L 162.062951 281.301111 \n",
       "L 162.195541 285.667223 \n",
       "L 162.261836 283.706967 \n",
       "L 162.328131 288.413218 \n",
       "L 162.394426 286.143443 \n",
       "L 162.460721 285.366447 \n",
       "L 162.593311 290.082923 \n",
       "L 162.858491 285.181456 \n",
       "L 162.924786 289.767148 \n",
       "L 162.991081 284.402907 \n",
       "L 163.057376 287.901661 \n",
       "L 163.12367 287.603356 \n",
       "L 163.189965 285.123211 \n",
       "L 163.322555 287.268994 \n",
       "L 163.38885 286.054633 \n",
       "L 163.455145 288.249115 \n",
       "L 163.65403 282.570018 \n",
       "L 163.720325 287.813699 \n",
       "L 163.78662 284.034592 \n",
       "L 163.985505 289.524999 \n",
       "L 164.118095 280.436338 \n",
       "L 164.18439 285.256142 \n",
       "L 164.250685 283.92214 \n",
       "L 164.31698 285.548627 \n",
       "L 164.44957 283.158745 \n",
       "L 164.515865 284.215925 \n",
       "L 164.648455 287.911618 \n",
       "L 164.71475 284.527336 \n",
       "L 164.781045 287.279389 \n",
       "L 164.84734 286.01215 \n",
       "L 164.913635 287.621096 \n",
       "L 165.11252 284.138739 \n",
       "L 165.24511 285.506468 \n",
       "L 165.311405 284.893688 \n",
       "L 165.3777 286.170998 \n",
       "L 165.51029 284.064195 \n",
       "L 165.576585 288.406763 \n",
       "L 165.64288 286.737933 \n",
       "L 165.709175 283.914217 \n",
       "L 165.77547 288.168343 \n",
       "L 165.841765 284.275837 \n",
       "L 165.974355 290.480319 \n",
       "L 166.106945 285.412801 \n",
       "L 166.17324 287.177855 \n",
       "L 166.239535 285.155342 \n",
       "L 166.372125 287.276367 \n",
       "L 166.43842 284.912374 \n",
       "L 166.637305 291.064287 \n",
       "L 166.7036 284.449459 \n",
       "L 166.769895 289.034853 \n",
       "L 166.83619 288.236517 \n",
       "L 166.96878 286.882672 \n",
       "L 167.10137 280.813423 \n",
       "L 167.23396 285.300801 \n",
       "L 167.36655 286.771222 \n",
       "L 167.49914 283.293866 \n",
       "L 167.565435 285.499068 \n",
       "L 167.63173 282.466237 \n",
       "L 167.698025 282.730391 \n",
       "L 167.830615 287.534123 \n",
       "L 168.0295 283.74729 \n",
       "L 168.228385 287.794999 \n",
       "L 168.360975 288.83266 \n",
       "L 168.42727 287.39581 \n",
       "L 168.493565 281.969991 \n",
       "L 168.55986 285.261692 \n",
       "L 168.626155 286.631286 \n",
       "L 168.69245 286.551982 \n",
       "L 168.82504 284.924068 \n",
       "L 168.891335 283.919682 \n",
       "L 168.95763 285.924216 \n",
       "L 169.023925 280.707694 \n",
       "L 169.09022 285.925812 \n",
       "L 169.156515 283.590871 \n",
       "L 169.222809 285.388197 \n",
       "L 169.355399 282.405562 \n",
       "L 169.421694 288.309974 \n",
       "L 169.487989 287.107646 \n",
       "L 169.554284 288.964815 \n",
       "L 169.686874 283.516341 \n",
       "L 169.753169 288.973741 \n",
       "L 169.819464 284.094561 \n",
       "L 169.885759 285.90659 \n",
       "L 169.952054 286.162735 \n",
       "L 170.018349 288.836276 \n",
       "L 170.084644 287.21846 \n",
       "L 170.150939 288.04664 \n",
       "L 170.217234 291.150498 \n",
       "L 170.349824 286.221066 \n",
       "L 170.482414 288.635523 \n",
       "L 170.548709 287.779351 \n",
       "L 170.615004 285.008442 \n",
       "L 170.747594 289.738138 \n",
       "L 170.813889 285.671856 \n",
       "L 170.880184 286.568676 \n",
       "L 170.946479 286.725504 \n",
       "L 171.012774 283.232795 \n",
       "L 171.145364 289.406615 \n",
       "L 171.211659 285.864968 \n",
       "L 171.277954 287.695555 \n",
       "L 171.344249 285.929894 \n",
       "L 171.410544 286.762974 \n",
       "L 171.476839 286.695774 \n",
       "L 171.543134 287.010264 \n",
       "L 171.609429 285.186301 \n",
       "L 171.675724 286.420406 \n",
       "L 171.742019 286.002122 \n",
       "L 171.808314 286.599239 \n",
       "L 171.874609 284.458992 \n",
       "L 171.940904 285.27059 \n",
       "L 172.007199 290.260824 \n",
       "L 172.073494 289.860349 \n",
       "L 172.206084 284.266812 \n",
       "L 172.404969 288.554538 \n",
       "L 172.471264 284.067048 \n",
       "L 172.537559 287.58323 \n",
       "L 172.603854 284.034733 \n",
       "L 172.670149 284.380267 \n",
       "L 172.869034 289.955924 \n",
       "L 173.001624 283.553839 \n",
       "L 173.067919 284.161436 \n",
       "L 173.134214 288.392696 \n",
       "L 173.266804 282.928856 \n",
       "L 173.399394 286.672569 \n",
       "L 173.465689 285.268359 \n",
       "L 173.531984 283.48928 \n",
       "L 173.598279 288.484598 \n",
       "L 173.664574 284.121254 \n",
       "L 173.730869 287.753095 \n",
       "L 173.797164 283.179352 \n",
       "L 173.863459 288.087429 \n",
       "L 173.929754 283.070247 \n",
       "L 173.996049 286.51776 \n",
       "L 174.062344 285.267695 \n",
       "L 174.128639 284.803594 \n",
       "L 174.327524 288.26773 \n",
       "L 174.393819 284.065085 \n",
       "L 174.526409 287.420611 \n",
       "L 174.725294 283.586648 \n",
       "L 174.791589 288.083474 \n",
       "L 174.857884 285.254362 \n",
       "L 174.924179 286.654561 \n",
       "L 174.990474 284.924054 \n",
       "L 175.056769 287.435159 \n",
       "L 175.123064 284.386397 \n",
       "L 175.189359 288.501335 \n",
       "L 175.255654 286.647655 \n",
       "L 175.321948 285.717673 \n",
       "L 175.388243 288.051766 \n",
       "L 175.454538 286.025115 \n",
       "L 175.520833 288.238848 \n",
       "L 175.587128 287.195255 \n",
       "L 175.719718 282.820033 \n",
       "L 175.786013 283.876916 \n",
       "L 175.852308 288.405195 \n",
       "L 175.918603 287.321972 \n",
       "L 175.984898 283.791454 \n",
       "L 176.051193 284.686975 \n",
       "L 176.117488 289.728845 \n",
       "L 176.183783 286.69278 \n",
       "L 176.250078 288.47498 \n",
       "L 176.316373 286.981918 \n",
       "L 176.515258 289.411939 \n",
       "L 176.581553 284.708189 \n",
       "L 176.647848 286.101241 \n",
       "L 176.714143 285.419848 \n",
       "L 176.780438 286.951807 \n",
       "L 176.913028 284.48594 \n",
       "L 177.045618 290.707652 \n",
       "L 177.111913 290.107498 \n",
       "L 177.244503 285.308074 \n",
       "L 177.310798 286.273493 \n",
       "L 177.377093 283.695315 \n",
       "L 177.443388 291.377817 \n",
       "L 177.509683 282.705336 \n",
       "L 177.575978 291.018724 \n",
       "L 177.642273 286.107145 \n",
       "L 177.708568 286.835838 \n",
       "L 177.774863 286.036598 \n",
       "L 177.907453 278.824652 \n",
       "L 178.040043 288.83886 \n",
       "L 178.172633 280.725603 \n",
       "L 178.238928 287.396022 \n",
       "L 178.305223 283.395259 \n",
       "L 178.437813 287.677025 \n",
       "L 178.504108 288.070212 \n",
       "L 178.636698 283.873117 \n",
       "L 178.835583 288.697596 \n",
       "L 178.901878 288.671792 \n",
       "L 178.968173 289.974906 \n",
       "L 179.034468 285.534447 \n",
       "L 179.100763 288.412794 \n",
       "L 179.167058 287.905333 \n",
       "L 179.233353 290.020313 \n",
       "L 179.432238 284.030878 \n",
       "L 179.631123 290.631597 \n",
       "L 179.763713 285.666009 \n",
       "L 179.830008 286.869918 \n",
       "L 179.896303 286.132327 \n",
       "L 179.962598 287.064866 \n",
       "L 180.095188 284.612289 \n",
       "L 180.161483 289.152574 \n",
       "L 180.227778 287.531213 \n",
       "L 180.294073 285.726105 \n",
       "L 180.360368 289.114609 \n",
       "L 180.426663 286.927458 \n",
       "L 180.492958 288.946157 \n",
       "L 180.691843 285.690979 \n",
       "L 180.758138 285.726966 \n",
       "L 180.824433 288.554383 \n",
       "L 180.890728 286.622063 \n",
       "L 180.957023 288.709672 \n",
       "L 181.023318 285.843175 \n",
       "L 181.089613 288.541502 \n",
       "L 181.155908 284.282122 \n",
       "L 181.354793 290.037502 \n",
       "L 181.421087 289.255845 \n",
       "L 181.553677 282.78077 \n",
       "L 181.619972 288.053023 \n",
       "L 181.686267 285.326604 \n",
       "L 181.752562 285.496158 \n",
       "L 181.818857 285.470185 \n",
       "L 181.885152 289.326124 \n",
       "L 181.951447 283.433916 \n",
       "L 182.084037 289.334443 \n",
       "L 182.150332 289.076476 \n",
       "L 182.216627 289.583795 \n",
       "L 182.349217 286.155561 \n",
       "L 182.415512 286.431112 \n",
       "L 182.481807 287.794053 \n",
       "L 182.548102 284.798735 \n",
       "L 182.614397 287.590349 \n",
       "L 182.680692 287.153802 \n",
       "L 182.746987 284.689051 \n",
       "L 182.879577 290.529836 \n",
       "L 183.078462 284.579283 \n",
       "L 183.144757 285.116558 \n",
       "L 183.277347 290.032869 \n",
       "L 183.343642 289.746951 \n",
       "L 183.476232 285.945274 \n",
       "L 183.741412 291.18986 \n",
       "L 183.940297 283.607184 \n",
       "L 184.072887 288.969617 \n",
       "L 184.139182 287.239617 \n",
       "L 184.205477 288.014579 \n",
       "L 184.271772 290.410139 \n",
       "L 184.404362 287.418535 \n",
       "L 184.470657 288.021372 \n",
       "L 184.669542 284.949208 \n",
       "L 184.802132 289.97033 \n",
       "L 185.001017 282.776024 \n",
       "L 185.199902 289.993916 \n",
       "L 185.332492 288.131791 \n",
       "L 185.398787 290.671383 \n",
       "L 185.465082 285.618695 \n",
       "L 185.531377 285.88741 \n",
       "L 185.597672 290.406933 \n",
       "L 185.796557 284.193045 \n",
       "L 185.995442 290.01311 \n",
       "L 186.128032 284.985576 \n",
       "L 186.194327 290.228213 \n",
       "L 186.260622 288.020497 \n",
       "L 186.326917 283.119807 \n",
       "L 186.393212 291.084639 \n",
       "L 186.459507 287.801892 \n",
       "L 186.525802 286.729275 \n",
       "L 186.658392 288.704093 \n",
       "L 186.724687 284.298054 \n",
       "L 186.857277 287.320531 \n",
       "L 186.989867 285.560082 \n",
       "L 187.056162 289.830648 \n",
       "L 187.122457 287.021267 \n",
       "L 187.188752 286.94079 \n",
       "L 187.255047 285.893314 \n",
       "L 187.387637 291.060869 \n",
       "L 187.453932 287.096249 \n",
       "L 187.520226 288.049012 \n",
       "L 187.652816 286.210501 \n",
       "L 187.719111 289.065134 \n",
       "L 187.785406 287.168576 \n",
       "L 187.851701 287.767543 \n",
       "L 187.917996 283.199803 \n",
       "L 187.984291 285.460171 \n",
       "L 188.050586 284.163569 \n",
       "L 188.249471 288.97826 \n",
       "L 188.382061 283.416642 \n",
       "L 188.514651 290.554341 \n",
       "L 188.580946 289.548726 \n",
       "L 188.647241 289.178166 \n",
       "L 188.713536 283.90183 \n",
       "L 188.779831 286.347345 \n",
       "L 188.846126 288.767607 \n",
       "L 188.912421 288.738597 \n",
       "L 188.978716 285.687575 \n",
       "L 189.045011 286.830979 \n",
       "L 189.111306 284.930649 \n",
       "L 189.177601 287.767684 \n",
       "L 189.376486 282.087654 \n",
       "L 189.442781 290.650522 \n",
       "L 189.509076 287.377111 \n",
       "L 189.575371 289.827286 \n",
       "L 189.641666 282.416282 \n",
       "L 189.707961 286.357175 \n",
       "L 189.774256 284.60897 \n",
       "L 189.840551 287.51024 \n",
       "L 189.906846 287.313823 \n",
       "L 189.973141 281.194761 \n",
       "L 190.039436 289.808629 \n",
       "L 190.105731 283.403903 \n",
       "L 190.172026 290.010116 \n",
       "L 190.238321 289.647662 \n",
       "L 190.370911 286.562391 \n",
       "L 190.437206 287.919598 \n",
       "L 190.503501 292.321654 \n",
       "L 190.569796 287.586422 \n",
       "L 190.636091 289.689609 \n",
       "L 190.702386 289.722207 \n",
       "L 190.768681 288.972766 \n",
       "L 190.834976 284.405421 \n",
       "L 190.901271 286.018915 \n",
       "L 190.967566 286.782521 \n",
       "L 191.033861 284.944745 \n",
       "L 191.100156 285.371743 \n",
       "L 191.166451 286.628277 \n",
       "L 191.232746 282.562518 \n",
       "L 191.299041 289.580406 \n",
       "L 191.365336 285.220466 \n",
       "L 191.431631 287.637874 \n",
       "L 191.497926 287.48671 \n",
       "L 191.696811 286.857489 \n",
       "L 191.763106 290.013266 \n",
       "L 191.829401 287.30782 \n",
       "L 191.895696 288.317064 \n",
       "L 191.961991 287.139891 \n",
       "L 192.094581 288.288393 \n",
       "L 192.160876 288.467 \n",
       "L 192.359761 282.451817 \n",
       "L 192.426056 282.897614 \n",
       "L 192.558646 285.022693 \n",
       "L 192.691236 290.573111 \n",
       "L 192.757531 284.212761 \n",
       "L 192.823826 287.233685 \n",
       "L 192.956416 289.955458 \n",
       "L 193.022711 284.18152 \n",
       "L 193.089006 288.264256 \n",
       "L 193.155301 281.756202 \n",
       "L 193.221596 288.088968 \n",
       "L 193.287891 283.744493 \n",
       "L 193.354186 286.836163 \n",
       "L 193.420481 286.135887 \n",
       "L 193.486776 287.446062 \n",
       "L 193.553071 285.810719 \n",
       "L 193.68566 288.028547 \n",
       "L 193.81825 284.87979 \n",
       "L 193.884545 288.953445 \n",
       "L 193.95084 286.980195 \n",
       "L 194.017135 289.341166 \n",
       "L 194.08343 286.329253 \n",
       "L 194.149725 287.022199 \n",
       "L 194.21602 287.085614 \n",
       "L 194.282315 290.932641 \n",
       "L 194.34861 290.215007 \n",
       "L 194.4812 279.417998 \n",
       "L 194.680085 288.035976 \n",
       "L 194.74638 285.982504 \n",
       "L 194.87897 290.007037 \n",
       "L 194.945265 287.940218 \n",
       "L 195.01156 288.870059 \n",
       "L 195.077855 287.956616 \n",
       "L 195.14415 289.424891 \n",
       "L 195.210445 285.622776 \n",
       "L 195.27674 285.88056 \n",
       "L 195.343035 286.520119 \n",
       "L 195.475625 293.772275 \n",
       "L 195.54192 286.28893 \n",
       "L 195.608215 288.947866 \n",
       "L 195.8071 286.610609 \n",
       "L 195.873395 285.47599 \n",
       "L 195.93969 289.590546 \n",
       "L 196.005985 287.881252 \n",
       "L 196.07228 288.619422 \n",
       "L 196.138575 286.922783 \n",
       "L 196.20487 286.979249 \n",
       "L 196.271165 286.647486 \n",
       "L 196.33746 291.482572 \n",
       "L 196.47005 285.871931 \n",
       "L 196.536345 286.744755 \n",
       "L 196.60264 289.668565 \n",
       "L 196.73523 285.913172 \n",
       "L 196.801525 286.236079 \n",
       "L 196.86782 284.353531 \n",
       "L 197.00041 288.25807 \n",
       "L 197.066705 287.610065 \n",
       "L 197.133 283.930939 \n",
       "L 197.199295 288.982667 \n",
       "L 197.26559 285.956192 \n",
       "L 197.331885 287.744169 \n",
       "L 197.39818 287.487303 \n",
       "L 197.464475 282.518071 \n",
       "L 197.597065 289.475891 \n",
       "L 197.66336 288.988867 \n",
       "L 197.729655 282.740461 \n",
       "L 197.92854 288.984178 \n",
       "L 197.994835 288.44097 \n",
       "L 198.19372 282.21351 \n",
       "L 198.32631 289.999961 \n",
       "L 198.392605 283.421642 \n",
       "L 198.4589 290.254158 \n",
       "L 198.525195 288.188808 \n",
       "L 198.59149 286.791913 \n",
       "L 198.657785 287.010674 \n",
       "L 198.72408 290.806856 \n",
       "L 198.790375 285.683268 \n",
       "L 198.85667 290.03873 \n",
       "L 199.055555 281.841198 \n",
       "L 199.12185 288.312333 \n",
       "L 199.188145 283.827625 \n",
       "L 199.25444 285.459169 \n",
       "L 199.38703 287.156458 \n",
       "L 199.453325 284.649844 \n",
       "L 199.51962 290.75193 \n",
       "L 199.585915 284.957385 \n",
       "L 199.65221 288.255598 \n",
       "L 199.718504 286.855117 \n",
       "L 199.851094 288.873986 \n",
       "L 200.049979 287.45318 \n",
       "L 200.116274 288.612953 \n",
       "L 200.182569 284.589395 \n",
       "L 200.248864 290.297221 \n",
       "L 200.315159 286.044945 \n",
       "L 200.381454 289.112138 \n",
       "L 200.447749 286.73025 \n",
       "L 200.514044 290.320807 \n",
       "L 200.580339 286.419587 \n",
       "L 200.646634 290.623122 \n",
       "L 200.712929 289.693437 \n",
       "L 200.779224 291.836452 \n",
       "L 200.845519 288.588703 \n",
       "L 200.911814 289.461965 \n",
       "L 200.978109 290.157566 \n",
       "L 201.110699 283.729155 \n",
       "L 201.176994 286.456323 \n",
       "L 201.309584 289.163491 \n",
       "L 201.375879 288.810444 \n",
       "L 201.442174 286.164854 \n",
       "L 201.508469 289.957901 \n",
       "L 201.574764 285.605348 \n",
       "L 201.641059 286.571938 \n",
       "L 201.707354 289.840379 \n",
       "L 201.773649 288.308759 \n",
       "L 201.839944 283.282963 \n",
       "L 201.972534 288.213439 \n",
       "L 202.038829 286.233071 \n",
       "L 202.237714 289.085275 \n",
       "L 202.304009 286.575286 \n",
       "L 202.370304 288.686608 \n",
       "L 202.436599 282.03486 \n",
       "L 202.502894 288.222309 \n",
       "L 202.569189 288.109504 \n",
       "L 202.635484 285.484083 \n",
       "L 202.701779 290.34976 \n",
       "L 202.768074 289.559926 \n",
       "L 202.834369 289.350714 \n",
       "L 202.900664 287.85809 \n",
       "L 202.966959 288.741309 \n",
       "L 203.033254 287.996783 \n",
       "L 203.099549 285.501342 \n",
       "L 203.232139 288.241235 \n",
       "L 203.298434 283.808756 \n",
       "L 203.364729 288.109151 \n",
       "L 203.431024 286.590229 \n",
       "L 203.563614 288.281275 \n",
       "L 203.629909 285.484026 \n",
       "L 203.696204 286.440123 \n",
       "L 203.762499 287.932154 \n",
       "L 203.895089 282.677032 \n",
       "L 204.093974 290.711381 \n",
       "L 204.160269 286.534327 \n",
       "L 204.226564 287.451443 \n",
       "L 204.292859 287.386079 \n",
       "L 204.359154 284.945296 \n",
       "L 204.491744 289.804787 \n",
       "L 204.624334 285.745849 \n",
       "L 204.690629 288.502902 \n",
       "L 204.756924 288.040665 \n",
       "L 204.955809 282.564919 \n",
       "L 205.022104 287.933255 \n",
       "L 205.088399 287.321718 \n",
       "L 205.154694 286.649308 \n",
       "L 205.220989 283.514208 \n",
       "L 205.287284 287.444551 \n",
       "L 205.353579 282.244977 \n",
       "L 205.419874 288.477734 \n",
       "L 205.486169 288.135265 \n",
       "L 205.685054 286.3774 \n",
       "L 205.751349 289.111361 \n",
       "L 205.883938 280.992511 \n",
       "L 205.950233 289.355925 \n",
       "L 206.016528 288.230331 \n",
       "L 206.149118 285.608921 \n",
       "L 206.348003 288.332346 \n",
       "L 206.414298 288.347924 \n",
       "L 206.480593 290.167707 \n",
       "L 206.546888 285.630445 \n",
       "L 206.613183 286.354124 \n",
       "L 206.679478 289.755468 \n",
       "L 206.745773 289.592072 \n",
       "L 206.878363 286.45025 \n",
       "L 206.944658 291.016578 \n",
       "L 207.010953 285.514674 \n",
       "L 207.077248 289.00357 \n",
       "L 207.276133 284.174515 \n",
       "L 207.342428 288.625806 \n",
       "L 207.408723 284.543267 \n",
       "L 207.475018 284.895609 \n",
       "L 207.607608 288.016401 \n",
       "L 207.673903 287.56446 \n",
       "L 207.740198 286.960366 \n",
       "L 207.806493 284.860159 \n",
       "L 207.872788 288.374914 \n",
       "L 207.939083 287.112081 \n",
       "L 208.005378 286.544948 \n",
       "L 208.071673 283.38002 \n",
       "L 208.137968 286.366144 \n",
       "L 208.204263 285.789308 \n",
       "L 208.270558 284.701932 \n",
       "L 208.336853 291.754747 \n",
       "L 208.403148 284.714361 \n",
       "L 208.469443 288.393049 \n",
       "L 208.535738 287.823303 \n",
       "L 208.602033 284.811616 \n",
       "L 208.734623 291.509717 \n",
       "L 208.933508 285.465341 \n",
       "L 208.999803 285.765298 \n",
       "L 209.066098 287.400937 \n",
       "L 209.132393 281.865716 \n",
       "L 209.198688 290.121537 \n",
       "L 209.264983 289.099144 \n",
       "L 209.331278 284.705082 \n",
       "L 209.397573 286.974108 \n",
       "L 209.463868 288.52389 \n",
       "L 209.530163 284.201476 \n",
       "L 209.596458 286.908574 \n",
       "L 209.662753 285.835845 \n",
       "L 209.729048 289.296917 \n",
       "L 209.795343 285.454875 \n",
       "L 209.861638 286.099871 \n",
       "L 210.060523 289.787641 \n",
       "L 210.126818 284.965789 \n",
       "L 210.259408 289.544475 \n",
       "L 210.325703 287.611252 \n",
       "L 210.391998 288.104716 \n",
       "L 210.458293 289.626194 \n",
       "L 210.524588 289.506327 \n",
       "L 210.657178 284.336823 \n",
       "L 210.723473 285.021803 \n",
       "L 210.789768 287.280336 \n",
       "L 210.856063 284.679306 \n",
       "L 210.922358 285.375783 \n",
       "L 211.054948 288.388628 \n",
       "L 211.121243 284.624309 \n",
       "L 211.187538 286.987441 \n",
       "L 211.253833 286.471167 \n",
       "L 211.320128 286.803198 \n",
       "L 211.386423 287.100768 \n",
       "L 211.452718 288.863548 \n",
       "L 211.519013 288.196081 \n",
       "L 211.585308 284.499597 \n",
       "L 211.651603 286.754331 \n",
       "L 211.717898 290.711692 \n",
       "L 211.784193 284.679475 \n",
       "L 211.850488 285.965076 \n",
       "L 211.916783 286.867927 \n",
       "L 211.983077 285.869826 \n",
       "L 212.115667 291.112971 \n",
       "L 212.314552 285.59717 \n",
       "L 212.380847 288.948488 \n",
       "L 212.447142 287.814462 \n",
       "L 212.513437 288.170673 \n",
       "L 212.579732 285.602933 \n",
       "L 212.646027 285.735765 \n",
       "L 212.712322 287.671898 \n",
       "L 212.778617 287.49065 \n",
       "L 212.844912 285.118409 \n",
       "L 212.911207 291.313526 \n",
       "L 212.977502 287.151825 \n",
       "L 213.043797 289.197586 \n",
       "L 213.110092 286.844143 \n",
       "L 213.176387 288.750334 \n",
       "L 213.242682 284.284735 \n",
       "L 213.308977 290.953459 \n",
       "L 213.308977 290.953459 \n",
       "\" clip-path=\"url(#pec08b2ec4f)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 50.14375 299.078125 \n",
       "L 50.14375 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 221.07875 299.078125 \n",
       "L 221.07875 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 50.14375 299.078125 \n",
       "L 221.07875 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 50.14375 189.718125 \n",
       "L 221.07875 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_38\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(105.30375 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_3\">\n",
       "    <g id=\"patch_19\">\n",
       "     <path d=\"M 134.488125 227.630625 \n",
       "L 214.07875 227.630625 \n",
       "Q 216.07875 227.630625 216.07875 225.630625 \n",
       "L 216.07875 196.718125 \n",
       "Q 216.07875 194.718125 214.07875 194.718125 \n",
       "L 134.488125 194.718125 \n",
       "Q 132.488125 194.718125 132.488125 196.718125 \n",
       "L 132.488125 225.630625 \n",
       "Q 132.488125 227.630625 134.488125 227.630625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_36\">\n",
       "     <path d=\"M 136.488125 202.816562 \n",
       "L 146.488125 202.816562 \n",
       "L 156.488125 202.816562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_39\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(164.488125 206.316562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_37\">\n",
       "     <path d=\"M 136.488125 217.772812 \n",
       "L 146.488125 217.772812 \n",
       "L 156.488125 217.772812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_40\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(164.488125 221.272812) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 275.09875 299.078125 \n",
       "L 446.03375 299.078125 \n",
       "L 446.03375 189.718125 \n",
       "L 275.09875 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_7\">\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_38\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"282.868523\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_41\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(279.687273 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"349.163512\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_42\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(336.438512 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_40\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mae681c3b4b\" x=\"415.458501\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_43\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(402.733501 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_44\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(349.750625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_8\">\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"286.404546\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_45\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(252.195625 290.203765) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_42\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"263.475668\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_46\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(252.195625 267.274887) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_43\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"240.54679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_47\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(252.195625 244.346009) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_44\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"217.617912\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_48\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(252.195625 221.417131) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_17\">\n",
       "     <g id=\"line2d_45\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m05c44a6906\" x=\"275.09875\" y=\"194.689034\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_49\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(252.195625 198.488253) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_50\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(246.115938 252.960625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_46\">\n",
       "    <path d=\"M 282.868523 201.854308 \n",
       "L 283.067408 208.123924 \n",
       "L 283.133703 212.60222 \n",
       "L 283.266293 200.06299 \n",
       "L 283.465178 216.184857 \n",
       "L 283.664063 203.645627 \n",
       "L 283.862948 213.497879 \n",
       "L 283.995538 204.541286 \n",
       "L 284.061833 210.810901 \n",
       "L 284.128128 204.541286 \n",
       "L 284.194423 209.915242 \n",
       "L 284.260717 202.749968 \n",
       "L 284.327012 205.436946 \n",
       "L 284.393307 200.958649 \n",
       "L 284.459602 208.123924 \n",
       "L 284.525897 203.645627 \n",
       "L 284.658487 207.228264 \n",
       "L 284.724782 202.749968 \n",
       "L 284.791077 216.184857 \n",
       "L 284.857372 209.019583 \n",
       "L 284.923667 209.915242 \n",
       "L 284.989962 208.123924 \n",
       "L 285.056257 210.810901 \n",
       "L 285.255142 201.854308 \n",
       "L 285.387732 211.706561 \n",
       "L 285.586617 197.376012 \n",
       "L 285.652912 196.480353 \n",
       "L 285.719207 197.376012 \n",
       "L 285.984387 210.810901 \n",
       "L 286.116977 198.271671 \n",
       "L 286.249567 211.706561 \n",
       "L 286.448452 198.271671 \n",
       "L 286.581042 200.06299 \n",
       "L 286.647337 196.480353 \n",
       "L 286.713632 207.228264 \n",
       "L 286.846222 198.271671 \n",
       "L 286.912517 217.976176 \n",
       "L 286.978812 197.376012 \n",
       "L 287.045107 200.958649 \n",
       "L 287.111402 205.436946 \n",
       "L 287.376582 194.689034 \n",
       "L 287.442877 194.689034 \n",
       "L 287.509172 209.019583 \n",
       "L 287.575467 200.958649 \n",
       "L 287.641762 195.584693 \n",
       "L 287.708057 197.376012 \n",
       "L 287.973237 215.289198 \n",
       "L 288.105827 199.167331 \n",
       "L 288.304712 206.332605 \n",
       "L 288.371007 207.228264 \n",
       "L 288.437302 205.436946 \n",
       "L 288.503597 200.06299 \n",
       "L 288.702482 212.60222 \n",
       "L 288.901367 202.749968 \n",
       "L 289.100252 209.915242 \n",
       "L 289.232842 203.645627 \n",
       "L 289.299137 205.436946 \n",
       "L 289.365432 200.958649 \n",
       "L 289.431727 207.228264 \n",
       "L 289.498022 200.06299 \n",
       "L 289.564317 204.541286 \n",
       "L 289.630612 199.167331 \n",
       "L 289.696907 207.228264 \n",
       "L 289.763202 200.958649 \n",
       "L 289.829497 202.749968 \n",
       "L 289.895792 204.541286 \n",
       "L 289.962087 201.854308 \n",
       "L 290.028382 217.080516 \n",
       "L 290.094677 208.123924 \n",
       "L 290.227267 205.436946 \n",
       "L 290.293562 209.019583 \n",
       "L 290.492446 201.854308 \n",
       "L 290.625036 208.123924 \n",
       "L 290.757626 198.271671 \n",
       "L 290.823921 199.167331 \n",
       "L 290.890216 196.480353 \n",
       "L 290.956511 197.376012 \n",
       "L 291.221691 207.228264 \n",
       "L 291.354281 198.271671 \n",
       "L 291.420576 199.167331 \n",
       "L 291.486871 207.228264 \n",
       "L 291.553166 201.854308 \n",
       "L 291.685756 197.376012 \n",
       "L 291.752051 199.167331 \n",
       "L 291.884641 196.480353 \n",
       "L 291.950936 202.749968 \n",
       "L 292.017231 195.584693 \n",
       "L 292.083526 196.480353 \n",
       "L 292.149821 216.184857 \n",
       "L 292.216116 198.271671 \n",
       "L 292.282411 200.958649 \n",
       "L 292.348706 202.749968 \n",
       "L 292.415001 201.854308 \n",
       "L 292.613886 194.689034 \n",
       "L 292.680181 194.689034 \n",
       "L 292.746476 203.645627 \n",
       "L 292.812771 199.167331 \n",
       "L 292.879066 196.480353 \n",
       "L 292.945361 197.376012 \n",
       "L 293.077951 206.332605 \n",
       "L 293.210541 210.810901 \n",
       "L 293.343131 200.06299 \n",
       "L 293.608311 207.228264 \n",
       "L 293.740901 199.167331 \n",
       "L 293.939786 211.706561 \n",
       "L 294.138671 201.854308 \n",
       "L 294.204966 208.123924 \n",
       "L 294.271261 203.645627 \n",
       "L 294.337556 207.228264 \n",
       "L 294.403851 205.436946 \n",
       "L 294.602736 198.271671 \n",
       "L 294.669031 203.645627 \n",
       "L 294.735326 200.958649 \n",
       "L 294.801621 203.645627 \n",
       "L 294.867916 199.167331 \n",
       "L 294.934211 205.436946 \n",
       "L 295.000506 200.06299 \n",
       "L 295.066801 200.958649 \n",
       "L 295.133096 202.749968 \n",
       "L 295.199391 200.06299 \n",
       "L 295.265686 215.289198 \n",
       "L 295.331981 208.123924 \n",
       "L 295.464571 205.436946 \n",
       "L 295.530866 203.645627 \n",
       "L 295.597161 205.436946 \n",
       "L 295.729751 199.167331 \n",
       "L 295.862341 205.436946 \n",
       "L 296.061226 195.584693 \n",
       "L 296.127521 196.480353 \n",
       "L 296.260111 200.958649 \n",
       "L 296.326406 197.376012 \n",
       "L 296.458996 204.541286 \n",
       "L 296.591585 197.376012 \n",
       "L 296.724175 205.436946 \n",
       "L 296.92306 196.480353 \n",
       "L 297.05565 198.271671 \n",
       "L 297.121945 195.584693 \n",
       "L 297.18824 200.958649 \n",
       "L 297.254535 194.689034 \n",
       "L 297.32083 197.376012 \n",
       "L 297.387125 208.123924 \n",
       "L 297.45342 198.271671 \n",
       "L 297.519715 200.06299 \n",
       "L 297.652305 200.06299 \n",
       "L 297.85119 194.689034 \n",
       "L 297.917485 195.584693 \n",
       "L 297.98378 204.541286 \n",
       "L 298.050075 199.167331 \n",
       "L 298.182665 196.480353 \n",
       "L 298.24896 196.480353 \n",
       "L 298.447845 209.915242 \n",
       "L 298.580435 198.271671 \n",
       "L 298.713025 205.436946 \n",
       "L 298.845615 204.541286 \n",
       "L 298.978205 198.271671 \n",
       "L 299.17709 210.810901 \n",
       "L 299.243385 201.854308 \n",
       "L 299.30968 204.541286 \n",
       "L 299.375975 201.854308 \n",
       "L 299.44227 204.541286 \n",
       "L 299.508565 200.958649 \n",
       "L 299.57486 206.332605 \n",
       "L 299.641155 203.645627 \n",
       "L 299.773745 203.645627 \n",
       "L 299.84004 199.167331 \n",
       "L 299.906335 201.854308 \n",
       "L 299.97263 200.958649 \n",
       "L 300.038925 201.854308 \n",
       "L 300.10522 200.958649 \n",
       "L 300.171515 204.541286 \n",
       "L 300.304105 198.271671 \n",
       "L 300.3704 203.645627 \n",
       "L 300.436695 199.167331 \n",
       "L 300.50299 212.60222 \n",
       "L 300.569285 209.019583 \n",
       "L 300.63558 202.749968 \n",
       "L 300.701875 205.436946 \n",
       "L 300.90076 202.749968 \n",
       "L 300.967055 199.167331 \n",
       "L 301.099645 206.332605 \n",
       "L 301.29853 195.584693 \n",
       "L 301.364825 195.584693 \n",
       "L 301.497415 200.06299 \n",
       "L 301.56371 196.480353 \n",
       "L 301.6963 204.541286 \n",
       "L 301.82889 196.480353 \n",
       "L 301.895185 198.271671 \n",
       "L 301.96148 207.228264 \n",
       "L 302.160365 195.584693 \n",
       "L 302.292955 196.480353 \n",
       "L 302.35925 195.584693 \n",
       "L 302.425545 201.854308 \n",
       "L 302.49184 195.584693 \n",
       "L 302.558135 197.376012 \n",
       "L 302.624429 205.436946 \n",
       "L 302.690724 197.376012 \n",
       "L 302.757019 200.06299 \n",
       "L 302.823314 200.06299 \n",
       "L 302.889609 200.958649 \n",
       "L 303.088494 194.689034 \n",
       "L 303.154789 194.689034 \n",
       "L 303.221084 203.645627 \n",
       "L 303.353674 196.480353 \n",
       "L 303.419969 196.480353 \n",
       "L 303.486264 198.271671 \n",
       "L 303.618854 209.915242 \n",
       "L 303.685149 206.332605 \n",
       "L 303.751444 194.689034 \n",
       "L 303.817739 197.376012 \n",
       "L 304.082919 201.854308 \n",
       "L 304.149214 201.854308 \n",
       "L 304.215509 200.06299 \n",
       "L 304.414394 209.019583 \n",
       "L 304.480689 202.749968 \n",
       "L 304.546984 204.541286 \n",
       "L 304.613279 201.854308 \n",
       "L 304.679574 202.749968 \n",
       "L 304.745869 202.749968 \n",
       "L 304.812164 206.332605 \n",
       "L 304.878459 202.749968 \n",
       "L 304.944754 204.541286 \n",
       "L 305.011049 203.645627 \n",
       "L 305.077344 200.06299 \n",
       "L 305.143639 203.645627 \n",
       "L 305.342524 199.167331 \n",
       "L 305.408819 203.645627 \n",
       "L 305.541409 198.271671 \n",
       "L 305.607704 202.749968 \n",
       "L 305.673999 199.167331 \n",
       "L 305.740294 210.810901 \n",
       "L 305.806589 209.019583 \n",
       "L 305.872884 204.541286 \n",
       "L 305.939179 206.332605 \n",
       "L 306.204359 200.958649 \n",
       "L 306.270654 200.958649 \n",
       "L 306.336949 203.645627 \n",
       "L 306.535834 195.584693 \n",
       "L 306.734719 199.167331 \n",
       "L 306.801014 198.271671 \n",
       "L 306.867309 203.645627 \n",
       "L 306.933604 202.749968 \n",
       "L 307.066194 197.376012 \n",
       "L 307.132489 199.167331 \n",
       "L 307.198784 206.332605 \n",
       "L 307.397669 196.480353 \n",
       "L 307.530259 196.480353 \n",
       "L 307.596554 195.584693 \n",
       "L 307.662849 201.854308 \n",
       "L 307.795439 196.480353 \n",
       "L 307.861734 204.541286 \n",
       "L 307.928029 197.376012 \n",
       "L 307.994324 200.06299 \n",
       "L 308.060619 196.480353 \n",
       "L 308.126914 199.167331 \n",
       "L 308.193209 197.376012 \n",
       "L 308.325799 194.689034 \n",
       "L 308.392094 195.584693 \n",
       "L 308.458389 200.958649 \n",
       "L 308.524684 195.584693 \n",
       "L 308.590979 196.480353 \n",
       "L 308.856158 208.123924 \n",
       "L 308.988748 194.689034 \n",
       "L 308.988748 194.689034 \n",
       "\" clip-path=\"url(#p184d359044)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_47\">\n",
       "    <path d=\"M 282.868523 294.107216 \n",
       "L 283.133703 269.924415 \n",
       "L 283.199998 279.776667 \n",
       "L 283.266293 278.881008 \n",
       "L 283.332588 280.672326 \n",
       "L 283.398883 269.028756 \n",
       "L 283.465178 274.402711 \n",
       "L 283.531473 273.507052 \n",
       "L 283.730358 262.759141 \n",
       "L 283.796653 268.133096 \n",
       "L 283.995538 243.054636 \n",
       "L 284.194423 260.967822 \n",
       "L 284.260717 264.550459 \n",
       "L 284.327012 244.845955 \n",
       "L 284.393307 269.924415 \n",
       "L 284.459602 258.280844 \n",
       "L 284.658487 254.698207 \n",
       "L 284.724782 249.324251 \n",
       "L 284.791077 251.11557 \n",
       "L 284.857372 251.11557 \n",
       "L 284.923667 245.741614 \n",
       "L 285.056257 254.698207 \n",
       "L 285.122552 245.741614 \n",
       "L 285.188847 251.11557 \n",
       "L 285.255142 248.428592 \n",
       "L 285.387732 254.698207 \n",
       "L 285.586617 243.054636 \n",
       "L 285.652912 243.054636 \n",
       "L 285.719207 252.011229 \n",
       "L 285.918092 231.411065 \n",
       "L 285.984387 244.845955 \n",
       "L 286.050682 228.724087 \n",
       "L 286.116977 235.889362 \n",
       "L 286.183272 240.367658 \n",
       "L 286.249567 234.993702 \n",
       "L 286.315862 242.158977 \n",
       "L 286.382157 232.306725 \n",
       "L 286.581042 243.054636 \n",
       "L 286.647337 240.367658 \n",
       "L 286.713632 232.306725 \n",
       "L 286.779927 234.993702 \n",
       "L 286.912517 229.619747 \n",
       "L 286.978812 235.889362 \n",
       "L 287.111402 221.558813 \n",
       "L 287.177697 228.724087 \n",
       "L 287.243992 221.558813 \n",
       "L 287.310287 226.037109 \n",
       "L 287.376582 224.245791 \n",
       "L 287.509172 231.411065 \n",
       "L 287.641762 222.454472 \n",
       "L 287.973237 236.785021 \n",
       "L 288.039532 218.871835 \n",
       "L 288.105827 225.14145 \n",
       "L 288.172122 226.932769 \n",
       "L 288.238417 233.202384 \n",
       "L 288.503597 217.080516 \n",
       "L 288.636187 230.515406 \n",
       "L 288.702482 222.454472 \n",
       "L 288.768777 225.14145 \n",
       "L 288.835072 225.14145 \n",
       "L 288.901367 224.245791 \n",
       "L 288.967662 230.515406 \n",
       "L 289.166547 222.454472 \n",
       "L 289.232842 221.558813 \n",
       "L 289.431727 227.828428 \n",
       "L 289.498022 226.037109 \n",
       "L 289.564317 229.619747 \n",
       "L 289.630612 222.454472 \n",
       "L 289.696907 227.828428 \n",
       "L 289.763202 218.871835 \n",
       "L 289.829497 224.245791 \n",
       "L 289.895792 216.184857 \n",
       "L 289.962087 227.828428 \n",
       "L 290.028382 224.245791 \n",
       "L 290.160972 219.767494 \n",
       "L 290.227267 219.767494 \n",
       "L 290.359857 229.619747 \n",
       "L 290.426151 229.619747 \n",
       "L 290.492446 232.306725 \n",
       "L 290.558741 231.411065 \n",
       "L 290.691331 217.080516 \n",
       "L 290.757626 224.245791 \n",
       "L 290.890216 217.080516 \n",
       "L 290.956511 227.828428 \n",
       "L 291.022806 224.245791 \n",
       "L 291.089101 229.619747 \n",
       "L 291.221691 220.663154 \n",
       "L 291.287986 232.306725 \n",
       "L 291.354281 223.350132 \n",
       "L 291.420576 226.932769 \n",
       "L 291.486871 226.932769 \n",
       "L 291.553166 224.245791 \n",
       "L 291.619461 227.828428 \n",
       "L 291.818346 217.080516 \n",
       "L 291.884641 225.14145 \n",
       "L 291.950936 217.080516 \n",
       "L 292.017231 219.767494 \n",
       "L 292.083526 221.558813 \n",
       "L 292.149821 219.767494 \n",
       "L 292.216116 206.332605 \n",
       "L 292.282411 212.60222 \n",
       "L 292.348706 224.245791 \n",
       "L 292.415001 217.080516 \n",
       "L 292.481296 221.558813 \n",
       "L 292.613886 213.497879 \n",
       "L 292.680181 216.184857 \n",
       "L 292.746476 213.497879 \n",
       "L 292.945361 220.663154 \n",
       "L 293.011656 209.915242 \n",
       "L 293.077951 217.080516 \n",
       "L 293.144246 212.60222 \n",
       "L 293.210541 219.767494 \n",
       "L 293.276836 217.080516 \n",
       "L 293.343131 217.976176 \n",
       "L 293.409426 210.810901 \n",
       "L 293.475721 211.706561 \n",
       "L 293.542016 217.080516 \n",
       "L 293.608311 216.184857 \n",
       "L 293.674606 212.60222 \n",
       "L 293.873491 217.080516 \n",
       "L 293.939786 209.915242 \n",
       "L 294.006081 212.60222 \n",
       "L 294.138671 217.976176 \n",
       "L 294.204966 213.497879 \n",
       "L 294.271261 220.663154 \n",
       "L 294.403851 213.497879 \n",
       "L 294.536441 215.289198 \n",
       "L 294.602736 217.976176 \n",
       "L 294.669031 214.393539 \n",
       "L 294.735326 216.184857 \n",
       "L 294.801621 216.184857 \n",
       "L 294.867916 212.60222 \n",
       "L 295.000506 222.454472 \n",
       "L 295.199391 216.184857 \n",
       "L 295.265686 212.60222 \n",
       "L 295.331981 216.184857 \n",
       "L 295.398276 214.393539 \n",
       "L 295.464571 215.289198 \n",
       "L 295.597161 222.454472 \n",
       "L 295.729751 211.706561 \n",
       "L 295.796046 211.706561 \n",
       "L 295.862341 217.976176 \n",
       "L 295.928636 213.497879 \n",
       "L 295.994931 218.871835 \n",
       "L 296.127521 213.497879 \n",
       "L 296.193816 219.767494 \n",
       "L 296.260111 218.871835 \n",
       "L 296.326406 209.915242 \n",
       "L 296.52529 220.663154 \n",
       "L 296.591585 217.976176 \n",
       "L 296.65788 206.332605 \n",
       "L 296.724175 211.706561 \n",
       "L 296.856765 222.454472 \n",
       "L 296.92306 206.332605 \n",
       "L 296.989355 211.706561 \n",
       "L 297.18824 217.080516 \n",
       "L 297.254535 209.915242 \n",
       "L 297.32083 215.289198 \n",
       "L 297.45342 206.332605 \n",
       "L 297.58601 215.289198 \n",
       "L 297.7186 209.915242 \n",
       "L 297.784895 209.019583 \n",
       "L 297.85119 213.497879 \n",
       "L 297.98378 209.915242 \n",
       "L 298.050075 210.810901 \n",
       "L 298.11637 207.228264 \n",
       "L 298.182665 212.60222 \n",
       "L 298.24896 204.541286 \n",
       "L 298.38155 212.60222 \n",
       "L 298.447845 211.706561 \n",
       "L 298.51414 213.497879 \n",
       "L 298.580435 208.123924 \n",
       "L 298.64673 209.019583 \n",
       "L 298.713025 212.60222 \n",
       "L 298.845615 207.228264 \n",
       "L 298.978205 209.915242 \n",
       "L 299.110795 203.645627 \n",
       "L 299.30968 214.393539 \n",
       "L 299.44227 200.06299 \n",
       "L 299.57486 210.810901 \n",
       "L 299.70745 211.706561 \n",
       "L 299.773745 205.436946 \n",
       "L 299.84004 215.289198 \n",
       "L 299.906335 209.019583 \n",
       "L 300.038925 210.810901 \n",
       "L 300.10522 204.541286 \n",
       "L 300.171515 207.228264 \n",
       "L 300.304105 209.019583 \n",
       "L 300.3704 203.645627 \n",
       "L 300.436695 209.915242 \n",
       "L 300.50299 207.228264 \n",
       "L 300.569285 208.123924 \n",
       "L 300.63558 210.810901 \n",
       "L 300.701875 201.854308 \n",
       "L 300.76817 204.541286 \n",
       "L 300.834465 201.854308 \n",
       "L 300.967055 209.019583 \n",
       "L 301.03335 208.123924 \n",
       "L 301.099645 207.228264 \n",
       "L 301.232235 212.60222 \n",
       "L 301.43112 205.436946 \n",
       "L 301.497415 205.436946 \n",
       "L 301.630005 213.497879 \n",
       "L 301.762595 204.541286 \n",
       "L 301.82889 204.541286 \n",
       "L 301.96148 209.915242 \n",
       "L 302.09407 209.019583 \n",
       "L 302.22666 199.167331 \n",
       "L 302.425545 207.228264 \n",
       "L 302.49184 207.228264 \n",
       "L 302.558135 201.854308 \n",
       "L 302.690724 214.393539 \n",
       "L 302.757019 203.645627 \n",
       "L 302.823314 210.810901 \n",
       "L 302.889609 202.749968 \n",
       "L 302.955904 207.228264 \n",
       "L 303.022199 207.228264 \n",
       "L 303.088494 202.749968 \n",
       "L 303.154789 205.436946 \n",
       "L 303.221084 205.436946 \n",
       "L 303.287379 202.749968 \n",
       "L 303.353674 207.228264 \n",
       "L 303.419969 204.541286 \n",
       "L 303.552559 204.541286 \n",
       "L 303.685149 209.915242 \n",
       "L 303.751444 207.228264 \n",
       "L 303.817739 204.541286 \n",
       "L 303.950329 209.915242 \n",
       "L 304.016624 201.854308 \n",
       "L 304.082919 207.228264 \n",
       "L 304.149214 208.123924 \n",
       "L 304.215509 207.228264 \n",
       "L 304.281804 208.123924 \n",
       "L 304.348099 201.854308 \n",
       "L 304.414394 209.019583 \n",
       "L 304.480689 206.332605 \n",
       "L 304.546984 208.123924 \n",
       "L 304.679574 201.854308 \n",
       "L 304.745869 212.60222 \n",
       "L 304.812164 208.123924 \n",
       "L 304.944754 209.915242 \n",
       "L 305.011049 214.393539 \n",
       "L 305.077344 203.645627 \n",
       "L 305.143639 209.019583 \n",
       "L 305.276229 202.749968 \n",
       "L 305.342524 207.228264 \n",
       "L 305.408819 205.436946 \n",
       "L 305.475114 207.228264 \n",
       "L 305.607704 204.541286 \n",
       "L 305.673999 207.228264 \n",
       "L 305.740294 201.854308 \n",
       "L 305.806589 208.123924 \n",
       "L 305.872884 207.228264 \n",
       "L 305.939179 209.915242 \n",
       "L 306.071769 201.854308 \n",
       "L 306.138064 208.123924 \n",
       "L 306.204359 205.436946 \n",
       "L 306.270654 204.541286 \n",
       "L 306.336949 209.019583 \n",
       "L 306.403244 207.228264 \n",
       "L 306.469539 206.332605 \n",
       "L 306.535834 202.749968 \n",
       "L 306.668424 209.915242 \n",
       "L 306.801014 208.123924 \n",
       "L 306.867309 209.915242 \n",
       "L 306.933604 204.541286 \n",
       "L 306.999899 207.228264 \n",
       "L 307.066194 207.228264 \n",
       "L 307.132489 203.645627 \n",
       "L 307.265079 209.019583 \n",
       "L 307.331374 202.749968 \n",
       "L 307.397669 204.541286 \n",
       "L 307.463964 209.019583 \n",
       "L 307.530259 200.958649 \n",
       "L 307.596554 207.228264 \n",
       "L 307.662849 204.541286 \n",
       "L 307.861734 211.706561 \n",
       "L 308.060619 202.749968 \n",
       "L 308.126914 209.019583 \n",
       "L 308.193209 206.332605 \n",
       "L 308.259504 206.332605 \n",
       "L 308.392094 202.749968 \n",
       "L 308.524684 208.123924 \n",
       "L 308.590979 206.332605 \n",
       "L 308.657274 208.123924 \n",
       "L 308.789863 202.749968 \n",
       "L 308.856158 209.915242 \n",
       "L 308.922453 205.436946 \n",
       "L 308.988748 200.958649 \n",
       "L 309.055043 205.436946 \n",
       "L 309.121338 204.541286 \n",
       "L 309.187633 204.541286 \n",
       "L 309.253928 207.228264 \n",
       "L 309.386518 204.541286 \n",
       "L 309.452813 205.436946 \n",
       "L 309.519108 208.123924 \n",
       "L 309.585403 203.645627 \n",
       "L 309.651698 214.393539 \n",
       "L 309.717993 209.915242 \n",
       "L 309.784288 199.167331 \n",
       "L 309.850583 202.749968 \n",
       "L 309.983173 200.06299 \n",
       "L 310.049468 211.706561 \n",
       "L 310.115763 204.541286 \n",
       "L 310.182058 202.749968 \n",
       "L 310.314648 208.123924 \n",
       "L 310.380943 206.332605 \n",
       "L 310.447238 212.60222 \n",
       "L 310.646123 205.436946 \n",
       "L 310.712418 205.436946 \n",
       "L 310.845008 204.541286 \n",
       "L 310.911303 204.541286 \n",
       "L 310.977598 202.749968 \n",
       "L 311.043893 212.60222 \n",
       "L 311.110188 206.332605 \n",
       "L 311.176483 210.810901 \n",
       "L 311.242778 204.541286 \n",
       "L 311.309073 205.436946 \n",
       "L 311.375368 204.541286 \n",
       "L 311.441663 200.958649 \n",
       "L 311.507958 209.019583 \n",
       "L 311.574253 204.541286 \n",
       "L 311.640548 205.436946 \n",
       "L 311.706843 204.541286 \n",
       "L 311.773138 206.332605 \n",
       "L 311.839433 204.541286 \n",
       "L 311.905728 209.019583 \n",
       "L 311.972023 206.332605 \n",
       "L 312.038318 204.541286 \n",
       "L 312.104613 198.271671 \n",
       "L 312.237203 207.228264 \n",
       "L 312.369793 202.749968 \n",
       "L 312.436088 207.228264 \n",
       "L 312.502383 205.436946 \n",
       "L 312.568678 204.541286 \n",
       "L 312.701268 213.497879 \n",
       "L 312.767563 203.645627 \n",
       "L 312.833858 209.019583 \n",
       "L 312.900153 204.541286 \n",
       "L 312.966448 209.019583 \n",
       "L 313.032743 206.332605 \n",
       "L 313.099038 201.854308 \n",
       "L 313.165333 208.123924 \n",
       "L 313.231628 202.749968 \n",
       "L 313.297923 204.541286 \n",
       "L 313.430513 208.123924 \n",
       "L 313.496808 203.645627 \n",
       "L 313.563103 208.123924 \n",
       "L 313.629398 207.228264 \n",
       "L 313.695693 208.123924 \n",
       "L 313.761988 201.854308 \n",
       "L 313.828283 209.019583 \n",
       "L 313.894578 203.048516 \n",
       "L 313.960873 208.123924 \n",
       "L 314.093463 200.06299 \n",
       "L 314.159758 201.854308 \n",
       "L 314.226053 207.228264 \n",
       "L 314.358643 202.749968 \n",
       "L 314.491233 203.645627 \n",
       "L 314.557528 203.645627 \n",
       "L 314.623823 210.810901 \n",
       "L 314.690118 200.958649 \n",
       "L 314.756413 205.436946 \n",
       "L 314.822707 206.332605 \n",
       "L 314.955297 201.854308 \n",
       "L 315.087887 201.854308 \n",
       "L 315.154182 206.332605 \n",
       "L 315.353067 199.167331 \n",
       "L 315.551952 209.019583 \n",
       "L 315.684542 202.749968 \n",
       "L 315.883427 207.228264 \n",
       "L 315.949722 201.854308 \n",
       "L 316.016017 205.436946 \n",
       "L 316.082312 205.436946 \n",
       "L 316.148607 210.810901 \n",
       "L 316.214902 205.436946 \n",
       "L 316.281197 206.332605 \n",
       "L 316.347492 206.332605 \n",
       "L 316.546377 201.854308 \n",
       "L 316.745262 208.123924 \n",
       "L 316.877852 200.958649 \n",
       "L 316.944147 209.915242 \n",
       "L 317.010442 201.854308 \n",
       "L 317.076737 204.541286 \n",
       "L 317.275622 208.123924 \n",
       "L 317.341917 205.436946 \n",
       "L 317.408212 206.332605 \n",
       "L 317.474507 205.436946 \n",
       "L 317.540802 209.915242 \n",
       "L 317.673392 200.958649 \n",
       "L 317.739687 202.749968 \n",
       "L 317.805982 202.749968 \n",
       "L 318.004867 209.019583 \n",
       "L 318.071162 203.645627 \n",
       "L 318.137457 208.123924 \n",
       "L 318.203752 207.228264 \n",
       "L 318.270047 208.123924 \n",
       "L 318.336342 206.332605 \n",
       "L 318.402637 209.915242 \n",
       "L 318.667817 203.645627 \n",
       "L 318.734112 208.123924 \n",
       "L 318.800407 207.228264 \n",
       "L 318.866702 208.123924 \n",
       "L 318.932997 203.645627 \n",
       "L 318.999292 204.541286 \n",
       "L 319.065587 206.332605 \n",
       "L 319.131882 200.958649 \n",
       "L 319.330767 207.228264 \n",
       "L 319.397062 207.228264 \n",
       "L 319.463357 200.06299 \n",
       "L 319.529652 208.123924 \n",
       "L 319.595947 202.749968 \n",
       "L 319.728537 200.958649 \n",
       "L 319.861127 205.436946 \n",
       "L 319.993717 202.749968 \n",
       "L 320.192602 204.541286 \n",
       "L 320.258897 204.541286 \n",
       "L 320.325192 203.645627 \n",
       "L 320.590372 207.228264 \n",
       "L 320.722962 206.332605 \n",
       "L 320.789257 208.123924 \n",
       "L 320.921846 203.645627 \n",
       "L 321.054436 208.123924 \n",
       "L 321.120731 202.749968 \n",
       "L 321.187026 209.915242 \n",
       "L 321.253321 206.332605 \n",
       "L 321.385911 201.854308 \n",
       "L 321.452206 205.436946 \n",
       "L 321.518501 204.541286 \n",
       "L 321.651091 200.06299 \n",
       "L 321.717386 200.958649 \n",
       "L 321.783681 203.645627 \n",
       "L 321.849976 201.854308 \n",
       "L 321.916271 201.854308 \n",
       "L 322.048861 208.123924 \n",
       "L 322.115156 203.645627 \n",
       "L 322.181451 205.436946 \n",
       "L 322.247746 208.123924 \n",
       "L 322.380336 198.271671 \n",
       "L 322.446631 201.854308 \n",
       "L 322.512926 209.019583 \n",
       "L 322.579221 203.645627 \n",
       "L 322.645516 204.541286 \n",
       "L 322.711811 205.436946 \n",
       "L 322.778106 211.706561 \n",
       "L 322.844401 203.645627 \n",
       "L 322.910696 206.332605 \n",
       "L 322.976991 209.019583 \n",
       "L 323.043286 202.749968 \n",
       "L 323.175876 209.019583 \n",
       "L 323.242171 203.645627 \n",
       "L 323.308466 207.228264 \n",
       "L 323.507351 201.854308 \n",
       "L 323.573646 208.123924 \n",
       "L 323.639941 200.06299 \n",
       "L 323.706236 200.958649 \n",
       "L 323.772531 206.332605 \n",
       "L 323.838826 200.958649 \n",
       "L 323.905121 215.289198 \n",
       "L 323.971416 203.645627 \n",
       "L 324.037711 207.228264 \n",
       "L 324.236596 199.167331 \n",
       "L 324.302891 203.645627 \n",
       "L 324.369186 202.749968 \n",
       "L 324.435481 202.749968 \n",
       "L 324.501776 209.019583 \n",
       "L 324.568071 199.167331 \n",
       "L 324.634366 204.541286 \n",
       "L 324.700661 206.332605 \n",
       "L 324.766956 200.06299 \n",
       "L 324.833251 205.436946 \n",
       "L 324.899546 201.854308 \n",
       "L 325.164726 204.541286 \n",
       "L 325.231021 201.854308 \n",
       "L 325.363611 210.810901 \n",
       "L 325.429906 209.019583 \n",
       "L 325.496201 206.332605 \n",
       "L 325.562496 207.228264 \n",
       "L 325.628791 207.228264 \n",
       "L 325.761381 204.541286 \n",
       "L 325.827676 206.332605 \n",
       "L 325.893971 205.436946 \n",
       "L 325.960266 206.332605 \n",
       "L 326.159151 200.06299 \n",
       "L 326.225446 201.854308 \n",
       "L 326.291741 201.854308 \n",
       "L 326.358036 200.958649 \n",
       "L 326.424331 206.332605 \n",
       "L 326.490626 201.854308 \n",
       "L 326.556921 202.749968 \n",
       "L 326.623216 200.958649 \n",
       "L 326.689511 204.541286 \n",
       "L 326.888396 200.958649 \n",
       "L 326.954691 202.749968 \n",
       "L 327.020985 197.376012 \n",
       "L 327.153575 209.915242 \n",
       "L 327.21987 203.645627 \n",
       "L 327.286165 208.123924 \n",
       "L 327.35246 205.436946 \n",
       "L 327.48505 199.167331 \n",
       "L 327.551345 204.541286 \n",
       "L 327.61764 200.958649 \n",
       "L 327.816525 207.228264 \n",
       "L 327.949115 201.854308 \n",
       "L 328.28059 212.60222 \n",
       "L 328.346885 203.645627 \n",
       "L 328.41318 205.436946 \n",
       "L 328.612065 197.376012 \n",
       "L 328.81095 203.645627 \n",
       "L 328.877245 203.645627 \n",
       "L 328.94354 207.228264 \n",
       "L 329.142425 199.167331 \n",
       "L 329.20872 206.332605 \n",
       "L 329.275015 202.749968 \n",
       "L 329.407605 209.019583 \n",
       "L 329.4739 208.123924 \n",
       "L 329.60649 208.123924 \n",
       "L 329.87167 201.854308 \n",
       "L 329.937965 202.749968 \n",
       "L 330.00426 209.019583 \n",
       "L 330.13685 199.167331 \n",
       "L 330.26944 203.645627 \n",
       "L 330.335735 201.854308 \n",
       "L 330.40203 202.749968 \n",
       "L 330.468325 206.332605 \n",
       "L 330.53462 204.541286 \n",
       "L 330.600915 206.332605 \n",
       "L 330.66721 203.645627 \n",
       "L 330.733505 204.541286 \n",
       "L 330.866095 209.019583 \n",
       "L 331.06498 201.854308 \n",
       "L 331.131275 205.436946 \n",
       "L 331.19757 202.749968 \n",
       "L 331.263865 208.123924 \n",
       "L 331.33016 206.332605 \n",
       "L 331.396455 200.06299 \n",
       "L 331.529045 207.228264 \n",
       "L 331.59534 200.958649 \n",
       "L 331.661635 204.541286 \n",
       "L 331.72793 201.854308 \n",
       "L 331.794225 210.810901 \n",
       "L 331.926815 201.854308 \n",
       "L 331.99311 202.749968 \n",
       "L 332.059405 209.019583 \n",
       "L 332.1257 202.749968 \n",
       "L 332.191995 205.436946 \n",
       "L 332.25829 206.332605 \n",
       "L 332.324585 204.541286 \n",
       "L 332.39088 206.332605 \n",
       "L 332.52347 203.645627 \n",
       "L 332.589765 209.019583 \n",
       "L 332.65606 200.06299 \n",
       "L 332.722355 204.541286 \n",
       "L 332.78865 202.749968 \n",
       "L 332.854945 206.332605 \n",
       "L 332.987535 202.749968 \n",
       "L 333.120124 204.541286 \n",
       "L 333.186419 207.228264 \n",
       "L 333.319009 200.06299 \n",
       "L 333.385304 205.436946 \n",
       "L 333.451599 200.06299 \n",
       "L 333.517894 200.958649 \n",
       "L 333.584189 205.436946 \n",
       "L 333.650484 200.958649 \n",
       "L 333.783074 207.228264 \n",
       "L 333.849369 199.167331 \n",
       "L 333.915664 207.228264 \n",
       "L 333.981959 201.854308 \n",
       "L 334.048254 201.854308 \n",
       "L 334.114549 205.436946 \n",
       "L 334.180844 200.958649 \n",
       "L 334.247139 202.749968 \n",
       "L 334.379729 205.436946 \n",
       "L 334.446024 201.854308 \n",
       "L 334.512319 206.332605 \n",
       "L 334.711204 200.06299 \n",
       "L 334.777499 199.167331 \n",
       "L 334.843794 207.228264 \n",
       "L 334.910089 204.541286 \n",
       "L 334.976384 205.436946 \n",
       "L 335.108974 201.854308 \n",
       "L 335.175269 202.749968 \n",
       "L 335.241564 201.854308 \n",
       "L 335.374154 205.436946 \n",
       "L 335.440449 204.541286 \n",
       "L 335.573039 200.06299 \n",
       "L 335.705629 208.123924 \n",
       "L 335.904514 200.958649 \n",
       "L 335.970809 207.228264 \n",
       "L 336.037104 200.06299 \n",
       "L 336.103399 206.332605 \n",
       "L 336.169694 201.854308 \n",
       "L 336.235989 203.645627 \n",
       "L 336.302284 208.123924 \n",
       "L 336.501169 200.06299 \n",
       "L 336.633759 204.541286 \n",
       "L 336.700054 203.645627 \n",
       "L 336.832644 199.167331 \n",
       "L 337.164119 208.123924 \n",
       "L 337.296709 200.958649 \n",
       "L 337.429299 204.541286 \n",
       "L 337.495594 200.958649 \n",
       "L 337.561889 208.123924 \n",
       "L 337.628184 202.749968 \n",
       "L 337.694479 207.228264 \n",
       "L 337.760774 202.749968 \n",
       "L 337.827069 207.228264 \n",
       "L 337.893364 204.541286 \n",
       "L 337.959659 204.541286 \n",
       "L 338.025954 209.019583 \n",
       "L 338.092249 201.854308 \n",
       "L 338.158544 202.749968 \n",
       "L 338.291134 200.958649 \n",
       "L 338.357429 209.915242 \n",
       "L 338.423724 207.228264 \n",
       "L 338.622609 203.645627 \n",
       "L 338.688904 204.541286 \n",
       "L 338.755199 204.541286 \n",
       "L 338.887789 202.749968 \n",
       "L 338.954084 207.228264 \n",
       "L 339.020379 204.541286 \n",
       "L 339.285558 198.271671 \n",
       "L 339.484443 206.332605 \n",
       "L 339.617033 204.541286 \n",
       "L 339.749623 202.749968 \n",
       "L 339.815918 204.541286 \n",
       "L 339.882213 201.854308 \n",
       "L 339.948508 210.810901 \n",
       "L 340.014803 202.749968 \n",
       "L 340.081098 205.436946 \n",
       "L 340.279983 203.645627 \n",
       "L 340.346278 204.541286 \n",
       "L 340.412573 201.854308 \n",
       "L 340.478868 206.332605 \n",
       "L 340.545163 201.854308 \n",
       "L 340.611458 207.228264 \n",
       "L 340.677753 203.645627 \n",
       "L 340.744048 205.436946 \n",
       "L 340.876638 200.06299 \n",
       "L 341.009228 209.019583 \n",
       "L 341.075523 208.123924 \n",
       "L 341.208113 202.749968 \n",
       "L 341.274408 205.436946 \n",
       "L 341.406998 199.167331 \n",
       "L 341.473293 200.06299 \n",
       "L 341.539588 204.541286 \n",
       "L 341.605883 198.271671 \n",
       "L 341.672178 200.958649 \n",
       "L 341.738473 202.749968 \n",
       "L 341.804768 200.06299 \n",
       "L 341.937358 203.645627 \n",
       "L 342.069948 202.749968 \n",
       "L 342.136243 202.749968 \n",
       "L 342.268833 199.167331 \n",
       "L 342.335128 203.645627 \n",
       "L 342.401423 202.749968 \n",
       "L 342.467718 203.645627 \n",
       "L 342.600308 202.749968 \n",
       "L 342.732898 211.706561 \n",
       "L 342.799193 200.06299 \n",
       "L 342.865488 205.436946 \n",
       "L 342.931783 198.271671 \n",
       "L 342.998078 201.854308 \n",
       "L 343.130668 200.06299 \n",
       "L 343.263258 204.541286 \n",
       "L 343.395848 201.854308 \n",
       "L 343.594733 205.436946 \n",
       "L 343.661028 197.376012 \n",
       "L 343.727323 204.541286 \n",
       "L 343.793618 200.06299 \n",
       "L 343.859913 200.958649 \n",
       "L 343.926208 200.06299 \n",
       "L 344.058798 205.436946 \n",
       "L 344.257683 200.06299 \n",
       "L 344.456568 207.228264 \n",
       "L 344.522863 199.167331 \n",
       "L 344.655453 207.228264 \n",
       "L 344.721748 200.06299 \n",
       "L 344.788043 209.019583 \n",
       "L 344.854338 199.167331 \n",
       "L 344.920633 200.958649 \n",
       "L 344.986928 210.213791 \n",
       "L 345.053223 205.436946 \n",
       "L 345.119518 206.332605 \n",
       "L 345.185813 200.958649 \n",
       "L 345.252108 204.541286 \n",
       "L 345.318402 200.06299 \n",
       "L 345.384697 200.958649 \n",
       "L 345.450992 204.541286 \n",
       "L 345.583582 200.958649 \n",
       "L 345.649877 202.749968 \n",
       "L 345.716172 200.958649 \n",
       "L 345.848762 205.436946 \n",
       "L 345.915057 199.167331 \n",
       "L 345.981352 208.123924 \n",
       "L 346.113942 199.167331 \n",
       "L 346.180237 203.645627 \n",
       "L 346.246532 200.958649 \n",
       "L 346.312827 197.376012 \n",
       "L 346.379122 205.436946 \n",
       "L 346.445417 200.958649 \n",
       "L 346.511712 202.749968 \n",
       "L 346.578007 200.06299 \n",
       "L 346.776892 206.332605 \n",
       "L 346.843187 205.436946 \n",
       "L 346.909482 201.854308 \n",
       "L 346.975777 205.436946 \n",
       "L 347.042072 200.06299 \n",
       "L 347.108367 209.019583 \n",
       "L 347.174662 204.541286 \n",
       "L 347.240957 206.332605 \n",
       "L 347.307252 200.958649 \n",
       "L 347.373547 201.854308 \n",
       "L 347.439842 207.228264 \n",
       "L 347.506137 204.541286 \n",
       "L 347.572432 200.958649 \n",
       "L 347.705022 207.228264 \n",
       "L 347.903907 200.06299 \n",
       "L 348.102792 209.019583 \n",
       "L 348.169087 198.271671 \n",
       "L 348.235382 201.854308 \n",
       "L 348.301677 206.332605 \n",
       "L 348.367972 203.645627 \n",
       "L 348.500562 200.958649 \n",
       "L 348.633152 203.645627 \n",
       "L 348.699447 201.854308 \n",
       "L 348.765742 203.645627 \n",
       "L 348.964627 200.06299 \n",
       "L 349.030922 209.019583 \n",
       "L 349.097217 203.645627 \n",
       "L 349.229807 200.958649 \n",
       "L 349.362397 204.541286 \n",
       "L 349.561282 200.958649 \n",
       "L 349.627577 205.436946 \n",
       "L 349.693872 200.958649 \n",
       "L 349.760167 203.645627 \n",
       "L 349.892757 202.749968 \n",
       "L 350.025347 203.645627 \n",
       "L 350.091642 203.645627 \n",
       "L 350.157937 206.332605 \n",
       "L 350.224232 202.749968 \n",
       "L 350.290527 204.541286 \n",
       "L 350.356822 203.645627 \n",
       "L 350.423117 198.271671 \n",
       "L 350.489412 205.436946 \n",
       "L 350.555707 201.854308 \n",
       "L 350.622002 207.228264 \n",
       "L 350.754592 200.06299 \n",
       "L 350.820887 207.228264 \n",
       "L 350.887182 202.749968 \n",
       "L 350.953477 202.749968 \n",
       "L 351.019772 204.541286 \n",
       "L 351.086067 203.645627 \n",
       "L 351.218657 199.167331 \n",
       "L 351.284952 203.645627 \n",
       "L 351.351247 202.749968 \n",
       "L 351.483836 200.958649 \n",
       "L 351.550131 203.645627 \n",
       "L 351.616426 199.167331 \n",
       "L 351.749016 206.332605 \n",
       "L 351.881606 199.167331 \n",
       "L 352.080491 204.541286 \n",
       "L 352.146786 200.958649 \n",
       "L 352.213081 206.332605 \n",
       "L 352.279376 202.749968 \n",
       "L 352.345671 205.436946 \n",
       "L 352.411966 200.06299 \n",
       "L 352.478261 201.854308 \n",
       "L 352.610851 202.749968 \n",
       "L 352.876031 202.749968 \n",
       "L 353.008621 200.06299 \n",
       "L 353.141211 205.436946 \n",
       "L 353.273801 198.271671 \n",
       "L 353.406391 205.436946 \n",
       "L 353.472686 204.541286 \n",
       "L 353.538981 207.228264 \n",
       "L 353.605276 201.854308 \n",
       "L 353.671571 205.436946 \n",
       "L 353.737866 200.958649 \n",
       "L 353.804161 205.436946 \n",
       "L 353.870456 202.749968 \n",
       "L 353.936751 202.749968 \n",
       "L 354.003046 204.541286 \n",
       "L 354.069341 197.376012 \n",
       "L 354.135636 203.645627 \n",
       "L 354.201931 202.749968 \n",
       "L 354.268226 201.854308 \n",
       "L 354.334521 198.271671 \n",
       "L 354.400816 199.167331 \n",
       "L 354.533406 203.645627 \n",
       "L 354.665996 200.958649 \n",
       "L 354.732291 200.958649 \n",
       "L 354.798586 198.271671 \n",
       "L 354.931176 201.854308 \n",
       "L 354.997471 200.958649 \n",
       "L 355.063766 203.645627 \n",
       "L 355.130061 200.06299 \n",
       "L 355.262651 204.541286 \n",
       "L 355.395241 201.854308 \n",
       "L 355.527831 205.436946 \n",
       "L 355.594126 198.271671 \n",
       "L 355.660421 199.167331 \n",
       "L 355.726716 208.123924 \n",
       "L 355.793011 205.436946 \n",
       "L 355.991896 197.376012 \n",
       "L 356.190781 201.854308 \n",
       "L 356.257076 201.854308 \n",
       "L 356.323371 200.958649 \n",
       "L 356.389666 201.854308 \n",
       "L 356.455961 200.06299 \n",
       "L 356.522256 204.541286 \n",
       "L 356.588551 200.06299 \n",
       "L 356.654846 207.228264 \n",
       "L 356.721141 205.436946 \n",
       "L 356.853731 200.958649 \n",
       "L 356.920026 207.228264 \n",
       "L 356.986321 203.645627 \n",
       "L 357.052616 209.019583 \n",
       "L 357.185206 199.167331 \n",
       "L 357.317796 209.915242 \n",
       "L 357.384091 202.749968 \n",
       "L 357.450386 205.436946 \n",
       "L 357.51668 209.019583 \n",
       "L 357.582975 200.06299 \n",
       "L 357.64927 201.854308 \n",
       "L 357.78186 205.436946 \n",
       "L 357.848155 201.854308 \n",
       "L 357.91445 204.541286 \n",
       "L 357.980745 200.06299 \n",
       "L 358.04704 206.332605 \n",
       "L 358.113335 205.436946 \n",
       "L 358.17963 201.854308 \n",
       "L 358.245925 206.332605 \n",
       "L 358.44481 200.958649 \n",
       "L 358.511105 205.436946 \n",
       "L 358.5774 202.749968 \n",
       "L 358.643695 203.645627 \n",
       "L 358.776285 201.854308 \n",
       "L 358.908875 204.541286 \n",
       "L 358.97517 198.271671 \n",
       "L 359.041465 209.915242 \n",
       "L 359.10776 202.749968 \n",
       "L 359.24035 209.019583 \n",
       "L 359.439235 199.167331 \n",
       "L 359.50553 200.06299 \n",
       "L 359.63812 204.541286 \n",
       "L 359.704415 199.167331 \n",
       "L 359.77071 201.854308 \n",
       "L 359.837005 201.854308 \n",
       "L 359.9033 205.436946 \n",
       "L 360.03589 201.854308 \n",
       "L 360.102185 202.749968 \n",
       "L 360.16848 200.06299 \n",
       "L 360.30107 207.228264 \n",
       "L 360.56625 198.271671 \n",
       "L 360.632545 205.436946 \n",
       "L 360.69884 202.749968 \n",
       "L 360.897725 199.167331 \n",
       "L 361.09661 202.749968 \n",
       "L 361.162905 201.854308 \n",
       "L 361.2292 200.06299 \n",
       "L 361.36179 205.436946 \n",
       "L 361.428085 202.749968 \n",
       "L 361.49438 209.019583 \n",
       "L 361.560675 197.376012 \n",
       "L 361.62697 203.645627 \n",
       "L 361.75956 197.376012 \n",
       "L 361.825855 202.749968 \n",
       "L 361.89215 201.854308 \n",
       "L 361.958445 202.749968 \n",
       "L 362.091035 198.271671 \n",
       "L 362.223625 202.749968 \n",
       "L 362.28992 200.06299 \n",
       "L 362.356215 201.854308 \n",
       "L 362.42251 201.854308 \n",
       "L 362.5551 202.749968 \n",
       "L 362.621395 199.167331 \n",
       "L 362.68769 200.958649 \n",
       "L 362.753985 200.958649 \n",
       "L 362.82028 201.854308 \n",
       "L 363.019165 200.06299 \n",
       "L 363.151755 203.645627 \n",
       "L 363.284345 201.854308 \n",
       "L 363.35064 202.749968 \n",
       "L 363.416935 206.332605 \n",
       "L 363.48323 204.541286 \n",
       "L 363.549525 206.332605 \n",
       "L 363.682114 202.749968 \n",
       "L 363.748409 206.332605 \n",
       "L 363.814704 205.436946 \n",
       "L 363.880999 204.541286 \n",
       "L 364.013589 198.271671 \n",
       "L 364.146179 203.645627 \n",
       "L 364.278769 200.06299 \n",
       "L 364.411359 204.541286 \n",
       "L 364.477654 203.645627 \n",
       "L 364.543949 202.749968 \n",
       "L 364.610244 207.228264 \n",
       "L 364.676539 204.541286 \n",
       "L 364.742834 203.645627 \n",
       "L 364.809129 200.06299 \n",
       "L 364.941719 203.645627 \n",
       "L 365.008014 204.541286 \n",
       "L 365.074309 208.123924 \n",
       "L 365.140604 202.749968 \n",
       "L 365.206899 208.123924 \n",
       "L 365.273194 198.271671 \n",
       "L 365.339489 203.645627 \n",
       "L 365.472079 199.167331 \n",
       "L 365.604669 203.645627 \n",
       "L 365.670964 202.749968 \n",
       "L 365.737259 203.645627 \n",
       "L 365.803554 200.06299 \n",
       "L 365.869849 206.332605 \n",
       "L 366.002439 200.06299 \n",
       "L 366.068734 206.332605 \n",
       "L 366.201324 200.06299 \n",
       "L 366.267619 200.958649 \n",
       "L 366.333914 204.541286 \n",
       "L 366.400209 196.480353 \n",
       "L 366.466504 208.123924 \n",
       "L 366.532799 203.645627 \n",
       "L 366.599094 206.332605 \n",
       "L 366.665389 205.436946 \n",
       "L 366.731684 205.436946 \n",
       "L 366.864274 197.376012 \n",
       "L 366.930569 202.749968 \n",
       "L 366.996864 196.480353 \n",
       "L 367.063159 198.271671 \n",
       "L 367.195749 203.645627 \n",
       "L 367.328339 200.958649 \n",
       "L 367.394634 202.749968 \n",
       "L 367.460929 199.167331 \n",
       "L 367.527224 202.749968 \n",
       "L 367.593519 201.854308 \n",
       "L 367.659814 199.167331 \n",
       "L 367.792404 201.854308 \n",
       "L 367.858699 200.06299 \n",
       "L 368.057584 204.541286 \n",
       "L 368.256469 199.167331 \n",
       "L 368.389059 204.541286 \n",
       "L 368.521649 200.06299 \n",
       "L 368.587944 202.749968 \n",
       "L 368.720534 197.376012 \n",
       "L 368.853124 202.749968 \n",
       "L 368.919419 202.749968 \n",
       "L 368.985714 205.436946 \n",
       "L 369.052009 203.645627 \n",
       "L 369.250894 200.958649 \n",
       "L 369.317189 205.436946 \n",
       "L 369.383484 202.749968 \n",
       "L 369.449779 200.06299 \n",
       "L 369.516074 201.854308 \n",
       "L 369.582369 201.854308 \n",
       "L 369.648664 204.541286 \n",
       "L 369.781253 198.271671 \n",
       "L 369.847548 204.541286 \n",
       "L 369.913843 203.645627 \n",
       "L 369.980138 200.958649 \n",
       "L 370.112728 206.332605 \n",
       "L 370.311613 196.480353 \n",
       "L 370.510498 203.645627 \n",
       "L 370.576793 202.749968 \n",
       "L 370.643088 209.019583 \n",
       "L 370.709383 201.854308 \n",
       "L 370.775678 203.645627 \n",
       "L 370.974563 198.271671 \n",
       "L 371.040858 200.06299 \n",
       "L 371.107153 208.123924 \n",
       "L 371.173448 198.271671 \n",
       "L 371.239743 202.749968 \n",
       "L 371.372333 199.167331 \n",
       "L 371.504923 203.645627 \n",
       "L 371.571218 202.749968 \n",
       "L 371.637513 199.167331 \n",
       "L 371.703808 203.645627 \n",
       "L 371.770103 200.958649 \n",
       "L 371.968988 204.541286 \n",
       "L 372.167873 202.749968 \n",
       "L 372.234168 202.749968 \n",
       "L 372.300463 204.541286 \n",
       "L 372.499348 200.958649 \n",
       "L 372.565643 203.645627 \n",
       "L 372.631938 200.06299 \n",
       "L 372.698233 203.645627 \n",
       "L 372.764528 202.749968 \n",
       "L 372.830823 196.480353 \n",
       "L 372.897118 202.749968 \n",
       "L 372.963413 201.854308 \n",
       "L 373.029708 203.645627 \n",
       "L 373.096003 200.06299 \n",
       "L 373.294888 207.228264 \n",
       "L 373.361183 200.06299 \n",
       "L 373.427478 204.541286 \n",
       "L 373.493773 202.749968 \n",
       "L 373.560068 203.645627 \n",
       "L 373.626363 206.332605 \n",
       "L 373.692658 200.958649 \n",
       "L 373.825248 206.332605 \n",
       "L 374.024133 200.06299 \n",
       "L 374.090428 205.436946 \n",
       "L 374.156723 201.854308 \n",
       "L 374.223018 200.958649 \n",
       "L 374.289313 196.480353 \n",
       "L 374.355608 204.541286 \n",
       "L 374.421903 200.06299 \n",
       "L 374.554493 199.167331 \n",
       "L 374.687083 204.541286 \n",
       "L 374.753378 200.06299 \n",
       "L 374.819673 205.436946 \n",
       "L 374.885968 198.271671 \n",
       "L 374.952263 203.645627 \n",
       "L 375.018558 196.480353 \n",
       "L 375.084853 199.167331 \n",
       "L 375.217443 205.436946 \n",
       "L 375.416328 198.271671 \n",
       "L 375.482623 205.436946 \n",
       "L 375.548918 202.749968 \n",
       "L 375.615213 205.436946 \n",
       "L 375.681508 199.167331 \n",
       "L 375.747803 201.854308 \n",
       "L 375.814098 200.958649 \n",
       "L 375.880392 201.854308 \n",
       "L 375.946687 204.541286 \n",
       "L 376.079277 195.883242 \n",
       "L 376.145572 200.958649 \n",
       "L 376.211867 199.167331 \n",
       "L 376.278162 199.167331 \n",
       "L 376.344457 204.541286 \n",
       "L 376.477047 199.167331 \n",
       "L 376.675932 201.854308 \n",
       "L 376.742227 201.854308 \n",
       "L 376.874817 200.06299 \n",
       "L 377.007407 205.436946 \n",
       "L 377.073702 199.167331 \n",
       "L 377.139997 200.06299 \n",
       "L 377.206292 201.854308 \n",
       "L 377.338882 199.167331 \n",
       "L 377.537767 202.749968 \n",
       "L 377.604062 204.541286 \n",
       "L 377.670357 200.06299 \n",
       "L 377.736652 201.854308 \n",
       "L 377.935537 200.06299 \n",
       "L 378.001832 201.854308 \n",
       "L 378.068127 199.167331 \n",
       "L 378.200717 204.541286 \n",
       "L 378.267012 201.854308 \n",
       "L 378.333307 205.436946 \n",
       "L 378.465897 198.271671 \n",
       "L 378.598487 204.541286 \n",
       "L 378.664782 200.958649 \n",
       "L 378.731077 200.958649 \n",
       "L 378.797372 206.332605 \n",
       "L 378.929962 200.958649 \n",
       "L 378.996257 203.645627 \n",
       "L 379.062552 200.958649 \n",
       "L 379.128847 203.645627 \n",
       "L 379.195142 200.958649 \n",
       "L 379.327732 204.541286 \n",
       "L 379.460322 197.376012 \n",
       "L 379.526617 199.167331 \n",
       "L 379.592912 199.167331 \n",
       "L 379.725502 200.06299 \n",
       "L 379.858092 203.645627 \n",
       "L 379.924387 202.749968 \n",
       "L 379.990682 200.06299 \n",
       "L 380.056977 202.749968 \n",
       "L 380.123272 198.271671 \n",
       "L 380.255862 202.749968 \n",
       "L 380.388452 199.167331 \n",
       "L 380.521042 206.332605 \n",
       "L 380.719927 200.06299 \n",
       "L 380.786222 201.854308 \n",
       "L 380.852517 198.271671 \n",
       "L 380.918812 199.167331 \n",
       "L 380.985107 203.645627 \n",
       "L 381.051402 200.958649 \n",
       "L 381.250287 204.541286 \n",
       "L 381.316582 197.376012 \n",
       "L 381.382877 202.749968 \n",
       "L 381.449172 200.958649 \n",
       "L 381.581762 198.271671 \n",
       "L 381.846942 207.228264 \n",
       "L 382.045826 200.958649 \n",
       "L 382.112121 201.854308 \n",
       "L 382.311006 198.271671 \n",
       "L 382.443596 203.645627 \n",
       "L 382.509891 202.749968 \n",
       "L 382.576186 200.06299 \n",
       "L 382.642481 201.854308 \n",
       "L 382.708776 204.541286 \n",
       "L 382.907661 200.06299 \n",
       "L 383.040251 201.854308 \n",
       "L 383.106546 204.541286 \n",
       "L 383.172841 200.06299 \n",
       "L 383.239136 200.958649 \n",
       "L 383.305431 204.541286 \n",
       "L 383.371726 200.06299 \n",
       "L 383.438021 202.749968 \n",
       "L 383.570611 202.749968 \n",
       "L 383.636906 199.167331 \n",
       "L 383.703201 202.749968 \n",
       "L 383.769496 200.06299 \n",
       "L 383.835791 201.854308 \n",
       "L 383.902086 203.645627 \n",
       "L 383.968381 197.376012 \n",
       "L 384.167266 207.228264 \n",
       "L 384.233561 199.167331 \n",
       "L 384.299856 204.541286 \n",
       "L 384.432446 200.06299 \n",
       "L 384.498741 201.854308 \n",
       "L 384.631331 204.541286 \n",
       "L 384.697626 200.958649 \n",
       "L 384.830216 203.645627 \n",
       "L 384.962806 199.167331 \n",
       "L 385.029101 202.749968 \n",
       "L 385.095396 200.958649 \n",
       "L 385.227986 204.541286 \n",
       "L 385.360576 199.167331 \n",
       "L 385.426871 203.645627 \n",
       "L 385.493166 200.06299 \n",
       "L 385.559461 200.958649 \n",
       "L 385.625756 200.06299 \n",
       "L 385.758346 202.749968 \n",
       "L 385.824641 198.271671 \n",
       "L 385.957231 202.749968 \n",
       "L 386.023526 198.271671 \n",
       "L 386.089821 202.749968 \n",
       "L 386.156116 200.958649 \n",
       "L 386.222411 199.167331 \n",
       "L 386.355001 202.749968 \n",
       "L 386.421296 200.958649 \n",
       "L 386.487591 201.854308 \n",
       "L 386.553886 201.854308 \n",
       "L 386.686476 198.271671 \n",
       "L 386.752771 205.436946 \n",
       "L 386.819066 200.958649 \n",
       "L 387.017951 206.332605 \n",
       "L 387.150541 201.854308 \n",
       "L 387.216836 205.436946 \n",
       "L 387.283131 198.271671 \n",
       "L 387.349426 201.854308 \n",
       "L 387.415721 202.749968 \n",
       "L 387.548311 197.376012 \n",
       "L 387.614606 199.167331 \n",
       "L 387.680901 200.06299 \n",
       "L 387.747196 199.167331 \n",
       "L 387.813491 202.749968 \n",
       "L 387.879786 198.271671 \n",
       "L 387.946081 204.541286 \n",
       "L 388.07867 200.06299 \n",
       "L 388.144965 202.749968 \n",
       "L 388.21126 200.06299 \n",
       "L 388.277555 200.958649 \n",
       "L 388.34385 200.958649 \n",
       "L 388.410145 200.06299 \n",
       "L 388.60903 205.436946 \n",
       "L 388.675325 200.06299 \n",
       "L 388.74162 203.645627 \n",
       "L 388.940505 198.271671 \n",
       "L 389.073095 207.228264 \n",
       "L 389.205685 204.541286 \n",
       "L 389.27198 202.749968 \n",
       "L 389.338275 203.645627 \n",
       "L 389.40457 206.332605 \n",
       "L 389.603455 200.06299 \n",
       "L 389.66975 202.749968 \n",
       "L 389.736045 200.06299 \n",
       "L 389.80234 200.958649 \n",
       "L 389.868635 200.958649 \n",
       "L 390.001225 201.854308 \n",
       "L 390.06752 205.436946 \n",
       "L 390.266405 201.854308 \n",
       "L 390.3327 201.854308 \n",
       "L 390.398995 200.958649 \n",
       "L 390.46529 203.645627 \n",
       "L 390.531585 199.167331 \n",
       "L 390.59788 200.958649 \n",
       "L 390.664175 200.958649 \n",
       "L 390.73047 198.271671 \n",
       "L 390.796765 204.541286 \n",
       "L 390.929355 197.376012 \n",
       "L 390.99565 201.854308 \n",
       "L 391.061945 200.958649 \n",
       "L 391.12824 200.958649 \n",
       "L 391.194535 202.749968 \n",
       "L 391.327125 200.06299 \n",
       "L 391.39342 204.541286 \n",
       "L 391.52601 198.271671 \n",
       "L 391.592305 196.480353 \n",
       "L 391.6586 203.645627 \n",
       "L 391.724895 199.167331 \n",
       "L 391.857485 200.06299 \n",
       "L 391.92378 200.06299 \n",
       "L 392.05637 207.228264 \n",
       "L 392.255255 200.958649 \n",
       "L 392.32155 200.06299 \n",
       "L 392.45414 205.436946 \n",
       "L 392.520435 202.749968 \n",
       "L 392.58673 205.436946 \n",
       "L 392.653025 204.541286 \n",
       "L 392.785615 200.958649 \n",
       "L 392.85191 201.854308 \n",
       "L 392.9845 204.541286 \n",
       "L 393.11709 199.167331 \n",
       "L 393.183385 200.06299 \n",
       "L 393.315975 199.167331 \n",
       "L 393.448565 205.436946 \n",
       "L 393.51486 202.749968 \n",
       "L 393.581155 200.958649 \n",
       "L 393.64745 203.645627 \n",
       "L 393.713745 200.958649 \n",
       "L 393.846335 204.541286 \n",
       "L 393.91263 201.854308 \n",
       "L 393.978925 206.332605 \n",
       "L 394.04522 201.854308 \n",
       "L 394.111515 203.645627 \n",
       "L 394.177809 202.749968 \n",
       "L 394.310399 206.332605 \n",
       "L 394.509284 199.167331 \n",
       "L 394.641874 204.541286 \n",
       "L 394.708169 199.167331 \n",
       "L 394.774464 203.645627 \n",
       "L 394.840759 200.958649 \n",
       "L 394.907054 200.958649 \n",
       "L 394.973349 199.167331 \n",
       "L 395.105939 200.958649 \n",
       "L 395.172234 196.480353 \n",
       "L 395.304824 200.958649 \n",
       "L 395.371119 200.958649 \n",
       "L 395.503709 199.167331 \n",
       "L 395.570004 201.854308 \n",
       "L 395.702594 197.376012 \n",
       "L 395.768889 203.645627 \n",
       "L 395.835184 200.06299 \n",
       "L 395.967774 203.645627 \n",
       "L 396.100364 199.167331 \n",
       "L 396.166659 202.749968 \n",
       "L 396.232954 199.167331 \n",
       "L 396.299249 200.958649 \n",
       "L 396.498134 202.749968 \n",
       "L 396.564429 202.749968 \n",
       "L 396.697019 200.958649 \n",
       "L 396.829609 203.645627 \n",
       "L 396.895904 202.749968 \n",
       "L 397.028494 196.480353 \n",
       "L 397.094789 202.749968 \n",
       "L 397.161084 201.854308 \n",
       "L 397.227379 202.749968 \n",
       "L 397.359969 197.376012 \n",
       "L 397.426264 204.541286 \n",
       "L 397.492559 200.06299 \n",
       "L 397.558854 203.645627 \n",
       "L 397.625149 201.854308 \n",
       "L 397.824034 198.271671 \n",
       "L 397.890329 204.541286 \n",
       "L 397.956624 203.645627 \n",
       "L 398.022919 203.645627 \n",
       "L 398.089214 200.958649 \n",
       "L 398.221804 205.436946 \n",
       "L 398.354394 200.958649 \n",
       "L 398.486984 203.645627 \n",
       "L 398.553279 199.167331 \n",
       "L 398.619574 202.749968 \n",
       "L 398.685869 200.06299 \n",
       "L 398.752164 204.541286 \n",
       "L 398.818459 200.958649 \n",
       "L 398.884754 204.541286 \n",
       "L 398.951049 200.958649 \n",
       "L 399.017344 202.749968 \n",
       "L 399.083639 203.645627 \n",
       "L 399.282524 199.167331 \n",
       "L 399.348819 202.749968 \n",
       "L 399.415114 201.854308 \n",
       "L 399.547704 200.958649 \n",
       "L 399.680294 204.541286 \n",
       "L 399.746589 200.958649 \n",
       "L 399.812884 202.749968 \n",
       "L 399.879179 200.06299 \n",
       "L 399.945474 204.541286 \n",
       "L 400.011769 200.06299 \n",
       "L 400.078064 204.541286 \n",
       "L 400.210654 199.167331 \n",
       "L 400.276948 202.749968 \n",
       "L 400.343243 199.167331 \n",
       "L 400.409538 200.958649 \n",
       "L 400.542128 199.167331 \n",
       "L 400.674718 205.436946 \n",
       "L 400.741013 204.541286 \n",
       "L 400.807308 199.167331 \n",
       "L 400.873603 200.06299 \n",
       "L 400.939898 204.541286 \n",
       "L 401.006193 203.645627 \n",
       "L 401.072488 197.376012 \n",
       "L 401.138783 200.06299 \n",
       "L 401.205078 199.167331 \n",
       "L 401.271373 201.854308 \n",
       "L 401.337668 199.167331 \n",
       "L 401.403963 200.06299 \n",
       "L 401.470258 199.167331 \n",
       "L 401.536553 203.645627 \n",
       "L 401.602848 200.958649 \n",
       "L 401.801733 202.749968 \n",
       "L 401.868028 202.749968 \n",
       "L 402.066913 197.376012 \n",
       "L 402.199503 201.854308 \n",
       "L 402.265798 200.958649 \n",
       "L 402.332093 204.541286 \n",
       "L 402.398388 197.376012 \n",
       "L 402.464683 205.436946 \n",
       "L 402.530978 197.376012 \n",
       "L 402.597273 200.958649 \n",
       "L 402.663568 200.06299 \n",
       "L 402.729863 200.958649 \n",
       "L 402.862453 209.915242 \n",
       "L 402.995043 199.167331 \n",
       "L 403.127633 207.228264 \n",
       "L 403.193928 200.06299 \n",
       "L 403.260223 204.541286 \n",
       "L 403.392813 200.06299 \n",
       "L 403.525403 200.06299 \n",
       "L 403.591698 203.645627 \n",
       "L 403.657993 200.06299 \n",
       "L 403.724288 202.749968 \n",
       "L 403.856878 198.271671 \n",
       "L 403.923173 197.376012 \n",
       "L 404.122058 200.958649 \n",
       "L 404.188353 197.376012 \n",
       "L 404.387238 204.541286 \n",
       "L 404.586123 197.376012 \n",
       "L 404.718713 202.749968 \n",
       "L 404.917598 200.06299 \n",
       "L 405.050188 201.854308 \n",
       "L 405.116483 199.167331 \n",
       "L 405.182778 200.06299 \n",
       "L 405.249073 200.958649 \n",
       "L 405.315368 198.271671 \n",
       "L 405.381663 200.06299 \n",
       "L 405.447958 198.271671 \n",
       "L 405.646843 201.854308 \n",
       "L 405.779433 200.06299 \n",
       "L 405.845728 202.749968 \n",
       "L 405.912023 200.06299 \n",
       "L 405.978318 202.749968 \n",
       "L 406.044613 199.167331 \n",
       "L 406.110908 203.645627 \n",
       "L 406.309793 198.271671 \n",
       "L 406.376087 199.167331 \n",
       "L 406.508677 206.332605 \n",
       "L 406.574972 200.958649 \n",
       "L 406.641267 202.749968 \n",
       "L 406.707562 202.749968 \n",
       "L 406.773857 201.854308 \n",
       "L 406.840152 199.167331 \n",
       "L 406.906447 203.645627 \n",
       "L 407.039037 198.271671 \n",
       "L 407.105332 199.167331 \n",
       "L 407.171627 198.271671 \n",
       "L 407.370512 202.749968 \n",
       "L 407.436807 200.06299 \n",
       "L 407.503102 202.749968 \n",
       "L 407.569397 200.06299 \n",
       "L 407.635692 200.958649 \n",
       "L 407.701987 202.749968 \n",
       "L 407.834577 197.376012 \n",
       "L 408.099757 203.645627 \n",
       "L 408.232347 197.376012 \n",
       "L 408.298642 198.271671 \n",
       "L 408.364937 199.167331 \n",
       "L 408.431232 201.854308 \n",
       "L 408.497527 200.06299 \n",
       "L 408.696412 196.480353 \n",
       "L 408.895297 204.541286 \n",
       "L 409.027887 199.167331 \n",
       "L 409.094182 200.958649 \n",
       "L 409.160477 200.06299 \n",
       "L 409.226772 197.376012 \n",
       "L 409.293067 198.271671 \n",
       "L 409.359362 200.958649 \n",
       "L 409.425657 200.06299 \n",
       "L 409.624542 202.749968 \n",
       "L 409.690837 197.376012 \n",
       "L 409.757132 199.167331 \n",
       "L 409.823427 200.06299 \n",
       "L 409.889722 199.167331 \n",
       "L 409.956017 205.436946 \n",
       "L 410.154902 197.376012 \n",
       "L 410.287492 200.06299 \n",
       "L 410.353787 198.271671 \n",
       "L 410.486377 202.749968 \n",
       "L 410.552672 196.480353 \n",
       "L 410.751557 202.749968 \n",
       "L 410.884147 200.958649 \n",
       "L 410.950442 198.271671 \n",
       "L 411.083032 203.645627 \n",
       "L 411.149327 196.480353 \n",
       "L 411.215622 199.167331 \n",
       "L 411.281917 203.645627 \n",
       "L 411.348212 197.376012 \n",
       "L 411.414507 200.06299 \n",
       "L 411.480802 200.958649 \n",
       "L 411.613392 199.167331 \n",
       "L 411.679687 204.541286 \n",
       "L 411.745982 200.958649 \n",
       "L 411.812277 200.958649 \n",
       "L 411.944867 201.854308 \n",
       "L 412.011162 198.271671 \n",
       "L 412.077457 199.167331 \n",
       "L 412.210047 201.854308 \n",
       "L 412.342637 197.376012 \n",
       "L 412.408932 201.854308 \n",
       "L 412.475226 200.06299 \n",
       "L 412.607816 200.958649 \n",
       "L 412.674111 198.271671 \n",
       "L 412.872996 203.645627 \n",
       "L 413.005586 202.749968 \n",
       "L 413.071881 202.749968 \n",
       "L 413.204471 198.271671 \n",
       "L 413.337061 204.541286 \n",
       "L 413.469651 197.376012 \n",
       "L 413.535946 199.167331 \n",
       "L 413.602241 199.167331 \n",
       "L 413.668536 204.541286 \n",
       "L 413.734831 201.854308 \n",
       "L 413.801126 199.167331 \n",
       "L 413.867421 200.06299 \n",
       "L 413.933716 201.854308 \n",
       "L 414.000011 200.958649 \n",
       "L 414.066306 201.854308 \n",
       "L 414.132601 199.167331 \n",
       "L 414.198896 204.541286 \n",
       "L 414.265191 203.645627 \n",
       "L 414.331486 205.436946 \n",
       "L 414.397781 198.271671 \n",
       "L 414.464076 201.854308 \n",
       "L 414.530371 197.376012 \n",
       "L 414.596666 205.436946 \n",
       "L 414.662961 201.854308 \n",
       "L 414.729256 204.541286 \n",
       "L 414.795551 200.06299 \n",
       "L 414.861846 200.958649 \n",
       "L 414.928141 207.228264 \n",
       "L 414.994436 198.271671 \n",
       "L 415.060731 204.541286 \n",
       "L 415.193321 197.376012 \n",
       "L 415.259616 202.749968 \n",
       "L 415.325911 200.958649 \n",
       "L 415.458501 195.584693 \n",
       "L 415.524796 200.958649 \n",
       "L 415.591091 197.376012 \n",
       "L 415.657386 197.376012 \n",
       "L 415.789976 203.645627 \n",
       "L 415.856271 201.854308 \n",
       "L 415.922566 200.06299 \n",
       "L 415.988861 202.749968 \n",
       "L 416.055156 201.854308 \n",
       "L 416.121451 200.958649 \n",
       "L 416.187746 205.436946 \n",
       "L 416.254041 198.271671 \n",
       "L 416.320336 200.958649 \n",
       "L 416.452926 200.06299 \n",
       "L 416.519221 200.06299 \n",
       "L 416.651811 200.958649 \n",
       "L 416.718106 198.271671 \n",
       "L 416.784401 200.06299 \n",
       "L 416.850696 200.06299 \n",
       "L 416.916991 199.167331 \n",
       "L 417.049581 200.06299 \n",
       "L 417.182171 200.06299 \n",
       "L 417.314761 205.436946 \n",
       "L 417.646236 198.271671 \n",
       "L 417.712531 204.541286 \n",
       "L 417.778826 201.854308 \n",
       "L 417.911416 198.271671 \n",
       "L 418.110301 205.436946 \n",
       "L 418.176596 200.06299 \n",
       "L 418.242891 203.645627 \n",
       "L 418.441776 199.167331 \n",
       "L 418.508071 202.749968 \n",
       "L 418.574365 200.958649 \n",
       "L 418.706955 200.958649 \n",
       "L 418.77325 202.749968 \n",
       "L 418.972135 196.480353 \n",
       "L 419.03843 202.749968 \n",
       "L 419.104725 200.06299 \n",
       "L 419.17102 200.06299 \n",
       "L 419.237315 196.480353 \n",
       "L 419.30361 198.271671 \n",
       "L 419.4362 209.019583 \n",
       "L 419.635085 200.06299 \n",
       "L 419.70138 202.749968 \n",
       "L 419.767675 197.376012 \n",
       "L 419.83397 198.271671 \n",
       "L 420.032855 200.958649 \n",
       "L 420.09915 199.167331 \n",
       "L 420.165445 202.749968 \n",
       "L 420.23174 200.958649 \n",
       "L 420.430625 194.689034 \n",
       "L 420.49692 201.854308 \n",
       "L 420.563215 197.376012 \n",
       "L 420.7621 202.749968 \n",
       "L 420.828395 201.854308 \n",
       "L 420.89469 198.271671 \n",
       "L 420.960985 200.958649 \n",
       "L 421.02728 199.167331 \n",
       "L 421.093575 200.958649 \n",
       "L 421.15987 200.06299 \n",
       "L 421.226165 200.958649 \n",
       "L 421.29246 196.480353 \n",
       "L 421.42505 202.749968 \n",
       "L 421.55764 198.271671 \n",
       "L 421.69023 201.854308 \n",
       "L 421.756525 200.958649 \n",
       "L 421.82282 201.854308 \n",
       "L 421.95541 199.167331 \n",
       "L 422.021705 200.06299 \n",
       "L 422.088 204.541286 \n",
       "L 422.154295 198.271671 \n",
       "L 422.22059 201.854308 \n",
       "L 422.286885 199.167331 \n",
       "L 422.35318 200.06299 \n",
       "L 422.419475 204.541286 \n",
       "L 422.552065 197.376012 \n",
       "L 422.61836 199.167331 \n",
       "L 422.684655 206.332605 \n",
       "L 422.817245 199.167331 \n",
       "L 422.88354 199.167331 \n",
       "L 422.949835 200.06299 \n",
       "L 423.14872 205.436946 \n",
       "L 423.28131 197.376012 \n",
       "L 423.347605 204.541286 \n",
       "L 423.4139 197.376012 \n",
       "L 423.480195 200.06299 \n",
       "L 423.54649 200.958649 \n",
       "L 423.67908 196.480353 \n",
       "L 423.745375 201.854308 \n",
       "L 423.81167 198.271671 \n",
       "L 424.010555 205.436946 \n",
       "L 424.07685 198.271671 \n",
       "L 424.143145 205.436946 \n",
       "L 424.20944 201.854308 \n",
       "L 424.34203 200.06299 \n",
       "L 424.408325 204.541286 \n",
       "L 424.47462 197.376012 \n",
       "L 424.540915 203.645627 \n",
       "L 424.60721 198.271671 \n",
       "L 424.673504 200.06299 \n",
       "L 424.739799 199.167331 \n",
       "L 424.872389 200.06299 \n",
       "L 424.938684 199.167331 \n",
       "L 425.004979 200.958649 \n",
       "L 425.071274 198.271671 \n",
       "L 425.137569 203.645627 \n",
       "L 425.203864 198.271671 \n",
       "L 425.270159 201.854308 \n",
       "L 425.469044 196.480353 \n",
       "L 425.535339 201.854308 \n",
       "L 425.667929 197.376012 \n",
       "L 425.734224 196.480353 \n",
       "L 425.866814 199.167331 \n",
       "L 425.933109 196.480353 \n",
       "L 426.131994 202.749968 \n",
       "L 426.264584 199.167331 \n",
       "L 426.330879 199.167331 \n",
       "L 426.397174 202.749968 \n",
       "L 426.463469 197.376012 \n",
       "L 426.529764 202.749968 \n",
       "L 426.596059 200.958649 \n",
       "L 426.662354 197.376012 \n",
       "L 426.794944 205.436946 \n",
       "L 426.861239 198.271671 \n",
       "L 426.927534 200.06299 \n",
       "L 426.993829 200.958649 \n",
       "L 427.126419 199.167331 \n",
       "L 427.192714 199.167331 \n",
       "L 427.259009 202.749968 \n",
       "L 427.325304 198.271671 \n",
       "L 427.391599 207.228264 \n",
       "L 427.524189 200.06299 \n",
       "L 427.590484 202.749968 \n",
       "L 427.656779 197.376012 \n",
       "L 427.723074 198.271671 \n",
       "L 427.855664 199.167331 \n",
       "L 427.988254 199.167331 \n",
       "L 428.054549 202.749968 \n",
       "L 428.120844 200.958649 \n",
       "L 428.187139 199.167331 \n",
       "L 428.253434 204.541286 \n",
       "L 428.319729 200.06299 \n",
       "L 428.386024 201.854308 \n",
       "L 428.518614 200.06299 \n",
       "L 428.584909 202.749968 \n",
       "L 428.651204 201.854308 \n",
       "L 428.717499 200.06299 \n",
       "L 428.783794 200.958649 \n",
       "L 428.850089 205.436946 \n",
       "L 429.048974 197.376012 \n",
       "L 429.115269 203.645627 \n",
       "L 429.181564 200.06299 \n",
       "L 429.247859 200.06299 \n",
       "L 429.314154 203.645627 \n",
       "L 429.446744 198.271671 \n",
       "L 429.579334 202.749968 \n",
       "L 429.645629 199.167331 \n",
       "L 429.711924 200.06299 \n",
       "L 429.910809 205.436946 \n",
       "L 429.977104 199.167331 \n",
       "L 430.043399 200.958649 \n",
       "L 430.109694 201.854308 \n",
       "L 430.175989 204.541286 \n",
       "L 430.242284 200.06299 \n",
       "L 430.308579 207.228264 \n",
       "L 430.441169 199.167331 \n",
       "L 430.640054 201.854308 \n",
       "L 430.706349 198.271671 \n",
       "L 430.838938 208.123924 \n",
       "L 430.905233 198.271671 \n",
       "L 430.971528 200.06299 \n",
       "L 431.104118 201.854308 \n",
       "L 431.303003 199.167331 \n",
       "L 431.369298 200.958649 \n",
       "L 431.435593 198.271671 \n",
       "L 431.501888 202.749968 \n",
       "L 431.568183 201.854308 \n",
       "L 431.634478 196.480353 \n",
       "L 431.700773 198.271671 \n",
       "L 431.833363 201.854308 \n",
       "L 431.899658 195.584693 \n",
       "L 431.965953 201.854308 \n",
       "L 432.032248 197.376012 \n",
       "L 432.231133 203.645627 \n",
       "L 432.297428 199.167331 \n",
       "L 432.363723 202.749968 \n",
       "L 432.430018 201.854308 \n",
       "L 432.496313 201.854308 \n",
       "L 432.562608 199.167331 \n",
       "L 432.761493 202.749968 \n",
       "L 432.827788 200.06299 \n",
       "L 432.894083 200.958649 \n",
       "L 432.960378 201.854308 \n",
       "L 433.026673 204.541286 \n",
       "L 433.092968 202.749968 \n",
       "L 433.159263 200.06299 \n",
       "L 433.225558 204.541286 \n",
       "L 433.291853 196.480353 \n",
       "L 433.358148 203.645627 \n",
       "L 433.424443 199.167331 \n",
       "L 433.557033 203.645627 \n",
       "L 433.689623 196.480353 \n",
       "L 433.954803 202.749968 \n",
       "L 434.021098 200.958649 \n",
       "L 434.087393 205.436946 \n",
       "L 434.153688 197.376012 \n",
       "L 434.219983 199.167331 \n",
       "L 434.286278 202.749968 \n",
       "L 434.418868 200.06299 \n",
       "L 434.485163 204.541286 \n",
       "L 434.684048 198.271671 \n",
       "L 434.750343 203.645627 \n",
       "L 434.816638 201.854308 \n",
       "L 435.015523 197.376012 \n",
       "L 435.081818 203.645627 \n",
       "L 435.148113 200.06299 \n",
       "L 435.214408 199.167331 \n",
       "L 435.280703 200.06299 \n",
       "L 435.413293 199.167331 \n",
       "L 435.479588 199.167331 \n",
       "L 435.678473 203.645627 \n",
       "L 435.744768 200.958649 \n",
       "L 435.811063 203.645627 \n",
       "L 435.877358 201.854308 \n",
       "L 436.009948 199.167331 \n",
       "L 436.076243 202.749968 \n",
       "L 436.142538 201.854308 \n",
       "L 436.208833 201.854308 \n",
       "L 436.407718 200.06299 \n",
       "L 436.474013 200.06299 \n",
       "L 436.540308 203.645627 \n",
       "L 436.606603 201.854308 \n",
       "L 436.672898 197.376012 \n",
       "L 436.739193 202.749968 \n",
       "L 436.805488 201.854308 \n",
       "L 436.871783 200.958649 \n",
       "L 436.938077 202.749968 \n",
       "L 437.004372 196.480353 \n",
       "L 437.070667 197.376012 \n",
       "L 437.269552 202.749968 \n",
       "L 437.335847 198.271671 \n",
       "L 437.402142 200.06299 \n",
       "L 437.468437 198.271671 \n",
       "L 437.601027 202.749968 \n",
       "L 437.733617 200.06299 \n",
       "L 437.799912 203.645627 \n",
       "L 437.866207 197.376012 \n",
       "L 437.932502 200.06299 \n",
       "L 437.998797 199.167331 \n",
       "L 438.065092 201.854308 \n",
       "L 438.131387 199.167331 \n",
       "L 438.197682 203.645627 \n",
       "L 438.263977 195.883242 \n",
       "L 438.263977 195.883242 \n",
       "\" clip-path=\"url(#p184d359044)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 275.09875 299.078125 \n",
       "L 275.09875 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 446.03375 299.078125 \n",
       "L 446.03375 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 275.09875 299.078125 \n",
       "L 446.03375 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 275.09875 189.718125 \n",
       "L 446.03375 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_51\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(331.573125 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_4\">\n",
       "    <g id=\"patch_25\">\n",
       "     <path d=\"M 361.63375 294.078125 \n",
       "L 439.03375 294.078125 \n",
       "Q 441.03375 294.078125 441.03375 292.078125 \n",
       "L 441.03375 263.165625 \n",
       "Q 441.03375 261.165625 439.03375 261.165625 \n",
       "L 361.63375 261.165625 \n",
       "Q 359.63375 261.165625 359.63375 263.165625 \n",
       "L 359.63375 292.078125 \n",
       "Q 359.63375 294.078125 361.63375 294.078125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_48\">\n",
       "     <path d=\"M 363.63375 269.264062 \n",
       "L 373.63375 269.264062 \n",
       "L 383.63375 269.264062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_52\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(391.63375 272.764062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_49\">\n",
       "     <path d=\"M 363.63375 284.220312 \n",
       "L 373.63375 284.220312 \n",
       "L 383.63375 284.220312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_53\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(391.63375 287.720312) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pb435515c99\">\n",
       "   <rect x=\"50.14375\" y=\"22.318125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p9e8601982c\">\n",
       "   <rect x=\"275.09875\" y=\"22.318125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pec08b2ec4f\">\n",
       "   <rect x=\"50.14375\" y=\"189.718125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p184d359044\">\n",
       "   <rect x=\"275.09875\" y=\"189.718125\" width=\"170.935\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_acc一直在92%左右，如何才能提高？\n",
    "# 使用CNN会好一点吗？\n",
    "# 我们来试一试：\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "\n",
    "net = Net1()  \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params= net.parameters(), lr= 0.5)   \n",
    "   \n",
    "trainer= Trainer(\n",
    "    device= 'auto', \n",
    "    train_dataloader= data.DataLoader(train_dataset, batch_size= 128, shuffle= True),\n",
    "    val_dataloader= data.DataLoader(test_dataset, batch_size= 128), \n",
    "    model= net, \n",
    "    loss_fn= loss_fn, \n",
    "    optimizer= opt, \n",
    "    is_tqdm= False\n",
    ")\n",
    "\n",
    "trainer.train(epochs= 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"453.29625pt\" height=\"336.634375pt\" viewBox=\"0 0 453.29625 336.634375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-28T11:05:21.400494</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 336.634375 \n",
       "L 453.29625 336.634375 \n",
       "L 453.29625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 56.50625 131.678125 \n",
       "L 224.24625 131.678125 \n",
       "L 224.24625 22.318125 \n",
       "L 56.50625 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m336d9ce2f4\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"64.130795\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(60.949545 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"102.253523\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(99.072273 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"140.37625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(137.195 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"178.498977\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(175.317727 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"216.621705\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(213.440455 146.276562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(125.148125 159.954687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m851cf65a3e\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"129.060325\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1.525 -->\n",
       "      <g transform=\"translate(20.878125 132.859544) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"106.733758\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1.550 -->\n",
       "      <g transform=\"translate(20.878125 110.532977) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"84.407191\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1.575 -->\n",
       "      <g transform=\"translate(20.878125 88.20641) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"62.080624\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1.600 -->\n",
       "      <g transform=\"translate(20.878125 65.879842) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"39.754057\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1.625 -->\n",
       "      <g transform=\"translate(20.878125 43.553275) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 86.655937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_11\">\n",
       "    <path d=\"M 64.130795 101.969955 \n",
       "L 102.253523 116.688569 \n",
       "L 140.37625 120.255608 \n",
       "L 178.498977 123.192355 \n",
       "L 216.621705 126.707216 \n",
       "\" clip-path=\"url(#pae7921fb36)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 64.130795 27.289034 \n",
       "L 102.253523 97.586038 \n",
       "L 140.37625 108.879269 \n",
       "L 178.498977 115.481965 \n",
       "L 216.621705 119.008292 \n",
       "\" clip-path=\"url(#pae7921fb36)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 56.50625 131.678125 \n",
       "L 56.50625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 224.24625 131.678125 \n",
       "L 224.24625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 56.50625 131.678125 \n",
       "L 224.24625 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 56.50625 22.318125 \n",
       "L 224.24625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_13\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(110.06875 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 137.655625 60.230625 \n",
       "L 217.24625 60.230625 \n",
       "Q 219.24625 60.230625 219.24625 58.230625 \n",
       "L 219.24625 29.318125 \n",
       "Q 219.24625 27.318125 217.24625 27.318125 \n",
       "L 137.655625 27.318125 \n",
       "Q 135.655625 27.318125 135.655625 29.318125 \n",
       "L 135.655625 58.230625 \n",
       "Q 135.655625 60.230625 137.655625 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_13\">\n",
       "     <path d=\"M 139.655625 35.416562 \n",
       "L 149.655625 35.416562 \n",
       "L 159.655625 35.416562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(167.655625 38.916562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_14\">\n",
       "     <path d=\"M 139.655625 50.372812 \n",
       "L 149.655625 50.372812 \n",
       "L 159.655625 50.372812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(167.655625 53.872812) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 278.35625 131.678125 \n",
       "L 446.09625 131.678125 \n",
       "L 446.09625 22.318125 \n",
       "L 278.35625 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"285.980795\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(282.799545 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"324.103523\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(320.922273 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"362.22625\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(359.045 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"400.348977\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(397.167727 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"438.471705\" y=\"131.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(435.290455 146.276562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_21\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(346.998125 159.954687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"119.877402\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.850 -->\n",
       "      <g transform=\"translate(242.728125 123.676621) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"93.151735\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.875 -->\n",
       "      <g transform=\"translate(242.728125 96.950954) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"66.426068\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.900 -->\n",
       "      <g transform=\"translate(242.728125 70.225286) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"39.7004\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 0.925 -->\n",
       "      <g transform=\"translate(242.728125 43.499619) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_26\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(236.648437 85.560625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_24\">\n",
       "    <path d=\"M 285.980795 52.450066 \n",
       "L 324.103523 36.909429 \n",
       "L 362.22625 34.160744 \n",
       "L 400.348977 30.989186 \n",
       "L 438.471705 27.289034 \n",
       "\" clip-path=\"url(#p336acdb2ba)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_25\">\n",
       "    <path d=\"M 285.980795 126.707216 \n",
       "L 324.103523 57.910466 \n",
       "L 362.22625 45.302676 \n",
       "L 400.348977 38.322091 \n",
       "L 438.471705 35.087041 \n",
       "\" clip-path=\"url(#p336acdb2ba)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 278.35625 131.678125 \n",
       "L 278.35625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 446.09625 131.678125 \n",
       "L 446.09625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 278.35625 131.678125 \n",
       "L 446.09625 131.678125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 278.35625 22.318125 \n",
       "L 446.09625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_27\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(333.233125 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_2\">\n",
       "    <g id=\"patch_13\">\n",
       "     <path d=\"M 361.69625 126.678125 \n",
       "L 439.09625 126.678125 \n",
       "Q 441.09625 126.678125 441.09625 124.678125 \n",
       "L 441.09625 95.765625 \n",
       "Q 441.09625 93.765625 439.09625 93.765625 \n",
       "L 361.69625 93.765625 \n",
       "Q 359.69625 93.765625 359.69625 95.765625 \n",
       "L 359.69625 124.678125 \n",
       "Q 359.69625 126.678125 361.69625 126.678125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_26\">\n",
       "     <path d=\"M 363.69625 101.864062 \n",
       "L 373.69625 101.864062 \n",
       "L 383.69625 101.864062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_28\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(391.69625 105.364062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_27\">\n",
       "     <path d=\"M 363.69625 116.820312 \n",
       "L 373.69625 116.820312 \n",
       "L 383.69625 116.820312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_29\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(391.69625 120.320312) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 56.50625 299.078125 \n",
       "L 224.24625 299.078125 \n",
       "L 224.24625 189.718125 \n",
       "L 56.50625 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_5\">\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"64.130795\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(60.949545 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"129.186644\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(116.461644 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"194.242493\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_32\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(181.517493 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_33\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(129.560625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"277.420382\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_34\">\n",
       "      <!-- 1.6 -->\n",
       "      <g transform=\"translate(33.603125 281.219601) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"253.347877\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_35\">\n",
       "      <!-- 1.8 -->\n",
       "      <g transform=\"translate(33.603125 257.147095) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"229.275371\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_36\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(33.603125 233.07459) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_34\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"56.50625\" y=\"205.202866\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_37\">\n",
       "      <!-- 2.2 -->\n",
       "      <g transform=\"translate(33.603125 209.002084) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_38\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(27.523438 254.055937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_35\">\n",
       "    <path d=\"M 64.130795 285.494544 \n",
       "L 64.325963 277.004923 \n",
       "L 64.391019 277.548122 \n",
       "L 64.521131 287.380087 \n",
       "L 64.716298 273.253495 \n",
       "L 64.911466 281.897604 \n",
       "L 65.106633 274.67771 \n",
       "L 65.236745 281.075904 \n",
       "L 65.301801 277.127688 \n",
       "L 65.366857 282.593527 \n",
       "L 65.431912 279.663167 \n",
       "L 65.62708 284.419897 \n",
       "L 65.692136 279.154576 \n",
       "L 65.757192 282.804863 \n",
       "L 65.887303 280.21717 \n",
       "L 65.952359 284.154984 \n",
       "L 66.017415 269.767625 \n",
       "L 66.082471 277.713903 \n",
       "L 66.147527 280.702546 \n",
       "L 66.212583 279.162008 \n",
       "L 66.277638 275.595343 \n",
       "L 66.472806 282.253744 \n",
       "L 66.602918 277.271616 \n",
       "L 66.798085 290.113975 \n",
       "L 66.863141 291.647424 \n",
       "L 66.993253 285.213862 \n",
       "L 67.058309 285.767464 \n",
       "L 67.18842 279.035786 \n",
       "L 67.318532 289.784293 \n",
       "L 67.383588 287.148663 \n",
       "L 67.448644 277.72752 \n",
       "L 67.5137 283.213102 \n",
       "L 67.643811 289.990292 \n",
       "L 67.773923 283.662291 \n",
       "L 67.838979 290.747267 \n",
       "L 67.904035 280.463158 \n",
       "L 67.969091 290.802767 \n",
       "L 68.034146 290.538958 \n",
       "L 68.099202 273.400565 \n",
       "L 68.164258 288.886719 \n",
       "L 68.229314 286.398245 \n",
       "L 68.29437 282.502329 \n",
       "L 68.554593 293.976087 \n",
       "L 68.619649 293.74327 \n",
       "L 68.684705 285.032083 \n",
       "L 68.749761 288.21549 \n",
       "L 68.814817 290.643715 \n",
       "L 69.009984 280.850821 \n",
       "L 69.07504 273.880144 \n",
       "L 69.140096 275.195211 \n",
       "L 69.270207 286.994346 \n",
       "L 69.335263 285.650697 \n",
       "L 69.530431 279.690744 \n",
       "L 69.660543 287.684702 \n",
       "L 69.85571 275.829809 \n",
       "L 70.050878 283.253822 \n",
       "L 70.115934 278.014831 \n",
       "L 70.180989 281.998028 \n",
       "L 70.246045 277.849609 \n",
       "L 70.376157 283.862723 \n",
       "L 70.441213 280.183667 \n",
       "L 70.506269 284.629784 \n",
       "L 70.571324 278.979081 \n",
       "L 70.766492 287.074611 \n",
       "L 70.831548 280.643115 \n",
       "L 70.896604 287.454339 \n",
       "L 70.96166 284.296372 \n",
       "L 71.026715 283.21534 \n",
       "L 71.091771 287.421812 \n",
       "L 71.156827 271.687216 \n",
       "L 71.221883 279.286523 \n",
       "L 71.351995 281.336311 \n",
       "L 71.41705 279.391739 \n",
       "L 71.612218 286.111996 \n",
       "L 71.74233 281.669968 \n",
       "L 71.937497 292.467431 \n",
       "L 72.002553 291.902193 \n",
       "L 72.327832 280.455711 \n",
       "L 72.457944 289.873985 \n",
       "L 72.588056 278.638352 \n",
       "L 72.783223 291.596373 \n",
       "L 72.848279 291.216314 \n",
       "L 72.913335 289.162551 \n",
       "L 72.978391 292.689228 \n",
       "L 73.043447 283.697961 \n",
       "L 73.108503 291.230992 \n",
       "L 73.173558 290.608275 \n",
       "L 73.238614 273.190075 \n",
       "L 73.30367 289.825373 \n",
       "L 73.368726 287.466665 \n",
       "L 73.433782 285.884417 \n",
       "L 73.694005 294.037641 \n",
       "L 73.759061 293.115144 \n",
       "L 73.824117 283.557835 \n",
       "L 73.889173 289.381335 \n",
       "L 73.954229 292.481636 \n",
       "L 74.149396 283.471888 \n",
       "L 74.214452 276.634937 \n",
       "L 74.279508 277.439879 \n",
       "L 74.40962 288.049264 \n",
       "L 74.669843 280.430487 \n",
       "L 74.799955 289.212225 \n",
       "L 74.995122 276.144239 \n",
       "L 75.060178 284.579006 \n",
       "L 75.125234 281.860542 \n",
       "L 75.19029 283.804454 \n",
       "L 75.255346 279.218799 \n",
       "L 75.320401 282.469299 \n",
       "L 75.385457 279.630754 \n",
       "L 75.450513 283.765972 \n",
       "L 75.515569 283.711505 \n",
       "L 75.580625 282.036826 \n",
       "L 75.645681 286.983901 \n",
       "L 75.710737 281.5531 \n",
       "L 75.905904 288.307391 \n",
       "L 75.97096 282.674308 \n",
       "L 76.036016 286.12907 \n",
       "L 76.101072 284.030225 \n",
       "L 76.166127 284.406065 \n",
       "L 76.231183 287.349812 \n",
       "L 76.296239 272.465527 \n",
       "L 76.361295 278.724714 \n",
       "L 76.491407 281.66737 \n",
       "L 76.556463 278.435409 \n",
       "L 76.75163 285.425988 \n",
       "L 76.816686 284.603699 \n",
       "L 76.881742 280.808365 \n",
       "L 77.076909 290.683546 \n",
       "L 77.141965 292.092251 \n",
       "L 77.467244 281.708995 \n",
       "L 77.597356 291.811712 \n",
       "L 77.727468 280.074663 \n",
       "L 77.922635 291.926126 \n",
       "L 78.052747 289.147816 \n",
       "L 78.117803 292.637387 \n",
       "L 78.182859 284.177568 \n",
       "L 78.31297 291.268097 \n",
       "L 78.378026 278.55097 \n",
       "L 78.443082 290.152084 \n",
       "L 78.508138 287.556342 \n",
       "L 78.573194 284.861108 \n",
       "L 78.768361 293.098026 \n",
       "L 78.833417 294.093958 \n",
       "L 78.898473 293.460049 \n",
       "L 78.963529 284.293546 \n",
       "L 79.028585 289.549842 \n",
       "L 79.093641 291.066417 \n",
       "L 79.158696 290.554856 \n",
       "L 79.223752 289.582584 \n",
       "L 79.353864 276.929666 \n",
       "L 79.41892 277.425889 \n",
       "L 79.549032 289.031982 \n",
       "L 79.614087 286.22275 \n",
       "L 79.809255 281.345337 \n",
       "L 79.939367 289.314285 \n",
       "L 80.134534 277.7362 \n",
       "L 80.19959 284.299687 \n",
       "L 80.264646 281.234983 \n",
       "L 80.329702 284.537324 \n",
       "L 80.394758 280.604188 \n",
       "L 80.459813 281.407895 \n",
       "L 80.524869 279.623996 \n",
       "L 80.654981 283.846581 \n",
       "L 80.720037 281.123798 \n",
       "L 80.785093 286.480016 \n",
       "L 80.850149 281.71383 \n",
       "L 81.045316 287.987323 \n",
       "L 81.110372 280.899735 \n",
       "L 81.175428 287.140584 \n",
       "L 81.240484 284.690463 \n",
       "L 81.305539 283.956216 \n",
       "L 81.370595 288.587901 \n",
       "L 81.435651 271.842594 \n",
       "L 81.500707 279.798759 \n",
       "L 81.565763 282.130147 \n",
       "L 81.630819 280.923639 \n",
       "L 81.695875 278.839027 \n",
       "L 81.891042 285.469291 \n",
       "L 81.956098 284.660705 \n",
       "L 82.021154 281.300168 \n",
       "L 82.216321 292.459568 \n",
       "L 82.281377 292.439667 \n",
       "L 82.411489 288.705428 \n",
       "L 82.476545 290.190552 \n",
       "L 82.606656 280.890494 \n",
       "L 82.736768 292.123459 \n",
       "L 82.86688 281.344117 \n",
       "L 83.062047 292.328625 \n",
       "L 83.127103 292.233266 \n",
       "L 83.192159 289.431984 \n",
       "L 83.257215 292.670116 \n",
       "L 83.322271 284.396681 \n",
       "L 83.387327 291.915767 \n",
       "L 83.452382 291.705219 \n",
       "L 83.517438 276.989542 \n",
       "L 83.582494 290.23981 \n",
       "L 83.64755 288.282539 \n",
       "L 83.712606 283.829951 \n",
       "L 83.907773 293.314499 \n",
       "L 83.972829 294.107216 \n",
       "L 84.037885 292.901899 \n",
       "L 84.102941 285.151274 \n",
       "L 84.167997 288.926234 \n",
       "L 84.233053 292.888813 \n",
       "L 84.298109 291.409744 \n",
       "L 84.363164 290.412463 \n",
       "L 84.493276 277.189701 \n",
       "L 84.558332 277.771095 \n",
       "L 84.688444 289.183026 \n",
       "L 84.948667 281.893299 \n",
       "L 85.078779 288.487104 \n",
       "L 85.273946 276.951002 \n",
       "L 85.469114 285.194205 \n",
       "L 85.53417 279.850197 \n",
       "L 85.599225 282.713837 \n",
       "L 85.664281 281.888263 \n",
       "L 85.729337 283.559026 \n",
       "L 85.794393 283.534906 \n",
       "L 85.859449 281.09018 \n",
       "L 85.924505 287.275043 \n",
       "L 85.989561 282.554097 \n",
       "L 86.184728 287.952585 \n",
       "L 86.249784 283.168622 \n",
       "L 86.31484 286.155586 \n",
       "L 86.379896 287.007575 \n",
       "L 86.444952 285.009972 \n",
       "L 86.510007 287.979603 \n",
       "L 86.575063 273.18887 \n",
       "L 86.640119 278.896866 \n",
       "L 86.770231 283.235098 \n",
       "L 86.835287 282.037974 \n",
       "L 86.900342 282.649959 \n",
       "L 87.030454 286.626111 \n",
       "L 87.160566 281.269893 \n",
       "L 87.420789 292.646412 \n",
       "L 87.746068 283.22745 \n",
       "L 87.87618 292.021327 \n",
       "L 88.006292 280.991291 \n",
       "L 88.201459 292.355213 \n",
       "L 88.266515 291.962886 \n",
       "L 88.331571 289.751837 \n",
       "L 88.396627 293.058611 \n",
       "L 88.461683 284.350508 \n",
       "L 88.526739 292.711683 \n",
       "L 88.591795 291.588165 \n",
       "L 88.65685 280.965206 \n",
       "L 88.721906 290.680117 \n",
       "L 88.786962 288.16296 \n",
       "L 88.852018 287.495835 \n",
       "L 89.112241 293.978483 \n",
       "L 89.177297 293.545091 \n",
       "L 89.242353 284.972379 \n",
       "L 89.307409 289.417148 \n",
       "L 89.372465 292.294261 \n",
       "L 89.437521 290.699903 \n",
       "L 89.502576 290.293487 \n",
       "L 89.632688 277.253953 \n",
       "L 89.697744 278.900983 \n",
       "L 89.7628 293.102417 \n",
       "L 89.7628 293.102417 \n",
       "\" clip-path=\"url(#pb8b62f9e74)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_36\">\n",
       "    <path d=\"M 64.130795 194.689034 \n",
       "L 64.195851 196.149264 \n",
       "L 64.651242 222.419566 \n",
       "L 64.716298 221.394321 \n",
       "L 65.041577 233.93393 \n",
       "L 65.106633 230.860948 \n",
       "L 65.366857 239.931489 \n",
       "L 65.431912 238.969993 \n",
       "L 65.496968 247.103417 \n",
       "L 65.562024 238.23325 \n",
       "L 65.62708 243.061076 \n",
       "L 65.692136 247.16543 \n",
       "L 65.757192 245.728832 \n",
       "L 65.887303 234.328825 \n",
       "L 66.017415 248.665505 \n",
       "L 66.147527 248.589215 \n",
       "L 66.342694 253.390899 \n",
       "L 66.40775 249.362562 \n",
       "L 66.472806 251.998809 \n",
       "L 66.537862 253.328613 \n",
       "L 66.602918 257.930668 \n",
       "L 66.667974 254.834714 \n",
       "L 66.733029 248.339066 \n",
       "L 66.798085 258.327241 \n",
       "L 66.863141 251.450473 \n",
       "L 66.928197 253.904167 \n",
       "L 66.993253 261.266152 \n",
       "L 67.058309 255.486128 \n",
       "L 67.123364 257.838164 \n",
       "L 67.18842 258.915451 \n",
       "L 67.253476 258.833622 \n",
       "L 67.383588 263.94325 \n",
       "L 67.448644 257.38711 \n",
       "L 67.5137 260.224277 \n",
       "L 67.578755 261.557137 \n",
       "L 67.708867 254.357933 \n",
       "L 67.773923 264.006167 \n",
       "L 67.838979 255.81294 \n",
       "L 67.904035 258.065786 \n",
       "L 68.099202 265.250397 \n",
       "L 68.164258 261.079136 \n",
       "L 68.229314 266.207216 \n",
       "L 68.29437 265.223838 \n",
       "L 68.424481 260.904288 \n",
       "L 68.489537 264.139234 \n",
       "L 68.554593 257.8989 \n",
       "L 68.619649 269.155525 \n",
       "L 68.684705 264.006885 \n",
       "L 68.749761 268.079687 \n",
       "L 68.814817 266.600288 \n",
       "L 68.879872 264.105314 \n",
       "L 68.944928 272.168144 \n",
       "L 69.009984 266.285959 \n",
       "L 69.07504 267.075691 \n",
       "L 69.140096 268.1088 \n",
       "L 69.205152 272.342462 \n",
       "L 69.270207 264.484297 \n",
       "L 69.335263 267.020895 \n",
       "L 69.400319 273.498736 \n",
       "L 69.465375 264.056889 \n",
       "L 69.530431 266.980117 \n",
       "L 69.595487 270.235065 \n",
       "L 69.660543 260.43613 \n",
       "L 69.725598 270.860638 \n",
       "L 69.790654 266.036485 \n",
       "L 69.920766 272.69434 \n",
       "L 69.985822 269.36834 \n",
       "L 70.050878 270.856893 \n",
       "L 70.115934 272.121899 \n",
       "L 70.311101 269.803768 \n",
       "L 70.376157 270.638095 \n",
       "L 70.441213 266.688688 \n",
       "L 70.506269 276.963299 \n",
       "L 70.571324 269.879901 \n",
       "L 70.831548 275.877661 \n",
       "L 70.96166 265.024224 \n",
       "L 71.091771 274.3742 \n",
       "L 71.156827 266.1054 \n",
       "L 71.221883 270.997133 \n",
       "L 71.286939 271.098734 \n",
       "L 71.351995 273.557077 \n",
       "L 71.41705 270.921919 \n",
       "L 71.482106 279.604683 \n",
       "L 71.612218 270.099387 \n",
       "L 71.677274 276.625109 \n",
       "L 71.74233 272.832687 \n",
       "L 71.807386 271.779318 \n",
       "L 71.937497 276.611234 \n",
       "L 72.132665 270.313751 \n",
       "L 72.197721 278.426369 \n",
       "L 72.262777 271.992649 \n",
       "L 72.327832 274.497064 \n",
       "L 72.392888 275.567694 \n",
       "L 72.523 273.376345 \n",
       "L 72.653112 275.735842 \n",
       "L 72.718167 272.552234 \n",
       "L 72.783223 276.725131 \n",
       "L 72.848279 271.794528 \n",
       "L 72.913335 277.935886 \n",
       "L 72.978391 262.638556 \n",
       "L 73.108503 281.162582 \n",
       "L 73.173558 279.105619 \n",
       "L 73.238614 270.997463 \n",
       "L 73.30367 275.114617 \n",
       "L 73.368726 277.879454 \n",
       "L 73.433782 274.393843 \n",
       "L 73.498838 279.807238 \n",
       "L 73.563893 270.532032 \n",
       "L 73.628949 275.993179 \n",
       "L 73.694005 278.68032 \n",
       "L 73.759061 274.580844 \n",
       "L 73.824117 274.676749 \n",
       "L 73.889173 281.064081 \n",
       "L 73.954229 279.967423 \n",
       "L 74.019284 275.581296 \n",
       "L 74.08434 281.617854 \n",
       "L 74.279508 272.266286 \n",
       "L 74.344564 269.844116 \n",
       "L 74.40962 272.136377 \n",
       "L 74.474675 269.499125 \n",
       "L 74.604787 281.4777 \n",
       "L 74.669843 278.877782 \n",
       "L 74.799955 275.890574 \n",
       "L 74.86501 276.67187 \n",
       "L 74.930066 282.359218 \n",
       "L 74.995122 275.483771 \n",
       "L 75.060178 276.487809 \n",
       "L 75.125234 278.154038 \n",
       "L 75.255346 274.367155 \n",
       "L 75.320401 276.211073 \n",
       "L 75.385457 277.215614 \n",
       "L 75.450513 276.980187 \n",
       "L 75.515569 274.798595 \n",
       "L 75.580625 277.919055 \n",
       "L 75.645681 276.977604 \n",
       "L 75.710737 274.518659 \n",
       "L 75.775792 279.132824 \n",
       "L 75.840848 277.825002 \n",
       "L 75.905904 280.273186 \n",
       "L 75.97096 271.11578 \n",
       "L 76.036016 273.160847 \n",
       "L 76.166127 278.729808 \n",
       "L 76.231183 275.30093 \n",
       "L 76.296239 277.353875 \n",
       "L 76.426351 283.411984 \n",
       "L 76.621518 274.088941 \n",
       "L 76.686574 274.740613 \n",
       "L 76.75163 276.988968 \n",
       "L 76.816686 283.248585 \n",
       "L 77.011853 272.086588 \n",
       "L 77.141965 279.580076 \n",
       "L 77.207021 275.034926 \n",
       "L 77.272077 282.044732 \n",
       "L 77.337133 280.408993 \n",
       "L 77.402189 274.873249 \n",
       "L 77.467244 276.55384 \n",
       "L 77.5323 280.557154 \n",
       "L 77.662412 276.971406 \n",
       "L 77.85758 279.263437 \n",
       "L 77.922635 270.912493 \n",
       "L 78.052747 282.194184 \n",
       "L 78.117803 274.174385 \n",
       "L 78.182859 279.132924 \n",
       "L 78.247915 276.058522 \n",
       "L 78.31297 277.57339 \n",
       "L 78.378026 280.496747 \n",
       "L 78.443082 280.027241 \n",
       "L 78.63825 274.698113 \n",
       "L 78.703306 277.463252 \n",
       "L 78.768361 275.616751 \n",
       "L 78.898473 272.48474 \n",
       "L 79.158696 282.385547 \n",
       "L 79.223752 284.345099 \n",
       "L 79.353864 276.976557 \n",
       "L 79.483976 279.9752 \n",
       "L 79.549032 278.999356 \n",
       "L 79.614087 274.235107 \n",
       "L 79.679143 274.892232 \n",
       "L 79.874311 281.757449 \n",
       "L 80.004423 275.01392 \n",
       "L 80.134534 279.586245 \n",
       "L 80.19959 273.683844 \n",
       "L 80.264646 278.998681 \n",
       "L 80.329702 275.111116 \n",
       "L 80.394758 277.728151 \n",
       "L 80.459813 279.103525 \n",
       "L 80.524869 275.508206 \n",
       "L 80.654981 283.633092 \n",
       "L 80.720037 277.372441 \n",
       "L 80.785093 279.732871 \n",
       "L 80.850149 281.793019 \n",
       "L 80.915204 281.526412 \n",
       "L 81.045316 276.111108 \n",
       "L 81.240484 282.767715 \n",
       "L 81.370595 277.521018 \n",
       "L 81.500707 277.756374 \n",
       "L 81.565763 280.072984 \n",
       "L 81.695875 274.902434 \n",
       "L 81.76093 277.057625 \n",
       "L 81.891042 275.88782 \n",
       "L 81.956098 273.068803 \n",
       "L 82.021154 282.581015 \n",
       "L 82.08621 275.411297 \n",
       "L 82.151266 277.424382 \n",
       "L 82.216321 277.840412 \n",
       "L 82.281377 282.039896 \n",
       "L 82.346433 276.149662 \n",
       "L 82.541601 286.417142 \n",
       "L 82.671712 277.420738 \n",
       "L 82.801824 281.494057 \n",
       "L 82.86688 279.797553 \n",
       "L 82.931936 281.742699 \n",
       "L 83.127103 274.976012 \n",
       "L 83.192159 279.540646 \n",
       "L 83.257215 277.303942 \n",
       "L 83.322271 275.619994 \n",
       "L 83.387327 278.95908 \n",
       "L 83.452382 274.595178 \n",
       "L 83.517438 275.030076 \n",
       "L 83.582494 286.224271 \n",
       "L 83.64755 272.77746 \n",
       "L 83.712606 280.040083 \n",
       "L 83.777662 282.165014 \n",
       "L 83.842718 280.232509 \n",
       "L 83.907773 284.022534 \n",
       "L 83.972829 277.000906 \n",
       "L 84.037885 277.74699 \n",
       "L 84.102941 279.829349 \n",
       "L 84.167997 276.792338 \n",
       "L 84.298109 281.066032 \n",
       "L 84.363164 278.773685 \n",
       "L 84.42822 280.386481 \n",
       "L 84.493276 285.570805 \n",
       "L 84.558332 279.449549 \n",
       "L 84.623388 281.736974 \n",
       "L 84.688444 279.855521 \n",
       "L 84.818555 283.145277 \n",
       "L 84.883611 276.623143 \n",
       "L 84.948667 278.69398 \n",
       "L 85.013723 278.857709 \n",
       "L 85.078779 280.312371 \n",
       "L 85.143835 273.893574 \n",
       "L 85.273946 279.481345 \n",
       "L 85.339002 280.046741 \n",
       "L 85.404058 282.866374 \n",
       "L 85.469114 276.124596 \n",
       "L 85.53417 277.372757 \n",
       "L 85.664281 277.289451 \n",
       "L 85.729337 278.345344 \n",
       "L 85.859449 286.664378 \n",
       "L 85.924505 275.372844 \n",
       "L 85.989561 283.593132 \n",
       "L 86.054616 281.634513 \n",
       "L 86.119672 285.901678 \n",
       "L 86.184728 284.424187 \n",
       "L 86.249784 273.096682 \n",
       "L 86.31484 277.271558 \n",
       "L 86.379896 275.993782 \n",
       "L 86.444952 278.396166 \n",
       "L 86.510007 274.046168 \n",
       "L 86.575063 283.877917 \n",
       "L 86.640119 278.027471 \n",
       "L 86.705175 283.237508 \n",
       "L 86.770231 277.369041 \n",
       "L 86.835287 281.679308 \n",
       "L 86.900342 280.470131 \n",
       "L 86.965398 275.462736 \n",
       "L 87.030454 284.937843 \n",
       "L 87.09551 284.835826 \n",
       "L 87.290678 273.815964 \n",
       "L 87.355733 283.822877 \n",
       "L 87.420789 280.977158 \n",
       "L 87.550901 276.747973 \n",
       "L 87.681013 280.717181 \n",
       "L 87.811124 275.909399 \n",
       "L 87.941236 283.335551 \n",
       "L 88.071348 276.741029 \n",
       "L 88.201459 284.929851 \n",
       "L 88.266515 278.64907 \n",
       "L 88.331571 282.154267 \n",
       "L 88.396627 279.333385 \n",
       "L 88.461683 286.560883 \n",
       "L 88.526739 282.254318 \n",
       "L 88.591795 281.486022 \n",
       "L 88.65685 277.321218 \n",
       "L 88.786962 283.612732 \n",
       "L 88.98213 279.063995 \n",
       "L 89.047185 280.607474 \n",
       "L 89.112241 276.643661 \n",
       "L 89.177297 283.229674 \n",
       "L 89.242353 280.178358 \n",
       "L 89.307409 283.541133 \n",
       "L 89.502576 273.610338 \n",
       "L 89.632688 278.9702 \n",
       "L 89.697744 277.360432 \n",
       "L 89.7628 284.47587 \n",
       "L 89.827856 276.346521 \n",
       "L 89.892911 276.606929 \n",
       "L 89.957967 276.281366 \n",
       "L 90.088079 283.135994 \n",
       "L 90.218191 277.023203 \n",
       "L 90.413358 283.829951 \n",
       "L 90.478414 283.946402 \n",
       "L 90.54347 276.982497 \n",
       "L 90.608526 280.277648 \n",
       "L 90.673582 281.714706 \n",
       "L 90.933805 275.872782 \n",
       "L 91.128973 282.751516 \n",
       "L 91.259084 279.621743 \n",
       "L 91.32414 282.267059 \n",
       "L 91.389196 275.984915 \n",
       "L 91.454252 279.804713 \n",
       "L 91.519308 277.870013 \n",
       "L 91.584364 283.941179 \n",
       "L 91.649419 280.164469 \n",
       "L 91.714475 283.66054 \n",
       "L 91.779531 278.121309 \n",
       "L 91.844587 280.597171 \n",
       "L 92.039754 276.205606 \n",
       "L 92.169866 278.765377 \n",
       "L 92.234922 279.821874 \n",
       "L 92.299978 283.810681 \n",
       "L 92.495145 274.431909 \n",
       "L 92.690313 285.874201 \n",
       "L 92.885481 276.936152 \n",
       "L 92.950536 281.150816 \n",
       "L 93.015592 279.253665 \n",
       "L 93.080648 279.41804 \n",
       "L 93.21076 282.599395 \n",
       "L 93.405927 275.521794 \n",
       "L 93.470983 281.768282 \n",
       "L 93.536039 279.294544 \n",
       "L 93.666151 283.8346 \n",
       "L 93.861318 276.899535 \n",
       "L 93.99143 281.309408 \n",
       "L 94.056486 277.568196 \n",
       "L 94.121542 279.948656 \n",
       "L 94.186598 284.032808 \n",
       "L 94.251653 279.505493 \n",
       "L 94.316709 284.231632 \n",
       "L 94.381765 278.593929 \n",
       "L 94.446821 278.786512 \n",
       "L 94.511877 283.029615 \n",
       "L 94.576933 282.343751 \n",
       "L 94.641988 279.321246 \n",
       "L 94.707044 279.997354 \n",
       "L 94.7721 285.140255 \n",
       "L 94.837156 282.539376 \n",
       "L 94.967268 277.622935 \n",
       "L 95.032324 284.581129 \n",
       "L 95.097379 281.835404 \n",
       "L 95.162435 283.63556 \n",
       "L 95.422659 274.019122 \n",
       "L 95.55277 283.46767 \n",
       "L 95.617826 282.332502 \n",
       "L 95.682882 278.316031 \n",
       "L 95.747938 284.763525 \n",
       "L 95.812994 282.757671 \n",
       "L 95.87805 285.237666 \n",
       "L 96.008161 281.20138 \n",
       "L 96.203329 284.077388 \n",
       "L 96.398496 273.774554 \n",
       "L 96.463552 280.424375 \n",
       "L 96.528608 275.910748 \n",
       "L 96.593664 282.11181 \n",
       "L 96.65872 281.924191 \n",
       "L 96.723776 275.103927 \n",
       "L 96.788831 283.218109 \n",
       "L 96.853887 276.456645 \n",
       "L 96.918943 281.039272 \n",
       "L 97.049055 279.499768 \n",
       "L 97.114111 284.579379 \n",
       "L 97.179167 282.292197 \n",
       "L 97.244222 276.351515 \n",
       "L 97.309278 285.010575 \n",
       "L 97.374334 280.170122 \n",
       "L 97.43939 278.9833 \n",
       "L 97.504446 283.573532 \n",
       "L 97.569502 278.316346 \n",
       "L 97.699613 286.569607 \n",
       "L 97.764669 286.234789 \n",
       "L 97.959837 276.341672 \n",
       "L 98.089948 283.091686 \n",
       "L 98.155004 283.05758 \n",
       "L 98.22006 285.0786 \n",
       "L 98.285116 275.586246 \n",
       "L 98.350172 286.747483 \n",
       "L 98.415228 279.424095 \n",
       "L 98.480284 284.178716 \n",
       "L 98.545339 282.273458 \n",
       "L 98.610395 278.400571 \n",
       "L 98.675451 281.810122 \n",
       "L 98.740507 279.428385 \n",
       "L 98.870619 282.951015 \n",
       "L 98.935674 281.424711 \n",
       "L 99.00073 281.653051 \n",
       "L 99.065786 283.903572 \n",
       "L 99.130842 279.376057 \n",
       "L 99.195898 280.473833 \n",
       "L 99.260954 280.188115 \n",
       "L 99.32601 281.096063 \n",
       "L 99.391065 277.426836 \n",
       "L 99.456121 281.798213 \n",
       "L 99.521177 279.363746 \n",
       "L 99.586233 276.173753 \n",
       "L 99.716345 282.699317 \n",
       "L 99.846456 279.443709 \n",
       "L 99.911512 282.388259 \n",
       "L 99.976568 280.80868 \n",
       "L 100.10668 282.983729 \n",
       "L 100.171736 282.066512 \n",
       "L 100.236791 284.701598 \n",
       "L 100.301847 276.756453 \n",
       "L 100.366903 284.257746 \n",
       "L 100.431959 281.179226 \n",
       "L 100.497015 279.421297 \n",
       "L 100.692182 287.47744 \n",
       "L 100.88735 279.743015 \n",
       "L 100.952406 281.422157 \n",
       "L 101.017462 287.5698 \n",
       "L 101.082517 283.934479 \n",
       "L 101.147573 279.691907 \n",
       "L 101.212629 283.707172 \n",
       "L 101.277685 282.841021 \n",
       "L 101.407797 278.156119 \n",
       "L 101.472853 286.115482 \n",
       "L 101.537908 282.958792 \n",
       "L 101.602964 278.55605 \n",
       "L 101.66802 283.355552 \n",
       "L 101.733076 281.472864 \n",
       "L 101.798132 283.644843 \n",
       "L 101.863188 278.908631 \n",
       "L 101.928243 281.053621 \n",
       "L 101.993299 284.642798 \n",
       "L 102.058355 277.781799 \n",
       "L 102.188467 286.268966 \n",
       "L 102.318579 280.039079 \n",
       "L 102.383634 280.798751 \n",
       "L 102.44869 286.712143 \n",
       "L 102.578802 280.65907 \n",
       "L 102.643858 285.447912 \n",
       "L 102.708914 282.543063 \n",
       "L 102.839025 279.101315 \n",
       "L 102.904081 283.232831 \n",
       "L 102.969137 283.185108 \n",
       "L 103.034193 282.382936 \n",
       "L 103.099249 283.291774 \n",
       "L 103.294416 278.701269 \n",
       "L 103.359472 283.240335 \n",
       "L 103.424528 281.88756 \n",
       "L 103.55464 279.682365 \n",
       "L 103.619696 285.785313 \n",
       "L 103.684751 280.306919 \n",
       "L 103.749807 282.50517 \n",
       "L 103.814863 284.14626 \n",
       "L 103.879919 276.018547 \n",
       "L 103.944975 283.675778 \n",
       "L 104.010031 280.909994 \n",
       "L 104.205198 286.668567 \n",
       "L 104.270254 281.223218 \n",
       "L 104.33531 283.332423 \n",
       "L 104.400366 283.703643 \n",
       "L 104.530477 282.42875 \n",
       "L 104.595533 286.074217 \n",
       "L 104.660589 280.008216 \n",
       "L 104.725645 284.27485 \n",
       "L 104.790701 283.520357 \n",
       "L 104.855757 286.177295 \n",
       "L 105.050924 277.749788 \n",
       "L 105.181036 285.996721 \n",
       "L 105.311148 279.386488 \n",
       "L 105.376203 283.249202 \n",
       "L 105.506315 280.022349 \n",
       "L 105.571371 283.586087 \n",
       "L 105.701483 278.724599 \n",
       "L 105.766539 283.613406 \n",
       "L 105.831594 282.553251 \n",
       "L 105.961706 278.616313 \n",
       "L 106.091818 282.712029 \n",
       "L 106.286985 277.135249 \n",
       "L 106.417097 287.663552 \n",
       "L 106.547209 281.5137 \n",
       "L 106.612265 286.303388 \n",
       "L 106.67732 285.978083 \n",
       "L 106.872488 279.811557 \n",
       "L 106.937544 283.861919 \n",
       "L 107.0026 278.361917 \n",
       "L 107.132711 285.043088 \n",
       "L 107.197767 277.587135 \n",
       "L 107.262823 284.827131 \n",
       "L 107.327879 279.500227 \n",
       "L 107.392935 288.601489 \n",
       "L 107.457991 284.788089 \n",
       "L 107.588102 280.809771 \n",
       "L 107.653158 285.232601 \n",
       "L 107.78327 279.021997 \n",
       "L 107.913382 283.231726 \n",
       "L 108.043493 281.873427 \n",
       "L 108.108549 279.740691 \n",
       "L 108.238661 284.81317 \n",
       "L 108.368772 279.458832 \n",
       "L 108.433828 287.299707 \n",
       "L 108.498884 285.101959 \n",
       "L 108.56394 279.114286 \n",
       "L 108.628996 281.100009 \n",
       "L 108.694052 281.237035 \n",
       "L 108.824163 276.680981 \n",
       "L 109.019331 284.876963 \n",
       "L 109.149443 275.80701 \n",
       "L 109.409666 286.944041 \n",
       "L 109.474722 281.6512 \n",
       "L 109.604834 287.923516 \n",
       "L 109.669889 281.145679 \n",
       "L 109.734945 284.570813 \n",
       "L 109.800001 286.790327 \n",
       "L 109.865057 285.585799 \n",
       "L 109.995169 283.987108 \n",
       "L 110.060225 277.846195 \n",
       "L 110.12528 279.416906 \n",
       "L 110.320448 286.174382 \n",
       "L 110.385504 275.981887 \n",
       "L 110.45056 282.202118 \n",
       "L 110.515615 278.3091 \n",
       "L 110.580671 281.523485 \n",
       "L 110.645727 280.58905 \n",
       "L 110.710783 280.581876 \n",
       "L 110.775839 285.786949 \n",
       "L 110.840895 284.908859 \n",
       "L 110.905951 287.1473 \n",
       "L 110.971006 279.884691 \n",
       "L 111.036062 280.468897 \n",
       "L 111.101118 281.148377 \n",
       "L 111.166174 287.312879 \n",
       "L 111.23123 283.926343 \n",
       "L 111.296286 284.848424 \n",
       "L 111.361342 280.306618 \n",
       "L 111.426397 281.310083 \n",
       "L 111.556509 283.857945 \n",
       "L 111.621565 283.213791 \n",
       "L 111.686621 282.760269 \n",
       "L 111.751677 285.239488 \n",
       "L 111.816732 283.010805 \n",
       "L 111.881788 283.531276 \n",
       "L 111.946844 283.869452 \n",
       "L 112.0119 284.879072 \n",
       "L 112.076956 277.912355 \n",
       "L 112.142012 279.840096 \n",
       "L 112.207068 284.898945 \n",
       "L 112.337179 280.594704 \n",
       "L 112.467291 285.703341 \n",
       "L 112.532347 280.869144 \n",
       "L 112.597403 282.068593 \n",
       "L 112.662459 281.540373 \n",
       "L 112.727514 284.175444 \n",
       "L 112.79257 282.968922 \n",
       "L 112.857626 280.113145 \n",
       "L 112.922682 281.075559 \n",
       "L 112.987738 283.974525 \n",
       "L 113.052794 283.061124 \n",
       "L 113.117849 282.955004 \n",
       "L 113.247961 279.247768 \n",
       "L 113.313017 280.33012 \n",
       "L 113.378073 286.256397 \n",
       "L 113.443129 284.789625 \n",
       "L 113.508185 283.110856 \n",
       "L 113.57324 285.659004 \n",
       "L 113.638296 276.462226 \n",
       "L 113.703352 279.169426 \n",
       "L 113.833464 285.702839 \n",
       "L 113.89852 279.220019 \n",
       "L 113.963575 280.190726 \n",
       "L 114.028631 285.302492 \n",
       "L 114.093687 285.287598 \n",
       "L 114.158743 286.691396 \n",
       "L 114.353911 282.156692 \n",
       "L 114.418966 284.082941 \n",
       "L 114.484022 283.708492 \n",
       "L 114.549078 276.99894 \n",
       "L 114.614134 282.376293 \n",
       "L 114.67919 281.300254 \n",
       "L 114.744246 279.921839 \n",
       "L 114.809302 283.710114 \n",
       "L 114.874357 283.683282 \n",
       "L 115.004469 276.688471 \n",
       "L 115.134581 286.58001 \n",
       "L 115.199637 281.839177 \n",
       "L 115.264692 283.436878 \n",
       "L 115.329748 286.318612 \n",
       "L 115.524916 275.499425 \n",
       "L 115.589972 284.883549 \n",
       "L 115.655028 282.292915 \n",
       "L 115.850195 276.087304 \n",
       "L 116.045363 285.98784 \n",
       "L 116.110418 279.411641 \n",
       "L 116.175474 282.065852 \n",
       "L 116.24053 282.175115 \n",
       "L 116.305586 286.093601 \n",
       "L 116.370642 284.674149 \n",
       "L 116.435698 281.098072 \n",
       "L 116.500754 281.194679 \n",
       "L 116.565809 280.639757 \n",
       "L 116.630865 284.290719 \n",
       "L 116.695921 280.336391 \n",
       "L 116.760977 286.767083 \n",
       "L 116.826033 284.966669 \n",
       "L 116.956145 280.231361 \n",
       "L 117.086256 281.059719 \n",
       "L 117.151312 287.930331 \n",
       "L 117.216368 279.466666 \n",
       "L 117.281424 280.100346 \n",
       "L 117.476591 285.593677 \n",
       "L 117.541647 282.239037 \n",
       "L 117.671759 285.628557 \n",
       "L 117.736815 285.148419 \n",
       "L 117.801871 283.504445 \n",
       "L 117.866926 284.446757 \n",
       "L 117.931982 284.644204 \n",
       "L 118.062094 278.368474 \n",
       "L 118.12715 287.761437 \n",
       "L 118.192206 281.105935 \n",
       "L 118.257261 285.043619 \n",
       "L 118.322317 285.14598 \n",
       "L 118.452429 277.22263 \n",
       "L 118.517485 285.7792 \n",
       "L 118.582541 285.264052 \n",
       "L 118.647597 283.766043 \n",
       "L 118.712652 279.503283 \n",
       "L 118.777708 281.031524 \n",
       "L 118.90782 282.400728 \n",
       "L 118.972876 282.421217 \n",
       "L 119.037932 287.174576 \n",
       "L 119.233099 278.005174 \n",
       "L 119.298155 286.368171 \n",
       "L 119.363211 280.346033 \n",
       "L 119.428267 282.279542 \n",
       "L 119.493323 280.896392 \n",
       "L 119.558378 286.33451 \n",
       "L 119.623434 282.225061 \n",
       "L 119.753546 285.336971 \n",
       "L 119.883658 279.583505 \n",
       "L 119.948714 280.328097 \n",
       "L 120.013769 284.942176 \n",
       "L 120.078825 282.267547 \n",
       "L 120.143881 280.978191 \n",
       "L 120.273993 286.098221 \n",
       "L 120.339049 284.66412 \n",
       "L 120.404104 278.989297 \n",
       "L 120.599272 284.87339 \n",
       "L 120.79444 280.86451 \n",
       "L 120.859495 283.662678 \n",
       "L 120.924551 283.423807 \n",
       "L 121.054663 279.205756 \n",
       "L 121.119719 281.502236 \n",
       "L 121.184775 280.432697 \n",
       "L 121.444998 283.888191 \n",
       "L 121.510054 288.321753 \n",
       "L 121.57511 281.757665 \n",
       "L 121.640166 282.44175 \n",
       "L 121.705221 282.389637 \n",
       "L 121.770277 282.846172 \n",
       "L 121.835333 279.611699 \n",
       "L 121.965445 282.869258 \n",
       "L 122.030501 278.077949 \n",
       "L 122.160612 285.217392 \n",
       "L 122.290724 280.892891 \n",
       "L 122.35578 286.454749 \n",
       "L 122.420836 286.137019 \n",
       "L 122.485892 285.264153 \n",
       "L 122.681059 286.744484 \n",
       "L 122.746115 281.243679 \n",
       "L 122.811171 282.308253 \n",
       "L 123.006338 279.08216 \n",
       "L 123.201506 285.194119 \n",
       "L 123.266562 280.11765 \n",
       "L 123.331618 288.003766 \n",
       "L 123.461729 281.917361 \n",
       "L 123.526785 282.43393 \n",
       "L 123.591841 284.533163 \n",
       "L 123.656897 283.837843 \n",
       "L 123.852064 287.573129 \n",
       "L 123.91712 286.240399 \n",
       "L 124.112288 279.167504 \n",
       "L 124.177344 288.489414 \n",
       "L 124.2424 284.665913 \n",
       "L 124.307455 280.056512 \n",
       "L 124.372511 286.858066 \n",
       "L 124.437567 283.384751 \n",
       "L 124.502623 283.149926 \n",
       "L 124.567679 281.159181 \n",
       "L 124.632735 282.749019 \n",
       "L 124.69779 280.176048 \n",
       "L 124.762846 284.339604 \n",
       "L 124.827902 282.938891 \n",
       "L 124.892958 274.846762 \n",
       "L 124.958014 281.075057 \n",
       "L 125.02307 280.705731 \n",
       "L 125.088126 275.843067 \n",
       "L 125.153181 284.925216 \n",
       "L 125.218237 284.480246 \n",
       "L 125.283293 281.442173 \n",
       "L 125.348349 282.712948 \n",
       "L 125.413405 282.043153 \n",
       "L 125.478461 284.441836 \n",
       "L 125.543517 283.325937 \n",
       "L 125.608572 275.626694 \n",
       "L 125.738684 283.280955 \n",
       "L 125.80374 283.88809 \n",
       "L 125.868796 283.174892 \n",
       "L 125.933852 288.034514 \n",
       "L 125.998907 281.35198 \n",
       "L 126.063963 282.127665 \n",
       "L 126.194075 285.830969 \n",
       "L 126.259131 279.933446 \n",
       "L 126.324187 285.982215 \n",
       "L 126.389243 285.190029 \n",
       "L 126.454298 278.931617 \n",
       "L 126.519354 282.609525 \n",
       "L 126.58441 286.636312 \n",
       "L 126.714522 281.451945 \n",
       "L 126.779578 286.685039 \n",
       "L 126.844633 279.84001 \n",
       "L 126.909689 283.906255 \n",
       "L 126.974745 279.704447 \n",
       "L 127.039801 286.25634 \n",
       "L 127.104857 281.080825 \n",
       "L 127.169913 283.701376 \n",
       "L 127.234969 283.740231 \n",
       "L 127.300024 284.130993 \n",
       "L 127.36508 281.696211 \n",
       "L 127.495192 287.481243 \n",
       "L 127.560248 280.760111 \n",
       "L 127.625304 284.500807 \n",
       "L 127.69036 287.980808 \n",
       "L 127.820471 281.438156 \n",
       "L 127.950583 287.511991 \n",
       "L 128.015639 284.358572 \n",
       "L 128.080695 285.409803 \n",
       "L 128.14575 286.374427 \n",
       "L 128.210806 279.379845 \n",
       "L 128.275862 282.136116 \n",
       "L 128.405974 279.891549 \n",
       "L 128.47103 281.371164 \n",
       "L 128.536086 288.03021 \n",
       "L 128.731253 281.484515 \n",
       "L 128.796309 283.77933 \n",
       "L 128.861365 277.069979 \n",
       "L 128.926421 280.125571 \n",
       "L 128.991476 282.91975 \n",
       "L 129.056532 282.904527 \n",
       "L 129.121588 279.911235 \n",
       "L 129.186644 285.6939 \n",
       "L 129.2517 281.743718 \n",
       "L 129.316756 278.794921 \n",
       "L 129.511923 288.463702 \n",
       "L 129.576979 279.890947 \n",
       "L 129.642035 286.975378 \n",
       "L 129.707091 281.41969 \n",
       "L 129.772147 281.875823 \n",
       "L 129.837203 285.375754 \n",
       "L 129.902258 284.219393 \n",
       "L 130.03237 279.130399 \n",
       "L 130.097426 284.622008 \n",
       "L 130.162482 280.498254 \n",
       "L 130.227538 281.51413 \n",
       "L 130.357649 284.038547 \n",
       "L 130.422705 278.353767 \n",
       "L 130.552817 288.852957 \n",
       "L 130.617873 284.9017 \n",
       "L 130.682929 283.189929 \n",
       "L 130.747984 286.751171 \n",
       "L 130.878096 282.310348 \n",
       "L 130.943152 287.716412 \n",
       "L 131.073264 281.846065 \n",
       "L 131.203375 284.396538 \n",
       "L 131.333487 280.246599 \n",
       "L 131.398543 287.127944 \n",
       "L 131.463599 282.698241 \n",
       "L 131.528655 279.584007 \n",
       "L 131.59371 284.733767 \n",
       "L 131.658766 283.929729 \n",
       "L 131.853934 279.131418 \n",
       "L 131.91899 285.286522 \n",
       "L 131.984046 281.594194 \n",
       "L 132.049101 286.737554 \n",
       "L 132.114157 282.99439 \n",
       "L 132.179213 285.63599 \n",
       "L 132.244269 283.796376 \n",
       "L 132.309325 284.934916 \n",
       "L 132.374381 285.728149 \n",
       "L 132.439436 280.957243 \n",
       "L 132.569548 286.660905 \n",
       "L 132.69966 283.255487 \n",
       "L 132.764716 284.923495 \n",
       "L 132.829772 282.635122 \n",
       "L 132.894827 283.290683 \n",
       "L 132.959883 285.692953 \n",
       "L 133.155051 281.720502 \n",
       "L 133.220107 288.314608 \n",
       "L 133.415274 279.366802 \n",
       "L 133.48033 286.20668 \n",
       "L 133.545386 283.914219 \n",
       "L 133.610442 287.558006 \n",
       "L 133.675498 282.018675 \n",
       "L 133.740553 285.427882 \n",
       "L 133.805609 281.683469 \n",
       "L 133.870665 285.326554 \n",
       "L 133.935721 282.128698 \n",
       "L 134.000777 289.567533 \n",
       "L 134.065833 280.58245 \n",
       "L 134.130889 281.812647 \n",
       "L 134.261 284.509474 \n",
       "L 134.326056 287.489579 \n",
       "L 134.391112 285.415155 \n",
       "L 134.456168 279.287355 \n",
       "L 134.586279 290.445622 \n",
       "L 134.651335 281.349727 \n",
       "L 134.716391 284.62119 \n",
       "L 134.781447 280.50296 \n",
       "L 134.846503 282.84435 \n",
       "L 134.911559 285.300282 \n",
       "L 134.976615 281.278358 \n",
       "L 135.04167 287.884 \n",
       "L 135.106726 284.45363 \n",
       "L 135.171782 287.423548 \n",
       "L 135.36695 282.575577 \n",
       "L 135.432006 287.674472 \n",
       "L 135.497061 280.747599 \n",
       "L 135.562117 282.44043 \n",
       "L 135.627173 288.080228 \n",
       "L 135.692229 284.406696 \n",
       "L 135.757285 280.482643 \n",
       "L 135.822341 284.897883 \n",
       "L 135.887396 282.323792 \n",
       "L 135.952452 281.451514 \n",
       "L 136.082564 283.755096 \n",
       "L 136.14762 280.176335 \n",
       "L 136.212676 282.703894 \n",
       "L 136.277732 282.57704 \n",
       "L 136.342787 286.956596 \n",
       "L 136.407843 281.857916 \n",
       "L 136.472899 286.024987 \n",
       "L 136.537955 281.413477 \n",
       "L 136.603011 285.028094 \n",
       "L 136.668067 281.033691 \n",
       "L 136.733122 281.740088 \n",
       "L 136.863234 286.742834 \n",
       "L 136.92829 281.651602 \n",
       "L 136.993346 282.514424 \n",
       "L 137.058402 287.076347 \n",
       "L 137.123458 283.26665 \n",
       "L 137.253569 286.402793 \n",
       "L 137.448737 276.926639 \n",
       "L 137.513793 283.658216 \n",
       "L 137.578849 280.97987 \n",
       "L 137.70896 287.481931 \n",
       "L 137.774016 285.963074 \n",
       "L 137.839072 283.707875 \n",
       "L 137.904128 288.359317 \n",
       "L 137.969184 279.711922 \n",
       "L 138.034239 283.280883 \n",
       "L 138.099295 285.250091 \n",
       "L 138.164351 282.709863 \n",
       "L 138.229407 287.832619 \n",
       "L 138.294463 284.021085 \n",
       "L 138.359519 285.190531 \n",
       "L 138.424575 286.056726 \n",
       "L 138.48963 288.471177 \n",
       "L 138.749854 278.711284 \n",
       "L 138.81491 284.581933 \n",
       "L 138.879965 281.135306 \n",
       "L 138.945021 285.865319 \n",
       "L 139.010077 283.390634 \n",
       "L 139.075133 286.018932 \n",
       "L 139.140189 285.263177 \n",
       "L 139.270301 280.685155 \n",
       "L 139.400412 281.402242 \n",
       "L 139.465468 287.692723 \n",
       "L 139.530524 284.745016 \n",
       "L 139.725692 283.307141 \n",
       "L 139.790747 286.068678 \n",
       "L 139.855803 283.049674 \n",
       "L 139.920859 286.582147 \n",
       "L 140.050971 281.210218 \n",
       "L 140.116027 284.550266 \n",
       "L 140.181082 278.245695 \n",
       "L 140.246138 279.855449 \n",
       "L 140.441306 286.068233 \n",
       "L 140.506362 285.623535 \n",
       "L 140.636473 278.54602 \n",
       "L 140.701529 285.668015 \n",
       "L 140.766585 281.231856 \n",
       "L 140.831641 284.837878 \n",
       "L 140.896697 281.620552 \n",
       "L 140.961753 286.876776 \n",
       "L 141.091864 280.220097 \n",
       "L 141.221976 283.575885 \n",
       "L 141.287032 283.510256 \n",
       "L 141.352088 282.959868 \n",
       "L 141.417144 287.79758 \n",
       "L 141.482199 282.816844 \n",
       "L 141.547255 288.069467 \n",
       "L 141.612311 281.679581 \n",
       "L 141.677367 287.51839 \n",
       "L 141.742423 286.540049 \n",
       "L 141.872535 278.47557 \n",
       "L 141.93759 283.362095 \n",
       "L 142.002646 279.42685 \n",
       "L 142.067702 287.088299 \n",
       "L 142.132758 286.326288 \n",
       "L 142.197814 279.344089 \n",
       "L 142.26287 286.267288 \n",
       "L 142.327925 282.597616 \n",
       "L 142.392981 284.58024 \n",
       "L 142.458037 283.78451 \n",
       "L 142.588149 282.516218 \n",
       "L 142.718261 286.725817 \n",
       "L 142.848372 281.997396 \n",
       "L 142.913428 286.874796 \n",
       "L 142.978484 279.374981 \n",
       "L 143.04354 279.474702 \n",
       "L 143.173651 283.360359 \n",
       "L 143.238707 279.686641 \n",
       "L 143.303763 279.943705 \n",
       "L 143.368819 283.770807 \n",
       "L 143.433875 280.291896 \n",
       "L 143.498931 280.878944 \n",
       "L 143.629042 286.879976 \n",
       "L 143.694098 279.638086 \n",
       "L 143.759154 282.752018 \n",
       "L 143.82421 284.001686 \n",
       "L 143.889266 287.677054 \n",
       "L 143.954322 279.410435 \n",
       "L 144.019378 282.805724 \n",
       "L 144.149489 283.224322 \n",
       "L 144.214545 284.825653 \n",
       "L 144.279601 279.177591 \n",
       "L 144.409713 286.977057 \n",
       "L 144.474768 285.511647 \n",
       "L 144.539824 281.016171 \n",
       "L 144.60488 283.118604 \n",
       "L 144.669936 281.136439 \n",
       "L 144.734992 288.634691 \n",
       "L 144.800048 282.717238 \n",
       "L 144.865104 283.88139 \n",
       "L 144.930159 283.164102 \n",
       "L 144.995215 283.783591 \n",
       "L 145.060271 283.530616 \n",
       "L 145.125327 283.435386 \n",
       "L 145.190383 286.419409 \n",
       "L 145.255439 283.907504 \n",
       "L 145.320494 285.045958 \n",
       "L 145.38555 288.080214 \n",
       "L 145.450606 282.395089 \n",
       "L 145.515662 283.888133 \n",
       "L 145.580718 285.524905 \n",
       "L 145.645774 285.179282 \n",
       "L 145.71083 281.950607 \n",
       "L 145.775885 282.879446 \n",
       "L 145.840941 287.574751 \n",
       "L 145.905997 282.614375 \n",
       "L 145.971053 284.587026 \n",
       "L 146.101165 282.1847 \n",
       "L 146.166221 286.146374 \n",
       "L 146.231276 280.377943 \n",
       "L 146.426444 290.573982 \n",
       "L 146.621611 282.141956 \n",
       "L 146.686667 282.466099 \n",
       "L 146.816779 284.976225 \n",
       "L 146.881835 282.50669 \n",
       "L 146.946891 284.126646 \n",
       "L 147.077002 288.082294 \n",
       "L 147.142058 281.523887 \n",
       "L 147.27217 288.887178 \n",
       "L 147.337226 279.276035 \n",
       "L 147.402282 284.129042 \n",
       "L 147.467337 283.353959 \n",
       "L 147.532393 283.736529 \n",
       "L 147.597449 285.034924 \n",
       "L 147.662505 283.888191 \n",
       "L 147.727561 284.486516 \n",
       "L 147.792617 285.029916 \n",
       "L 147.857673 281.016301 \n",
       "L 147.922728 282.902719 \n",
       "L 148.05284 285.865821 \n",
       "L 148.117896 279.117227 \n",
       "L 148.182952 279.570319 \n",
       "L 148.248008 284.831637 \n",
       "L 148.313064 279.92752 \n",
       "L 148.378119 283.28437 \n",
       "L 148.443175 285.965628 \n",
       "L 148.508231 283.485347 \n",
       "L 148.573287 288.284606 \n",
       "L 148.638343 286.965994 \n",
       "L 148.703399 283.039358 \n",
       "L 148.768454 286.573711 \n",
       "L 148.898566 278.897856 \n",
       "L 149.093734 289.877371 \n",
       "L 149.223845 280.592293 \n",
       "L 149.288901 285.795773 \n",
       "L 149.353957 282.413627 \n",
       "L 149.419013 286.779007 \n",
       "L 149.484069 284.691783 \n",
       "L 149.549125 285.569026 \n",
       "L 149.679236 281.666438 \n",
       "L 149.744292 285.446104 \n",
       "L 149.809348 281.384335 \n",
       "L 149.874404 282.778821 \n",
       "L 150.004516 286.211157 \n",
       "L 150.069571 285.289793 \n",
       "L 150.134627 281.933762 \n",
       "L 150.199683 284.633816 \n",
       "L 150.394851 279.635589 \n",
       "L 150.459907 280.236153 \n",
       "L 150.524962 286.24027 \n",
       "L 150.590018 283.834313 \n",
       "L 150.655074 282.01404 \n",
       "L 150.72013 287.791181 \n",
       "L 150.785186 283.474213 \n",
       "L 150.850242 288.058605 \n",
       "L 151.045409 278.788335 \n",
       "L 151.240577 286.599337 \n",
       "L 151.305633 287.061138 \n",
       "L 151.435744 281.180288 \n",
       "L 151.630912 288.607515 \n",
       "L 151.956191 280.795465 \n",
       "L 152.021247 287.351318 \n",
       "L 152.086303 285.305662 \n",
       "L 152.151359 286.711627 \n",
       "L 152.216414 282.364986 \n",
       "L 152.28147 285.086951 \n",
       "L 152.346526 282.655884 \n",
       "L 152.411582 284.544254 \n",
       "L 152.476638 282.713407 \n",
       "L 152.60675 284.697494 \n",
       "L 152.671805 282.337854 \n",
       "L 152.736861 283.388912 \n",
       "L 152.801917 285.406488 \n",
       "L 152.866973 279.180546 \n",
       "L 152.932029 288.668811 \n",
       "L 152.997085 282.042077 \n",
       "L 153.127196 280.30194 \n",
       "L 153.322364 288.795507 \n",
       "L 153.452476 283.335192 \n",
       "L 153.517531 283.689739 \n",
       "L 153.582587 281.299867 \n",
       "L 153.647643 285.080379 \n",
       "L 153.712699 280.741831 \n",
       "L 153.777755 285.201006 \n",
       "L 153.842811 283.548566 \n",
       "L 153.907867 281.464083 \n",
       "L 153.972922 285.620551 \n",
       "L 154.037978 283.298417 \n",
       "L 154.103034 283.587249 \n",
       "L 154.16809 286.359217 \n",
       "L 154.233146 280.79449 \n",
       "L 154.298202 285.502493 \n",
       "L 154.363257 284.162215 \n",
       "L 154.428313 285.297828 \n",
       "L 154.493369 283.463423 \n",
       "L 154.558425 288.093959 \n",
       "L 154.623481 283.815617 \n",
       "L 154.688537 286.531067 \n",
       "L 154.753593 287.126007 \n",
       "L 154.818648 282.450215 \n",
       "L 154.883704 283.788126 \n",
       "L 154.94876 286.371543 \n",
       "L 155.078872 281.979633 \n",
       "L 155.274039 289.006198 \n",
       "L 155.339095 281.821615 \n",
       "L 155.404151 286.362561 \n",
       "L 155.469207 280.376006 \n",
       "L 155.534263 285.149481 \n",
       "L 155.599319 282.465496 \n",
       "L 155.72943 284.741701 \n",
       "L 155.794486 284.38097 \n",
       "L 155.859542 284.856373 \n",
       "L 155.924598 287.580146 \n",
       "L 155.989654 281.715825 \n",
       "L 156.05471 285.12694 \n",
       "L 156.119765 284.520436 \n",
       "L 156.249877 286.097791 \n",
       "L 156.314933 282.617201 \n",
       "L 156.379989 283.033202 \n",
       "L 156.445045 284.241045 \n",
       "L 156.5101 288.855942 \n",
       "L 156.640212 282.674566 \n",
       "L 156.705268 286.318339 \n",
       "L 156.770324 282.535344 \n",
       "L 156.83538 286.817546 \n",
       "L 156.900436 284.734326 \n",
       "L 156.965491 283.524217 \n",
       "L 157.030547 284.4369 \n",
       "L 157.095603 284.076498 \n",
       "L 157.160659 283.006715 \n",
       "L 157.290771 284.120074 \n",
       "L 157.355826 281.648287 \n",
       "L 157.485938 288.978261 \n",
       "L 157.550994 285.362281 \n",
       "L 157.61605 287.186313 \n",
       "L 157.811217 284.442998 \n",
       "L 157.876273 280.224301 \n",
       "L 157.941329 286.584285 \n",
       "L 158.006385 283.127701 \n",
       "L 158.136497 285.893341 \n",
       "L 158.201553 285.779674 \n",
       "L 158.266608 285.377332 \n",
       "L 158.331664 287.494228 \n",
       "L 158.461776 280.885903 \n",
       "L 158.526832 289.490798 \n",
       "L 158.591888 285.646105 \n",
       "L 158.656943 289.120181 \n",
       "L 158.721999 285.472232 \n",
       "L 158.787055 288.626899 \n",
       "L 158.852111 284.079856 \n",
       "L 158.917167 285.208223 \n",
       "L 158.982223 284.741429 \n",
       "L 159.047279 285.210074 \n",
       "L 159.112334 284.649083 \n",
       "L 159.242446 288.188902 \n",
       "L 159.307502 280.2174 \n",
       "L 159.372558 284.29356 \n",
       "L 159.437614 282.683247 \n",
       "L 159.502669 283.132837 \n",
       "L 159.632781 289.609617 \n",
       "L 159.697837 279.43137 \n",
       "L 159.762893 283.507931 \n",
       "L 159.827949 287.286335 \n",
       "L 160.023116 280.85359 \n",
       "L 160.153228 288.918429 \n",
       "L 160.218284 283.244252 \n",
       "L 160.28334 288.87702 \n",
       "L 160.348396 278.085137 \n",
       "L 160.413451 279.884117 \n",
       "L 160.543563 287.524015 \n",
       "L 160.608619 282.655899 \n",
       "L 160.673675 286.947815 \n",
       "L 160.738731 282.641034 \n",
       "L 160.803786 285.8249 \n",
       "L 160.868842 285.20686 \n",
       "L 160.933898 285.116135 \n",
       "L 160.998954 282.85019 \n",
       "L 161.06401 285.853855 \n",
       "L 161.129066 282.048261 \n",
       "L 161.194122 283.071527 \n",
       "L 161.259177 286.020367 \n",
       "L 161.324233 285.273436 \n",
       "L 161.389289 284.913824 \n",
       "L 161.454345 286.196063 \n",
       "L 161.519401 282.250802 \n",
       "L 161.649512 286.350293 \n",
       "L 161.714568 286.39516 \n",
       "L 161.779624 283.951209 \n",
       "L 161.909736 286.302211 \n",
       "L 161.974792 283.757765 \n",
       "L 162.039848 292.144149 \n",
       "L 162.104903 290.193665 \n",
       "L 162.235015 278.94918 \n",
       "L 162.300071 285.775628 \n",
       "L 162.365127 282.837764 \n",
       "L 162.430183 284.71576 \n",
       "L 162.495239 277.47529 \n",
       "L 162.62535 286.981748 \n",
       "L 162.755462 280.402307 \n",
       "L 162.885574 287.369785 \n",
       "L 162.950629 280.837879 \n",
       "L 163.015685 287.958253 \n",
       "L 163.080741 284.326748 \n",
       "L 163.145797 283.472419 \n",
       "L 163.210853 286.986168 \n",
       "L 163.275909 285.690456 \n",
       "L 163.40602 283.899038 \n",
       "L 163.471076 277.900159 \n",
       "L 163.536132 280.643215 \n",
       "L 163.666244 287.133841 \n",
       "L 163.796355 279.251398 \n",
       "L 163.861411 281.235801 \n",
       "L 163.926467 288.419724 \n",
       "L 163.991523 282.166635 \n",
       "L 164.056579 286.212018 \n",
       "L 164.121635 284.014743 \n",
       "L 164.186691 284.572778 \n",
       "L 164.251746 285.574306 \n",
       "L 164.316802 284.113632 \n",
       "L 164.381858 278.694597 \n",
       "L 164.51197 284.816772 \n",
       "L 164.577026 284.340594 \n",
       "L 164.642082 290.216192 \n",
       "L 164.707137 284.481164 \n",
       "L 164.772193 286.612652 \n",
       "L 164.837249 286.660403 \n",
       "L 164.902305 282.889059 \n",
       "L 164.967361 286.641521 \n",
       "L 165.032417 284.74163 \n",
       "L 165.097472 285.172912 \n",
       "L 165.162528 278.975265 \n",
       "L 165.357696 289.442143 \n",
       "L 165.422752 280.511957 \n",
       "L 165.487808 285.353815 \n",
       "L 165.552863 286.613786 \n",
       "L 165.617919 283.645575 \n",
       "L 165.682975 286.650833 \n",
       "L 165.878143 278.479731 \n",
       "L 166.07331 287.623262 \n",
       "L 166.138366 280.451292 \n",
       "L 166.203422 284.64122 \n",
       "L 166.268478 287.588841 \n",
       "L 166.333534 283.484773 \n",
       "L 166.398589 286.800572 \n",
       "L 166.463645 282.224803 \n",
       "L 166.528701 282.843331 \n",
       "L 166.593757 284.86471 \n",
       "L 166.658813 282.002131 \n",
       "L 166.723869 285.368695 \n",
       "L 166.788925 284.565547 \n",
       "L 166.85398 284.522904 \n",
       "L 166.919036 285.1571 \n",
       "L 166.984092 281.191422 \n",
       "L 167.049148 288.008572 \n",
       "L 167.114204 281.096465 \n",
       "L 167.17926 284.478869 \n",
       "L 167.244315 282.550725 \n",
       "L 167.309371 284.624906 \n",
       "L 167.374427 281.330012 \n",
       "L 167.504539 285.555209 \n",
       "L 167.634651 283.001148 \n",
       "L 167.829818 288.667706 \n",
       "L 167.894874 280.317379 \n",
       "L 167.95993 286.999325 \n",
       "L 168.024986 284.124278 \n",
       "L 168.090041 287.41085 \n",
       "L 168.285209 281.695536 \n",
       "L 168.415321 287.66272 \n",
       "L 168.480377 282.120979 \n",
       "L 168.545432 284.299227 \n",
       "L 168.675544 286.304019 \n",
       "L 168.7406 285.97781 \n",
       "L 168.805656 283.932398 \n",
       "L 168.870712 285.772557 \n",
       "L 169.000823 283.452461 \n",
       "L 169.065879 283.523212 \n",
       "L 169.130935 288.419494 \n",
       "L 169.261047 282.885085 \n",
       "L 169.391158 289.52829 \n",
       "L 169.456214 285.578166 \n",
       "L 169.52127 285.635574 \n",
       "L 169.586326 287.31562 \n",
       "L 169.651382 279.518866 \n",
       "L 169.716438 284.476802 \n",
       "L 169.781494 284.559994 \n",
       "L 169.846549 286.671078 \n",
       "L 169.911605 285.263966 \n",
       "L 169.976661 281.712754 \n",
       "L 170.041717 282.995983 \n",
       "L 170.106773 283.096149 \n",
       "L 170.171829 282.614202 \n",
       "L 170.30194 287.463408 \n",
       "L 170.366996 281.354218 \n",
       "L 170.497108 287.305246 \n",
       "L 170.62722 281.158507 \n",
       "L 170.757331 288.973426 \n",
       "L 170.952499 281.5596 \n",
       "L 171.017555 281.450768 \n",
       "L 171.147666 285.971569 \n",
       "L 171.212722 282.873979 \n",
       "L 171.277778 286.973082 \n",
       "L 171.342834 284.582277 \n",
       "L 171.40789 283.341763 \n",
       "L 171.472946 283.905638 \n",
       "L 171.538001 286.390396 \n",
       "L 171.733169 283.390347 \n",
       "L 171.863281 284.848123 \n",
       "L 171.928337 282.297004 \n",
       "L 172.058448 285.760835 \n",
       "L 172.123504 284.572879 \n",
       "L 172.18856 280.895645 \n",
       "L 172.253616 284.783197 \n",
       "L 172.318672 282.542303 \n",
       "L 172.383728 282.097476 \n",
       "L 172.448783 282.935505 \n",
       "L 172.643951 288.574528 \n",
       "L 172.774063 282.588576 \n",
       "L 172.904174 288.908227 \n",
       "L 173.034286 282.493633 \n",
       "L 173.099342 283.021408 \n",
       "L 173.164398 282.385404 \n",
       "L 173.229454 283.230779 \n",
       "L 173.359565 288.16583 \n",
       "L 173.489677 281.406762 \n",
       "L 173.619789 289.662232 \n",
       "L 173.7499 280.740095 \n",
       "L 173.814956 289.549626 \n",
       "L 173.880012 284.265064 \n",
       "L 173.945068 288.5779 \n",
       "L 174.010124 286.500003 \n",
       "L 174.07518 279.268258 \n",
       "L 174.140235 287.633364 \n",
       "L 174.205291 285.086664 \n",
       "L 174.270347 289.54393 \n",
       "L 174.465515 284.637676 \n",
       "L 174.660682 287.144057 \n",
       "L 174.725738 287.326367 \n",
       "L 174.790794 280.319043 \n",
       "L 174.85585 287.188092 \n",
       "L 174.920906 285.467253 \n",
       "L 174.985961 283.709898 \n",
       "L 175.051017 284.937857 \n",
       "L 175.116073 288.008788 \n",
       "L 175.246185 282.788506 \n",
       "L 175.311241 286.712373 \n",
       "L 175.376297 283.826077 \n",
       "L 175.441352 285.507472 \n",
       "L 175.571464 281.998214 \n",
       "L 175.701576 286.835697 \n",
       "L 175.831687 280.834507 \n",
       "L 175.896743 286.04981 \n",
       "L 175.961799 284.76453 \n",
       "L 176.026855 285.835015 \n",
       "L 176.091911 283.266707 \n",
       "L 176.156967 283.325048 \n",
       "L 176.222023 284.68118 \n",
       "L 176.287078 281.747822 \n",
       "L 176.352134 282.513735 \n",
       "L 176.41719 283.506669 \n",
       "L 176.482246 280.752234 \n",
       "L 176.547302 282.341541 \n",
       "L 176.612358 281.437625 \n",
       "L 176.807525 286.058476 \n",
       "L 176.872581 285.716556 \n",
       "L 176.937637 283.018338 \n",
       "L 177.002693 288.001829 \n",
       "L 177.067749 287.80055 \n",
       "L 177.132804 288.380366 \n",
       "L 177.262916 283.336641 \n",
       "L 177.327972 286.569894 \n",
       "L 177.393028 281.468861 \n",
       "L 177.458084 282.160982 \n",
       "L 177.52314 287.635014 \n",
       "L 177.588195 283.954164 \n",
       "L 177.653251 284.734757 \n",
       "L 177.718307 282.884712 \n",
       "L 177.783363 287.610177 \n",
       "L 177.848419 279.05926 \n",
       "L 177.913475 284.731184 \n",
       "L 177.97853 289.250349 \n",
       "L 178.043586 280.036281 \n",
       "L 178.108642 284.223339 \n",
       "L 178.173698 286.011127 \n",
       "L 178.30381 281.974066 \n",
       "L 178.368866 286.842914 \n",
       "L 178.433921 283.142594 \n",
       "L 178.564033 289.16301 \n",
       "L 178.629089 283.697329 \n",
       "L 178.694145 285.18231 \n",
       "L 178.759201 287.378064 \n",
       "L 178.824257 283.50268 \n",
       "L 178.954368 287.482362 \n",
       "L 179.08448 282.986111 \n",
       "L 179.149536 287.041839 \n",
       "L 179.214592 284.936064 \n",
       "L 179.279647 288.076325 \n",
       "L 179.344703 286.721398 \n",
       "L 179.409759 282.116387 \n",
       "L 179.539871 286.668352 \n",
       "L 179.604927 281.44764 \n",
       "L 179.669983 284.931429 \n",
       "L 179.735038 283.072115 \n",
       "L 179.800094 285.472978 \n",
       "L 179.86515 285.264899 \n",
       "L 179.930206 283.103983 \n",
       "L 179.995262 284.087145 \n",
       "L 180.125373 289.491659 \n",
       "L 180.255485 284.085093 \n",
       "L 180.320541 285.663739 \n",
       "L 180.450653 281.955428 \n",
       "L 180.515709 286.579393 \n",
       "L 180.580764 284.865097 \n",
       "L 180.64582 281.909699 \n",
       "L 180.710876 286.080688 \n",
       "L 180.840988 281.149783 \n",
       "L 180.9711 286.144409 \n",
       "L 181.101211 285.261972 \n",
       "L 181.231323 287.888104 \n",
       "L 181.361435 282.564514 \n",
       "L 181.42649 285.294571 \n",
       "L 181.491546 284.354167 \n",
       "L 181.556602 283.326181 \n",
       "L 181.621658 285.545437 \n",
       "L 181.75177 281.250824 \n",
       "L 181.816826 281.748496 \n",
       "L 181.881881 289.7531 \n",
       "L 181.946937 280.220255 \n",
       "L 182.011993 285.544777 \n",
       "L 182.077049 281.731005 \n",
       "L 182.142105 284.165874 \n",
       "L 182.207161 284.625738 \n",
       "L 182.272216 282.561731 \n",
       "L 182.337272 282.969295 \n",
       "L 182.402328 282.4664 \n",
       "L 182.467384 285.201824 \n",
       "L 182.53244 282.739607 \n",
       "L 182.597496 285.998443 \n",
       "L 182.662552 282.921386 \n",
       "L 182.727607 288.775692 \n",
       "L 182.792663 285.240851 \n",
       "L 182.857719 283.01069 \n",
       "L 182.922775 285.361965 \n",
       "L 182.987831 282.954889 \n",
       "L 183.052887 289.792328 \n",
       "L 183.117943 280.604733 \n",
       "L 183.182998 287.191263 \n",
       "L 183.248054 285.304974 \n",
       "L 183.31311 286.080085 \n",
       "L 183.378166 284.78334 \n",
       "L 183.443222 287.761523 \n",
       "L 183.508278 285.794654 \n",
       "L 183.573333 288.468795 \n",
       "L 183.638389 284.936035 \n",
       "L 183.703445 285.743516 \n",
       "L 183.768501 285.618069 \n",
       "L 183.963669 283.064654 \n",
       "L 184.028724 285.385296 \n",
       "L 184.09378 285.097526 \n",
       "L 184.158836 284.854608 \n",
       "L 184.223892 287.008766 \n",
       "L 184.288948 281.409761 \n",
       "L 184.354004 283.947234 \n",
       "L 184.419059 288.515642 \n",
       "L 184.484115 280.942737 \n",
       "L 184.549171 281.195325 \n",
       "L 184.614227 286.975464 \n",
       "L 184.679283 286.680405 \n",
       "L 184.744339 285.665662 \n",
       "L 184.809395 280.782581 \n",
       "L 184.87445 281.639578 \n",
       "L 185.004562 283.716441 \n",
       "L 185.069618 283.530114 \n",
       "L 185.134674 285.024952 \n",
       "L 185.264786 280.460374 \n",
       "L 185.459953 285.148807 \n",
       "L 185.525009 285.712108 \n",
       "L 185.720176 288.203251 \n",
       "L 185.785232 279.190576 \n",
       "L 185.850288 284.496574 \n",
       "L 185.915344 281.826938 \n",
       "L 185.9804 283.536671 \n",
       "L 186.110512 285.784481 \n",
       "L 186.175567 287.535838 \n",
       "L 186.240623 286.667377 \n",
       "L 186.305679 281.415586 \n",
       "L 186.370735 284.437445 \n",
       "L 186.435791 286.382978 \n",
       "L 186.500847 281.913947 \n",
       "L 186.565902 285.000617 \n",
       "L 186.76107 280.167252 \n",
       "L 186.826126 288.24179 \n",
       "L 186.891182 284.146188 \n",
       "L 186.956238 283.671474 \n",
       "L 187.086349 287.810895 \n",
       "L 187.151405 281.091715 \n",
       "L 187.281517 286.750238 \n",
       "L 187.346573 283.402055 \n",
       "L 187.411629 286.761286 \n",
       "L 187.476684 282.306976 \n",
       "L 187.54174 286.574973 \n",
       "L 187.606796 281.262145 \n",
       "L 187.671852 284.870607 \n",
       "L 187.736908 288.781632 \n",
       "L 187.801964 288.369419 \n",
       "L 187.867019 279.101932 \n",
       "L 187.932075 287.035167 \n",
       "L 187.997131 285.596747 \n",
       "L 188.062187 282.733925 \n",
       "L 188.192299 286.491523 \n",
       "L 188.257355 284.449727 \n",
       "L 188.32241 284.467189 \n",
       "L 188.387466 287.310067 \n",
       "L 188.452522 283.013043 \n",
       "L 188.582634 287.410104 \n",
       "L 188.712746 283.83536 \n",
       "L 188.777801 284.478381 \n",
       "L 188.972969 288.223152 \n",
       "L 189.168136 281.592257 \n",
       "L 189.233192 287.142536 \n",
       "L 189.298248 284.61865 \n",
       "L 189.363304 283.119694 \n",
       "L 189.42836 290.241618 \n",
       "L 189.493416 285.831457 \n",
       "L 189.558472 287.475647 \n",
       "L 189.623527 281.915267 \n",
       "L 189.688583 283.747778 \n",
       "L 189.753639 287.360214 \n",
       "L 189.818695 281.888995 \n",
       "L 189.883751 289.639476 \n",
       "L 189.948807 287.468989 \n",
       "L 190.013862 287.635559 \n",
       "L 190.078918 288.52418 \n",
       "L 190.274086 284.53817 \n",
       "L 190.339142 288.039694 \n",
       "L 190.404198 284.410427 \n",
       "L 190.534309 289.220318 \n",
       "L 190.599365 283.187935 \n",
       "L 190.664421 286.406782 \n",
       "L 190.794533 283.213116 \n",
       "L 190.859589 284.096801 \n",
       "L 190.924644 287.445903 \n",
       "L 191.054756 284.038547 \n",
       "L 191.119812 285.645015 \n",
       "L 191.184868 280.095382 \n",
       "L 191.249924 286.010826 \n",
       "L 191.314979 285.334431 \n",
       "L 191.510147 289.339839 \n",
       "L 191.575203 280.894842 \n",
       "L 191.640259 282.426282 \n",
       "L 191.705315 289.666379 \n",
       "L 191.77037 284.338728 \n",
       "L 191.835426 285.062387 \n",
       "L 191.900482 282.875184 \n",
       "L 192.030594 287.375194 \n",
       "L 192.160705 282.625322 \n",
       "L 192.225761 286.807832 \n",
       "L 192.290817 284.917311 \n",
       "L 192.355873 285.444411 \n",
       "L 192.420929 284.879818 \n",
       "L 192.485985 285.151274 \n",
       "L 192.681152 281.879195 \n",
       "L 192.746208 285.516597 \n",
       "L 192.811264 283.384521 \n",
       "L 192.87632 285.77052 \n",
       "L 192.941376 283.770678 \n",
       "L 193.006432 286.458422 \n",
       "L 193.071487 285.905365 \n",
       "L 193.136543 284.87946 \n",
       "L 193.201599 284.973829 \n",
       "L 193.266655 284.749392 \n",
       "L 193.396767 279.158981 \n",
       "L 193.461822 289.295187 \n",
       "L 193.526878 285.398754 \n",
       "L 193.65699 283.676567 \n",
       "L 193.852158 289.977694 \n",
       "L 194.047325 280.912218 \n",
       "L 194.112381 286.357467 \n",
       "L 194.177437 283.99642 \n",
       "L 194.372604 288.382146 \n",
       "L 194.502716 283.963333 \n",
       "L 194.632828 285.166125 \n",
       "L 194.697884 280.805624 \n",
       "L 194.893051 287.758467 \n",
       "L 194.958107 288.279713 \n",
       "L 195.023163 289.876366 \n",
       "L 195.153275 285.156296 \n",
       "L 195.21833 284.446987 \n",
       "L 195.348442 288.957155 \n",
       "L 195.413498 286.970915 \n",
       "L 195.478554 288.414085 \n",
       "L 195.54361 287.583245 \n",
       "L 195.608665 283.206301 \n",
       "L 195.673721 289.265501 \n",
       "L 195.738777 283.058542 \n",
       "L 195.803833 283.29582 \n",
       "L 195.868889 281.395871 \n",
       "L 195.933945 281.968398 \n",
       "L 195.999001 285.342179 \n",
       "L 196.064056 281.289808 \n",
       "L 196.129112 286.954788 \n",
       "L 196.194168 284.186923 \n",
       "L 196.259224 284.63967 \n",
       "L 196.32428 288.474821 \n",
       "L 196.454391 278.800947 \n",
       "L 196.584503 287.578739 \n",
       "L 196.714615 282.163091 \n",
       "L 196.779671 283.281127 \n",
       "L 196.844727 281.514202 \n",
       "L 196.909782 288.207082 \n",
       "L 196.974838 283.598972 \n",
       "L 197.10495 289.404464 \n",
       "L 197.170006 281.680973 \n",
       "L 197.235062 286.076699 \n",
       "L 197.300118 285.040175 \n",
       "L 197.365173 288.206536 \n",
       "L 197.430229 283.832763 \n",
       "L 197.495285 287.048597 \n",
       "L 197.560341 285.661932 \n",
       "L 197.625397 286.378932 \n",
       "L 197.690453 283.325507 \n",
       "L 197.755508 284.249195 \n",
       "L 197.820564 282.760426 \n",
       "L 197.950676 286.051948 \n",
       "L 198.080788 284.380296 \n",
       "L 198.275955 289.395037 \n",
       "L 198.341011 281.533859 \n",
       "L 198.406067 292.661736 \n",
       "L 198.471123 281.780937 \n",
       "L 198.536179 282.2438 \n",
       "L 198.601234 286.798018 \n",
       "L 198.66629 281.46361 \n",
       "L 198.731346 284.375833 \n",
       "L 198.796402 281.781124 \n",
       "L 198.861458 286.068506 \n",
       "L 198.926514 284.050054 \n",
       "L 198.99157 283.442316 \n",
       "L 199.056625 286.251906 \n",
       "L 199.121681 285.512221 \n",
       "L 199.186737 283.615773 \n",
       "L 199.251793 286.21219 \n",
       "L 199.381905 280.577313 \n",
       "L 199.446961 281.314889 \n",
       "L 199.512016 285.719253 \n",
       "L 199.642128 280.617661 \n",
       "L 199.77224 288.095724 \n",
       "L 199.837296 284.505614 \n",
       "L 199.902351 288.605133 \n",
       "L 200.097519 282.222063 \n",
       "L 200.162575 286.389148 \n",
       "L 200.227631 285.955298 \n",
       "L 200.292687 283.099291 \n",
       "L 200.357742 283.507616 \n",
       "L 200.422798 286.998708 \n",
       "L 200.487854 284.871181 \n",
       "L 200.55291 283.686123 \n",
       "L 200.683022 286.520536 \n",
       "L 200.748077 284.689115 \n",
       "L 200.813133 287.626936 \n",
       "L 200.878189 286.553293 \n",
       "L 200.943245 281.375009 \n",
       "L 201.008301 282.913982 \n",
       "L 201.138413 284.59136 \n",
       "L 201.203468 280.504883 \n",
       "L 201.268524 283.004104 \n",
       "L 201.33358 285.611009 \n",
       "L 201.398636 282.106085 \n",
       "L 201.463692 285.979661 \n",
       "L 201.528748 281.055213 \n",
       "L 201.593804 285.142221 \n",
       "L 201.658859 280.131009 \n",
       "L 201.723915 287.247767 \n",
       "L 201.788971 285.482979 \n",
       "L 201.854027 280.830231 \n",
       "L 201.919083 283.756889 \n",
       "L 202.11425 288.145915 \n",
       "L 202.244362 285.178221 \n",
       "L 202.374474 289.958152 \n",
       "L 202.504585 283.502709 \n",
       "L 202.569641 287.64533 \n",
       "L 202.634697 286.29785 \n",
       "L 202.764809 281.923675 \n",
       "L 202.829865 285.842907 \n",
       "L 202.89492 280.664781 \n",
       "L 202.959976 282.803974 \n",
       "L 203.025032 283.470683 \n",
       "L 203.090088 285.598024 \n",
       "L 203.155144 281.238614 \n",
       "L 203.2202 287.05108 \n",
       "L 203.285256 280.059669 \n",
       "L 203.350311 282.72876 \n",
       "L 203.415367 285.156153 \n",
       "L 203.480423 278.356235 \n",
       "L 203.545479 282.016279 \n",
       "L 203.675591 287.750231 \n",
       "L 203.740647 284.999627 \n",
       "L 203.805702 287.097281 \n",
       "L 203.870758 285.350444 \n",
       "L 203.935814 287.081154 \n",
       "L 204.00087 283.330256 \n",
       "L 204.065926 285.41283 \n",
       "L 204.130982 285.75511 \n",
       "L 204.196037 281.456522 \n",
       "L 204.261093 284.821952 \n",
       "L 204.326149 281.605845 \n",
       "L 204.456261 287.002439 \n",
       "L 204.586373 281.283294 \n",
       "L 204.78154 286.244216 \n",
       "L 204.911652 282.830862 \n",
       "L 205.041763 287.994181 \n",
       "L 205.106819 284.941502 \n",
       "L 205.171875 286.211602 \n",
       "L 205.236931 285.946029 \n",
       "L 205.301987 288.832439 \n",
       "L 205.432099 281.821012 \n",
       "L 205.497154 284.077704 \n",
       "L 205.627266 288.975004 \n",
       "L 205.757378 281.028281 \n",
       "L 205.88749 283.269089 \n",
       "L 206.017601 286.370366 \n",
       "L 206.082657 285.691547 \n",
       "L 206.147713 285.643752 \n",
       "L 206.212769 284.604474 \n",
       "L 206.277825 287.941365 \n",
       "L 206.34288 283.612803 \n",
       "L 206.407936 283.662664 \n",
       "L 206.538048 286.757785 \n",
       "L 206.603104 286.591316 \n",
       "L 206.66816 282.860721 \n",
       "L 206.733216 284.60555 \n",
       "L 206.798271 284.527007 \n",
       "L 206.863327 287.436318 \n",
       "L 206.928383 286.380797 \n",
       "L 207.058495 287.698462 \n",
       "L 207.188607 285.156813 \n",
       "L 207.253662 287.984209 \n",
       "L 207.318718 286.107433 \n",
       "L 207.44883 285.35769 \n",
       "L 207.513886 287.243778 \n",
       "L 207.578942 281.817468 \n",
       "L 207.643997 286.026537 \n",
       "L 207.709053 284.746551 \n",
       "L 207.774109 286.864178 \n",
       "L 207.839165 284.116545 \n",
       "L 207.904221 289.733055 \n",
       "L 207.969277 281.104959 \n",
       "L 208.034333 285.387247 \n",
       "L 208.294556 283.102663 \n",
       "L 208.359612 287.740058 \n",
       "L 208.424668 287.715063 \n",
       "L 208.554779 281.002024 \n",
       "L 208.619835 289.88905 \n",
       "L 208.684891 285.939285 \n",
       "L 208.749947 288.225447 \n",
       "L 208.815003 284.538687 \n",
       "L 208.880059 285.618198 \n",
       "L 209.01017 281.478173 \n",
       "L 209.075226 282.839773 \n",
       "L 209.140282 284.885658 \n",
       "L 209.205338 283.931767 \n",
       "L 209.270394 280.998222 \n",
       "L 209.33545 284.857205 \n",
       "L 209.400505 283.875564 \n",
       "L 209.465561 285.003515 \n",
       "L 209.530617 283.220003 \n",
       "L 209.595673 285.906685 \n",
       "L 209.660729 284.391989 \n",
       "L 209.725785 282.257489 \n",
       "L 209.79084 287.274813 \n",
       "L 209.855896 284.067287 \n",
       "L 209.920952 281.637641 \n",
       "L 209.986008 285.506238 \n",
       "L 210.051064 284.035333 \n",
       "L 210.11612 284.530953 \n",
       "L 210.181176 283.232558 \n",
       "L 210.441399 287.077983 \n",
       "L 210.506455 283.989677 \n",
       "L 210.571511 288.23948 \n",
       "L 210.636566 284.761602 \n",
       "L 210.701622 289.899639 \n",
       "L 210.766678 288.269511 \n",
       "L 210.831734 285.409329 \n",
       "L 210.961846 289.875764 \n",
       "L 211.091957 284.502271 \n",
       "L 211.222069 285.979001 \n",
       "L 211.287125 284.658122 \n",
       "L 211.417237 286.940339 \n",
       "L 211.482293 284.91269 \n",
       "L 211.547348 288.547553 \n",
       "L 211.612404 286.444934 \n",
       "L 211.67746 288.093486 \n",
       "L 211.872628 281.018539 \n",
       "L 212.002739 287.29216 \n",
       "L 212.067795 287.142048 \n",
       "L 212.197907 289.712207 \n",
       "L 212.328019 284.598605 \n",
       "L 212.393074 286.228059 \n",
       "L 212.45813 286.055707 \n",
       "L 212.588242 288.792938 \n",
       "L 212.653298 285.194578 \n",
       "L 212.718354 286.497794 \n",
       "L 212.783409 289.01792 \n",
       "L 212.848465 285.72241 \n",
       "L 212.913521 286.507967 \n",
       "L 212.978577 287.74155 \n",
       "L 213.043633 286.983958 \n",
       "L 213.108689 283.073607 \n",
       "L 213.303856 289.012382 \n",
       "L 213.433968 282.81297 \n",
       "L 213.499024 285.118991 \n",
       "L 213.56408 281.741135 \n",
       "L 213.759247 288.022074 \n",
       "L 213.954415 284.457102 \n",
       "L 214.019471 281.861604 \n",
       "L 214.084526 285.774609 \n",
       "L 214.149582 285.451269 \n",
       "L 214.279694 284.419639 \n",
       "L 214.409806 287.74013 \n",
       "L 214.539917 287.02983 \n",
       "L 214.735085 280.46752 \n",
       "L 214.800141 287.669435 \n",
       "L 214.865197 285.2619 \n",
       "L 214.930252 285.198825 \n",
       "L 214.995308 283.446578 \n",
       "L 215.060364 285.694445 \n",
       "L 215.12542 284.176061 \n",
       "L 215.190476 284.928387 \n",
       "L 215.255532 280.485197 \n",
       "L 215.320588 288.520736 \n",
       "L 215.385643 282.914355 \n",
       "L 215.645867 286.560223 \n",
       "L 215.710923 287.371076 \n",
       "L 215.775979 286.437975 \n",
       "L 215.971146 281.442102 \n",
       "L 216.101258 288.982092 \n",
       "L 216.166314 284.040785 \n",
       "L 216.296425 288.564212 \n",
       "L 216.361481 281.599905 \n",
       "L 216.426537 286.506015 \n",
       "L 216.621705 282.150407 \n",
       "L 216.621705 282.150407 \n",
       "\" clip-path=\"url(#pb8b62f9e74)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 56.50625 299.078125 \n",
       "L 56.50625 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 224.24625 299.078125 \n",
       "L 224.24625 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 56.50625 299.078125 \n",
       "L 224.24625 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 56.50625 189.718125 \n",
       "L 224.24625 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_39\">\n",
       "    <!-- loss curve -->\n",
       "    <g transform=\"translate(110.06875 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"193.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"224.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"279.931641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"343.310547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"384.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"443.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_3\">\n",
       "    <g id=\"patch_19\">\n",
       "     <path d=\"M 137.655625 227.630625 \n",
       "L 217.24625 227.630625 \n",
       "Q 219.24625 227.630625 219.24625 225.630625 \n",
       "L 219.24625 196.718125 \n",
       "Q 219.24625 194.718125 217.24625 194.718125 \n",
       "L 137.655625 194.718125 \n",
       "Q 135.655625 194.718125 135.655625 196.718125 \n",
       "L 135.655625 225.630625 \n",
       "Q 135.655625 227.630625 137.655625 227.630625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_37\">\n",
       "     <path d=\"M 139.655625 202.816562 \n",
       "L 149.655625 202.816562 \n",
       "L 159.655625 202.816562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_40\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(167.655625 206.316562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_38\">\n",
       "     <path d=\"M 139.655625 217.772812 \n",
       "L 149.655625 217.772812 \n",
       "L 159.655625 217.772812 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_41\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(167.655625 221.272812) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 278.35625 299.078125 \n",
       "L 446.09625 299.078125 \n",
       "L 446.09625 189.718125 \n",
       "L 278.35625 189.718125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_7\">\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"285.980795\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_42\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(282.799545 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_40\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"351.036644\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_43\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(338.311644 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m336d9ce2f4\" x=\"416.092493\" y=\"299.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_44\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(403.367493 313.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_45\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(351.410625 327.354687) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_8\">\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_42\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"293.528081\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_46\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(255.453125 297.3273) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_43\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"268.818319\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_47\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(255.453125 272.617538) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_44\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"244.108557\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_48\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(255.453125 247.907776) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_17\">\n",
       "     <g id=\"line2d_45\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"219.398796\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_49\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(255.453125 223.198015) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_18\">\n",
       "     <g id=\"line2d_46\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m851cf65a3e\" x=\"278.35625\" y=\"194.689034\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_50\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(255.453125 198.488253) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_51\">\n",
       "     <!-- acc -->\n",
       "     <g transform=\"translate(249.373437 252.960625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_47\">\n",
       "    <path d=\"M 285.980795 202.410835 \n",
       "L 286.110907 208.202185 \n",
       "L 286.175963 212.063085 \n",
       "L 286.241019 210.132635 \n",
       "L 286.306075 208.202185 \n",
       "L 286.371131 199.515159 \n",
       "L 286.566298 215.923986 \n",
       "L 286.761466 206.271735 \n",
       "L 286.956633 213.993535 \n",
       "L 287.086745 207.23696 \n",
       "L 287.151801 212.063085 \n",
       "L 287.216857 204.341285 \n",
       "L 287.281912 208.202185 \n",
       "L 287.346968 203.37606 \n",
       "L 287.412024 205.30651 \n",
       "L 287.47708 203.37606 \n",
       "L 287.542136 208.202185 \n",
       "L 287.607192 206.271735 \n",
       "L 287.672248 209.16741 \n",
       "L 287.802359 204.341285 \n",
       "L 287.867415 217.854436 \n",
       "L 287.932471 210.132635 \n",
       "L 288.062583 208.202185 \n",
       "L 288.127638 213.02831 \n",
       "L 288.192694 210.132635 \n",
       "L 288.322806 207.23696 \n",
       "L 288.452918 212.063085 \n",
       "L 288.648085 198.549934 \n",
       "L 288.713141 196.619484 \n",
       "L 288.778197 202.410835 \n",
       "L 288.843253 200.480384 \n",
       "L 289.03842 209.16741 \n",
       "L 289.168532 199.515159 \n",
       "L 289.233588 201.44561 \n",
       "L 289.298644 209.16741 \n",
       "L 289.3637 204.341285 \n",
       "L 289.493811 198.549934 \n",
       "L 289.623923 205.30651 \n",
       "L 289.688979 197.584709 \n",
       "L 289.754035 207.23696 \n",
       "L 289.819091 196.619484 \n",
       "L 289.884146 197.584709 \n",
       "L 289.949202 217.854436 \n",
       "L 290.079314 200.480384 \n",
       "L 290.14437 206.271735 \n",
       "L 290.209426 203.37606 \n",
       "L 290.274481 201.44561 \n",
       "L 290.404593 194.689034 \n",
       "L 290.469649 194.689034 \n",
       "L 290.534705 201.44561 \n",
       "L 290.599761 200.480384 \n",
       "L 290.664817 197.584709 \n",
       "L 290.92504 213.02831 \n",
       "L 291.055152 210.132635 \n",
       "L 291.120207 200.480384 \n",
       "L 291.185263 202.410835 \n",
       "L 291.380431 208.202185 \n",
       "L 291.510543 200.480384 \n",
       "L 291.70571 212.063085 \n",
       "L 291.900878 205.30651 \n",
       "L 292.096045 213.02831 \n",
       "L 292.226157 203.37606 \n",
       "L 292.291213 208.202185 \n",
       "L 292.356269 202.410835 \n",
       "L 292.421324 209.16741 \n",
       "L 292.48638 201.44561 \n",
       "L 292.551436 206.271735 \n",
       "L 292.616492 200.480384 \n",
       "L 292.681548 208.202185 \n",
       "L 292.746604 199.515159 \n",
       "L 292.81166 203.37606 \n",
       "L 292.876715 205.30651 \n",
       "L 292.941771 200.480384 \n",
       "L 293.006827 216.889211 \n",
       "L 293.071883 208.202185 \n",
       "L 293.201995 209.16741 \n",
       "L 293.26705 209.16741 \n",
       "L 293.462218 201.44561 \n",
       "L 293.527274 204.341285 \n",
       "L 293.59233 207.23696 \n",
       "L 293.787497 195.654259 \n",
       "L 293.852553 197.584709 \n",
       "L 293.982665 202.410835 \n",
       "L 294.047721 201.44561 \n",
       "L 294.112777 203.37606 \n",
       "L 294.177832 209.16741 \n",
       "L 294.307944 198.549934 \n",
       "L 294.438056 208.202185 \n",
       "L 294.633223 196.619484 \n",
       "L 294.763335 199.515159 \n",
       "L 294.828391 195.654259 \n",
       "L 294.893447 203.37606 \n",
       "L 295.023558 197.584709 \n",
       "L 295.088614 218.819661 \n",
       "L 295.15367 198.549934 \n",
       "L 295.218726 201.44561 \n",
       "L 295.283782 201.44561 \n",
       "L 295.348838 200.480384 \n",
       "L 295.544005 194.689034 \n",
       "L 295.609061 195.654259 \n",
       "L 295.674117 205.30651 \n",
       "L 295.739173 198.549934 \n",
       "L 295.804229 195.654259 \n",
       "L 296.129508 211.09786 \n",
       "L 296.25962 200.480384 \n",
       "L 296.389731 207.23696 \n",
       "L 296.454787 206.271735 \n",
       "L 296.519843 207.23696 \n",
       "L 296.649955 198.549934 \n",
       "L 296.845122 213.993535 \n",
       "L 296.910178 204.341285 \n",
       "L 296.975234 207.23696 \n",
       "L 297.04029 203.37606 \n",
       "L 297.105346 210.132635 \n",
       "L 297.170401 206.271735 \n",
       "L 297.235457 209.16741 \n",
       "L 297.300513 203.37606 \n",
       "L 297.365569 205.30651 \n",
       "L 297.430625 205.30651 \n",
       "L 297.495681 199.515159 \n",
       "L 297.560737 206.271735 \n",
       "L 297.625792 202.410835 \n",
       "L 297.690848 203.37606 \n",
       "L 297.755904 198.549934 \n",
       "L 297.82096 206.271735 \n",
       "L 297.886016 202.410835 \n",
       "L 298.016127 204.341285 \n",
       "L 298.081183 199.515159 \n",
       "L 298.146239 217.854436 \n",
       "L 298.211295 209.16741 \n",
       "L 298.341407 206.271735 \n",
       "L 298.406463 211.09786 \n",
       "L 298.471518 208.202185 \n",
       "L 298.536574 207.23696 \n",
       "L 298.666686 203.37606 \n",
       "L 298.731742 208.202185 \n",
       "L 298.991965 196.619484 \n",
       "L 299.317244 208.202185 \n",
       "L 299.447356 197.584709 \n",
       "L 299.577468 208.202185 \n",
       "L 299.772635 196.619484 \n",
       "L 299.837691 196.619484 \n",
       "L 299.902747 198.549934 \n",
       "L 299.967803 195.654259 \n",
       "L 300.032859 204.341285 \n",
       "L 300.16297 196.619484 \n",
       "L 300.228026 211.09786 \n",
       "L 300.293082 198.549934 \n",
       "L 300.358138 201.44561 \n",
       "L 300.423194 204.341285 \n",
       "L 300.618361 195.654259 \n",
       "L 300.748473 194.689034 \n",
       "L 300.813529 204.341285 \n",
       "L 300.878585 198.549934 \n",
       "L 300.943641 197.584709 \n",
       "L 301.073752 199.515159 \n",
       "L 301.26892 211.09786 \n",
       "L 301.333976 210.132635 \n",
       "L 301.399032 199.515159 \n",
       "L 301.464087 203.37606 \n",
       "L 301.529143 203.37606 \n",
       "L 301.659255 208.202185 \n",
       "L 301.789367 198.549934 \n",
       "L 301.854423 207.23696 \n",
       "L 301.919478 206.271735 \n",
       "L 301.984534 209.16741 \n",
       "L 302.04959 204.341285 \n",
       "L 302.114646 206.271735 \n",
       "L 302.179702 204.341285 \n",
       "L 302.244758 209.16741 \n",
       "L 302.309813 206.271735 \n",
       "L 302.374869 209.16741 \n",
       "L 302.504981 204.341285 \n",
       "L 302.570037 208.202185 \n",
       "L 302.635093 201.44561 \n",
       "L 302.700149 206.271735 \n",
       "L 302.895316 198.549934 \n",
       "L 302.960372 206.271735 \n",
       "L 303.025428 201.44561 \n",
       "L 303.155539 203.37606 \n",
       "L 303.220595 198.549934 \n",
       "L 303.285651 216.889211 \n",
       "L 303.350707 210.132635 \n",
       "L 303.415763 207.23696 \n",
       "L 303.480819 209.16741 \n",
       "L 303.545875 210.132635 \n",
       "L 303.61093 209.16741 \n",
       "L 303.741042 203.37606 \n",
       "L 303.806098 204.341285 \n",
       "L 303.871154 207.23696 \n",
       "L 304.131377 195.654259 \n",
       "L 304.261489 198.549934 \n",
       "L 304.326545 197.584709 \n",
       "L 304.456656 209.16741 \n",
       "L 304.586768 196.619484 \n",
       "L 304.71688 207.23696 \n",
       "L 304.912047 196.619484 \n",
       "L 304.977103 196.619484 \n",
       "L 305.042159 200.480384 \n",
       "L 305.107215 195.654259 \n",
       "L 305.172271 204.341285 \n",
       "L 305.302382 196.619484 \n",
       "L 305.367438 213.02831 \n",
       "L 305.432494 198.549934 \n",
       "L 305.49755 200.480384 \n",
       "L 305.562606 206.271735 \n",
       "L 305.757773 195.654259 \n",
       "L 305.822829 194.689034 \n",
       "L 305.887885 195.654259 \n",
       "L 305.952941 203.37606 \n",
       "L 306.017997 199.515159 \n",
       "L 306.083053 195.654259 \n",
       "L 306.148109 196.619484 \n",
       "L 306.213164 198.549934 \n",
       "L 306.343276 211.09786 \n",
       "L 306.408332 210.132635 \n",
       "L 306.538444 198.549934 \n",
       "L 306.668555 205.30651 \n",
       "L 306.733611 204.341285 \n",
       "L 306.798667 206.271735 \n",
       "L 306.863723 205.30651 \n",
       "L 306.928779 200.480384 \n",
       "L 307.123946 211.09786 \n",
       "L 307.319114 203.37606 \n",
       "L 307.38417 210.132635 \n",
       "L 307.449225 209.16741 \n",
       "L 307.579337 204.341285 \n",
       "L 307.644393 204.341285 \n",
       "L 307.709449 208.202185 \n",
       "L 307.774505 200.480384 \n",
       "L 307.839561 206.271735 \n",
       "L 307.904616 201.44561 \n",
       "L 307.969672 205.30651 \n",
       "L 308.034728 200.480384 \n",
       "L 308.099784 206.271735 \n",
       "L 308.16484 200.480384 \n",
       "L 308.229896 203.37606 \n",
       "L 308.294952 203.37606 \n",
       "L 308.360007 199.515159 \n",
       "L 308.425063 216.889211 \n",
       "L 308.490119 210.132635 \n",
       "L 308.620231 204.341285 \n",
       "L 308.685287 207.23696 \n",
       "L 308.880454 202.410835 \n",
       "L 309.010566 208.202185 \n",
       "L 309.270789 196.619484 \n",
       "L 309.596068 205.30651 \n",
       "L 309.72618 196.619484 \n",
       "L 309.856292 206.271735 \n",
       "L 310.051459 196.619484 \n",
       "L 310.116515 195.654259 \n",
       "L 310.181571 197.584709 \n",
       "L 310.246627 195.654259 \n",
       "L 310.311683 204.341285 \n",
       "L 310.376739 194.689034 \n",
       "L 310.441795 196.619484 \n",
       "L 310.50685 207.23696 \n",
       "L 310.571906 198.549934 \n",
       "L 310.636962 201.44561 \n",
       "L 310.702018 201.44561 \n",
       "L 310.962241 194.689034 \n",
       "L 311.027297 195.654259 \n",
       "L 311.092353 203.37606 \n",
       "L 311.157409 199.515159 \n",
       "L 311.222465 196.619484 \n",
       "L 311.287521 197.584709 \n",
       "L 311.352576 198.549934 \n",
       "L 311.482688 211.09786 \n",
       "L 311.547744 210.132635 \n",
       "L 311.6128 194.689034 \n",
       "L 311.6128 194.689034 \n",
       "\" clip-path=\"url(#pe8097749a7)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_48\">\n",
       "    <path d=\"M 285.980795 292.176766 \n",
       "L 286.045851 294.107216 \n",
       "L 286.241019 272.872264 \n",
       "L 286.306075 275.76794 \n",
       "L 286.371131 263.220014 \n",
       "L 286.436186 264.185239 \n",
       "L 286.891577 243.915512 \n",
       "L 286.956633 254.532988 \n",
       "L 287.021689 248.741638 \n",
       "L 287.216857 244.880738 \n",
       "L 287.281912 245.845963 \n",
       "L 287.346968 235.228487 \n",
       "L 287.412024 249.706863 \n",
       "L 287.47708 242.950287 \n",
       "L 287.607192 236.193712 \n",
       "L 287.737303 251.637313 \n",
       "L 287.802359 246.811188 \n",
       "L 287.932471 232.332812 \n",
       "L 287.997527 228.471911 \n",
       "L 288.062583 233.298037 \n",
       "L 288.127638 225.576236 \n",
       "L 288.192694 229.437136 \n",
       "L 288.25775 234.263262 \n",
       "L 288.322806 231.367587 \n",
       "L 288.387862 229.437136 \n",
       "L 288.452918 223.645786 \n",
       "L 288.583029 235.228487 \n",
       "L 288.648085 226.541461 \n",
       "L 288.713141 235.228487 \n",
       "L 288.843253 219.784886 \n",
       "L 288.908309 228.471911 \n",
       "L 288.973364 222.680561 \n",
       "L 289.03842 224.611011 \n",
       "L 289.103476 223.645786 \n",
       "L 289.233588 220.750111 \n",
       "L 289.298644 231.367587 \n",
       "L 289.3637 224.611011 \n",
       "L 289.428755 222.680561 \n",
       "L 289.493811 225.576236 \n",
       "L 289.558867 235.228487 \n",
       "L 289.623923 222.680561 \n",
       "L 289.688979 229.437136 \n",
       "L 289.754035 230.402362 \n",
       "L 289.819091 219.784886 \n",
       "L 289.884146 224.611011 \n",
       "L 289.949202 217.854436 \n",
       "L 290.014258 223.645786 \n",
       "L 290.079314 219.784886 \n",
       "L 290.209426 222.680561 \n",
       "L 290.274481 221.715336 \n",
       "L 290.339537 219.784886 \n",
       "L 290.404593 226.541461 \n",
       "L 290.469649 216.889211 \n",
       "L 290.534705 221.715336 \n",
       "L 290.599761 215.923986 \n",
       "L 290.664817 218.819661 \n",
       "L 290.729872 218.819661 \n",
       "L 290.794928 211.09786 \n",
       "L 290.859984 217.854436 \n",
       "L 290.92504 214.95876 \n",
       "L 290.990096 214.95876 \n",
       "L 291.055152 213.02831 \n",
       "L 291.120207 219.784886 \n",
       "L 291.185263 217.854436 \n",
       "L 291.250319 207.23696 \n",
       "L 291.315375 220.750111 \n",
       "L 291.380431 219.784886 \n",
       "L 291.445487 213.993535 \n",
       "L 291.510543 227.506686 \n",
       "L 291.70571 211.09786 \n",
       "L 291.835822 215.923986 \n",
       "L 291.965934 211.09786 \n",
       "L 292.096045 214.95876 \n",
       "L 292.161101 215.923986 \n",
       "L 292.226157 213.02831 \n",
       "L 292.291213 217.854436 \n",
       "L 292.356269 206.271735 \n",
       "L 292.421324 220.750111 \n",
       "L 292.48638 212.063085 \n",
       "L 292.681548 209.16741 \n",
       "L 292.81166 222.680561 \n",
       "L 292.876715 215.923986 \n",
       "L 292.941771 212.063085 \n",
       "L 293.006827 218.819661 \n",
       "L 293.071883 214.95876 \n",
       "L 293.201995 212.063085 \n",
       "L 293.26705 214.95876 \n",
       "L 293.332106 204.341285 \n",
       "L 293.462218 215.923986 \n",
       "L 293.527274 208.202185 \n",
       "L 293.657386 216.889211 \n",
       "L 293.787497 209.16741 \n",
       "L 293.982665 215.923986 \n",
       "L 294.047721 207.23696 \n",
       "L 294.112777 215.923986 \n",
       "L 294.177832 212.063085 \n",
       "L 294.242888 211.09786 \n",
       "L 294.307944 208.202185 \n",
       "L 294.373 213.993535 \n",
       "L 294.438056 211.09786 \n",
       "L 294.503112 208.202185 \n",
       "L 294.568167 212.063085 \n",
       "L 294.633223 209.16741 \n",
       "L 294.698279 215.923986 \n",
       "L 294.763335 208.202185 \n",
       "L 294.828391 224.611011 \n",
       "L 294.958503 204.341285 \n",
       "L 295.023558 205.30651 \n",
       "L 295.088614 212.063085 \n",
       "L 295.15367 209.16741 \n",
       "L 295.218726 206.271735 \n",
       "L 295.283782 215.923986 \n",
       "L 295.348838 207.23696 \n",
       "L 295.413893 216.889211 \n",
       "L 295.478949 210.132635 \n",
       "L 295.544005 204.341285 \n",
       "L 295.609061 213.02831 \n",
       "L 295.674117 211.09786 \n",
       "L 295.804229 207.23696 \n",
       "L 295.869284 213.993535 \n",
       "L 295.93434 205.30651 \n",
       "L 295.999396 211.09786 \n",
       "L 296.129508 212.063085 \n",
       "L 296.194564 218.819661 \n",
       "L 296.25962 215.923986 \n",
       "L 296.324675 219.784886 \n",
       "L 296.454787 202.410835 \n",
       "L 296.519843 208.202185 \n",
       "L 296.649955 212.063085 \n",
       "L 296.71501 210.132635 \n",
       "L 296.780066 203.37606 \n",
       "L 296.910178 213.02831 \n",
       "L 296.975234 209.16741 \n",
       "L 297.04029 210.132635 \n",
       "L 297.105346 213.993535 \n",
       "L 297.170401 212.063085 \n",
       "L 297.430625 208.202185 \n",
       "L 297.560737 213.02831 \n",
       "L 297.625792 206.271735 \n",
       "L 297.690848 209.16741 \n",
       "L 297.755904 206.271735 \n",
       "L 297.82096 216.889211 \n",
       "L 297.886016 213.02831 \n",
       "L 298.016127 209.16741 \n",
       "L 298.146239 212.063085 \n",
       "L 298.276351 201.44561 \n",
       "L 298.471518 211.09786 \n",
       "L 298.536574 211.09786 \n",
       "L 298.60163 209.16741 \n",
       "L 298.666686 203.37606 \n",
       "L 298.861853 216.889211 \n",
       "L 298.991965 207.23696 \n",
       "L 299.057021 212.063085 \n",
       "L 299.122077 205.30651 \n",
       "L 299.187133 206.271735 \n",
       "L 299.252189 213.02831 \n",
       "L 299.317244 210.132635 \n",
       "L 299.3823 206.271735 \n",
       "L 299.577468 210.132635 \n",
       "L 299.642524 208.202185 \n",
       "L 299.70758 209.16741 \n",
       "L 299.772635 214.95876 \n",
       "L 299.902747 206.271735 \n",
       "L 299.967803 214.95876 \n",
       "L 300.032859 208.202185 \n",
       "L 300.097915 213.02831 \n",
       "L 300.228026 207.23696 \n",
       "L 300.423194 210.132635 \n",
       "L 300.48825 213.993535 \n",
       "L 300.553306 210.132635 \n",
       "L 300.618361 211.09786 \n",
       "L 300.748473 215.923986 \n",
       "L 300.878585 209.16741 \n",
       "L 300.943641 211.09786 \n",
       "L 301.073752 202.410835 \n",
       "L 301.203864 212.063085 \n",
       "L 301.333976 208.202185 \n",
       "L 301.464087 213.993535 \n",
       "L 301.529143 211.09786 \n",
       "L 301.724311 207.23696 \n",
       "L 301.789367 208.202185 \n",
       "L 301.854423 213.02831 \n",
       "L 301.919478 210.132635 \n",
       "L 301.984534 208.202185 \n",
       "L 302.04959 213.993535 \n",
       "L 302.114646 209.16741 \n",
       "L 302.179702 213.02831 \n",
       "L 302.309813 208.202185 \n",
       "L 302.374869 210.132635 \n",
       "L 302.504981 204.341285 \n",
       "L 302.570037 211.09786 \n",
       "L 302.700149 203.37606 \n",
       "L 302.765204 204.341285 \n",
       "L 302.895316 212.063085 \n",
       "L 303.090484 206.271735 \n",
       "L 303.155539 206.271735 \n",
       "L 303.350707 212.063085 \n",
       "L 303.415763 207.23696 \n",
       "L 303.480819 210.132635 \n",
       "L 303.545875 213.02831 \n",
       "L 303.61093 211.09786 \n",
       "L 303.675986 211.09786 \n",
       "L 303.806098 213.02831 \n",
       "L 303.871154 204.341285 \n",
       "L 303.93621 213.993535 \n",
       "L 304.001266 210.132635 \n",
       "L 304.131377 205.30651 \n",
       "L 304.196433 211.09786 \n",
       "L 304.261489 202.410835 \n",
       "L 304.326545 208.202185 \n",
       "L 304.391601 202.410835 \n",
       "L 304.521712 210.132635 \n",
       "L 304.586768 207.23696 \n",
       "L 304.651824 208.202185 \n",
       "L 304.71688 207.23696 \n",
       "L 304.781936 203.37606 \n",
       "L 304.977103 212.063085 \n",
       "L 305.042159 209.16741 \n",
       "L 305.107215 210.132635 \n",
       "L 305.172271 213.02831 \n",
       "L 305.237327 208.202185 \n",
       "L 305.302382 213.993535 \n",
       "L 305.367438 211.09786 \n",
       "L 305.432494 201.44561 \n",
       "L 305.49755 216.889211 \n",
       "L 305.562606 207.23696 \n",
       "L 305.627662 204.341285 \n",
       "L 305.692718 207.23696 \n",
       "L 305.757773 203.37606 \n",
       "L 305.887885 211.09786 \n",
       "L 305.952941 208.202185 \n",
       "L 306.017997 210.132635 \n",
       "L 306.148109 207.23696 \n",
       "L 306.213164 210.132635 \n",
       "L 306.343276 202.410835 \n",
       "L 306.408332 209.16741 \n",
       "L 306.473388 205.30651 \n",
       "L 306.538444 207.23696 \n",
       "L 306.668555 203.37606 \n",
       "L 306.733611 212.063085 \n",
       "L 306.798667 208.202185 \n",
       "L 306.863723 209.16741 \n",
       "L 306.928779 208.202185 \n",
       "L 306.993835 213.02831 \n",
       "L 307.123946 207.23696 \n",
       "L 307.189002 208.202185 \n",
       "L 307.254058 204.341285 \n",
       "L 307.319114 213.993535 \n",
       "L 307.38417 208.202185 \n",
       "L 307.514281 212.063085 \n",
       "L 307.709449 201.44561 \n",
       "L 307.774505 213.993535 \n",
       "L 307.904616 204.341285 \n",
       "L 308.034728 202.410835 \n",
       "L 308.099784 214.95876 \n",
       "L 308.16484 210.132635 \n",
       "L 308.229896 210.132635 \n",
       "L 308.294952 209.16741 \n",
       "L 308.360007 213.993535 \n",
       "L 308.425063 202.410835 \n",
       "L 308.490119 209.16741 \n",
       "L 308.555175 204.341285 \n",
       "L 308.620231 211.09786 \n",
       "L 308.685287 205.30651 \n",
       "L 308.750342 207.23696 \n",
       "L 308.815398 212.063085 \n",
       "L 308.94551 201.44561 \n",
       "L 309.140678 213.993535 \n",
       "L 309.205733 205.30651 \n",
       "L 309.270789 206.271735 \n",
       "L 309.400901 211.09786 \n",
       "L 309.465957 209.16741 \n",
       "L 309.531013 205.30651 \n",
       "L 309.661124 211.09786 \n",
       "L 309.791236 204.341285 \n",
       "L 309.921348 211.09786 \n",
       "L 310.051459 203.37606 \n",
       "L 310.116515 210.132635 \n",
       "L 310.181571 206.271735 \n",
       "L 310.246627 206.271735 \n",
       "L 310.311683 200.480384 \n",
       "L 310.50685 210.132635 \n",
       "L 310.767074 205.30651 \n",
       "L 310.962241 212.063085 \n",
       "L 311.027297 205.30651 \n",
       "L 311.092353 207.23696 \n",
       "L 311.157409 205.30651 \n",
       "L 311.352576 215.923986 \n",
       "L 311.417632 208.202185 \n",
       "L 311.482688 209.16741 \n",
       "L 311.547744 210.132635 \n",
       "L 311.6128 202.410835 \n",
       "L 311.742911 212.063085 \n",
       "L 311.807967 212.063085 \n",
       "L 311.938079 203.37606 \n",
       "L 312.068191 211.09786 \n",
       "L 312.198302 202.410835 \n",
       "L 312.263358 204.341285 \n",
       "L 312.328414 205.30651 \n",
       "L 312.39347 211.09786 \n",
       "L 312.458526 207.23696 \n",
       "L 312.523582 207.23696 \n",
       "L 312.588638 209.16741 \n",
       "L 312.653693 207.23696 \n",
       "L 312.783805 213.993535 \n",
       "L 312.978973 204.341285 \n",
       "L 313.109084 206.271735 \n",
       "L 313.17414 205.30651 \n",
       "L 313.239196 212.063085 \n",
       "L 313.304252 207.23696 \n",
       "L 313.369308 211.09786 \n",
       "L 313.434364 202.410835 \n",
       "L 313.499419 209.16741 \n",
       "L 313.564475 204.341285 \n",
       "L 313.759643 213.02831 \n",
       "L 313.824699 209.16741 \n",
       "L 313.889754 212.063085 \n",
       "L 313.95481 211.09786 \n",
       "L 314.084922 209.16741 \n",
       "L 314.149978 204.341285 \n",
       "L 314.215034 207.23696 \n",
       "L 314.28009 208.202185 \n",
       "L 314.345145 215.923986 \n",
       "L 314.540313 200.480384 \n",
       "L 314.605369 208.202185 \n",
       "L 314.670425 205.30651 \n",
       "L 314.735481 209.16741 \n",
       "L 314.800536 206.271735 \n",
       "L 314.930648 209.16741 \n",
       "L 315.06076 204.341285 \n",
       "L 315.255927 213.993535 \n",
       "L 315.320983 205.30651 \n",
       "L 315.386039 209.16741 \n",
       "L 315.516151 204.341285 \n",
       "L 315.581207 206.271735 \n",
       "L 315.646262 204.341285 \n",
       "L 315.711318 212.063085 \n",
       "L 315.776374 208.202185 \n",
       "L 315.84143 205.30651 \n",
       "L 315.906486 211.09786 \n",
       "L 315.971542 210.132635 \n",
       "L 316.036598 203.37606 \n",
       "L 316.101653 208.202185 \n",
       "L 316.166709 204.341285 \n",
       "L 316.296821 209.16741 \n",
       "L 316.426933 203.697796 \n",
       "L 316.557044 208.202185 \n",
       "L 316.6221 202.410835 \n",
       "L 316.687156 205.30651 \n",
       "L 316.817268 211.09786 \n",
       "L 316.947379 205.30651 \n",
       "L 317.012435 205.30651 \n",
       "L 317.272659 213.993535 \n",
       "L 317.337714 205.30651 \n",
       "L 317.40277 206.271735 \n",
       "L 317.467826 206.271735 \n",
       "L 317.532882 210.132635 \n",
       "L 317.72805 202.410835 \n",
       "L 317.858161 207.23696 \n",
       "L 317.988273 205.30651 \n",
       "L 318.053329 202.410835 \n",
       "L 318.248496 213.993535 \n",
       "L 318.313552 206.271735 \n",
       "L 318.378608 213.02831 \n",
       "L 318.50872 204.341285 \n",
       "L 318.573776 213.02831 \n",
       "L 318.638831 202.410835 \n",
       "L 318.703887 210.132635 \n",
       "L 318.768943 208.202185 \n",
       "L 318.899055 210.132635 \n",
       "L 318.964111 204.341285 \n",
       "L 319.029167 206.271735 \n",
       "L 319.094222 213.02831 \n",
       "L 319.159278 203.37606 \n",
       "L 319.224334 208.202185 \n",
       "L 319.28939 209.16741 \n",
       "L 319.354446 204.341285 \n",
       "L 319.419502 210.132635 \n",
       "L 319.549613 200.480384 \n",
       "L 319.614669 200.480384 \n",
       "L 319.809837 213.02831 \n",
       "L 320.005004 204.341285 \n",
       "L 320.07006 201.44561 \n",
       "L 320.135116 213.02831 \n",
       "L 320.200172 201.44561 \n",
       "L 320.265228 208.202185 \n",
       "L 320.395339 205.30651 \n",
       "L 320.460395 212.063085 \n",
       "L 320.525451 208.202185 \n",
       "L 320.590507 210.132635 \n",
       "L 320.720619 205.30651 \n",
       "L 320.785674 204.341285 \n",
       "L 320.85073 205.30651 \n",
       "L 320.915786 204.341285 \n",
       "L 320.980842 209.16741 \n",
       "L 321.045898 205.30651 \n",
       "L 321.110954 210.132635 \n",
       "L 321.17601 207.23696 \n",
       "L 321.241065 211.09786 \n",
       "L 321.306121 205.30651 \n",
       "L 321.436233 212.063085 \n",
       "L 321.566345 205.30651 \n",
       "L 321.6314 207.23696 \n",
       "L 321.696456 209.16741 \n",
       "L 321.761512 204.341285 \n",
       "L 321.826568 208.202185 \n",
       "L 321.891624 207.23696 \n",
       "L 321.95668 205.30651 \n",
       "L 322.021736 207.23696 \n",
       "L 322.086791 202.410835 \n",
       "L 322.151847 210.132635 \n",
       "L 322.216903 204.341285 \n",
       "L 322.281959 207.23696 \n",
       "L 322.347015 212.063085 \n",
       "L 322.542182 199.515159 \n",
       "L 322.73735 209.16741 \n",
       "L 322.802406 207.23696 \n",
       "L 322.867462 199.515159 \n",
       "L 322.932517 204.341285 \n",
       "L 322.997573 209.16741 \n",
       "L 323.062629 203.37606 \n",
       "L 323.127685 204.341285 \n",
       "L 323.257797 210.132635 \n",
       "L 323.322853 202.410835 \n",
       "L 323.387908 206.271735 \n",
       "L 323.452964 208.202185 \n",
       "L 323.583076 205.30651 \n",
       "L 323.648132 205.30651 \n",
       "L 323.778243 209.16741 \n",
       "L 323.843299 202.410835 \n",
       "L 323.908355 210.132635 \n",
       "L 324.038467 200.480384 \n",
       "L 324.233634 208.202185 \n",
       "L 324.29869 202.410835 \n",
       "L 324.363746 208.202185 \n",
       "L 324.428802 206.271735 \n",
       "L 324.493858 201.44561 \n",
       "L 324.689025 210.132635 \n",
       "L 324.819137 205.30651 \n",
       "L 324.884193 205.30651 \n",
       "L 324.949249 203.37606 \n",
       "L 325.07936 208.202185 \n",
       "L 325.144416 209.16741 \n",
       "L 325.274528 205.30651 \n",
       "L 325.339584 209.16741 \n",
       "L 325.40464 208.202185 \n",
       "L 325.469696 199.515159 \n",
       "L 325.534751 209.16741 \n",
       "L 325.599807 206.271735 \n",
       "L 325.664863 204.341285 \n",
       "L 325.729919 211.09786 \n",
       "L 325.794975 202.410835 \n",
       "L 325.860031 206.271735 \n",
       "L 326.055198 201.44561 \n",
       "L 326.120254 206.271735 \n",
       "L 326.18531 205.30651 \n",
       "L 326.250366 204.341285 \n",
       "L 326.315422 206.271735 \n",
       "L 326.380477 205.30651 \n",
       "L 326.445533 201.44561 \n",
       "L 326.510589 208.202185 \n",
       "L 326.705757 201.44561 \n",
       "L 326.900924 210.132635 \n",
       "L 327.031036 201.44561 \n",
       "L 327.161148 208.202185 \n",
       "L 327.226203 204.341285 \n",
       "L 327.356315 209.16741 \n",
       "L 327.421371 205.30651 \n",
       "L 327.551483 210.132635 \n",
       "L 327.616539 204.341285 \n",
       "L 327.681594 208.202185 \n",
       "L 327.876762 208.202185 \n",
       "L 327.941818 204.341285 \n",
       "L 328.006874 211.09786 \n",
       "L 328.071929 209.16741 \n",
       "L 328.136985 211.09786 \n",
       "L 328.267097 200.480384 \n",
       "L 328.397209 206.271735 \n",
       "L 328.52732 201.44561 \n",
       "L 328.722488 207.23696 \n",
       "L 328.787544 204.341285 \n",
       "L 328.8526 212.063085 \n",
       "L 328.982711 201.44561 \n",
       "L 329.047767 210.132635 \n",
       "L 329.112823 202.410835 \n",
       "L 329.177879 208.202185 \n",
       "L 329.242935 199.515159 \n",
       "L 329.307991 202.410835 \n",
       "L 329.373046 203.37606 \n",
       "L 329.438102 207.23696 \n",
       "L 329.503158 203.37606 \n",
       "L 329.63327 208.202185 \n",
       "L 329.828437 204.341285 \n",
       "L 329.958549 208.202185 \n",
       "L 330.023605 207.23696 \n",
       "L 330.088661 201.44561 \n",
       "L 330.153717 205.30651 \n",
       "L 330.218772 209.16741 \n",
       "L 330.283828 202.410835 \n",
       "L 330.348884 203.37606 \n",
       "L 330.41394 209.16741 \n",
       "L 330.478996 207.23696 \n",
       "L 330.544052 207.23696 \n",
       "L 330.674163 212.063085 \n",
       "L 330.804275 203.37606 \n",
       "L 330.869331 204.341285 \n",
       "L 330.999443 212.063085 \n",
       "L 331.259666 201.44561 \n",
       "L 331.324722 205.30651 \n",
       "L 331.454834 199.515159 \n",
       "L 331.519889 208.202185 \n",
       "L 331.584945 204.341285 \n",
       "L 331.650001 201.44561 \n",
       "L 331.715057 202.410835 \n",
       "L 331.97528 210.132635 \n",
       "L 332.170448 202.410835 \n",
       "L 332.235504 213.02831 \n",
       "L 332.30056 206.271735 \n",
       "L 332.365615 211.09786 \n",
       "L 332.430671 206.271735 \n",
       "L 332.495727 208.202185 \n",
       "L 332.755951 199.515159 \n",
       "L 332.821006 209.16741 \n",
       "L 332.886062 208.202185 \n",
       "L 332.951118 207.23696 \n",
       "L 333.016174 202.410835 \n",
       "L 333.08123 205.30651 \n",
       "L 333.146286 202.410835 \n",
       "L 333.276397 206.271735 \n",
       "L 333.406509 203.37606 \n",
       "L 333.536621 206.271735 \n",
       "L 333.601677 203.37606 \n",
       "L 333.666732 205.30651 \n",
       "L 333.731788 205.30651 \n",
       "L 333.8619 203.37606 \n",
       "L 333.926956 209.16741 \n",
       "L 333.992012 207.23696 \n",
       "L 334.057068 203.37606 \n",
       "L 334.187179 207.23696 \n",
       "L 334.252235 207.23696 \n",
       "L 334.317291 201.44561 \n",
       "L 334.382347 207.23696 \n",
       "L 334.447403 206.271735 \n",
       "L 334.512459 206.271735 \n",
       "L 334.577514 203.37606 \n",
       "L 334.707626 208.202185 \n",
       "L 334.772682 207.23696 \n",
       "L 334.902794 204.341285 \n",
       "L 334.967849 205.30651 \n",
       "L 335.097961 210.132635 \n",
       "L 335.228073 202.410835 \n",
       "L 335.293129 203.37606 \n",
       "L 335.358185 204.341285 \n",
       "L 335.42324 201.44561 \n",
       "L 335.488296 212.063085 \n",
       "L 335.553352 210.132635 \n",
       "L 335.683464 201.44561 \n",
       "L 335.813575 209.16741 \n",
       "L 336.008743 201.44561 \n",
       "L 336.203911 207.23696 \n",
       "L 336.334022 203.37606 \n",
       "L 336.399078 211.09786 \n",
       "L 336.464134 207.23696 \n",
       "L 336.52919 207.23696 \n",
       "L 336.594246 208.202185 \n",
       "L 336.724357 203.37606 \n",
       "L 336.854469 211.09786 \n",
       "L 336.984581 201.44561 \n",
       "L 337.114692 206.271735 \n",
       "L 337.179748 201.44561 \n",
       "L 337.244804 204.341285 \n",
       "L 337.374916 213.993535 \n",
       "L 337.439972 203.37606 \n",
       "L 337.505028 205.30651 \n",
       "L 337.700195 211.09786 \n",
       "L 337.765251 210.132635 \n",
       "L 337.895363 202.410835 \n",
       "L 337.960418 209.16741 \n",
       "L 338.025474 207.23696 \n",
       "L 338.09053 207.23696 \n",
       "L 338.155586 201.44561 \n",
       "L 338.220642 203.37606 \n",
       "L 338.285698 208.202185 \n",
       "L 338.350754 207.23696 \n",
       "L 338.415809 208.202185 \n",
       "L 338.610977 201.44561 \n",
       "L 338.806145 209.16741 \n",
       "L 338.8712 205.30651 \n",
       "L 338.936256 208.202185 \n",
       "L 339.001312 198.549934 \n",
       "L 339.131424 209.16741 \n",
       "L 339.19648 209.16741 \n",
       "L 339.326591 202.410835 \n",
       "L 339.391647 207.23696 \n",
       "L 339.456703 202.410835 \n",
       "L 339.521759 204.341285 \n",
       "L 339.586815 202.410835 \n",
       "L 339.651871 205.30651 \n",
       "L 339.716926 203.37606 \n",
       "L 339.781982 203.37606 \n",
       "L 339.847038 204.341285 \n",
       "L 339.912094 211.09786 \n",
       "L 339.97715 200.480384 \n",
       "L 340.042206 207.23696 \n",
       "L 340.172317 202.410835 \n",
       "L 340.302429 211.09786 \n",
       "L 340.367485 201.44561 \n",
       "L 340.432541 203.37606 \n",
       "L 340.497597 203.37606 \n",
       "L 340.562652 210.132635 \n",
       "L 340.627708 205.30651 \n",
       "L 340.692764 206.271735 \n",
       "L 340.822876 206.271735 \n",
       "L 340.887932 199.515159 \n",
       "L 341.018043 211.09786 \n",
       "L 341.083099 209.16741 \n",
       "L 341.148155 201.44561 \n",
       "L 341.213211 211.09786 \n",
       "L 341.278267 206.271735 \n",
       "L 341.343323 207.23696 \n",
       "L 341.408378 200.480384 \n",
       "L 341.473434 207.23696 \n",
       "L 341.603546 202.410835 \n",
       "L 341.733658 209.16741 \n",
       "L 341.798714 207.23696 \n",
       "L 341.863769 204.341285 \n",
       "L 341.993881 208.202185 \n",
       "L 342.123993 201.44561 \n",
       "L 342.254104 210.132635 \n",
       "L 342.384216 209.16741 \n",
       "L 342.449272 202.410835 \n",
       "L 342.514328 205.30651 \n",
       "L 342.64444 207.23696 \n",
       "L 342.709495 203.37606 \n",
       "L 342.904663 210.132635 \n",
       "L 342.969719 207.23696 \n",
       "L 343.034775 209.16741 \n",
       "L 343.360054 200.480384 \n",
       "L 343.490166 207.23696 \n",
       "L 343.555221 205.30651 \n",
       "L 343.620277 206.271735 \n",
       "L 343.685333 210.132635 \n",
       "L 343.750389 205.30651 \n",
       "L 343.815445 206.271735 \n",
       "L 343.880501 210.132635 \n",
       "L 344.010612 202.410835 \n",
       "L 344.140724 206.271735 \n",
       "L 344.270836 200.480384 \n",
       "L 344.335892 203.37606 \n",
       "L 344.400947 199.515159 \n",
       "L 344.466003 201.44561 \n",
       "L 344.531059 201.44561 \n",
       "L 344.596115 208.202185 \n",
       "L 344.661171 206.271735 \n",
       "L 344.856338 209.16741 \n",
       "L 344.921394 204.341285 \n",
       "L 344.98645 207.23696 \n",
       "L 345.051506 203.37606 \n",
       "L 345.116562 206.271735 \n",
       "L 345.181618 199.515159 \n",
       "L 345.311729 207.23696 \n",
       "L 345.376785 207.23696 \n",
       "L 345.441841 203.37606 \n",
       "L 345.506897 205.30651 \n",
       "L 345.702064 200.480384 \n",
       "L 345.76712 201.44561 \n",
       "L 345.962288 209.16741 \n",
       "L 346.027344 199.515159 \n",
       "L 346.0924 203.37606 \n",
       "L 346.157455 208.202185 \n",
       "L 346.222511 201.44561 \n",
       "L 346.287567 204.341285 \n",
       "L 346.352623 204.341285 \n",
       "L 346.417679 208.202185 \n",
       "L 346.482735 205.30651 \n",
       "L 346.54779 209.16741 \n",
       "L 346.612846 204.341285 \n",
       "L 346.677902 205.30651 \n",
       "L 346.742958 213.02831 \n",
       "L 346.808014 208.202185 \n",
       "L 346.87307 207.23696 \n",
       "L 346.938126 213.993535 \n",
       "L 347.003181 202.410835 \n",
       "L 347.068237 204.341285 \n",
       "L 347.133293 208.202185 \n",
       "L 347.328461 203.37606 \n",
       "L 347.393517 205.30651 \n",
       "L 347.458572 213.02831 \n",
       "L 347.588684 204.341285 \n",
       "L 347.718796 206.271735 \n",
       "L 347.783852 199.515159 \n",
       "L 347.913963 207.23696 \n",
       "L 348.044075 201.44561 \n",
       "L 348.109131 207.23696 \n",
       "L 348.174187 201.44561 \n",
       "L 348.239243 202.410835 \n",
       "L 348.304298 211.09786 \n",
       "L 348.369354 205.30651 \n",
       "L 348.43441 200.480384 \n",
       "L 348.564522 208.202185 \n",
       "L 348.629578 202.410835 \n",
       "L 348.694633 208.202185 \n",
       "L 348.759689 204.341285 \n",
       "L 348.824745 208.202185 \n",
       "L 348.889801 200.480384 \n",
       "L 348.954857 206.271735 \n",
       "L 349.019913 203.37606 \n",
       "L 349.21508 206.271735 \n",
       "L 349.345192 201.44561 \n",
       "L 349.410248 208.202185 \n",
       "L 349.54036 200.480384 \n",
       "L 349.670471 207.23696 \n",
       "L 349.800583 200.480384 \n",
       "L 349.930695 204.341285 \n",
       "L 349.99575 202.410835 \n",
       "L 350.060806 210.132635 \n",
       "L 350.125862 206.271735 \n",
       "L 350.255974 209.16741 \n",
       "L 350.32103 208.202185 \n",
       "L 350.386086 199.515159 \n",
       "L 350.451141 205.30651 \n",
       "L 350.516197 204.341285 \n",
       "L 350.581253 209.16741 \n",
       "L 350.646309 204.341285 \n",
       "L 350.711365 211.09786 \n",
       "L 350.776421 208.202185 \n",
       "L 350.906532 203.37606 \n",
       "L 350.971588 209.16741 \n",
       "L 351.036644 202.410835 \n",
       "L 351.166756 210.132635 \n",
       "L 351.361923 199.515159 \n",
       "L 351.426979 208.202185 \n",
       "L 351.492035 202.410835 \n",
       "L 351.622147 206.271735 \n",
       "L 351.752258 202.410835 \n",
       "L 351.88237 210.132635 \n",
       "L 351.947426 203.37606 \n",
       "L 352.012482 208.202185 \n",
       "L 352.077538 206.271735 \n",
       "L 352.142593 206.271735 \n",
       "L 352.207649 205.30651 \n",
       "L 352.272705 209.16741 \n",
       "L 352.402817 198.549934 \n",
       "L 352.467873 203.37606 \n",
       "L 352.532929 205.30651 \n",
       "L 352.597984 200.480384 \n",
       "L 352.728096 206.271735 \n",
       "L 352.793152 199.515159 \n",
       "L 352.923264 208.202185 \n",
       "L 353.053375 203.37606 \n",
       "L 353.183487 210.132635 \n",
       "L 353.248543 200.480384 \n",
       "L 353.313599 204.341285 \n",
       "L 353.378655 209.16741 \n",
       "L 353.508766 204.341285 \n",
       "L 353.703934 209.16741 \n",
       "L 353.76899 200.480384 \n",
       "L 353.834046 206.271735 \n",
       "L 353.899101 201.44561 \n",
       "L 353.964157 205.30651 \n",
       "L 354.029213 203.37606 \n",
       "L 354.094269 205.30651 \n",
       "L 354.224381 201.44561 \n",
       "L 354.289436 208.202185 \n",
       "L 354.354492 204.341285 \n",
       "L 354.484604 201.44561 \n",
       "L 354.679772 206.271735 \n",
       "L 354.809883 202.410835 \n",
       "L 355.005051 207.23696 \n",
       "L 355.070107 200.480384 \n",
       "L 355.265274 210.132635 \n",
       "L 355.33033 201.44561 \n",
       "L 355.395386 204.341285 \n",
       "L 355.460442 199.515159 \n",
       "L 355.655609 208.202185 \n",
       "L 355.720665 201.44561 \n",
       "L 355.785721 208.202185 \n",
       "L 355.850777 198.549934 \n",
       "L 355.915833 208.202185 \n",
       "L 355.980889 207.23696 \n",
       "L 356.111 204.341285 \n",
       "L 356.176056 199.515159 \n",
       "L 356.241112 201.44561 \n",
       "L 356.306168 209.16741 \n",
       "L 356.436279 196.619484 \n",
       "L 356.631447 209.16741 \n",
       "L 356.761559 202.410835 \n",
       "L 356.826615 208.202185 \n",
       "L 356.89167 199.515159 \n",
       "L 356.956726 204.341285 \n",
       "L 357.021782 200.480384 \n",
       "L 357.21695 205.30651 \n",
       "L 357.282006 200.480384 \n",
       "L 357.347061 208.202185 \n",
       "L 357.412117 206.271735 \n",
       "L 357.477173 200.480384 \n",
       "L 357.542229 204.341285 \n",
       "L 357.607285 207.23696 \n",
       "L 357.672341 204.341285 \n",
       "L 357.737396 207.23696 \n",
       "L 357.802452 205.30651 \n",
       "L 357.867508 206.271735 \n",
       "L 357.932564 205.30651 \n",
       "L 357.99762 209.16741 \n",
       "L 358.192787 202.410835 \n",
       "L 358.257843 206.271735 \n",
       "L 358.322899 203.37606 \n",
       "L 358.387955 207.23696 \n",
       "L 358.453011 203.37606 \n",
       "L 358.583122 208.202185 \n",
       "L 358.713234 200.480384 \n",
       "L 358.77829 208.202185 \n",
       "L 358.843346 206.271735 \n",
       "L 358.908402 200.480384 \n",
       "L 358.973458 206.271735 \n",
       "L 359.038513 201.44561 \n",
       "L 359.103569 202.410835 \n",
       "L 359.233681 208.202185 \n",
       "L 359.298737 213.02831 \n",
       "L 359.363793 205.30651 \n",
       "L 359.428849 209.16741 \n",
       "L 359.55896 200.480384 \n",
       "L 359.689072 203.37606 \n",
       "L 359.754128 199.515159 \n",
       "L 359.819184 210.132635 \n",
       "L 359.884239 204.341285 \n",
       "L 359.949295 203.37606 \n",
       "L 360.014351 206.271735 \n",
       "L 360.079407 198.549934 \n",
       "L 360.144463 204.341285 \n",
       "L 360.209519 203.37606 \n",
       "L 360.274575 203.37606 \n",
       "L 360.33963 200.480384 \n",
       "L 360.599854 212.063085 \n",
       "L 360.66491 202.410835 \n",
       "L 360.729965 207.23696 \n",
       "L 360.795021 201.44561 \n",
       "L 360.860077 206.271735 \n",
       "L 360.925133 201.44561 \n",
       "L 360.990189 203.37606 \n",
       "L 361.055245 203.37606 \n",
       "L 361.120301 208.202185 \n",
       "L 361.185356 207.23696 \n",
       "L 361.250412 208.202185 \n",
       "L 361.315468 199.515159 \n",
       "L 361.380524 205.30651 \n",
       "L 361.44558 203.37606 \n",
       "L 361.510636 204.341285 \n",
       "L 361.575692 205.30651 \n",
       "L 361.770859 201.44561 \n",
       "L 361.900971 208.202185 \n",
       "L 361.966027 203.37606 \n",
       "L 362.096138 210.132635 \n",
       "L 362.22625 202.410835 \n",
       "L 362.356362 202.410835 \n",
       "L 362.486473 209.16741 \n",
       "L 362.551529 202.410835 \n",
       "L 362.616585 207.23696 \n",
       "L 362.681641 203.37606 \n",
       "L 362.746697 208.202185 \n",
       "L 362.811753 200.480384 \n",
       "L 362.941864 210.132635 \n",
       "L 363.00692 205.30651 \n",
       "L 363.071976 206.271735 \n",
       "L 363.202088 204.341285 \n",
       "L 363.267144 199.515159 \n",
       "L 363.332199 206.271735 \n",
       "L 363.397255 200.480384 \n",
       "L 363.462311 206.271735 \n",
       "L 363.592423 201.44561 \n",
       "L 363.722535 211.09786 \n",
       "L 363.78759 205.30651 \n",
       "L 363.852646 210.132635 \n",
       "L 363.982758 200.480384 \n",
       "L 364.047814 210.132635 \n",
       "L 364.11287 201.44561 \n",
       "L 364.177925 204.341285 \n",
       "L 364.308037 203.37606 \n",
       "L 364.373093 205.30651 \n",
       "L 364.438149 204.341285 \n",
       "L 364.503205 204.341285 \n",
       "L 364.568261 201.44561 \n",
       "L 364.698372 207.23696 \n",
       "L 364.763428 201.44561 \n",
       "L 364.828484 210.132635 \n",
       "L 364.89354 208.202185 \n",
       "L 365.023651 205.30651 \n",
       "L 365.088707 210.132635 \n",
       "L 365.153763 209.16741 \n",
       "L 365.218819 206.271735 \n",
       "L 365.283875 207.23696 \n",
       "L 365.348931 206.271735 \n",
       "L 365.479042 202.410835 \n",
       "L 365.544098 208.202185 \n",
       "L 365.609154 205.30651 \n",
       "L 365.739266 198.549934 \n",
       "L 365.804322 208.202185 \n",
       "L 365.869378 205.30651 \n",
       "L 365.934433 207.23696 \n",
       "L 366.064545 203.37606 \n",
       "L 366.129601 208.202185 \n",
       "L 366.259713 200.480384 \n",
       "L 366.519936 207.23696 \n",
       "L 366.584992 199.515159 \n",
       "L 366.650048 206.271735 \n",
       "L 366.715104 204.341285 \n",
       "L 366.780159 203.37606 \n",
       "L 366.975327 205.30651 \n",
       "L 367.040383 201.44561 \n",
       "L 367.105439 204.341285 \n",
       "L 367.170494 203.37606 \n",
       "L 367.23555 199.515159 \n",
       "L 367.300606 206.271735 \n",
       "L 367.365662 205.30651 \n",
       "L 367.430718 201.44561 \n",
       "L 367.495774 203.37606 \n",
       "L 367.56083 206.271735 \n",
       "L 367.625885 205.30651 \n",
       "L 367.690941 199.515159 \n",
       "L 367.755997 206.271735 \n",
       "L 367.821053 203.37606 \n",
       "L 367.886109 204.341285 \n",
       "L 367.951165 207.23696 \n",
       "L 368.016221 201.44561 \n",
       "L 368.081276 206.271735 \n",
       "L 368.276444 197.584709 \n",
       "L 368.471611 207.23696 \n",
       "L 368.666779 203.37606 \n",
       "L 368.731835 207.23696 \n",
       "L 368.927002 199.515159 \n",
       "L 368.992058 207.23696 \n",
       "L 369.12217 199.515159 \n",
       "L 369.187226 209.16741 \n",
       "L 369.252282 204.341285 \n",
       "L 369.317337 205.30651 \n",
       "L 369.447449 202.410835 \n",
       "L 369.512505 205.30651 \n",
       "L 369.577561 203.37606 \n",
       "L 369.642617 203.37606 \n",
       "L 369.707673 207.23696 \n",
       "L 369.772728 206.271735 \n",
       "L 369.90284 201.44561 \n",
       "L 369.967896 209.16741 \n",
       "L 370.032952 208.202185 \n",
       "L 370.098008 204.341285 \n",
       "L 370.163064 208.202185 \n",
       "L 370.293175 201.44561 \n",
       "L 370.358231 205.30651 \n",
       "L 370.423287 199.515159 \n",
       "L 370.488343 200.480384 \n",
       "L 370.553399 205.30651 \n",
       "L 370.618454 200.480384 \n",
       "L 370.748566 209.16741 \n",
       "L 370.943734 197.584709 \n",
       "L 371.073845 208.202185 \n",
       "L 371.138901 202.410835 \n",
       "L 371.203957 205.30651 \n",
       "L 371.269013 201.44561 \n",
       "L 371.334069 203.37606 \n",
       "L 371.399125 202.410835 \n",
       "L 371.529236 207.23696 \n",
       "L 371.594292 202.410835 \n",
       "L 371.659348 208.202185 \n",
       "L 371.724404 207.23696 \n",
       "L 371.78946 202.410835 \n",
       "L 371.854516 203.37606 \n",
       "L 371.919571 202.410835 \n",
       "L 371.984627 206.271735 \n",
       "L 372.049683 202.410835 \n",
       "L 372.179795 207.23696 \n",
       "L 372.244851 207.23696 \n",
       "L 372.309907 208.202185 \n",
       "L 372.374962 200.480384 \n",
       "L 372.440018 205.30651 \n",
       "L 372.505074 205.30651 \n",
       "L 372.57013 200.480384 \n",
       "L 372.635186 204.341285 \n",
       "L 372.700242 200.480384 \n",
       "L 372.895409 210.132635 \n",
       "L 373.090577 202.410835 \n",
       "L 373.155633 200.480384 \n",
       "L 373.285744 205.30651 \n",
       "L 373.480912 199.515159 \n",
       "L 373.676079 207.23696 \n",
       "L 373.741135 206.271735 \n",
       "L 373.806191 207.23696 \n",
       "L 373.936303 201.44561 \n",
       "L 374.001359 201.44561 \n",
       "L 374.066414 208.202185 \n",
       "L 374.13147 203.37606 \n",
       "L 374.196526 207.23696 \n",
       "L 374.261582 204.341285 \n",
       "L 374.326638 206.271735 \n",
       "L 374.45675 202.410835 \n",
       "L 374.586861 206.271735 \n",
       "L 374.651917 203.37606 \n",
       "L 374.716973 211.09786 \n",
       "L 374.782029 199.515159 \n",
       "L 374.847085 205.30651 \n",
       "L 374.977196 208.202185 \n",
       "L 375.172364 198.549934 \n",
       "L 375.302476 206.271735 \n",
       "L 375.367531 205.30651 \n",
       "L 375.432587 207.23696 \n",
       "L 375.497643 205.30651 \n",
       "L 375.562699 209.16741 \n",
       "L 375.692811 203.37606 \n",
       "L 375.757867 208.202185 \n",
       "L 375.822922 200.480384 \n",
       "L 375.887978 203.37606 \n",
       "L 375.953034 205.30651 \n",
       "L 376.01809 202.410835 \n",
       "L 376.083146 208.202185 \n",
       "L 376.148202 203.37606 \n",
       "L 376.213257 205.30651 \n",
       "L 376.408425 200.480384 \n",
       "L 376.473481 204.341285 \n",
       "L 376.538537 203.37606 \n",
       "L 376.603593 200.480384 \n",
       "L 376.668648 205.30651 \n",
       "L 376.733704 203.37606 \n",
       "L 376.79876 201.44561 \n",
       "L 376.863816 207.23696 \n",
       "L 376.928872 206.271735 \n",
       "L 377.124039 198.549934 \n",
       "L 377.189095 208.202185 \n",
       "L 377.254151 200.480384 \n",
       "L 377.319207 207.23696 \n",
       "L 377.384263 203.37606 \n",
       "L 377.449319 206.271735 \n",
       "L 377.514374 205.30651 \n",
       "L 377.774598 199.515159 \n",
       "L 377.839654 204.341285 \n",
       "L 377.90471 203.37606 \n",
       "L 377.969765 205.30651 \n",
       "L 378.099877 200.480384 \n",
       "L 378.229989 206.271735 \n",
       "L 378.3601 198.549934 \n",
       "L 378.425156 202.410835 \n",
       "L 378.490212 204.341285 \n",
       "L 378.555268 202.410835 \n",
       "L 378.620324 206.271735 \n",
       "L 378.750436 202.410835 \n",
       "L 378.815491 206.271735 \n",
       "L 378.880547 205.30651 \n",
       "L 378.945603 205.30651 \n",
       "L 379.010659 206.271735 \n",
       "L 379.075715 203.37606 \n",
       "L 379.140771 205.30651 \n",
       "L 379.205826 207.23696 \n",
       "L 379.335938 199.515159 \n",
       "L 379.400994 203.37606 \n",
       "L 379.46605 201.44561 \n",
       "L 379.531106 202.410835 \n",
       "L 379.596162 201.44561 \n",
       "L 379.726273 209.16741 \n",
       "L 379.791329 201.44561 \n",
       "L 379.856385 206.271735 \n",
       "L 379.921441 201.44561 \n",
       "L 379.986497 202.410835 \n",
       "L 380.051553 201.44561 \n",
       "L 380.116608 203.37606 \n",
       "L 380.181664 200.480384 \n",
       "L 380.24672 209.16741 \n",
       "L 380.311776 208.202185 \n",
       "L 380.376832 197.584709 \n",
       "L 380.441888 202.410835 \n",
       "L 380.506943 199.515159 \n",
       "L 380.702111 203.37606 \n",
       "L 380.962334 203.37606 \n",
       "L 381.092446 200.480384 \n",
       "L 381.157502 208.202185 \n",
       "L 381.222558 205.30651 \n",
       "L 381.352669 207.23696 \n",
       "L 381.482781 197.584709 \n",
       "L 381.547837 210.132635 \n",
       "L 381.612893 205.30651 \n",
       "L 381.677949 199.515159 \n",
       "L 381.873116 208.202185 \n",
       "L 382.003228 200.480384 \n",
       "L 382.068284 205.30651 \n",
       "L 382.13334 199.515159 \n",
       "L 382.198396 211.09786 \n",
       "L 382.263451 209.16741 \n",
       "L 382.393563 200.480384 \n",
       "L 382.458619 205.30651 \n",
       "L 382.523675 202.410835 \n",
       "L 382.588731 204.341285 \n",
       "L 382.653786 202.410835 \n",
       "L 382.718842 203.37606 \n",
       "L 382.783898 202.410835 \n",
       "L 382.848954 206.271735 \n",
       "L 382.91401 202.410835 \n",
       "L 382.979066 206.271735 \n",
       "L 383.044122 205.30651 \n",
       "L 383.174233 202.410835 \n",
       "L 383.239289 203.37606 \n",
       "L 383.304345 202.410835 \n",
       "L 383.369401 205.30651 \n",
       "L 383.499512 202.410835 \n",
       "L 383.564568 202.410835 \n",
       "L 383.629624 205.30651 \n",
       "L 383.69468 200.480384 \n",
       "L 383.759736 201.44561 \n",
       "L 383.824792 205.30651 \n",
       "L 383.889848 195.654259 \n",
       "L 383.954903 198.549934 \n",
       "L 384.085015 209.16741 \n",
       "L 384.150071 202.410835 \n",
       "L 384.215127 206.271735 \n",
       "L 384.280183 203.37606 \n",
       "L 384.345239 212.063085 \n",
       "L 384.47535 200.480384 \n",
       "L 384.605462 209.16741 \n",
       "L 384.735574 200.480384 \n",
       "L 384.800629 209.16741 \n",
       "L 384.865685 200.480384 \n",
       "L 384.930741 203.37606 \n",
       "L 384.995797 206.271735 \n",
       "L 385.060853 200.480384 \n",
       "L 385.125909 202.410835 \n",
       "L 385.25602 204.341285 \n",
       "L 385.321076 210.132635 \n",
       "L 385.386132 208.202185 \n",
       "L 385.516244 201.44561 \n",
       "L 385.5813 202.410835 \n",
       "L 385.646355 209.16741 \n",
       "L 385.711411 205.30651 \n",
       "L 385.776467 200.480384 \n",
       "L 385.841523 206.271735 \n",
       "L 385.906579 201.44561 \n",
       "L 385.971635 205.30651 \n",
       "L 386.036691 203.37606 \n",
       "L 386.101746 202.410835 \n",
       "L 386.231858 209.16741 \n",
       "L 386.296914 205.30651 \n",
       "L 386.492082 198.549934 \n",
       "L 386.557137 204.341285 \n",
       "L 386.622193 201.44561 \n",
       "L 386.687249 201.44561 \n",
       "L 386.752305 206.271735 \n",
       "L 386.817361 199.515159 \n",
       "L 386.882417 201.44561 \n",
       "L 386.947472 202.410835 \n",
       "L 387.012528 209.16741 \n",
       "L 387.207696 198.549934 \n",
       "L 387.272752 209.16741 \n",
       "L 387.337808 202.410835 \n",
       "L 387.402863 201.44561 \n",
       "L 387.467919 205.30651 \n",
       "L 387.532975 199.515159 \n",
       "L 387.598031 209.16741 \n",
       "L 387.663087 206.271735 \n",
       "L 387.728143 209.16741 \n",
       "L 387.858254 200.480384 \n",
       "L 387.92331 202.410835 \n",
       "L 387.988366 208.202185 \n",
       "L 388.118478 200.480384 \n",
       "L 388.313645 206.271735 \n",
       "L 388.443757 202.410835 \n",
       "L 388.508813 206.271735 \n",
       "L 388.638925 203.37606 \n",
       "L 388.70398 204.341285 \n",
       "L 388.769036 202.410835 \n",
       "L 388.834092 207.23696 \n",
       "L 388.899148 199.515159 \n",
       "L 389.094315 207.23696 \n",
       "L 389.159371 203.37606 \n",
       "L 389.224427 208.202185 \n",
       "L 389.354539 202.410835 \n",
       "L 389.419595 206.271735 \n",
       "L 389.484651 205.30651 \n",
       "L 389.614762 202.410835 \n",
       "L 389.679818 199.515159 \n",
       "L 389.744874 211.09786 \n",
       "L 389.80993 202.410835 \n",
       "L 389.874986 204.341285 \n",
       "L 389.940041 200.480384 \n",
       "L 390.135209 208.202185 \n",
       "L 390.265321 200.480384 \n",
       "L 390.330377 206.271735 \n",
       "L 390.395432 205.30651 \n",
       "L 390.5906 202.410835 \n",
       "L 390.655656 204.341285 \n",
       "L 390.720712 202.410835 \n",
       "L 390.915879 205.30651 \n",
       "L 390.980935 199.515159 \n",
       "L 391.045991 203.37606 \n",
       "L 391.111047 206.271735 \n",
       "L 391.241158 197.584709 \n",
       "L 391.306214 203.37606 \n",
       "L 391.37127 201.44561 \n",
       "L 391.436326 201.44561 \n",
       "L 391.501382 208.202185 \n",
       "L 391.696549 200.480384 \n",
       "L 391.826661 206.271735 \n",
       "L 391.891717 204.341285 \n",
       "L 392.021829 206.271735 \n",
       "L 392.15194 200.480384 \n",
       "L 392.216996 207.23696 \n",
       "L 392.282052 200.480384 \n",
       "L 392.347108 201.44561 \n",
       "L 392.47722 207.23696 \n",
       "L 392.607331 198.549934 \n",
       "L 392.737443 208.202185 \n",
       "L 392.802499 206.271735 \n",
       "L 392.867555 208.202185 \n",
       "L 393.127778 201.44561 \n",
       "L 393.25789 207.23696 \n",
       "L 393.322946 205.30651 \n",
       "L 393.453057 202.410835 \n",
       "L 393.648225 205.30651 \n",
       "L 393.713281 204.341285 \n",
       "L 393.778337 206.271735 \n",
       "L 393.843392 205.30651 \n",
       "L 393.908448 202.410835 \n",
       "L 393.973504 204.341285 \n",
       "L 394.03856 206.271735 \n",
       "L 394.103616 202.410835 \n",
       "L 394.233728 208.202185 \n",
       "L 394.493951 199.515159 \n",
       "L 394.624063 206.271735 \n",
       "L 394.754174 200.480384 \n",
       "L 394.949342 207.23696 \n",
       "L 395.014398 207.23696 \n",
       "L 395.209565 199.515159 \n",
       "L 395.339677 208.202185 \n",
       "L 395.469789 199.515159 \n",
       "L 395.5999 207.23696 \n",
       "L 395.664956 198.549934 \n",
       "L 395.730012 203.37606 \n",
       "L 395.795068 199.515159 \n",
       "L 395.92518 210.132635 \n",
       "L 395.990235 199.515159 \n",
       "L 396.055291 204.341285 \n",
       "L 396.120347 199.515159 \n",
       "L 396.185403 202.410835 \n",
       "L 396.250459 200.480384 \n",
       "L 396.315515 204.341285 \n",
       "L 396.380571 201.44561 \n",
       "L 396.445626 203.37606 \n",
       "L 396.575738 200.480384 \n",
       "L 396.640794 209.16741 \n",
       "L 396.70585 201.44561 \n",
       "L 396.770906 204.341285 \n",
       "L 396.901017 204.341285 \n",
       "L 396.966073 200.480384 \n",
       "L 397.096185 205.30651 \n",
       "L 397.161241 200.480384 \n",
       "L 397.226297 205.30651 \n",
       "L 397.291352 203.37606 \n",
       "L 397.421464 206.271735 \n",
       "L 397.551576 200.480384 \n",
       "L 397.681687 208.202185 \n",
       "L 397.746743 202.410835 \n",
       "L 397.811799 204.341285 \n",
       "L 397.876855 203.37606 \n",
       "L 398.006967 204.341285 \n",
       "L 398.072023 203.37606 \n",
       "L 398.202134 207.23696 \n",
       "L 398.26719 205.30651 \n",
       "L 398.332246 207.23696 \n",
       "L 398.397302 206.271735 \n",
       "L 398.462358 207.23696 \n",
       "L 398.592469 205.30651 \n",
       "L 398.657525 202.410835 \n",
       "L 398.722581 203.37606 \n",
       "L 398.787637 205.30651 \n",
       "L 398.852693 198.549934 \n",
       "L 398.917749 200.480384 \n",
       "L 398.982804 200.480384 \n",
       "L 399.04786 204.341285 \n",
       "L 399.112916 203.37606 \n",
       "L 399.177972 201.44561 \n",
       "L 399.243028 207.23696 \n",
       "L 399.308084 206.271735 \n",
       "L 399.37314 201.44561 \n",
       "L 399.438195 203.37606 \n",
       "L 399.503251 203.37606 \n",
       "L 399.568307 206.271735 \n",
       "L 399.633363 200.480384 \n",
       "L 399.698419 210.132635 \n",
       "L 399.763475 203.37606 \n",
       "L 399.82853 199.515159 \n",
       "L 399.893586 209.16741 \n",
       "L 399.958642 204.341285 \n",
       "L 400.023698 202.410835 \n",
       "L 400.088754 207.23696 \n",
       "L 400.15381 206.271735 \n",
       "L 400.218866 199.515159 \n",
       "L 400.283921 205.30651 \n",
       "L 400.348977 202.410835 \n",
       "L 400.414033 198.549934 \n",
       "L 400.544145 206.271735 \n",
       "L 400.609201 200.480384 \n",
       "L 400.674257 204.341285 \n",
       "L 400.739312 200.480384 \n",
       "L 400.804368 201.44561 \n",
       "L 400.93448 204.341285 \n",
       "L 400.999536 199.515159 \n",
       "L 401.064592 203.37606 \n",
       "L 401.129647 200.480384 \n",
       "L 401.194703 201.44561 \n",
       "L 401.259759 207.23696 \n",
       "L 401.324815 200.480384 \n",
       "L 401.389871 201.44561 \n",
       "L 401.454927 207.23696 \n",
       "L 401.519983 201.44561 \n",
       "L 401.585038 205.30651 \n",
       "L 401.71515 203.37606 \n",
       "L 401.780206 206.271735 \n",
       "L 401.845262 204.341285 \n",
       "L 401.975373 199.515159 \n",
       "L 402.105485 203.37606 \n",
       "L 402.170541 202.410835 \n",
       "L 402.300653 207.23696 \n",
       "L 402.365709 201.44561 \n",
       "L 402.430764 203.37606 \n",
       "L 402.49582 206.271735 \n",
       "L 402.560876 203.37606 \n",
       "L 402.625932 205.30651 \n",
       "L 402.690988 208.202185 \n",
       "L 402.8211 201.44561 \n",
       "L 402.886155 202.410835 \n",
       "L 402.951211 203.37606 \n",
       "L 403.081323 199.515159 \n",
       "L 403.211435 205.30651 \n",
       "L 403.27649 202.410835 \n",
       "L 403.406602 206.271735 \n",
       "L 403.471658 204.341285 \n",
       "L 403.60177 209.16741 \n",
       "L 403.731881 198.549934 \n",
       "L 403.796937 208.202185 \n",
       "L 403.861993 202.410835 \n",
       "L 403.927049 205.30651 \n",
       "L 403.992105 202.410835 \n",
       "L 404.057161 204.341285 \n",
       "L 404.187272 207.23696 \n",
       "L 404.317384 202.410835 \n",
       "L 404.38244 206.271735 \n",
       "L 404.447496 201.44561 \n",
       "L 404.512552 206.271735 \n",
       "L 404.577607 200.480384 \n",
       "L 404.642663 203.37606 \n",
       "L 404.707719 204.341285 \n",
       "L 404.772775 202.410835 \n",
       "L 404.837831 206.271735 \n",
       "L 404.902887 197.584709 \n",
       "L 404.967943 207.23696 \n",
       "L 405.032998 200.480384 \n",
       "L 405.228166 203.37606 \n",
       "L 405.293222 199.515159 \n",
       "L 405.488389 205.30651 \n",
       "L 405.618501 201.44561 \n",
       "L 405.813669 206.271735 \n",
       "L 406.073892 201.44561 \n",
       "L 406.138948 207.23696 \n",
       "L 406.204004 205.30651 \n",
       "L 406.269059 198.549934 \n",
       "L 406.399171 209.16741 \n",
       "L 406.529283 201.44561 \n",
       "L 406.72445 208.202185 \n",
       "L 406.854562 203.37606 \n",
       "L 406.919618 206.271735 \n",
       "L 406.984674 202.410835 \n",
       "L 407.114786 210.132635 \n",
       "L 407.309953 201.44561 \n",
       "L 407.440065 202.410835 \n",
       "L 407.570176 200.480384 \n",
       "L 407.635232 210.132635 \n",
       "L 407.700288 203.37606 \n",
       "L 407.765344 205.30651 \n",
       "L 407.8304 204.341285 \n",
       "L 407.895456 204.341285 \n",
       "L 408.025567 200.480384 \n",
       "L 408.155679 207.23696 \n",
       "L 408.220735 205.30651 \n",
       "L 408.285791 203.37606 \n",
       "L 408.350847 207.23696 \n",
       "L 408.546014 203.37606 \n",
       "L 408.61107 208.202185 \n",
       "L 408.676126 199.515159 \n",
       "L 408.741182 204.341285 \n",
       "L 408.871293 205.30651 \n",
       "L 408.936349 200.480384 \n",
       "L 409.001405 209.16741 \n",
       "L 409.131517 201.44561 \n",
       "L 409.196573 206.271735 \n",
       "L 409.261629 200.480384 \n",
       "L 409.326684 206.271735 \n",
       "L 409.39174 201.44561 \n",
       "L 409.456796 208.202185 \n",
       "L 409.586908 198.549934 \n",
       "L 409.651964 199.515159 \n",
       "L 409.717019 208.202185 \n",
       "L 409.782075 201.44561 \n",
       "L 409.847131 202.410835 \n",
       "L 409.912187 207.23696 \n",
       "L 409.977243 204.341285 \n",
       "L 410.042299 202.410835 \n",
       "L 410.17241 205.30651 \n",
       "L 410.237466 201.44561 \n",
       "L 410.302522 205.30651 \n",
       "L 410.432634 201.44561 \n",
       "L 410.49769 202.410835 \n",
       "L 410.562746 205.30651 \n",
       "L 410.627801 203.37606 \n",
       "L 410.822969 200.480384 \n",
       "L 411.018136 208.202185 \n",
       "L 411.083192 202.410835 \n",
       "L 411.148248 204.341285 \n",
       "L 411.213304 205.30651 \n",
       "L 411.27836 196.619484 \n",
       "L 411.343416 202.410835 \n",
       "L 411.408472 202.410835 \n",
       "L 411.473527 207.23696 \n",
       "L 411.538583 205.30651 \n",
       "L 411.603639 200.480384 \n",
       "L 411.668695 207.23696 \n",
       "L 411.733751 198.549934 \n",
       "L 411.798807 201.44561 \n",
       "L 411.928918 198.549934 \n",
       "L 411.993974 203.37606 \n",
       "L 412.05903 202.410835 \n",
       "L 412.124086 203.37606 \n",
       "L 412.189142 200.480384 \n",
       "L 412.254198 203.37606 \n",
       "L 412.319253 201.44561 \n",
       "L 412.384309 198.549934 \n",
       "L 412.449365 204.341285 \n",
       "L 412.514421 202.410835 \n",
       "L 412.644533 205.30651 \n",
       "L 412.709589 204.341285 \n",
       "L 412.774644 201.44561 \n",
       "L 412.904756 204.341285 \n",
       "L 412.969812 202.410835 \n",
       "L 413.034868 209.16741 \n",
       "L 413.230035 201.44561 \n",
       "L 413.360147 198.549934 \n",
       "L 413.425203 209.16741 \n",
       "L 413.490259 206.271735 \n",
       "L 413.555315 198.549934 \n",
       "L 413.62037 203.37606 \n",
       "L 413.685426 203.37606 \n",
       "L 413.815538 205.30651 \n",
       "L 413.880594 201.44561 \n",
       "L 413.94565 206.271735 \n",
       "L 414.010705 204.341285 \n",
       "L 414.075761 201.44561 \n",
       "L 414.140817 203.37606 \n",
       "L 414.205873 203.37606 \n",
       "L 414.270929 202.410835 \n",
       "L 414.531152 207.23696 \n",
       "L 414.596208 202.410835 \n",
       "L 414.661264 205.30651 \n",
       "L 414.856432 201.44561 \n",
       "L 414.921487 201.44561 \n",
       "L 415.051599 204.341285 \n",
       "L 415.116655 204.341285 \n",
       "L 415.181711 205.30651 \n",
       "L 415.246767 209.16741 \n",
       "L 415.311822 198.549934 \n",
       "L 415.376878 203.37606 \n",
       "L 415.50699 205.30651 \n",
       "L 415.702158 198.549934 \n",
       "L 415.897325 209.16741 \n",
       "L 415.962381 202.410835 \n",
       "L 416.027437 205.30651 \n",
       "L 416.222604 200.480384 \n",
       "L 416.28766 204.341285 \n",
       "L 416.352716 203.37606 \n",
       "L 416.417772 202.410835 \n",
       "L 416.482828 203.37606 \n",
       "L 416.547884 208.202185 \n",
       "L 416.743051 198.549934 \n",
       "L 416.808107 199.515159 \n",
       "L 416.873163 198.549934 \n",
       "L 417.003275 204.341285 \n",
       "L 417.06833 203.37606 \n",
       "L 417.198442 198.549934 \n",
       "L 417.263498 201.44561 \n",
       "L 417.328554 199.515159 \n",
       "L 417.39361 200.480384 \n",
       "L 417.458665 206.271735 \n",
       "L 417.523721 199.515159 \n",
       "L 417.718889 208.202185 \n",
       "L 417.849001 202.410835 \n",
       "L 417.914056 207.23696 \n",
       "L 417.979112 201.44561 \n",
       "L 418.044168 204.341285 \n",
       "L 418.109224 203.37606 \n",
       "L 418.17428 199.515159 \n",
       "L 418.304391 211.09786 \n",
       "L 418.434503 200.480384 \n",
       "L 418.564615 207.23696 \n",
       "L 418.759782 199.515159 \n",
       "L 418.824838 205.30651 \n",
       "L 418.95495 198.549934 \n",
       "L 419.020006 206.271735 \n",
       "L 419.085062 202.410835 \n",
       "L 419.150118 203.37606 \n",
       "L 419.215173 200.480384 \n",
       "L 419.280229 204.341285 \n",
       "L 419.345285 200.480384 \n",
       "L 419.410341 202.410835 \n",
       "L 419.475397 202.410835 \n",
       "L 419.540453 205.30651 \n",
       "L 419.605508 204.341285 \n",
       "L 419.670564 205.30651 \n",
       "L 419.865732 202.410835 \n",
       "L 419.930788 204.341285 \n",
       "L 420.125955 199.515159 \n",
       "L 420.191011 208.202185 \n",
       "L 420.256067 195.654259 \n",
       "L 420.386179 207.23696 \n",
       "L 420.451234 201.44561 \n",
       "L 420.51629 208.202185 \n",
       "L 420.581346 204.341285 \n",
       "L 420.646402 207.23696 \n",
       "L 420.711458 202.410835 \n",
       "L 420.776514 205.30651 \n",
       "L 420.84157 205.30651 \n",
       "L 420.971681 202.410835 \n",
       "L 421.036737 206.271735 \n",
       "L 421.101793 201.44561 \n",
       "L 421.231905 210.132635 \n",
       "L 421.296961 207.23696 \n",
       "L 421.362016 202.410835 \n",
       "L 421.492128 206.271735 \n",
       "L 421.62224 200.480384 \n",
       "L 421.687296 203.37606 \n",
       "L 421.752351 199.515159 \n",
       "L 421.947519 205.30651 \n",
       "L 422.077631 202.410835 \n",
       "L 422.207742 205.30651 \n",
       "L 422.272798 201.44561 \n",
       "L 422.40291 204.341285 \n",
       "L 422.467966 202.410835 \n",
       "L 422.533022 203.37606 \n",
       "L 422.598077 203.37606 \n",
       "L 422.663133 200.480384 \n",
       "L 422.728189 201.44561 \n",
       "L 422.793245 207.23696 \n",
       "L 422.858301 206.271735 \n",
       "L 422.988413 203.37606 \n",
       "L 423.053468 208.202185 \n",
       "L 423.118524 205.30651 \n",
       "L 423.18358 204.341285 \n",
       "L 423.248636 205.30651 \n",
       "L 423.313692 202.410835 \n",
       "L 423.378748 208.202185 \n",
       "L 423.443804 202.410835 \n",
       "L 423.508859 208.202185 \n",
       "L 423.573915 200.480384 \n",
       "L 423.638971 203.37606 \n",
       "L 423.704027 206.271735 \n",
       "L 423.834139 202.410835 \n",
       "L 423.899194 205.30651 \n",
       "L 423.96425 200.480384 \n",
       "L 424.029306 201.44561 \n",
       "L 424.094362 203.37606 \n",
       "L 424.224474 197.584709 \n",
       "L 424.354585 205.30651 \n",
       "L 424.419641 200.480384 \n",
       "L 424.484697 202.410835 \n",
       "L 424.549753 206.271735 \n",
       "L 424.614809 205.30651 \n",
       "L 424.679865 203.37606 \n",
       "L 424.74492 207.23696 \n",
       "L 424.809976 205.30651 \n",
       "L 424.875032 205.30651 \n",
       "L 424.940088 202.410835 \n",
       "L 425.005144 207.23696 \n",
       "L 425.0702 202.410835 \n",
       "L 425.135256 208.202185 \n",
       "L 425.200311 206.271735 \n",
       "L 425.265367 203.37606 \n",
       "L 425.330423 212.063085 \n",
       "L 425.395479 207.23696 \n",
       "L 425.525591 200.480384 \n",
       "L 425.590647 203.37606 \n",
       "L 425.720758 201.44561 \n",
       "L 425.785814 201.44561 \n",
       "L 425.85087 207.23696 \n",
       "L 425.915926 203.37606 \n",
       "L 425.980982 204.341285 \n",
       "L 426.046037 208.202185 \n",
       "L 426.111093 202.410835 \n",
       "L 426.176149 206.271735 \n",
       "L 426.306261 201.44561 \n",
       "L 426.436373 207.23696 \n",
       "L 426.696596 202.410835 \n",
       "L 426.761652 205.30651 \n",
       "L 426.891763 199.515159 \n",
       "L 427.021875 203.37606 \n",
       "L 427.086931 202.410835 \n",
       "L 427.151987 198.549934 \n",
       "L 427.282099 206.271735 \n",
       "L 427.347154 205.30651 \n",
       "L 427.477266 198.549934 \n",
       "L 427.607378 208.202185 \n",
       "L 427.867601 201.44561 \n",
       "L 427.997713 203.37606 \n",
       "L 428.127825 201.44561 \n",
       "L 428.19288 206.271735 \n",
       "L 428.257936 205.30651 \n",
       "L 428.322992 202.410835 \n",
       "L 428.388048 203.37606 \n",
       "L 428.453104 202.410835 \n",
       "L 428.51816 204.341285 \n",
       "L 428.648271 202.410835 \n",
       "L 428.778383 202.410835 \n",
       "L 428.843439 200.480384 \n",
       "L 428.908495 201.44561 \n",
       "L 428.973551 201.44561 \n",
       "L 429.038607 203.37606 \n",
       "L 429.103662 200.480384 \n",
       "L 429.168718 203.37606 \n",
       "L 429.233774 202.410835 \n",
       "L 429.29883 203.37606 \n",
       "L 429.363886 200.480384 \n",
       "L 429.428942 207.23696 \n",
       "L 429.493997 203.37606 \n",
       "L 429.559053 204.341285 \n",
       "L 429.624109 201.44561 \n",
       "L 429.689165 205.30651 \n",
       "L 429.754221 197.584709 \n",
       "L 429.819277 208.202185 \n",
       "L 429.884333 203.37606 \n",
       "L 429.949388 203.37606 \n",
       "L 430.144556 206.271735 \n",
       "L 430.274668 199.515159 \n",
       "L 430.404779 207.23696 \n",
       "L 430.469835 198.549934 \n",
       "L 430.534891 202.410835 \n",
       "L 430.599947 199.515159 \n",
       "L 430.665003 204.341285 \n",
       "L 430.730059 202.410835 \n",
       "L 430.86017 207.23696 \n",
       "L 430.925226 206.271735 \n",
       "L 430.990282 204.341285 \n",
       "L 431.055338 205.30651 \n",
       "L 431.120394 208.202185 \n",
       "L 431.315561 202.410835 \n",
       "L 431.380617 206.271735 \n",
       "L 431.445673 203.37606 \n",
       "L 431.510729 204.341285 \n",
       "L 431.575785 207.23696 \n",
       "L 431.64084 200.480384 \n",
       "L 431.705896 204.341285 \n",
       "L 431.770952 206.271735 \n",
       "L 431.836008 202.410835 \n",
       "L 432.031176 206.271735 \n",
       "L 432.226343 202.410835 \n",
       "L 432.291399 202.410835 \n",
       "L 432.356455 203.37606 \n",
       "L 432.421511 199.515159 \n",
       "L 432.486566 205.30651 \n",
       "L 432.551622 198.549934 \n",
       "L 432.616678 200.480384 \n",
       "L 432.681734 203.37606 \n",
       "L 432.811846 198.549934 \n",
       "L 432.941957 203.37606 \n",
       "L 433.007013 203.37606 \n",
       "L 433.072069 202.410835 \n",
       "L 433.137125 204.341285 \n",
       "L 433.267237 200.480384 \n",
       "L 433.332293 203.37606 \n",
       "L 433.52746 199.515159 \n",
       "L 433.722628 207.23696 \n",
       "L 433.917795 200.480384 \n",
       "L 433.982851 200.480384 \n",
       "L 434.047907 199.515159 \n",
       "L 434.112963 200.480384 \n",
       "L 434.178019 205.30651 \n",
       "L 434.243074 203.37606 \n",
       "L 434.438242 198.549934 \n",
       "L 434.503298 204.341285 \n",
       "L 434.568354 201.44561 \n",
       "L 434.633409 199.515159 \n",
       "L 434.763521 202.410835 \n",
       "L 434.828577 200.480384 \n",
       "L 434.958689 206.271735 \n",
       "L 435.153856 198.549934 \n",
       "L 435.283968 207.23696 \n",
       "L 435.349024 202.410835 \n",
       "L 435.41408 206.271735 \n",
       "L 435.609247 199.515159 \n",
       "L 435.674303 200.480384 \n",
       "L 435.869471 206.271735 \n",
       "L 435.934526 201.44561 \n",
       "L 435.999582 202.410835 \n",
       "L 436.129694 204.341285 \n",
       "L 436.19475 203.37606 \n",
       "L 436.389917 199.515159 \n",
       "L 436.585085 209.16741 \n",
       "L 436.650141 201.44561 \n",
       "L 436.715197 203.37606 \n",
       "L 436.780252 204.341285 \n",
       "L 436.910364 203.37606 \n",
       "L 436.97542 203.37606 \n",
       "L 437.040476 202.410835 \n",
       "L 437.105532 209.16741 \n",
       "L 437.170588 200.480384 \n",
       "L 437.235643 205.30651 \n",
       "L 437.300699 203.37606 \n",
       "L 437.365755 205.30651 \n",
       "L 437.560923 201.44561 \n",
       "L 437.691034 205.30651 \n",
       "L 437.75609 203.37606 \n",
       "L 437.821146 207.23696 \n",
       "L 437.951258 199.515159 \n",
       "L 438.016314 204.341285 \n",
       "L 438.146425 198.549934 \n",
       "L 438.211481 207.23696 \n",
       "L 438.276537 202.410835 \n",
       "L 438.471705 207.558697 \n",
       "L 438.471705 207.558697 \n",
       "\" clip-path=\"url(#pe8097749a7)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 278.35625 299.078125 \n",
       "L 278.35625 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 446.09625 299.078125 \n",
       "L 446.09625 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 278.35625 299.078125 \n",
       "L 446.09625 299.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 278.35625 189.718125 \n",
       "L 446.09625 189.718125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_52\">\n",
       "    <!-- acc curve -->\n",
       "    <g transform=\"translate(333.233125 183.718125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"171.240234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"203.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"258.007812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"321.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"362.5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"421.679688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_4\">\n",
       "    <g id=\"patch_25\">\n",
       "     <path d=\"M 361.69625 294.078125 \n",
       "L 439.09625 294.078125 \n",
       "Q 441.09625 294.078125 441.09625 292.078125 \n",
       "L 441.09625 263.165625 \n",
       "Q 441.09625 261.165625 439.09625 261.165625 \n",
       "L 361.69625 261.165625 \n",
       "Q 359.69625 261.165625 359.69625 263.165625 \n",
       "L 359.69625 292.078125 \n",
       "Q 359.69625 294.078125 361.69625 294.078125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_49\">\n",
       "     <path d=\"M 363.69625 269.264062 \n",
       "L 373.69625 269.264062 \n",
       "L 383.69625 269.264062 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_53\">\n",
       "     <!-- val_acc -->\n",
       "     <g transform=\"translate(391.69625 272.764062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_50\">\n",
       "     <path d=\"M 363.69625 284.220312 \n",
       "L 373.69625 284.220312 \n",
       "L 383.69625 284.220312 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_54\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(391.69625 287.720312) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"344.042969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"399.023438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pae7921fb36\">\n",
       "   <rect x=\"56.50625\" y=\"22.318125\" width=\"167.74\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p336acdb2ba\">\n",
       "   <rect x=\"278.35625\" y=\"22.318125\" width=\"167.74\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pb8b62f9e74\">\n",
       "   <rect x=\"56.50625\" y=\"189.718125\" width=\"167.74\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pe8097749a7\">\n",
       "   <rect x=\"278.35625\" y=\"189.718125\" width=\"167.74\" height=\"109.36\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.network = nn.Sequential(nn.Flatten(),\n",
    "        #                              nn.Linear(28*28, 2**5), nn.ReLU(),\n",
    "        #                              nn.Linear(2**5, 10), nn.Softmax())\n",
    "        self.num_hidden = 2**5\n",
    "        self.layer1 = nn.Flatten()\n",
    "        self.layer2 = nn.Linear(28*28, self.num_hidden)\n",
    "        self.layer3 = nn.Linear(self.num_hidden, self.num_hidden)\n",
    "        self.ac = nn.ReLU()\n",
    "        # self.ac = nn.Tanh()\n",
    "        self.dp = nn.Dropout()\n",
    "        self.bn = nn.BatchNorm1d(self.num_hidden)\n",
    "        self.layer4 = nn.Linear(self.num_hidden, 10)\n",
    "        self.layer5 = nn.Softmax()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.layer1(X)\n",
    "        y = self.layer2(y)\n",
    "        \n",
    "        for i in range(2):\n",
    "            y = y + self.dp(self.ac(self.bn(self.layer3(y))))\n",
    "                   \n",
    "        y = self.layer4(y)\n",
    "        y = self.layer5(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)\n",
    "\n",
    "trainer= Trainer(\n",
    "    device= 'auto', \n",
    "    train_dataloader= data.DataLoader(train_dataset, batch_size= 128, shuffle= True),\n",
    "    val_dataloader= data.DataLoader(test_dataset, batch_size= 128), \n",
    "    model= net, \n",
    "    loss_fn= loss_fn, \n",
    "    optimizer= opt, \n",
    "    is_tqdm= False\n",
    ")\n",
    "\n",
    "trainer.train(epochs= 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.3. <a id='toc8_8_3_'></a>[K折交叉验证](#toc0_)\n",
    "\n",
    "简述：把数据分成K份，分别只取1份做Test_data，（K-1）做Train_data，做K次，计算Test_acc的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1 # k必须大于1\n",
    "    fold_size = X.shape[0] // k # 窗口大小：X一维数据长度除以k向下取整数\n",
    "    print('fold_size: ', fold_size)\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size) # 切片范围 (窗口大小)\n",
    "        print(idx)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]),\n",
       " tensor([[  0,  -1,  -2],\n",
       "         [ -3,  -4,  -5],\n",
       "         [ -6,  -7,  -8],\n",
       "         [ -9, -10, -11],\n",
       "         [-12, -13, -14]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15).reshape(5, 3)\n",
    "y = torch.negative(torch.arange(15).reshape(5, 3))\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_size:  2\n",
      "slice(0, 2, None)\n",
      "slice(2, 4, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 0, -1, -2],\n",
       "         [-3, -4, -5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]),\n",
       " tensor([[ -6,  -7,  -8],\n",
       "         [ -9, -10, -11]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_fold_data(2, 1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.9. <a id='toc8_9_'></a>[可视化训练过程](#toc0_)\n",
    "* 清理上一次\n",
    "    * figure plt.clf() # 只是清理figure内容\n",
    "    * figure `plt.colse()` # 关闭（释放）figure\n",
    "    * axes plt.cla() # 只是清理axes内容\n",
    "* 绘图plot\n",
    "* 用jupyter的display来显示\n",
    "* 保持yupyter上的display直至下一次展示再清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印图片耗时： 3.3102283477783203 seconds\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"256.140024pt\" height=\"183.35625pt\" viewBox=\"0 0 256.140024 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-03-27T19:15:07.151762</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 256.140024 183.35625 \n",
       "L 256.140024 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 247.460938 145.8 \n",
       "L 247.460938 7.2 \n",
       "L 52.160938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m76273655c0\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76273655c0\" x=\"61.03821\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(57.85696 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76273655c0\" x=\"97.346073\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(90.983573 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76273655c0\" x=\"133.653936\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(127.291436 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76273655c0\" x=\"169.961799\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(163.599299 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76273655c0\" x=\"206.269661\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(199.907161 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76273655c0\" x=\"242.577524\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(236.215024 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(134.582813 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m3234d40189\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3234d40189\" x=\"52.160938\" y=\"139.500616\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(20.878125 143.299835) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3234d40189\" x=\"52.160938\" y=\"108.000363\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 111.799582) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3234d40189\" x=\"52.160938\" y=\"76.500111\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 80.29933) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3234d40189\" x=\"52.160938\" y=\"44.999858\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 48.799077) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3234d40189\" x=\"52.160938\" y=\"13.499606\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(29.257812 17.298824) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 61.03821 76.500111 \n",
       "L 62.853603 46.296059 \n",
       "L 63.942839 31.306315 \n",
       "L 64.668997 23.487016 \n",
       "L 65.395154 17.781178 \n",
       "L 66.121311 14.41628 \n",
       "L 66.48439 13.657422 \n",
       "L 66.847468 13.52647 \n",
       "L 67.210547 14.024726 \n",
       "L 67.573626 15.147218 \n",
       "L 67.936704 16.882727 \n",
       "L 68.662861 22.117481 \n",
       "L 69.389018 29.520304 \n",
       "L 70.478255 44.023271 \n",
       "L 71.930569 67.609479 \n",
       "L 74.47212 109.880058 \n",
       "L 75.561355 124.17905 \n",
       "L 76.287512 131.409817 \n",
       "L 77.01367 136.451524 \n",
       "L 77.376748 138.085002 \n",
       "L 77.739827 139.103144 \n",
       "L 78.102905 139.495779 \n",
       "L 78.465985 139.258981 \n",
       "L 78.829063 138.395119 \n",
       "L 79.192142 136.912826 \n",
       "L 79.9183 132.158191 \n",
       "L 80.644456 125.184661 \n",
       "L 81.733691 111.193588 \n",
       "L 83.186006 87.976446 \n",
       "L 85.727558 45.37071 \n",
       "L 86.816792 30.574697 \n",
       "L 87.542949 22.922185 \n",
       "L 88.269107 17.405639 \n",
       "L 88.632186 15.520684 \n",
       "L 88.995264 14.245016 \n",
       "L 89.358342 13.591377 \n",
       "L 89.72142 13.5663 \n",
       "L 90.0845 14.170042 \n",
       "L 90.44758 15.39657 \n",
       "L 90.810657 17.233606 \n",
       "L 91.536814 22.659935 \n",
       "L 92.262974 30.23274 \n",
       "L 93.35221 44.935574 \n",
       "L 94.804523 68.659431 \n",
       "L 97.346073 110.773717 \n",
       "L 98.43531 124.864713 \n",
       "L 99.161466 131.921389 \n",
       "L 99.887623 136.768595 \n",
       "L 100.250699 138.299581 \n",
       "L 100.613779 139.213094 \n",
       "L 100.976859 139.5 \n",
       "L 101.339936 139.157431 \n",
       "L 101.703016 138.188805 \n",
       "L 102.066096 136.603802 \n",
       "L 102.792252 131.654039 \n",
       "L 103.518409 124.505468 \n",
       "L 104.607646 110.304477 \n",
       "L 106.059959 86.933281 \n",
       "L 108.601512 44.45416 \n",
       "L 109.690748 29.85603 \n",
       "L 110.416905 22.372469 \n",
       "L 111.143062 17.046804 \n",
       "L 111.506142 15.263139 \n",
       "L 111.869218 14.091345 \n",
       "L 112.232298 13.543113 \n",
       "L 112.595378 13.62393 \n",
       "L 112.958455 14.33298 \n",
       "L 113.321531 15.663176 \n",
       "L 114.047688 20.127799 \n",
       "L 114.773844 26.839803 \n",
       "L 115.500004 35.531647 \n",
       "L 116.589241 51.505265 \n",
       "L 118.767711 88.524274 \n",
       "L 120.220024 111.657646 \n",
       "L 121.309264 125.5367 \n",
       "L 122.035417 132.41726 \n",
       "L 122.761577 137.068637 \n",
       "L 123.12465 138.496688 \n",
       "L 123.48773 139.305312 \n",
       "L 123.85081 139.486406 \n",
       "L 124.21389 139.038157 \n",
       "L 124.57697 137.965041 \n",
       "L 124.94005 136.277786 \n",
       "L 125.66621 131.134251 \n",
       "L 126.392363 123.812686 \n",
       "L 127.481603 109.405756 \n",
       "L 128.933909 85.887196 \n",
       "L 131.47547 43.546622 \n",
       "L 132.564703 29.150574 \n",
       "L 133.290863 21.838047 \n",
       "L 134.017016 16.704776 \n",
       "L 134.380096 15.022916 \n",
       "L 134.743169 13.955331 \n",
       "L 135.106249 13.512655 \n",
       "L 135.469329 13.699329 \n",
       "L 135.832409 14.513496 \n",
       "L 136.195489 15.947018 \n",
       "L 136.921642 20.608729 \n",
       "L 137.647795 27.498624 \n",
       "L 138.373955 36.34213 \n",
       "L 139.463195 52.481159 \n",
       "L 143.820135 122.080613 \n",
       "L 144.546295 129.812422 \n",
       "L 145.272455 135.418824 \n",
       "L 145.635535 137.35157 \n",
       "L 145.998615 138.676306 \n",
       "L 146.361688 139.379784 \n",
       "L 146.724768 139.455002 \n",
       "L 147.087848 138.901193 \n",
       "L 147.450928 137.723887 \n",
       "L 147.814008 135.934849 \n",
       "L 148.540161 130.599075 \n",
       "L 149.266314 123.106569 \n",
       "L 150.355554 108.497831 \n",
       "L 151.807867 84.838338 \n",
       "L 154.34942 42.648502 \n",
       "L 155.438653 28.458543 \n",
       "L 156.164813 21.319138 \n",
       "L 156.890967 16.379676 \n",
       "L 157.254047 14.800084 \n",
       "L 157.617127 13.836985 \n",
       "L 157.9802 13.5 \n",
       "L 158.34328 13.792483 \n",
       "L 158.70636 14.711527 \n",
       "L 159.06944 16.247942 \n",
       "L 159.7956 21.105487 \n",
       "L 160.52176 28.171456 \n",
       "L 161.610993 42.273508 \n",
       "L 163.063306 65.572561 \n",
       "L 165.604852 108.113047 \n",
       "L 166.694085 122.805393 \n",
       "L 167.420245 130.369284 \n",
       "L 168.146405 135.785564 \n",
       "L 168.509485 137.617268 \n",
       "L 168.872565 138.838302 \n",
       "L 169.235645 139.436471 \n",
       "L 169.598719 139.405803 \n",
       "L 169.961799 138.746602 \n",
       "L 170.324879 137.465452 \n",
       "L 170.687959 135.575146 \n",
       "L 171.414112 130.048604 \n",
       "L 172.140272 122.387189 \n",
       "L 173.229505 107.580864 \n",
       "L 174.681825 83.787122 \n",
       "L 176.860298 47.180317 \n",
       "L 177.949531 32.011043 \n",
       "L 178.675691 24.035789 \n",
       "L 179.401837 18.152229 \n",
       "L 180.127997 14.594697 \n",
       "L 180.49107 13.736377 \n",
       "L 180.854157 13.50516 \n",
       "L 181.21723 13.903364 \n",
       "L 181.580317 14.927048 \n",
       "L 181.943391 16.565904 \n",
       "L 182.669551 21.617851 \n",
       "L 183.395697 28.857637 \n",
       "L 184.484944 43.167632 \n",
       "L 185.937264 66.617405 \n",
       "L 188.478817 109.025016 \n",
       "L 189.56805 123.517244 \n",
       "L 190.29421 130.911035 \n",
       "L 191.020356 136.135545 \n",
       "L 191.383443 137.865714 \n",
       "L 191.746516 138.982679 \n",
       "L 192.109589 139.475359 \n",
       "L 192.472676 139.338811 \n",
       "L 192.835749 138.574415 \n",
       "L 193.198836 137.189747 \n",
       "L 193.924982 132.621265 \n",
       "L 194.651142 125.81533 \n",
       "L 195.377303 117.043337 \n",
       "L 196.466536 100.982161 \n",
       "L 200.823482 31.267348 \n",
       "L 201.549642 23.456775 \n",
       "L 202.275802 17.760889 \n",
       "L 203.001962 14.406768 \n",
       "L 203.365035 13.653487 \n",
       "L 203.728108 13.528126 \n",
       "L 204.091195 14.031973 \n",
       "L 204.454268 15.159956 \n",
       "L 204.817355 16.900887 \n",
       "L 205.543501 22.145731 \n",
       "L 206.269661 29.557607 \n",
       "L 207.358894 44.071183 \n",
       "L 208.811215 67.664925 \n",
       "L 211.352754 109.927376 \n",
       "L 212.441987 124.215479 \n",
       "L 213.168147 131.437166 \n",
       "L 213.894307 136.468666 \n",
       "L 214.25738 138.096733 \n",
       "L 214.620467 139.109392 \n",
       "L 214.983554 139.496451 \n",
       "L 215.346627 139.254062 \n",
       "L 215.709714 138.384635 \n",
       "L 216.072787 136.896915 \n",
       "L 216.798947 132.131916 \n",
       "L 217.525107 125.14904 \n",
       "L 218.61434 111.146777 \n",
       "L 220.066646 87.921465 \n",
       "L 222.6082 45.322085 \n",
       "L 223.697433 30.53644 \n",
       "L 224.423593 22.89276 \n",
       "L 225.149753 17.386248 \n",
       "L 225.512826 15.506659 \n",
       "L 225.875899 14.236477 \n",
       "L 226.238986 13.588381 \n",
       "L 226.602059 13.568895 \n",
       "L 226.965132 14.178179 \n",
       "L 227.328219 15.410201 \n",
       "L 227.691292 17.252562 \n",
       "L 228.417452 22.688977 \n",
       "L 229.143612 30.270686 \n",
       "L 230.232859 44.984106 \n",
       "L 231.685165 68.714942 \n",
       "L 234.226718 110.820693 \n",
       "L 235.315952 124.900541 \n",
       "L 236.042112 131.947998 \n",
       "L 236.768272 136.784904 \n",
       "L 237.131345 138.310437 \n",
       "L 237.494418 139.218403 \n",
       "L 237.857505 139.499726 \n",
       "L 238.220578 139.151573 \n",
       "L 238.583665 138.177401 \n",
       "L 238.583665 138.177401 \n",
       "\" clip-path=\"url(#pbd0a0325d7)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 52.160938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 247.460938 145.8 \n",
       "L 247.460938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 247.460938 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 7.2 \n",
       "L 247.460938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pbd0a0325d7\">\n",
       "   <rect x=\"52.160938\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def dl_plot(x, y):\n",
    "    '''再jupyter中持续刷新展示图片'''\n",
    "    plt.close()                                 # close figure （推荐）\n",
    "    fig = plt.figure(figsize=(3.5, 2.5))\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    # plt.show()                                # 普通展示\n",
    "    display.display(fig)                        # 在jupyter中展示 （推荐）\n",
    "    display.clear_output(wait=True)             # 等待 （必须）\n",
    "\n",
    "    \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    x = torch.arange(0, epoch+1, 0.1)\n",
    "    y = torch.sin(x)\n",
    "    if epoch % 2 == 0:\n",
    "        dl_plot(x, y)\n",
    "stop = time.time()\n",
    "print(f\"打印图片耗时： {stop - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10: \t train_loss=0.8999999761581421 \t train_acc=0.7833268642425537\n",
      "2/10: \t train_loss=1.899999976158142 \t train_acc=0.9463000893592834\n",
      "3/10: \t train_loss=2.9000000953674316 \t train_acc=0.23924924433231354\n",
      "4/10: \t train_loss=3.9000000953674316 \t train_acc=-0.6877662539482117\n",
      "5/10: \t train_loss=4.900000095367432 \t train_acc=-0.9824525713920593\n",
      "6/10: \t train_loss=5.900000095367432 \t train_acc=-0.37387657165527344\n",
      "7/10: \t train_loss=6.900000095367432 \t train_acc=0.5784398317337036\n",
      "8/10: \t train_loss=7.899999618530273 \t train_acc=0.9989413619041443\n",
      "9/10: \t train_loss=8.899999618530273 \t train_acc=0.5010212063789368\n",
      "10/10: \t train_loss=9.899999618530273 \t train_acc=-0.4575355648994446\n",
      "打印数值耗时： 0.001786947250366211 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    x = torch.arange(0, epoch+1, 0.1)\n",
    "    y = torch.sin(x)\n",
    "    # dl_plot(x, y)\n",
    "    print(f\"{epoch+1}/{10}: \\t train_loss={x[-1]} \\t train_acc={y[-1]}\")\n",
    "    \n",
    "stop = time.time()\n",
    "print(f\"打印数值耗时： {stop - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. <a id='toc9_'></a>[在 GPU 上训练](#toc0_)\n",
    "- 要实行运算的Tensor`必须`在同一张GPU卡上：\n",
    "\n",
    "|操作|函数|\n",
    "|:-|:-|\n",
    "|1. 张量传到GPU上 |x_gpu = x`.to`('cuda:0')|\n",
    "|2. 神经网络传到GPU上|net = net`.to`('cuda:0')|\n",
    "\n",
    "- CPU和GPU之间数据传输总结：\n",
    "\n",
    "|对象|方法一|方法二|\n",
    "|:-|:-|:-|\n",
    "|模型上GPU：|model.cuda()|model.`to(device)`|\n",
    "|数据上GPU：|data.cuda()|data.`to(device)`|\n",
    "|输出下GPU：|output=model(data)|output`.detach().cpu().numpy()`|\n",
    "||解释：||\n",
    "||output`.detach()`|将变量output从计算图中分离，使其不具有梯度，不进行反向传播|\n",
    "||`.cpu()`|将GPU数据转CPU|\n",
    "||.numpy()|将Tensor转numpy|\n",
    "||`.item()`|将只有`一个元素`的Tensor转为python数值|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. <a id='toc9_1_'></a>[查看GPU配置](#toc0_)\n",
    "都在`torch.cuda`模块中."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 是否有可用的GPU\n",
    "torch.cuda.is_available()      \n",
    "     \n",
    "# True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可用的GPU数量\n",
    "torch.cuda.device_count()  \n",
    "   \n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-SXM4-40GB'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回gpu名字，设备索引默认从0开始；\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "# \"Tesla T4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回当前设备索引；\n",
    "torch.cuda.current_device()\n",
    "\n",
    "# 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单机2卡: ['NVIDIA A100-SXM4-40GB', 'NVIDIA A100-SXM4-40GB']\n"
     ]
    }
   ],
   "source": [
    "def check_device():\n",
    "    '''判断是否有GPU，并列出GPU的代号/名称'''\n",
    "    if torch.cuda.is_available(): # 判断是否支持cuda/GPU\n",
    "        gpu_num = torch.cuda.device_count() # cuda/GPU计数\n",
    "        if gpu_num == 1:\n",
    "            print(f\"单机单卡: {[torch.cuda.get_device_name(gpu_name) for gpu_name in range(gpu_num)]}\")\n",
    "        else:\n",
    "            print(f\"单机{gpu_num}卡: {[torch.cuda.get_device_name(gpu_name) for gpu_name in range(gpu_num)]}\")\n",
    "    else:\n",
    "        print(f\"只有CPU\")\n",
    "    return None \n",
    "\n",
    "check_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda:0', 'cuda:1']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = [ 'cpu' if not torch.cuda.is_available() else ]\n",
    "device = [f'cuda:{i}' for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else ['cpu']\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device id: 1\n",
      "current device id: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=1), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda() -> 1\n",
    "torch.cuda.set_device(device= 1)\n",
    "x = torch.tensor([1]).cuda()\n",
    "print(f\"current device id: {torch.cuda.current_device()}\")\n",
    "\n",
    "# cuda() -> 0\n",
    "torch.cuda.set_device(device= 0)\n",
    "y = torch.tensor([1]).cuda()\n",
    "print(f\"current device id: {torch.cuda.current_device()}\")\n",
    "\n",
    "x.device, y.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. <a id='toc9_2_'></a>[单机单卡（GPU）](#toc0_)\n",
    "所有的张量必须存在于同一个设备上（同一个CPU或同一个GPU），才能正确计算，否则可能会出现异常错误。  \n",
    "1. 模型上GPU：model.cuda() 或 model.to(device)   \n",
    "2. 数据上GPU：data_gpu = data.cuda() 或 data_gpu = data.to(device)   \n",
    "3. 输出下GPU：output = model(data)  output.detach().cpu().numpy()，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.]]),\n",
       " tensor([[1.],\n",
       "         [1.]]),\n",
       " device(type='cpu'),\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((2, 1))\n",
    "y = torch.ones((2, 1))\n",
    "\n",
    "x, y, x.device, y.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.]], device='cuda:0'),\n",
       " tensor([[1.],\n",
       "         [1.]], device='cuda:0'),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x.to(device)\n",
    "y1 = y.to(device)\n",
    "\n",
    "x1, y1, x1.device, y1.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. <a id='toc9_3_'></a>[单机多卡（GPU）](#toc0_)\n",
    "\n",
    "目前PyTorch的单机多卡训练，主要有两种方式：\n",
    "\n",
    "|方法|函数|注释|\n",
    "|:-|:-|:-|\n",
    "|第一种：|torch.nn.`DataParallel`(module=net, device_ids=[0, 1], output_device=[0])|# 单机两卡|\n",
    "|第二种：|torch.nn.parallel.`DistributedDataParallel`()|# 单机多卡、多机多卡|\n",
    "\n",
    "\n",
    "DataParallel (DP) 和 DistributedDataParallel (DDP) 都是用于在多GPU上进行训练的工具，但它们有一些关键的区别：\n",
    "\n",
    "1. **目标环境：**\n",
    "   - `DataParallel` 适用于单机多卡的情况，通过将模型复制到每个GPU上，每个GPU计算不同的批次，最后通过梯度累积或平均来更新模型参数。\n",
    "   - `DistributedDataParallel` 适用于分布式环境，可以在单机或多台机器上的多个GPU上运行，每个GPU计算不同的批次，并通过分布式通信来同步梯度和更新模型参数。\n",
    "\n",
    "2. **通信方式：**\n",
    "   - `DataParallel` 使用单个进程内的多个GPU，通信相对较简单，仅涉及到进程内的数据传输。\n",
    "   - `DistributedDataParallel` 通过分布式通信协议，如NCCL或Gloo，实现跨进程和可能跨机器的通信，因此需要更复杂的设置。\n",
    "\n",
    "3. **启动方式：**\n",
    "   - `DataParallel` 只需在模型实例上调用 `nn.DataParallel(model)` 即可。\n",
    "   - `DistributedDataParallel` 需要在训练脚本中设置分布式环境变量，如`torch.distributed.launch` 或手动设置`os.environ`。\n",
    "\n",
    "4. **维护性：**\n",
    "   - `DataParallel` 更容易使用，因为它不涉及复杂的分布式设置。\n",
    "   - `DistributedDataParallel` 适用于更复杂的分布式场景，但需要更多的设置和管理。\n",
    "\n",
    "在单机多卡的情况下，如果简单性和易用性是首要考虑的因素，可以使用 DataParallel。在需要更高级的分布式设置时，或者在多机多卡的环境中，DistributedDataParallel 提供了更大的灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1. <a id='toc9_3_1_'></a>[DP](#toc0_)\n",
    "- 单机多线程\n",
    "\n",
    "- 参数详情\n",
    "```python\n",
    "torch.nn.DataParallel(module, device_ids, output_device)  \n",
    "\n",
    "Parameters\n",
    "    module (Module) – module to be parallelized                                                 # 神经网络\n",
    "    device_ids (list of int or torch.device) – CUDA devices (default: all devices)              # 默认使用所用GPU\n",
    "    output_device (int or torch.device) – device location of output (default: device_ids[0])    # 在cuda:0上进行参数分配、计算、汇总、更新\n",
    "Variables\n",
    "    module (Module) – the module to be parallelized\n",
    "```\n",
    "\n",
    "- 前提\n",
    " \n",
    "    1. 有一个前提: net模型被复制到cuda:[0, 1, 2等等]上，但是X, y必须提前在cuda:0上，而不能在cuda:1、cuda:2等等上；\n",
    "\n",
    "    2. 那如果cuda:0有其他人占满了，怎么办？那就需要手动指定其他GPU为cuda:0了：\n",
    "\n",
    "        - os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"      # 一定一定要放在所有访问显卡的代码之前，否则则无效，给我困扰了好一段时间才发现了。我之前看到有一个说法是放到import os之后并且在import torch之前。\n",
    "\n",
    "        - os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2, 3\"         # 只识别2、3而抛弃了其他GPU，把2当成pytorch逻辑上的cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=1.6309014558792114, train_acc=84.05500030517578, test_acc=84.62999725341797\n",
      "epoch 2/10: train_loss=1.5620430707931519, train_acc=91.41999816894531, test_acc=91.5199966430664\n",
      "epoch 3/10: train_loss=1.5354968309402466, train_acc=93.49500274658203, test_acc=93.54999542236328\n",
      "epoch 4/10: train_loss=1.5278639793395996, train_acc=94.12833404541016, test_acc=93.93000030517578\n",
      "epoch 5/10: train_loss=1.518404245376587, train_acc=95.00666809082031, test_acc=94.61000061035156\n",
      "epoch 6/10: train_loss=1.5110242366790771, train_acc=95.67833709716797, test_acc=95.20999908447266\n",
      "epoch 7/10: train_loss=1.507236361503601, train_acc=96.06000518798828, test_acc=95.68000030517578\n",
      "epoch 8/10: train_loss=1.5020065307617188, train_acc=96.47666931152344, test_acc=95.98999786376953\n",
      "epoch 9/10: train_loss=1.4993703365325928, train_acc=96.80000305175781, test_acc=96.15999603271484\n",
      "epoch 10/10: train_loss=1.4964629411697388, train_acc=97.03666687011719, test_acc=96.44999694824219\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 6.5389158725738525 s\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import time \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "dbs = './data/'\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), ])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root= dbs, train= True, download= True, transform= transforms)\n",
    "test_dataset = torchvision.datasets.MNIST(root= dbs, train= False, download= True, transform= transforms )\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(dataset= train_dataset, batch_size= 128,  shuffle= True)\n",
    "test_iter = data.DataLoader(dataset= test_dataset, batch_size= 128, shuffle= False) # test不需要batch训练\n",
    "\n",
    "\n",
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "\n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "    print('='*100, '\\n', f\"Runing on {device}\", '\\n','='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "\n",
    "    net.to(device)\n",
    "    net = nn.DataParallel(module= net)\n",
    "    # net = nn.DataParallel(module=net, device_ids=[0, 1], output_device=[0]) # 多GPU并行计算，等价于net = nn.DataParallel(module=net)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)# 计算loss\n",
    "            loss.backward()         # 计算梯度\n",
    "            opt.step()              # 更新网络参数\n",
    "\n",
    "        net.eval()  # 切换至评估模式\n",
    "                    # 模型默认是net.train()\n",
    "                    # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                    # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    \n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None\n",
    "\n",
    "\n",
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "net = Net()  \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params= net.parameters(), lr= 0.5)   \n",
    "\n",
    "train_steps(\n",
    "    epochs= 10, \n",
    "    train_dataset= train_dataset, \n",
    "    train_iter= train_iter, \n",
    "    test_dataset= test_dataset, \n",
    "    net= net,                        \n",
    "    loss_fn= loss_fn, \n",
    "    opt= opt, \n",
    "    device= device \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. <a id='toc9_3_2_'></a>[DDP](#toc0_)\n",
    "\n",
    "[中文官方教程：https://pytorch.ac.cn/docs/stable/notes/ddp.html](https://pytorch.ac.cn/docs/stable/notes/ddp.html)\n",
    "\n",
    "[https://pytorch.ac.cn/tutorials/beginner/ddp_series_multigpu.html](https://pytorch.ac.cn/tutorials/beginner/ddp_series_multigpu.html)\n",
    "\n",
    "1. 与 DataParallel 的单进程控制多 GPU 不同，在 distributed 的帮助下，我们只需要编写一份代码，torch 就会自动将其分配给 \n",
    " 个进程，分别在 n 个 GPU 上运行。\n",
    "2. 单机多进程\n",
    "\n",
    "\n",
    "详解: `torch.nn.parallel.DistributedDataParallel(module, device_ids, output_device)`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2.1. <a id='toc9_3_2_1_'></a>[在colab上测试可用](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前提：一般是一个进程对应一个GPU。\n",
    "\n",
    "```python\n",
    "- 假设有二台机器（节点, node），该节点上有两个GPUs：\n",
    "  - world_size: int   # node_nums * GPU_nums    -> 2 * 2 = 4\n",
    "  - rank: int         # [0, ... , world_size-1] -> [0, 1, 2, 3]\n",
    "  - loacal_rank: int  # node1 -> [0, 1] , node2 -> [0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.multiprocessing import Process\n",
    "import os\n",
    "\n",
    "\n",
    "# 定义卷积神经网络模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(local_rank, world_size):  \n",
    "    os.environ[\"MASTER_PORT\"] = \"12357\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "\n",
    "    # 设置每个进程的GPU\n",
    "    torch.cuda.set_device(local_rank) # 后续.cuda()，都会到local_rank序号的GPU上\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "    # 初始化进程组\n",
    "    dist.init_process_group(backend= 'nccl', world_size= world_size, rank= local_rank)\n",
    "\n",
    "    # 数据预处理和加载\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    trainset = torchvision.datasets.MNIST(root= './data', train= True, download= True, transform=transform)\n",
    "\n",
    "    # 使用DistributedSampler来对数据进行分布式采样\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size= 32, shuffle= False, sampler= train_sampler)\n",
    "\n",
    "    # 创建CNN模型实例，并放入多个GPU上\n",
    "    model = CNN().to(device)\n",
    "    ddp_model = nn.parallel.DistributedDataParallel(model, device_ids= [local_rank])\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ddp_model.parameters(), lr= 0.001)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_sampler.set_epoch(epoch) \n",
    "        ddp_model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Local Rank {local_rank}, Epoch {epoch + 1}/{num_epochs}, Training Loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size: int = torch.cuda.device_count()                         # 2\n",
    "    local_ranks: list = [i for i in range(torch.cuda.device_count())]   # [0, 1]\n",
    "\n",
    "    # Process格式：\n",
    "    # processes = []\n",
    "    # for local_rank in local_ranks:\n",
    "    #     p = Process(target= train, args= (local_rank, world_size))\n",
    "    #     p.start()\n",
    "    #     processes.append(p)\n",
    "\n",
    "    # for p in processes:\n",
    "    #     p.join()\n",
    "\n",
    "    mp.spawn(\n",
    "        train,\n",
    "        args= (0, world_size),\n",
    "        nprocs= world_size,\n",
    "        join= True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. <a id='toc9_4_'></a>[多机多卡（GPU）- 分布式训练](#toc0_)\n",
    "```shell\n",
    "目前PyTorch的多机多卡训练，主要有两种方式：   \n",
    "    1. torch.nn.parallel.DistributedDataParallel()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nproc_per_node=4 --master_port= 29500 train.py\n",
    "        # --nproc_per_node= N\t指定当前节点（机器）使用的 GPU 数量\n",
    "        # --nnodes= M\t        总节点数（默认 1，单机训练）\n",
    "        # --node_rank= K        当前节点序号（多机训练时使用）\n",
    "        # --master_addr= IP\t    主节点 IP 地址（默认 127.0.0.1）\n",
    "        # --master_port= PORT\t主节点端口（默认 29500）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建进程组：\n",
    "- 在初始化组进程之前，调用 set_device，它为每个进程设置默认 GPU。这对于防止 GPU:0 上的挂起或过度内存使用非常重要。\n",
    "- init_process_group 初始化分布式进程组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup(rank: int, world_size: int):\n",
    "   \"\"\"\n",
    "   Args:\n",
    "       rank: Unique identifier of each process\n",
    "      world_size: Total number of processes\n",
    "   \"\"\"\n",
    "   os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "   os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "   torch.cuda.set_device(rank)\n",
    "   init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据分发:\n",
    "- DistributedSampler 将输入数据分块到所有分布式进程中。\n",
    "- DataLoader 结合了数据集和采样器，并提供给定数据集上的可迭代对象。\n",
    "- `在每个 epoch 开始时在 DistributedSampler 上调用 set_epoch() 方法是必要的，以使混洗在多个 epoch 中正常工作。否则，每个 epoch 中将使用相同的顺序。`\n",
    "    ```python\n",
    "    for epoch in range(epochs):\n",
    "        sampler.set_epoch(epoch)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "\n",
    "# 使用 DistributedSampler 确保数据分片\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas= dist.get_world_size(),  # 总进程数\n",
    "    rank= dist.get_rank(),                # 当前进程的全局 rank\n",
    "    shuffle= True\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size= 64,\n",
    "    sampler= sampler,\n",
    "    shuffle= False                        # Sampler 已处理 shuffle\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sampler.set_epoch(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型并行化:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "ddp_model = DDP(model, device_ids= [local_rank])  # 关键包装操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型检查点:\n",
    "- 个进程都将保存其相同的模型副本,我们只需要从一个进程保存模型检查点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "    self._save_checkpoint(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行分布式训练任务：\n",
    "- 包括新参数 rank（替换 device）和 world_size。\n",
    "- rank 在调用 mp.spawn 时由 DDP 自动分配。\n",
    "- world_size 是整个训练作业中的进程数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "x = torch.tensor([1]).cuda()\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. <a id='toc10_'></a>[模型和参数的保存与加载](#toc0_)\n",
    "\n",
    "* torch.save( 张量名, 位置 )\n",
    "\n",
    "* 张量名称 = torch.load( 位置 )\n",
    "\n",
    "* torch.save会保存数据的`很多关系`，会有条件限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. <a id='toc10_1_'></a>[加载和保存-张量](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.ones((3, 5))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save()\n",
    "torch.save(x, './cache/tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.load()\n",
    "x1 = torch.load('./cache/tensor.pt', weights_only=True)\n",
    "\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. <a id='toc10_2_'></a>[加载和保存-模型参数](#toc0_)\n",
    "保存单个权重向量（或其他张量）确实有用， 但是如果我们想保存整个模型，并在以后加载它们， 单独保存每个向量则会变得很麻烦。 毕竟，我们可能有数百个参数散布在各处。 因此，深度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。 例如，如果我们有一个3层多层感知机，我们需要单独指定架构。 因为模型本身可以包含任意代码，所以模型本身难以序列化。 因此，为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。\n",
    "\n",
    "1. save和load函数可用于张量对象的文件读写。\n",
    "2. 我们可以通过参数字典保存和加载网络的全部参数。\n",
    "3. 保存架构必须在代码中完成，而不是在参数中完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size= (2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save()\n",
    "# 接下来，我们将模型的参数存储在一个叫做“mlp.params”的文件中。\n",
    "torch.save(net.state_dict(), './cache/mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.load()\n",
    "# 为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 \n",
    "# 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。\n",
    "net_params = torch.load('./cache/mlp.params', weights_only= True)\n",
    "clone = MLP()\n",
    "\n",
    "clone.load_state_dict(net_params)\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 完整的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存\n",
    "torch.save(\n",
    "    {\n",
    "        'epoch': '10', \n",
    "        'model_state_dict': net.state_dict(), \n",
    "        # 'opt_state_dict': opt.state_dict(), \n",
    "        'loss': 'loss'\n",
    "    }, \n",
    "    './cache/ckpt.pt'\n",
    ")\n",
    "\n",
    "# 重载\n",
    "check_point = torch.load('./cache/ckpt.pt')\n",
    "\n",
    "check_point['model_state_dict']\n",
    "check_point['loss']\n",
    "check_point['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. <a id='toc10_3_'></a>[safetensor](#toc0_)\n",
    "是有huggingface推出的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        net = nn.Sequential(nn.Linear(12, 128), nn.ReLU(), nn.Linear(128, 2))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return net(X)\n",
    "    \n",
    "\n",
    "net = DemoModel()\n",
    "\n",
    "state_dicts1 = net.state_dict()\n",
    "\n",
    "safetensors.torch.save_file(state_dicts1, './cache/demo.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import safetensors \n",
    "\n",
    "\n",
    "state_dicts2 = safetensors.torch.load_file('./cache/demo.safetensors')\n",
    "\n",
    "state_dicts2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. <a id='toc11_'></a>[神经网络类型](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. <a id='toc11_1_'></a>[CNN](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1. <a id='toc11_1_1_'></a>[概述](#toc0_)\n",
    "\n",
    "\n",
    "CBAPD: 卷积，批量归一化，激活，池化，丢弃\n",
    "\n",
    "卷积层就是特征提取，随后将特征传入FC（全连接层）；\n",
    "\n",
    "卷积本身是线性的，但是经过激活函数后可以编程非线性的。\n",
    "\n",
    "- 为什么要用CNN？\n",
    "  - 利用MLP处理图片像素矩阵，太占内存\n",
    "  - 解决办法，顶层设计一个新的算法具备如下特点：\n",
    "    - 局部性\n",
    "    - 平移不变性\n",
    "  - 刚好来自“信号处理中的卷积”符合此类特征：\n",
    "    - 局部性 （固定/通用的卷积核）\n",
    "    - 平移不变性 （特征图在整个图片的位置不固定，可以平移）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2. <a id='toc11_1_2_'></a>[简单CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.1. <a id='toc11_1_2_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1.2.1.1. <a id='toc11_1_2_1_1_'></a>[卷积计算过程](#toc0_)\n",
    "\n",
    " <img src=\"./Pytorch_Pictures/convolution/conv.gif\" width = \"500\" height = \"300\" alt=\"图片名称\" align=center />\n",
    "\n",
    "- 内积后求和\n",
    "\n",
    "- 输出大小：(Xh - Kh + 1, Xw - Kw + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def cov2d(X, kernel)-> torch.Tensor:\n",
    "    '''\n",
    "    手写二维convolution计算过程 (二维互关运算)\n",
    "    \n",
    "    Args: \n",
    "        X (2d): 输入图片像素矩阵\n",
    "        kernel (int): 卷积核\n",
    "\n",
    "    Return: \n",
    "        Y: 卷积计算结果\n",
    "    '''\n",
    "    h, w = kernel.shape\n",
    "    Y = torch.zeros(size=(X.shape[0] - h + 1, X.shape[1] - w + 1))   # 输出形状，暂时用0填充\n",
    "    # print(Y)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * kernel).sum()      # X取子集 * kernel 最后在求和\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(9, dtype=torch.float32).reshape(3, 3)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = torch.arange(4, dtype=torch.float32).reshape(2, 2)\n",
    "\n",
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov2d(X=X, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1.2.1.2. <a id='toc11_1_2_1_2_'></a>[从头卷积层](#toc0_)\n",
    "- 卷积层对输入和卷积核进行互关运算，并添加偏置；\n",
    "- 所以卷积层中两个被训练的参数是卷积核与偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Cov2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        # self.bias = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return cov2d(X, kernel=self.kernel) + self.bias                   # 将conv2d计算添加进来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]),\n",
       " tensor([[ 4.2942,  6.8109],\n",
       "         [11.8445, 14.3613]], grad_fn=<AddBackward0>),\n",
       " Parameter containing:\n",
       " tensor([[0.7042, 0.7619],\n",
       "         [0.6707, 0.3800]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov2d1 = Cov2d(kernel_size=(2, 2))\n",
    "\n",
    "X, cov2d1(X=X), cov2d1.kernel, cov2d1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.2. <a id='toc11_1_2_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5187, -0.3951],\n",
       "          [-0.1480, -0.0244]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    kernel_size = (2, 2), \n",
    "    bias = True\n",
    ")\n",
    "\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "conv2d(X.reshape(shape=(1,1,3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.3. <a id='toc11_1_2_3_'></a>[填充和步幅](#toc0_)\n",
    "\n",
    "- 填充 (padding)\n",
    "\n",
    "  - 输出大小：(Xh - Kh + Ph + 1, Xw - Kw + Pw + 1)\n",
    "\n",
    "  - 一般情况下Kh和Kw为奇数(1,3,5,7) 可得 (输入和输出形状一致)：\n",
    "\n",
    "    - Ph设置为：Kh - 1\n",
    "\n",
    "    - Pw设置为：Kw - 1\n",
    "\n",
    "  - padding填写时写一半 (输入和输出形状一致)：\n",
    "    - padding = (Ph/2, Pw/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]),\n",
       " torch.Size([1, 1, 3, 3]),\n",
       " tensor([[[[-0.8907, -0.1869,  0.2284],\n",
       "           [-0.5685,  0.4497,  0.2547],\n",
       "           [ 4.9647,  3.9560,  0.3975]]]], grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 1, 3, 3]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn  \n",
    "\n",
    "\n",
    "conv2d1 = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    kernel_size = (3, 3), \n",
    "    bias = True, \n",
    "    padding = (1, 1),           # ((3 - 1)/2, (3 - 1)/2)\n",
    "    stride = 1\n",
    ")\n",
    "\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "X = torch.arange(9, dtype=torch.float32).reshape(3, 3)\n",
    "Y = conv2d1(X.reshape(shape=(1,1,3,3)))\n",
    "\n",
    "X, X.reshape(shape=(1,1,3,3)).shape, Y, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步幅 (stride)\n",
    "\n",
    "  - 输出大小为：( (Xh - Kh + Ph + Sh)/Sh, (Xw - Kw + Pw + Sw)/Sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19., 20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27., 28., 29., 30., 31.],\n",
       "         [32., 33., 34., 35., 36., 37., 38., 39.],\n",
       "         [40., 41., 42., 43., 44., 45., 46., 47.],\n",
       "         [48., 49., 50., 51., 52., 53., 54., 55.],\n",
       "         [56., 57., 58., 59., 60., 61., 62., 63.]]),\n",
       " torch.Size([1, 1, 8, 8]),\n",
       " tensor([[[[ -2.2047,  -5.6431,  -6.8988,  -8.1545],\n",
       "           [-13.6358, -17.9951, -19.6276, -21.2601],\n",
       "           [-28.1641, -31.0550, -32.6875, -34.3200],\n",
       "           [-42.6923, -44.1149, -45.7474, -47.3799]]]],\n",
       "        grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 1, 4, 4]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "conv2d1 = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    kernel_size = (3, 3), \n",
    "    bias = True, \n",
    "    padding = (1, 1),           # ((3 - 1)/2, (3 - 1)/2)\n",
    "    stride = 2                  # (8 - 3 + 1 + 2 )/2 = 4\n",
    ")\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "X = torch.arange(64, dtype=torch.float32).reshape(8, 8)\n",
    "Y = conv2d1(X.reshape((1,1,8,8)))\n",
    "\n",
    "X, X.reshape((1,1,8,8)).shape, Y, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.4. <a id='toc11_1_2_4_'></a>[多输入和多输出通道](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19., 20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27., 28., 29., 30., 31.],\n",
       "         [32., 33., 34., 35., 36., 37., 38., 39.],\n",
       "         [40., 41., 42., 43., 44., 45., 46., 47.],\n",
       "         [48., 49., 50., 51., 52., 53., 54., 55.],\n",
       "         [56., 57., 58., 59., 60., 61., 62., 63.]]),\n",
       " torch.Size([1, 1, 8, 8]),\n",
       " tensor([[[[ -2.3830,  -4.8751,  -5.6074,  -6.3396],\n",
       "           [ -4.1800,  -7.9979,  -8.1388,  -8.2797],\n",
       "           [ -4.5518,  -9.1252,  -9.2662,  -9.4071],\n",
       "           [ -4.9237, -10.2526, -10.3935, -10.5344]],\n",
       " \n",
       "          [[ -0.6794,  -3.0746,  -3.6831,  -4.2917],\n",
       "           [  2.9294,  -2.9314,  -2.5534,  -2.1754],\n",
       "           [  8.4542,   0.0925,   0.4705,   0.8484],\n",
       "           [ 13.9789,   3.1163,   3.4943,   3.8723]],\n",
       " \n",
       "          [[ -1.6030,  -2.0202,  -2.3736,  -2.7271],\n",
       "           [ -5.1094,  -4.1152,  -4.3205,  -4.5257],\n",
       "           [ -8.3311,  -5.7571,  -5.9623,  -6.1675],\n",
       "           [-11.5528,  -7.3989,  -7.6042,  -7.8094]]]],\n",
       "        grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 3, 4, 4]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "conv2d1 = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 3, \n",
    "    kernel_size = (3, 3), \n",
    "    bias = True, \n",
    "    padding = (1, 1),           # ((3 - 1)/2, (3 - 1)/2)\n",
    "    stride = 2                  # (8 - 3 + 1 + 2 )/2 = 4\n",
    ")\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "X = torch.arange(64, dtype=torch.float32).reshape(8, 8)\n",
    "Y = conv2d1(X.reshape((1,1,8,8)))\n",
    "\n",
    "X, X.reshape((1,1,8,8)).shape, Y, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.5. <a id='toc11_1_2_5_'></a>[Pooling (汇聚层)](#toc0_)\n",
    "pooling层不包含参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 11.1.2.5.1. <a id='toc11_1_2_5_1_'></a>[平均Pooling](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AvgPool2d(kernel_size=(2, 2), stride=1, padding=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "nn.AvgPool2d(\n",
    "    kernel_size = (2, 2), \n",
    "    padding = 0, \n",
    "    stride = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1.2.5.2. <a id='toc11_1_2_5_2_'></a>[最大Pooling](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "nn.MaxPool2d(\n",
    "    kernel_size = (2, 2), \n",
    "    padding = 0, \n",
    "    stride = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3. <a id='toc11_1_3_'></a>[LeNet](#toc0_)\n",
    "\n",
    "- 最早被Yann LeCun用来识别手写数字的算法\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/LeNet.jpg\" width = \"700\" height = \"300\" alt=\"LeCun\" align=center >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4869, -0.4271, -0.2250,  0.0145, -0.0824, -0.5031, -0.2721,  0.5979,\n",
       "         -0.1851, -0.4968]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5), \n",
    "            nn.Sigmoid(), \n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(16 * 5 * 5, 120), \n",
    "            nn.Sigmoid(), \n",
    "            nn.Linear(120, 84), \n",
    "            nn.Sigmoid(), \n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "\n",
    "# 测试\n",
    "lenet = LeNet()\n",
    "\n",
    "X = torch.arange(28*28, dtype=torch.float32).reshape((1, 1, 28, 28))\n",
    "# X = torch.rand(size=(1,1,28,28), dtype=torch.float32)\n",
    "# X.shape\n",
    "\n",
    "lenet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.4. <a id='toc11_1_4_'></a>[AlexNet](#toc0_)\n",
    "\n",
    "- 第一个在大规模视觉比赛 (ImageNet) 中战胜传统给算法 (如支持向量机 supportvectormachines) 的**大型神经网络**\n",
    "\n",
    "- 证明算法学习的特征可以超越手动设计的特征\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/AlexNet.jpg\" width = \"500\" height = \"700\" alt=\"图片名称\" align=center >  \n",
    "\n",
    "- LeNet VS AlexNet：\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/LeNetVSAlexNet.jpg\" width = \"1000\" height = \"300\" alt=\"图片名称\" align=center >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import AlexNet\n",
    "\n",
    "\n",
    "alexnet = AlexNet()\n",
    "\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.5. <a id='toc11_1_5_'></a>[VGG](#toc0_)\n",
    "\n",
    "- 利用重复的神经网络块\n",
    "\n",
    "  - 卷积层，如Conv2d()\n",
    "  - 非线性激活，如nn.Relu()\n",
    "  - 汇聚层，如nn.MaxPooling()\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/VGG.jpg\" width = \"500\" height = \"500\" alt=\"图片名称\" align=center >  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模块设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (5): Flatten(start_dim=1, end_dim=-1)\n",
       "  (6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Dropout(p=0.5, inplace=False)\n",
       "  (9): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.5, inplace=False)\n",
       "  (12): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "conv_arch = (\n",
    "    (1, 64), \n",
    "    (1, 128), \n",
    "    (2, 256), \n",
    "    (2, 512), \n",
    "    (2, 512)\n",
    ")\n",
    "\n",
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "        \n",
    "    return nn.Sequential(\n",
    "        *conv_blks, \n",
    "        nn.Flatten(), \n",
    "        # 全连接部分\n",
    "        nn.Linear(out_channels*7*7, 4096), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(p=0.5), \n",
    "        nn.Linear(4096, 4096), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(p=0.5), \n",
    "        nn.Linear(4096, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "net = vgg(conv_arch)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vgg11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vgg11\n",
    "\n",
    "\n",
    "vgg = vgg11()\n",
    "\n",
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.6. <a id='toc11_1_6_'></a>[NiN](#toc0_)\n",
    "\n",
    "- 使用1 x 1卷积层来替代全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.7. <a id='toc11_1_7_'></a>[GoogLeNet](#toc0_)\n",
    "\n",
    "- 2014年的ImageNet挑战赛中，GoogLeNet大放异彩；\n",
    "\n",
    "- 解决了到底选多大的卷积核的问题？结论是：使用不同大小的卷积核组合更加有利。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.8. <a id='toc11_1_8_'></a>[批量规范化](#toc0_)\n",
    "\n",
    "- batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.9. <a id='toc11_1_9_'></a>[ResNet](#toc0_)\n",
    "```shell\n",
    "如果，CNN只需要弄懂一个神经网络模型的话，那就是ResNet。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.9.1. <a id='toc11_1_9_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.LSTM(), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet34 \n",
    "\n",
    "\n",
    "resnet = resnet34()\n",
    "\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. <a id='toc11_2_'></a>[序列数据](#toc0_)\n",
    "### 11.2.1. <a id='toc11_2_1_'></a>[什么是序列](#toc0_)\n",
    "\n",
    "在深度学习中，**序列**是一段具有连续关系的数据，通常带有时间先后顺序。例如，文本、语音、股票价格、气温、DNA序列等都可以被视为序列数据。为了处理不定长的数据，我们常常使用循环神经网络（RNN）来处理序列信息。总之，序列数据在许多领域中都有广泛的应用，包括自然语言处理、时间序列分析、音频处理和图像处理等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2. <a id='toc11_2_2_'></a>[语言模型](#toc0_)\n",
    "\n",
    "语言模型 (language model) 是定义在单词序列上的概率模型，可以用来计算一个句子或一段文字的概率。\n",
    "\n",
    "常见的语言模型包括：\n",
    "  - `n-gram模型`：基于统计的方法，通过计算n个连续词出现的概率来预测下一个词。\n",
    "  - `神经网络语言模型`：使用神经网络（如RNN、LSTM、Transformer等）来捕捉语言的复杂模式和长距离依赖关系。\n",
    "  - `预训练语言模型`：如GPT（生成式预训练变换器）和BERT（双向编码器表示）等，这些模型在大量文本上进行预训练，然后可以通过微调应用于特定任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3. <a id='toc11_2_3_'></a>[文本预处理](#toc0_)\n",
    "* token：最小单位（字符/单词/词组）\n",
    "* vocab：（token：indice）对照（查询）列表\n",
    "* cropus：token转化为indice后的文本，也称之为语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.1. <a id='toc11_2_3_1_'></a>[下载《Time machine》并读取数据](#toc0_)\n",
    "首先，我们从H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)中加载文本。\n",
    "这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，\n",
    "而现实中的文档集合可能会包含数十亿个单词。\n",
    "下面的函数 (**将数据集读取到由多条文本行组成的列表中**)，其中每条文本行都是一个字符串。\n",
    "为简单起见，我们在这里忽略了标点符号和字母大写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines: ['the time machine by h g wells', '', '', '', '', 'i', '', '', 'the time traveller for so it will be convenient to speak of him', 'was expounding a recondite matter to us his grey eyes shone and', 'twinkled and his usually pale face was flushed and animated the', 'fire burned brightly and the soft radiance of the incandescent', 'lights in the lilies of silver caught the bubbles that flashed and', 'passed in our glasses our chairs being his patents embraced and', 'caressed us rather than submitted to be sat upon and there was that', 'luxurious after dinner atmosphere when thought roams gracefully', 'free of the trammels of precision and he put it to us in this', 'way marking the points with a lean forefinger as we sat and lazily', 'admired his earnestness over this new paradox as we thought it', 'and his fecundity', '', 'you must follow me carefully i shall have to controvert one or two', 'ideas that are almost universally accepted the geometry for', 'instance they taught you at school is founded on a misconception', '', 'is not that rather a large thing to expect us to begin upon', 'said filby an argumentative person with red hair', '', 'i do not mean to ask you to accept anything without reasonable', 'ground for it you will soon admit as much as i need from you you', 'know of course that a mathematical line a line of thickness nil', 'has no real existence they taught you that neither has a', 'mathematical plane these things are mere abstractions', '', 'that is all right said the psychologist', '', 'nor having only length breadth and thickness can a cube have a', 'real existence', '', 'there i object said filby of course a solid body may exist all', 'real things', '', 'so most people think but wait a moment can an instantaneous', 'cube exist', '', 'don t follow you said filby', '', 'can a cube that does not last for any time at all have a real', 'existence', '', 'filby became pensive clearly the time traveller proceeded any', 'real body must have extension in four directions it must have', 'length breadth thickness and duration but through a natural', 'infirmity of the flesh which i will explain to you in a moment we', 'incline to overlook this fact there are really four dimensions', 'three which we call the three planes of space and a fourth time', 'there is however a tendency to draw an unreal distinction between', 'the former three dimensions and the latter because it happens that', 'our consciousness moves intermittently in one direction along the', 'latter from the beginning to the end of our lives', '', 'that said a very young man making spasmodic efforts to relight', 'his cigar over the lamp that very clear indeed', '', 'now it is very remarkable that this is so extensively overlooked', 'continued the time traveller with a slight accession of', 'cheerfulness really this is what is meant by the fourth dimension', 'though some people who talk about the fourth dimension do not know', 'they mean it it is only another way of looking at time there is', 'no difference between time and any of the three dimensions of space', 'except that our consciousness moves along it but some foolish', 'people have got hold of the wrong side of that idea you have all', 'heard what they have to say about this fourth dimension', '', 'i have not said the provincial mayor', '', 'it is simply this that space as our mathematicians have it is', 'spoken of as having three dimensions which one may call length', 'breadth and thickness and is always definable by reference to', 'three planes each at right angles to the others but some', 'philosophical people have been asking why three dimensions', 'particularly why not another direction at right angles to the other', 'three and have even tried to construct a four dimension geometry', 'professor simon newcomb was expounding this to the new york', 'mathematical society only a month or so ago you know how on a flat', 'surface which has only two dimensions we can represent a figure of', 'a three dimensional solid and similarly they think that by models', 'of three dimensions they could represent one of four if they could', 'master the perspective of the thing see', '', 'i think so murmured the provincial mayor and knitting his', 'brows he lapsed into an introspective state his lips moving as one', 'who repeats mystic words yes i think i see it now he said after', 'some time brightening in a quite transitory manner', '', 'well i do not mind telling you i have been at work upon this', 'geometry of four dimensions for some time some of my results', 'are curious for instance here is a portrait of a man at eight', 'years old another at fifteen another at seventeen another at', 'twenty three and so on all these are evidently sections as it', 'were three dimensional representations of his four dimensioned', 'being which is a fixed and unalterable thing', '', 'scientific people proceeded the time traveller after the pause', 'required for the proper assimilation of this know very well that', 'time is only a kind of space here is a popular scientific diagram', 'a weather record this line i trace with my finger shows the', 'movement of the barometer yesterday it was so high yesterday night', 'it fell then this morning it rose again and so gently upward to', 'here surely the mercury did not trace this line in any of the', 'dimensions of space generally recognized but certainly it traced', 'such a line and that line therefore we must conclude was along', 'the time dimension', '', 'but said the medical man staring hard at a coal in the fire if', 'time is really only a fourth dimension of space why is it and why', 'has it always been regarded as something different and why cannot', 'we move in time as we move about in the other dimensions of space', '', 'the time traveller smiled are you sure we can move freely in', 'space right and left we can go backward and forward freely enough', 'and men always have done so i admit we move freely in two', 'dimensions but how about up and down gravitation limits us there', '', 'not exactly said the medical man there are balloons', '', 'but before the balloons save for spasmodic jumping and the', 'inequalities of the surface man had no freedom of vertical', 'movement', '', 'still they could move a little up and down said the medical man', '', 'easier far easier down than up', '', 'and you cannot move at all in time you cannot get away from the', 'present moment', '', 'my dear sir that is just where you are wrong that is just where', 'the whole world has gone wrong we are always getting away from the', 'present moment our mental existences which are immaterial and have', 'no dimensions are passing along the time dimension with a uniform', 'velocity from the cradle to the grave just as we should travel down', 'if we began our existence fifty miles above the earth s surface', '', 'but the great difficulty is this interrupted the psychologist', 'you can move about in all directions of space but you cannot', 'move about in time', '', 'that is the germ of my great discovery but you are wrong to say', 'that we cannot move about in time for instance if i am recalling', 'an incident very vividly i go back to the instant of its occurrence', 'i become absent minded as you say i jump back for a moment of', 'course we have no means of staying back for any length of time any', 'more than a savage or an animal has of staying six feet above the', 'ground but a civilized man is better off than the savage in this', 'respect he can go up against gravitation in a balloon and why', 'should he not hope that ultimately he may be able to stop or', 'accelerate his drift along the time dimension or even turn about', 'and travel the other way', '', 'oh this began filby is all', '', 'why not said the time traveller', '', 'it s against reason said filby', '', 'what reason said the time traveller', '', 'you can show black is white by argument said filby but you will', 'never convince me', '', 'possibly not said the time traveller but now you begin to see', 'the object of my investigations into the geometry of four', 'dimensions long ago i had a vague inkling of a machine', '', 'to travel through time exclaimed the very young man', '', 'that shall travel indifferently in any direction of space and time', 'as the driver determines', '', 'filby contented himself with laughter', '', 'but i have experimental verification said the time traveller', '', 'it would be remarkably convenient for the historian the', 'psychologist suggested one might travel back and verify the', 'accepted account of the battle of hastings for instance', '', 'don t you think you would attract attention said the medical man', 'our ancestors had no great tolerance for anachronisms', '', 'one might get one s greek from the very lips of homer and plato', 'the very young man thought', '', 'in which case they would certainly plough you for the little go', 'the german scholars have improved greek so much', '', 'then there is the future said the very young man just think', 'one might invest all one s money leave it to accumulate at', 'interest and hurry on ahead', '', 'to discover a society said i erected on a strictly communistic', 'basis', '', 'of all the wild extravagant theories began the psychologist', '', 'yes so it seemed to me and so i never talked of it until', '', 'experimental verification cried i you are going to verify', 'that', '', 'the experiment cried filby who was getting brain weary', '', 'let s see your experiment anyhow said the psychologist though', 'it s all humbug you know', '', 'the time traveller smiled round at us then still smiling faintly', 'and with his hands deep in his trousers pockets he walked slowly', 'out of the room and we heard his slippers shuffling down the long', 'passage to his laboratory', '', 'the psychologist looked at us i wonder what he s got', '', 'some sleight of hand trick or other said the medical man and', 'filby tried to tell us about a conjurer he had seen at burslem but', 'before he had finished his preface the time traveller came back and', 'filby s anecdote collapsed', '', 'the thing the time traveller held in his hand was a glittering', 'metallic framework scarcely larger than a small clock and very', 'delicately made there was ivory in it and some transparent', 'crystalline substance and now i must be explicit for this that', 'follows unless his explanation is to be accepted is an absolutely', 'unaccountable thing he took one of the small octagonal tables that', 'were scattered about the room and set it in front of the fire with', 'two legs on the hearthrug on this table he placed the mechanism', 'then he drew up a chair and sat down the only other object on the', 'table was a small shaded lamp the bright light of which fell upon', 'the model there were also perhaps a dozen candles about two in', 'brass candlesticks upon the mantel and several in sconces so that', 'the room was brilliantly illuminated i sat in a low arm chair', 'nearest the fire and i drew this forward so as to be almost between', 'the time traveller and the fireplace filby sat behind him looking', 'over his shoulder the medical man and the provincial mayor watched', 'him in profile from the right the psychologist from the left the', 'very young man stood behind the psychologist we were all on the', 'alert it appears incredible to me that any kind of trick however', 'subtly conceived and however adroitly done could have been played', 'upon us under these conditions', '', 'the time traveller looked at us and then at the mechanism well', 'said the psychologist', '', 'this little affair said the time traveller resting his elbows', 'upon the table and pressing his hands together above the apparatus', 'is only a model it is my plan for a machine to travel through', 'time you will notice that it looks singularly askew and that there', 'is an odd twinkling appearance about this bar as though it was in', 'some way unreal he pointed to the part with his finger also', 'here is one little white lever and here is another', '', 'the medical man got up out of his chair and peered into the thing', 'it s beautifully made he said', '', 'it took two years to make retorted the time traveller then when', 'we had all imitated the action of the medical man he said now i', 'want you clearly to understand that this lever being pressed over', 'sends the machine gliding into the future and this other reverses', 'the motion this saddle represents the seat of a time traveller', 'presently i am going to press the lever and off the machine will', 'go it will vanish pass into future time and disappear have a', 'good look at the thing look at the table too and satisfy', 'yourselves there is no trickery i don t want to waste this model', 'and then be told i m a quack', '', 'there was a minute s pause perhaps the psychologist seemed about to', 'speak to me but changed his mind then the time traveller put forth', 'his finger towards the lever no he said suddenly lend me your', 'hand and turning to the psychologist he took that individual s', 'hand in his own and told him to put out his forefinger so that it', 'was the psychologist himself who sent forth the model time machine', 'on its interminable voyage we all saw the lever turn i am', 'absolutely certain there was no trickery there was a breath of', 'wind and the lamp flame jumped one of the candles on the mantel', 'was blown out and the little machine suddenly swung round became', 'indistinct was seen as a ghost for a second perhaps as an eddy of', 'faintly glittering brass and ivory and it was gone vanished save', 'for the lamp the table was bare', '', 'everyone was silent for a minute then filby said he was damned', '', 'the psychologist recovered from his stupor and suddenly looked', 'under the table at that the time traveller laughed cheerfully', 'well he said with a reminiscence of the psychologist then', 'getting up he went to the tobacco jar on the mantel and with his', 'back to us began to fill his pipe', '', 'we stared at each other look here said the medical man are you', 'in earnest about this do you seriously believe that that machine', 'has travelled into time', '', 'certainly said the time traveller stooping to light a spill at', 'the fire then he turned lighting his pipe to look at the', 'psychologist s face the psychologist to show that he was not', 'unhinged helped himself to a cigar and tried to light it uncut', 'what is more i have a big machine nearly finished in there he', 'indicated the laboratory and when that is put together i mean to', 'have a journey on my own account', '', 'you mean to say that that machine has travelled into the future', 'said filby', '', 'into the future or the past i don t for certain know which', '', 'after an interval the psychologist had an inspiration it must have', 'gone into the past if it has gone anywhere he said', '', 'why said the time traveller', '', 'because i presume that it has not moved in space and if it', 'travelled into the future it would still be here all this time', 'since it must have travelled through this time', '', 'but i said if it travelled into the past it would have been', 'visible when we came first into this room and last thursday when we', 'were here and the thursday before that and so forth', '', 'serious objections remarked the provincial mayor with an air of', 'impartiality turning towards the time traveller', '', 'not a bit said the time traveller and to the psychologist you', 'think you can explain that it s presentation below the threshold', 'you know diluted presentation', '', 'of course said the psychologist and reassured us that s a', 'simple point of psychology i should have thought of it it s plain', 'enough and helps the paradox delightfully we cannot see it nor', 'can we appreciate this machine any more than we can the spoke of', 'a wheel spinning or a bullet flying through the air if it is', 'travelling through time fifty times or a hundred times faster than', 'we are if it gets through a minute while we get through a second', 'the impression it creates will of course be only one fiftieth or', 'one hundredth of what it would make if it were not travelling in', 'time that s plain enough he passed his hand through the space in', 'which the machine had been you see he said laughing', '', 'we sat and stared at the vacant table for a minute or so then the', 'time traveller asked us what we thought of it all', '', 'it sounds plausible enough to night said the medical man but', 'wait until to morrow wait for the common sense of the morning', '', 'would you like to see the time machine itself asked the time', 'traveller and therewith taking the lamp in his hand he led the', 'way down the long draughty corridor to his laboratory i remember', 'vividly the flickering light his queer broad head in silhouette', 'the dance of the shadows how we all followed him puzzled but', 'incredulous and how there in the laboratory we beheld a larger', 'edition of the little mechanism which we had seen vanish from before', 'our eyes parts were of nickel parts of ivory parts had certainly', 'been filed or sawn out of rock crystal the thing was generally', 'complete but the twisted crystalline bars lay unfinished upon the', 'bench beside some sheets of drawings and i took one up for a better', 'look at it quartz it seemed to be', '', 'look here said the medical man are you perfectly serious', 'or is this a trick like that ghost you showed us last christmas', '', 'upon that machine said the time traveller holding the lamp', 'aloft i intend to explore time is that plain i was never more', 'serious in my life', '', 'none of us quite knew how to take it', '', 'i caught filby s eye over the shoulder of the medical man and he', 'winked at me solemnly', '', '', '', '', 'ii', '', '', 'i think that at that time none of us quite believed in the time', 'machine the fact is the time traveller was one of those men who', 'are too clever to be believed you never felt that you saw all round', 'him you always suspected some subtle reserve some ingenuity in', 'ambush behind his lucid frankness had filby shown the model and', 'explained the matter in the time traveller s words we should have', 'shown him far less scepticism for we should have perceived his', 'motives a pork butcher could understand filby but the time', 'traveller had more than a touch of whim among his elements and we', 'distrusted him things that would have made the frame of a less', 'clever man seemed tricks in his hands it is a mistake to do things', 'too easily the serious people who took him seriously never felt', 'quite sure of his deportment they were somehow aware that trusting', 'their reputations for judgment with him was like furnishing a', 'nursery with egg shell china so i don t think any of us said very', 'much about time travelling in the interval between that thursday and', 'the next though its odd potentialities ran no doubt in most of', 'our minds its plausibility that is its practical incredibleness', 'the curious possibilities of anachronism and of utter confusion it', 'suggested for my own part i was particularly preoccupied with the', 'trick of the model that i remember discussing with the medical man', 'whom i met on friday at the linnaean he said he had seen a similar', 'thing at tubingen and laid considerable stress on the blowing out', 'of the candle but how the trick was done he could not explain', '', 'the next thursday i went again to richmond i suppose i was one of', 'the time traveller s most constant guests and arriving late found', 'four or five men already assembled in his drawing room the medical', 'man was standing before the fire with a sheet of paper in one hand', 'and his watch in the other i looked round for the time traveller', 'and it s half past seven now said the medical man i suppose', 'we d better have dinner', '', 'where s said i naming our host', '', 'you ve just come it s rather odd he s unavoidably detained he', 'asks me in this note to lead off with dinner at seven if he s not', 'back says he ll explain when he comes', '', 'it seems a pity to let the dinner spoil said the editor of a', 'well known daily paper and thereupon the doctor rang the bell', '', 'the psychologist was the only person besides the doctor and myself', 'who had attended the previous dinner the other men were blank the', 'editor aforementioned a certain journalist and another a quiet', 'shy man with a beard whom i didn t know and who as far as my', 'observation went never opened his mouth all the evening there was', 'some speculation at the dinner table about the time traveller s', 'absence and i suggested time travelling in a half jocular spirit', 'the editor wanted that explained to him and the psychologist', 'volunteered a wooden account of the ingenious paradox and trick we', 'had witnessed that day week he was in the midst of his exposition', 'when the door from the corridor opened slowly and without noise i', 'was facing the door and saw it first hallo i said at last', 'and the door opened wider and the time traveller stood before us', 'i gave a cry of surprise good heavens man what s the matter', 'cried the medical man who saw him next and the whole tableful', 'turned towards the door', '', 'he was in an amazing plight his coat was dusty and dirty and', 'smeared with green down the sleeves his hair disordered and as it', 'seemed to me greyer either with dust and dirt or because its colour', 'had actually faded his face was ghastly pale his chin had a brown', 'cut on it a cut half healed his expression was haggard and drawn', 'as by intense suffering for a moment he hesitated in the doorway', 'as if he had been dazzled by the light then he came into the room', 'he walked with just such a limp as i have seen in footsore tramps', 'we stared at him in silence expecting him to speak', '', 'he said not a word but came painfully to the table and made a', 'motion towards the wine the editor filled a glass of champagne and', 'pushed it towards him he drained it and it seemed to do him good', 'for he looked round the table and the ghost of his old smile', 'flickered across his face what on earth have you been up to man', 'said the doctor the time traveller did not seem to hear don t let', 'me disturb you he said with a certain faltering articulation', 'i m all right he stopped held out his glass for more and took', 'it off at a draught that s good he said his eyes grew brighter', 'and a faint colour came into his cheeks his glance flickered over', 'our faces with a certain dull approval and then went round the warm', 'and comfortable room then he spoke again still as it were feeling', 'his way among his words i m going to wash and dress and then i ll', 'come down and explain things save me some of that mutton i m', 'starving for a bit of meat', '', 'he looked across at the editor who was a rare visitor and hoped he', 'was all right the editor began a question tell you presently', 'said the time traveller i m funny be all right in a minute', '', 'he put down his glass and walked towards the staircase door again', 'i remarked his lameness and the soft padding sound of his footfall', 'and standing up in my place i saw his feet as he went out he had', 'nothing on them but a pair of tattered blood stained socks then the', 'door closed upon him i had half a mind to follow till i remembered', 'how he detested any fuss about himself for a minute perhaps my', 'mind was wool gathering then remarkable behaviour of an eminent', 'scientist i heard the editor say thinking after his wont in', 'headlines and this brought my attention back to the bright', 'dinner table', '', 'what s the game said the journalist has he been doing the', 'amateur cadger i don t follow i met the eye of the psychologist', 'and read my own interpretation in his face i thought of the time', 'traveller limping painfully upstairs i don t think any one else had', 'noticed his lameness', '', 'the first to recover completely from this surprise was the medical', 'man who rang the bell the time traveller hated to have servants', 'waiting at dinner for a hot plate at that the editor turned to his', 'knife and fork with a grunt and the silent man followed suit the', 'dinner was resumed conversation was exclamatory for a little while', 'with gaps of wonderment and then the editor got fervent in his', 'curiosity does our friend eke out his modest income with a', 'crossing or has he his nebuchadnezzar phases he inquired i feel', 'assured it s this business of the time machine i said and took up', 'the psychologist s account of our previous meeting the new guests', 'were frankly incredulous the editor raised objections what was', 'this time travelling a man couldn t cover himself with dust by', 'rolling in a paradox could he and then as the idea came home to', 'him he resorted to caricature hadn t they any clothes brushes in', 'the future the journalist too would not believe at any price and', 'joined the editor in the easy work of heaping ridicule on the whole', 'thing they were both the new kind of journalist very joyous', 'irreverent young men our special correspondent in the day', 'after to morrow reports the journalist was saying or rather', 'shouting when the time traveller came back he was dressed in', 'ordinary evening clothes and nothing save his haggard look remained', 'of the change that had startled me', '', 'i say said the editor hilariously these chaps here say you have', 'been travelling into the middle of next week tell us all about', 'little rosebery will you what will you take for the lot', '', 'the time traveller came to the place reserved for him without a', 'word he smiled quietly in his old way where s my mutton he', 'said what a treat it is to stick a fork into meat again', '', 'story cried the editor', '', 'story be damned said the time traveller i want something to', 'eat i won t say a word until i get some peptone into my arteries', 'thanks and the salt', '', 'one word said i have you been time travelling', '', 'yes said the time traveller with his mouth full nodding his', 'head', '', 'i d give a shilling a line for a verbatim note said the editor', 'the time traveller pushed his glass towards the silent man and rang', 'it with his fingernail at which the silent man who had been', 'staring at his face started convulsively and poured him wine', 'the rest of the dinner was uncomfortable for my own part sudden', 'questions kept on rising to my lips and i dare say it was the same', 'with the others the journalist tried to relieve the tension by', 'telling anecdotes of hettie potter the time traveller devoted his', 'attention to his dinner and displayed the appetite of a tramp', 'the medical man smoked a cigarette and watched the time traveller', 'through his eyelashes the silent man seemed even more clumsy than', 'usual and drank champagne with regularity and determination out of', 'sheer nervousness at last the time traveller pushed his plate away', 'and looked round us i suppose i must apologize he said i was', 'simply starving i ve had a most amazing time he reached out his', 'hand for a cigar and cut the end but come into the smoking room', 'it s too long a story to tell over greasy plates and ringing the', 'bell in passing he led the way into the adjoining room', '', 'you have told blank and dash and chose about the machine he', 'said to me leaning back in his easy chair and naming the three new', 'guests', '', 'but the thing s a mere paradox said the editor', '', 'i can t argue to night i don t mind telling you the story but', 'i can t argue i will he went on tell you the story of what', 'has happened to me if you like but you must refrain from', 'interruptions i want to tell it badly most of it will sound like', 'lying so be it it s true every word of it all the same i was in', 'my laboratory at four o clock and since then i ve lived eight', 'days such days as no human being ever lived before i m nearly', 'worn out but i shan t sleep till i ve told this thing over to you', 'then i shall go to bed but no interruptions is it agreed', '', 'agreed said the editor and the rest of us echoed agreed and', 'with that the time traveller began his story as i have set it forth', 'he sat back in his chair at first and spoke like a weary man', 'afterwards he got more animated in writing it down i feel with only', 'too much keenness the inadequacy of pen and ink and above all my', 'own inadequacy to express its quality you read i will suppose', 'attentively enough but you cannot see the speaker s white', 'sincere face in the bright circle of the little lamp nor hear the', 'intonation of his voice you cannot know how his expression followed', 'the turns of his story most of us hearers were in shadow for the', 'candles in the smoking room had not been lighted and only the face', 'of the journalist and the legs of the silent man from the knees', 'downward were illuminated at first we glanced now and again at each', 'other after a time we ceased to do that and looked only at the', 'time traveller s face', '', '', '', '', 'iii', '', '', 'i told some of you last thursday of the principles of the time', 'machine and showed you the actual thing itself incomplete in the', 'workshop there it is now a little travel worn truly and one of', 'the ivory bars is cracked and a brass rail bent but the rest of', 'it s sound enough i expected to finish it on friday but on friday', 'when the putting together was nearly done i found that one of the', 'nickel bars was exactly one inch too short and this i had to get', 'remade so that the thing was not complete until this morning it', 'was at ten o clock to day that the first of all time machines began', 'its career i gave it a last tap tried all the screws again put', 'one more drop of oil on the quartz rod and sat myself in the', 'saddle i suppose a suicide who holds a pistol to his skull feels', 'much the same wonder at what will come next as i felt then i took', 'the starting lever in one hand and the stopping one in the other', 'pressed the first and almost immediately the second i seemed to', 'reel i felt a nightmare sensation of falling and looking round', 'i saw the laboratory exactly as before had anything happened for', 'a moment i suspected that my intellect had tricked me then i noted', 'the clock a moment before as it seemed it had stood at a minute', 'or so past ten now it was nearly half past three', '', 'i drew a breath set my teeth gripped the starting lever with both', 'hands and went off with a thud the laboratory got hazy and went', 'dark mrs watchett came in and walked apparently without seeing', 'me towards the garden door i suppose it took her a minute or so to', 'traverse the place but to me she seemed to shoot across the room', 'like a rocket i pressed the lever over to its extreme position the', 'night came like the turning out of a lamp and in another moment', 'came to morrow the laboratory grew faint and hazy then fainter', 'and ever fainter to morrow night came black then day again night', 'again day again faster and faster still an eddying murmur filled', 'my ears and a strange dumb confusedness descended on my mind', '', 'i am afraid i cannot convey the peculiar sensations of time', 'travelling they are excessively unpleasant there is a feeling', 'exactly like that one has upon a switchback of a helpless headlong', 'motion i felt the same horrible anticipation too of an imminent', 'smash as i put on pace night followed day like the flapping of a', 'black wing the dim suggestion of the laboratory seemed presently to', 'fall away from me and i saw the sun hopping swiftly across the sky', 'leaping it every minute and every minute marking a day i supposed', 'the laboratory had been destroyed and i had come into the open air', 'i had a dim impression of scaffolding but i was already going too', 'fast to be conscious of any moving things the slowest snail that', 'ever crawled dashed by too fast for me the twinkling succession of', 'darkness and light was excessively painful to the eye then in the', 'intermittent darknesses i saw the moon spinning swiftly through her', 'quarters from new to full and had a faint glimpse of the circling', 'stars presently as i went on still gaining velocity the', 'palpitation of night and day merged into one continuous greyness', 'the sky took on a wonderful deepness of blue a splendid luminous', 'color like that of early twilight the jerking sun became a streak', 'of fire a brilliant arch in space the moon a fainter fluctuating', 'band and i could see nothing of the stars save now and then a', 'brighter circle flickering in the blue', '', 'the landscape was misty and vague i was still on the hill side', 'upon which this house now stands and the shoulder rose above me', 'grey and dim i saw trees growing and changing like puffs of vapour', 'now brown now green they grew spread shivered and passed away', 'i saw huge buildings rise up faint and fair and pass like dreams', 'the whole surface of the earth seemed changed melting and flowing', 'under my eyes the little hands upon the dials that registered my', 'speed raced round faster and faster presently i noted that the sun', 'belt swayed up and down from solstice to solstice in a minute or', 'less and that consequently my pace was over a year a minute and', 'minute by minute the white snow flashed across the world and', 'vanished and was followed by the bright brief green of spring', '', 'the unpleasant sensations of the start were less poignant now they', 'merged at last into a kind of hysterical exhilaration i remarked', 'indeed a clumsy swaying of the machine for which i was unable to', 'account but my mind was too confused to attend to it so with a', 'kind of madness growing upon me i flung myself into futurity at', 'first i scarce thought of stopping scarce thought of anything but', 'these new sensations but presently a fresh series of impressions', 'grew up in my mind a certain curiosity and therewith a certain', 'dread until at last they took complete possession of me what', 'strange developments of humanity what wonderful advances upon our', 'rudimentary civilization i thought might not appear when i came to', 'look nearly into the dim elusive world that raced and fluctuated', 'before my eyes i saw great and splendid architecture rising about', 'me more massive than any buildings of our own time and yet as it', 'seemed built of glimmer and mist i saw a richer green flow up the', 'hill side and remain there without any wintry intermission even', 'through the veil of my confusion the earth seemed very fair and so', 'my mind came round to the business of stopping', '', 'the peculiar risk lay in the possibility of my finding some', 'substance in the space which i or the machine occupied so long', 'as i travelled at a high velocity through time this scarcely', 'mattered i was so to speak attenuated was slipping like a vapour', 'through the interstices of intervening substances but to come to', 'a stop involved the jamming of myself molecule by molecule into', 'whatever lay in my way meant bringing my atoms into such intimate', 'contact with those of the obstacle that a profound chemical', 'reaction possibly a far reaching explosion would result and blow', 'myself and my apparatus out of all possible dimensions into the', 'unknown this possibility had occurred to me again and again while i', 'was making the machine but then i had cheerfully accepted it as an', 'unavoidable risk one of the risks a man has got to take now the', 'risk was inevitable i no longer saw it in the same cheerful light', 'the fact is that insensibly the absolute strangeness of everything', 'the sickly jarring and swaying of the machine above all the', 'feeling of prolonged falling had absolutely upset my nerve i told', 'myself that i could never stop and with a gust of petulance i', 'resolved to stop forthwith like an impatient fool i lugged over', 'the lever and incontinently the thing went reeling over and i was', 'flung headlong through the air', '', 'there was the sound of a clap of thunder in my ears i may have', 'been stunned for a moment a pitiless hail was hissing round me', 'and i was sitting on soft turf in front of the overset machine', 'everything still seemed grey but presently i remarked that the', 'confusion in my ears was gone i looked round me i was on what', 'seemed to be a little lawn in a garden surrounded by rhododendron', 'bushes and i noticed that their mauve and purple blossoms were', 'dropping in a shower under the beating of the hail stones the', 'rebounding dancing hail hung in a cloud over the machine and drove', 'along the ground like smoke in a moment i was wet to the skin', 'fine hospitality said i to a man who has travelled innumerable', 'years to see you', '', 'presently i thought what a fool i was to get wet i stood up and', 'looked round me a colossal figure carved apparently in some white', 'stone loomed indistinctly beyond the rhododendrons through the hazy', 'downpour but all else of the world was invisible', '', 'my sensations would be hard to describe as the columns of hail', 'grew thinner i saw the white figure more distinctly it was very', 'large for a silver birch tree touched its shoulder it was of white', 'marble in shape something like a winged sphinx but the wings', 'instead of being carried vertically at the sides were spread so', 'that it seemed to hover the pedestal it appeared to me was of', 'bronze and was thick with verdigris it chanced that the face was', 'towards me the sightless eyes seemed to watch me there was the', 'faint shadow of a smile on the lips it was greatly weather worn', 'and that imparted an unpleasant suggestion of disease i stood', 'looking at it for a little space half a minute perhaps or half an', 'hour it seemed to advance and to recede as the hail drove before it', 'denser or thinner at last i tore my eyes from it for a moment and', 'saw that the hail curtain had worn threadbare and that the sky was', 'lightening with the promise of the sun', '', 'i looked up again at the crouching white shape and the full', 'temerity of my voyage came suddenly upon me what might appear when', 'that hazy curtain was altogether withdrawn what might not have', 'happened to men what if cruelty had grown into a common passion', 'what if in this interval the race had lost its manliness and had', 'developed into something inhuman unsympathetic and overwhelmingly', 'powerful i might seem some old world savage animal only the more', 'dreadful and disgusting for our common likeness a foul creature to', 'be incontinently slain', '', 'already i saw other vast shapes huge buildings with intricate', 'parapets and tall columns with a wooded hill side dimly creeping', 'in upon me through the lessening storm i was seized with a panic', 'fear i turned frantically to the time machine and strove hard to', 'readjust it as i did so the shafts of the sun smote through the', 'thunderstorm the grey downpour was swept aside and vanished like', 'the trailing garments of a ghost above me in the intense blue', 'of the summer sky some faint brown shreds of cloud whirled into', 'nothingness the great buildings about me stood out clear and', 'distinct shining with the wet of the thunderstorm and picked out', 'in white by the unmelted hailstones piled along their courses i', 'felt naked in a strange world i felt as perhaps a bird may feel in', 'the clear air knowing the hawk wings above and will swoop my fear', 'grew to frenzy i took a breathing space set my teeth and again', 'grappled fiercely wrist and knee with the machine it gave under', 'my desperate onset and turned over it struck my chin violently one', 'hand on the saddle the other on the lever i stood panting heavily', 'in attitude to mount again', '', 'but with this recovery of a prompt retreat my courage recovered i', 'looked more curiously and less fearfully at this world of the remote', 'future in a circular opening high up in the wall of the nearer', 'house i saw a group of figures clad in rich soft robes they had', 'seen me and their faces were directed towards me', '', 'then i heard voices approaching me coming through the bushes by', 'the white sphinx were the heads and shoulders of men running one of', 'these emerged in a pathway leading straight to the little lawn upon', 'which i stood with my machine he was a slight creature perhaps', 'four feet high clad in a purple tunic girdled at the waist with a', 'leather belt sandals or buskins i could not clearly distinguish', 'which were on his feet his legs were bare to the knees and his', 'head was bare noticing that i noticed for the first time how warm', 'the air was', '', 'he struck me as being a very beautiful and graceful creature but', 'indescribably frail his flushed face reminded me of the more', 'beautiful kind of consumptive that hectic beauty of which we used', 'to hear so much at the sight of him i suddenly regained confidence', 'i took my hands from the machine', '', '', '', '', 'iv', '', '', 'in another moment we were standing face to face i and this fragile', 'thing out of futurity he came straight up to me and laughed into my', 'eyes the absence from his bearing of any sign of fear struck me at', 'once then he turned to the two others who were following him and', 'spoke to them in a strange and very sweet and liquid tongue', '', 'there were others coming and presently a little group of perhaps', 'eight or ten of these exquisite creatures were about me one of them', 'addressed me it came into my head oddly enough that my voice was', 'too harsh and deep for them so i shook my head and pointing to my', 'ears shook it again he came a step forward hesitated and then', 'touched my hand then i felt other soft little tentacles upon my', 'back and shoulders they wanted to make sure i was real there was', 'nothing in this at all alarming indeed there was something in', 'these pretty little people that inspired confidence a graceful', 'gentleness a certain childlike ease and besides they looked so', 'frail that i could fancy myself flinging the whole dozen of them', 'about like nine pins but i made a sudden motion to warn them when i', 'saw their little pink hands feeling at the time machine happily', 'then when it was not too late i thought of a danger i had hitherto', 'forgotten and reaching over the bars of the machine i unscrewed the', 'little levers that would set it in motion and put these in my', 'pocket then i turned again to see what i could do in the way of', 'communication', '', 'and then looking more nearly into their features i saw some', 'further peculiarities in their dresden china type of prettiness', 'their hair which was uniformly curly came to a sharp end at the', 'neck and cheek there was not the faintest suggestion of it on the', 'face and their ears were singularly minute the mouths were small', 'with bright red rather thin lips and the little chins ran to a', 'point the eyes were large and mild and this may seem egotism on', 'my part i fancied even that there was a certain lack of the', 'interest i might have expected in them', '', 'as they made no effort to communicate with me but simply stood', 'round me smiling and speaking in soft cooing notes to each other i', 'began the conversation i pointed to the time machine and to myself', 'then hesitating for a moment how to express time i pointed to the', 'sun at once a quaintly pretty little figure in chequered purple and', 'white followed my gesture and then astonished me by imitating the', 'sound of thunder', '', 'for a moment i was staggered though the import of his gesture was', 'plain enough the question had come into my mind abruptly were', 'these creatures fools you may hardly understand how it took me', 'you see i had always anticipated that the people of the year eight', 'hundred and two thousand odd would be incredibly in front of us in', 'knowledge art everything then one of them suddenly asked me a', 'question that showed him to be on the intellectual level of one of', 'our five year old children asked me in fact if i had come from', 'the sun in a thunderstorm it let loose the judgment i had suspended', 'upon their clothes their frail light limbs and fragile features', 'a flow of disappointment rushed across my mind for a moment i felt', 'that i had built the time machine in vain', '', 'i nodded pointed to the sun and gave them such a vivid rendering', 'of a thunderclap as startled them they all withdrew a pace or so', 'and bowed then came one laughing towards me carrying a chain of', 'beautiful flowers altogether new to me and put it about my neck', 'the idea was received with melodious applause and presently they', 'were all running to and fro for flowers and laughingly flinging', 'them upon me until i was almost smothered with blossom you who', 'have never seen the like can scarcely imagine what delicate and', 'wonderful flowers countless years of culture had created then', 'someone suggested that their plaything should be exhibited in the', 'nearest building and so i was led past the sphinx of white marble', 'which had seemed to watch me all the while with a smile at my', 'astonishment towards a vast grey edifice of fretted stone as i', 'went with them the memory of my confident anticipations of a', 'profoundly grave and intellectual posterity came with irresistible', 'merriment to my mind', '', 'the building had a huge entry and was altogether of colossal', 'dimensions i was naturally most occupied with the growing crowd of', 'little people and with the big open portals that yawned before me', 'shadowy and mysterious my general impression of the world i saw', 'over their heads was a tangled waste of beautiful bushes and', 'flowers a long neglected and yet weedless garden i saw a number', 'of tall spikes of strange white flowers measuring a foot perhaps', 'across the spread of the waxen petals they grew scattered as if', 'wild among the variegated shrubs but as i say i did not examine', 'them closely at this time the time machine was left deserted on the', 'turf among the rhododendrons', '', 'the arch of the doorway was richly carved but naturally i did', 'not observe the carving very narrowly though i fancied i saw', 'suggestions of old phoenician decorations as i passed through and', 'it struck me that they were very badly broken and weather worn', 'several more brightly clad people met me in the doorway and so we', 'entered i dressed in dingy nineteenth century garments looking', 'grotesque enough garlanded with flowers and surrounded by an', 'eddying mass of bright soft colored robes and shining white limbs', 'in a melodious whirl of laughter and laughing speech', '', 'the big doorway opened into a proportionately great hall hung with', 'brown the roof was in shadow and the windows partially glazed', 'with coloured glass and partially unglazed admitted a tempered', 'light the floor was made up of huge blocks of some very hard white', 'metal not plates nor slabs blocks and it was so much worn as i', 'judged by the going to and fro of past generations as to be deeply', 'channelled along the more frequented ways transverse to the length', 'were innumerable tables made of slabs of polished stone raised', 'perhaps a foot from the floor and upon these were heaps of fruits', 'some i recognized as a kind of hypertrophied raspberry and orange', 'but for the most part they were strange', '', 'between the tables was scattered a great number of cushions', 'upon these my conductors seated themselves signing for me to do', 'likewise with a pretty absence of ceremony they began to eat the', 'fruit with their hands flinging peel and stalks and so forth into', 'the round openings in the sides of the tables i was not loath to', 'follow their example for i felt thirsty and hungry as i did so i', 'surveyed the hall at my leisure', '', 'and perhaps the thing that struck me most was its dilapidated look', 'the stained glass windows which displayed only a geometrical', 'pattern were broken in many places and the curtains that hung', 'across the lower end were thick with dust and it caught my eye that', 'the corner of the marble table near me was fractured nevertheless', 'the general effect was extremely rich and picturesque there were', 'perhaps a couple of hundred people dining in the hall and most of', 'them seated as near to me as they could come were watching me with', 'interest their little eyes shining over the fruit they were eating', 'all were clad in the same soft and yet strong silky material', '', 'fruit by the by was all their diet these people of the remote', 'future were strict vegetarians and while i was with them in spite', 'of some carnal cravings i had to be frugivorous also indeed i', 'found afterwards that horses cattle sheep dogs had followed the', 'ichthyosaurus into extinction but the fruits were very delightful', 'one in particular that seemed to be in season all the time i was', 'there a floury thing in a three sided husk was especially good', 'and i made it my staple at first i was puzzled by all these strange', 'fruits and by the strange flowers i saw but later i began to', 'perceive their import', '', 'however i am telling you of my fruit dinner in the distant future', 'now so soon as my appetite was a little checked i determined to', 'make a resolute attempt to learn the speech of these new men of', 'mine clearly that was the next thing to do the fruits seemed a', 'convenient thing to begin upon and holding one of these up i began', 'a series of interrogative sounds and gestures i had some', 'considerable difficulty in conveying my meaning at first my efforts', 'met with a stare of surprise or inextinguishable laughter but', 'presently a fair haired little creature seemed to grasp my intention', 'and repeated a name they had to chatter and explain the business', 'at great length to each other and my first attempts to make the', 'exquisite little sounds of their language caused an immense amount', 'of amusement however i felt like a schoolmaster amidst children', 'and persisted and presently i had a score of noun substantives at', 'least at my command and then i got to demonstrative pronouns and', 'even the verb to eat but it was slow work and the little people', 'soon tired and wanted to get away from my interrogations so i', 'determined rather of necessity to let them give their lessons in', 'little doses when they felt inclined and very little doses i found', 'they were before long for i never met people more indolent or more', 'easily fatigued', '', 'a queer thing i soon discovered about my little hosts and that was', 'their lack of interest they would come to me with eager cries of', 'astonishment like children but like children they would soon stop', 'examining me and wander away after some other toy the dinner and my', 'conversational beginnings ended i noted for the first time that', 'almost all those who had surrounded me at first were gone it is', 'odd too how speedily i came to disregard these little people i', 'went out through the portal into the sunlit world again as soon as', 'my hunger was satisfied i was continually meeting more of these men', 'of the future who would follow me a little distance chatter and', 'laugh about me and having smiled and gesticulated in a friendly', 'way leave me again to my own devices', '', 'the calm of evening was upon the world as i emerged from the great', 'hall and the scene was lit by the warm glow of the setting sun', 'at first things were very confusing everything was so entirely', 'different from the world i had known even the flowers the big', 'building i had left was situated on the slope of a broad river', 'valley but the thames had shifted perhaps a mile from its present', 'position i resolved to mount to the summit of a crest perhaps a', 'mile and a half away from which i could get a wider view of this', 'our planet in the year eight hundred and two thousand seven hundred', 'and one a d for that i should explain was the date the little', 'dials of my machine recorded', '', 'as i walked i was watching for every impression that could possibly', 'help to explain the condition of ruinous splendour in which i', 'found the world for ruinous it was a little way up the hill for', 'instance was a great heap of granite bound together by masses of', 'aluminium a vast labyrinth of precipitous walls and crumpled', 'heaps amidst which were thick heaps of very beautiful pagoda like', 'plants nettles possibly but wonderfully tinted with brown about', 'the leaves and incapable of stinging it was evidently the derelict', 'remains of some vast structure to what end built i could not', 'determine it was here that i was destined at a later date to have', 'a very strange experience the first intimation of a still stranger', 'discovery but of that i will speak in its proper place', '', 'looking round with a sudden thought from a terrace on which i', 'rested for a while i realized that there were no small houses to be', 'seen apparently the single house and possibly even the household', 'had vanished here and there among the greenery were palace like', 'buildings but the house and the cottage which form such', 'characteristic features of our own english landscape had', 'disappeared', '', 'communism said i to myself', '', 'and on the heels of that came another thought i looked at the', 'half dozen little figures that were following me then in a flash', 'i perceived that all had the same form of costume the same soft', 'hairless visage and the same girlish rotundity of limb it may seem', 'strange perhaps that i had not noticed this before but everything', 'was so strange now i saw the fact plainly enough in costume and', 'in all the differences of texture and bearing that now mark off the', 'sexes from each other these people of the future were alike and', 'the children seemed to my eyes to be but the miniatures of their', 'parents i judged then that the children of that time were', 'extremely precocious physically at least and i found afterwards', 'abundant verification of my opinion', '', 'seeing the ease and security in which these people were living i', 'felt that this close resemblance of the sexes was after all what', 'one would expect for the strength of a man and the softness of a', 'woman the institution of the family and the differentiation of', 'occupations are mere militant necessities of an age of physical', 'force where population is balanced and abundant much childbearing', 'becomes an evil rather than a blessing to the state where', 'violence comes but rarely and off spring are secure there is less', 'necessity indeed there is no necessity for an efficient family', 'and the specialization of the sexes with reference to their', 'children s needs disappears we see some beginnings of this even', 'in our own time and in this future age it was complete this i', 'must remind you was my speculation at the time later i was to', 'appreciate how far it fell short of the reality', '', 'while i was musing upon these things my attention was attracted by', 'a pretty little structure like a well under a cupola i thought in', 'a transitory way of the oddness of wells still existing and then', 'resumed the thread of my speculations there were no large buildings', 'towards the top of the hill and as my walking powers were evidently', 'miraculous i was presently left alone for the first time with a', 'strange sense of freedom and adventure i pushed on up to the crest', '', 'there i found a seat of some yellow metal that i did not recognize', 'corroded in places with a kind of pinkish rust and half smothered', 'in soft moss the arm rests cast and filed into the resemblance of', 'griffins heads i sat down on it and i surveyed the broad view of', 'our old world under the sunset of that long day it was as sweet and', 'fair a view as i have ever seen the sun had already gone below the', 'horizon and the west was flaming gold touched with some horizontal', 'bars of purple and crimson below was the valley of the thames in', 'which the river lay like a band of burnished steel i have already', 'spoken of the great palaces dotted about among the variegated', 'greenery some in ruins and some still occupied here and there rose', 'a white or silvery figure in the waste garden of the earth here and', 'there came the sharp vertical line of some cupola or obelisk there', 'were no hedges no signs of proprietary rights no evidences of', 'agriculture the whole earth had become a garden', '', 'so watching i began to put my interpretation upon the things i had', 'seen and as it shaped itself to me that evening my interpretation', 'was something in this way afterwards i found i had got only a', 'half truth or only a glimpse of one facet of the truth', '', 'it seemed to me that i had happened upon humanity upon the wane', 'the ruddy sunset set me thinking of the sunset of mankind for the', 'first time i began to realize an odd consequence of the social', 'effort in which we are at present engaged and yet come to think', 'it is a logical consequence enough strength is the outcome of need', 'security sets a premium on feebleness the work of ameliorating the', 'conditions of life the true civilizing process that makes life more', 'and more secure had gone steadily on to a climax one triumph of a', 'united humanity over nature had followed another things that are', 'now mere dreams had become projects deliberately put in hand and', 'carried forward and the harvest was what i saw', '', 'after all the sanitation and the agriculture of to day are still', 'in the rudimentary stage the science of our time has attacked but', 'a little department of the field of human disease but even so', 'it spreads its operations very steadily and persistently our', 'agriculture and horticulture destroy a weed just here and there and', 'cultivate perhaps a score or so of wholesome plants leaving the', 'greater number to fight out a balance as they can we improve our', 'favourite plants and animals and how few they are gradually by', 'selective breeding now a new and better peach now a seedless', 'grape now a sweeter and larger flower now a more convenient breed', 'of cattle we improve them gradually because our ideals are vague', 'and tentative and our knowledge is very limited because nature', 'too is shy and slow in our clumsy hands some day all this will', 'be better organized and still better that is the drift of the', 'current in spite of the eddies the whole world will be intelligent', 'educated and co operating things will move faster and faster', 'towards the subjugation of nature in the end wisely and carefully', 'we shall readjust the balance of animal and vegetable life to suit', 'our human needs', '', 'this adjustment i say must have been done and done well done', 'indeed for all time in the space of time across which my machine', 'had leaped the air was free from gnats the earth from weeds or', 'fungi everywhere were fruits and sweet and delightful flowers', 'brilliant butterflies flew hither and thither the ideal of', 'preventive medicine was attained diseases had been stamped out i', 'saw no evidence of any contagious diseases during all my stay and i', 'shall have to tell you later that even the processes of putrefaction', 'and decay had been profoundly affected by these changes', '', 'social triumphs too had been effected i saw mankind housed in', 'splendid shelters gloriously clothed and as yet i had found them', 'engaged in no toil there were no signs of struggle neither social', 'nor economical struggle the shop the advertisement traffic all', 'that commerce which constitutes the body of our world was gone it', 'was natural on that golden evening that i should jump at the idea of', 'a social paradise the difficulty of increasing population had been', 'met i guessed and population had ceased to increase', '', 'but with this change in condition comes inevitably adaptations to', 'the change what unless biological science is a mass of errors is', 'the cause of human intelligence and vigour hardship and freedom', 'conditions under which the active strong and subtle survive and', 'the weaker go to the wall conditions that put a premium upon the', 'loyal alliance of capable men upon self restraint patience and', 'decision and the institution of the family and the emotions that', 'arise therein the fierce jealousy the tenderness for offspring', 'parental self devotion all found their justification and support in', 'the imminent dangers of the young now where are these imminent', 'dangers there is a sentiment arising and it will grow against', 'connubial jealousy against fierce maternity against passion', 'of all sorts unnecessary things now and things that make us', 'uncomfortable savage survivals discords in a refined and pleasant', 'life', '', 'i thought of the physical slightness of the people their lack of', 'intelligence and those big abundant ruins and it strengthened my', 'belief in a perfect conquest of nature for after the battle comes', 'quiet humanity had been strong energetic and intelligent and had', 'used all its abundant vitality to alter the conditions under which', 'it lived and now came the reaction of the altered conditions', '', 'under the new conditions of perfect comfort and security that', 'restless energy that with us is strength would become weakness', 'even in our own time certain tendencies and desires once necessary', 'to survival are a constant source of failure physical courage and', 'the love of battle for instance are no great help may even be', 'hindrances to a civilized man and in a state of physical balance', 'and security power intellectual as well as physical would be out', 'of place for countless years i judged there had been no danger of', 'war or solitary violence no danger from wild beasts no wasting', 'disease to require strength of constitution no need of toil for', 'such a life what we should call the weak are as well equipped as', 'the strong are indeed no longer weak better equipped indeed they', 'are for the strong would be fretted by an energy for which there', 'was no outlet no doubt the exquisite beauty of the buildings i saw', 'was the outcome of the last surgings of the now purposeless energy', 'of mankind before it settled down into perfect harmony with the', 'conditions under which it lived the flourish of that triumph which', 'began the last great peace this has ever been the fate of energy in', 'security it takes to art and to eroticism and then come languor', 'and decay', '', 'even this artistic impetus would at last die away had almost died', 'in the time i saw to adorn themselves with flowers to dance to', 'sing in the sunlight so much was left of the artistic spirit and', 'no more even that would fade in the end into a contented', 'inactivity we are kept keen on the grindstone of pain and', 'necessity and it seemed to me that here was that hateful', 'grindstone broken at last', '', 'as i stood there in the gathering dark i thought that in this', 'simple explanation i had mastered the problem of the world mastered', 'the whole secret of these delicious people possibly the checks they', 'had devised for the increase of population had succeeded too well', 'and their numbers had rather diminished than kept stationary', 'that would account for the abandoned ruins very simple was my', 'explanation and plausible enough as most wrong theories are', '', '', '', '', 'v', '', '', 'as i stood there musing over this too perfect triumph of man the', 'full moon yellow and gibbous came up out of an overflow of silver', 'light in the north east the bright little figures ceased to move', 'about below a noiseless owl flitted by and i shivered with the', 'chill of the night i determined to descend and find where i could', 'sleep', '', 'i looked for the building i knew then my eye travelled along to', 'the figure of the white sphinx upon the pedestal of bronze growing', 'distinct as the light of the rising moon grew brighter i could see', 'the silver birch against it there was the tangle of rhododendron', 'bushes black in the pale light and there was the little lawn', 'i looked at the lawn again a queer doubt chilled my complacency', 'no said i stoutly to myself that was not the lawn', '', 'but it was the lawn for the white leprous face of the sphinx was', 'towards it can you imagine what i felt as this conviction came', 'home to me but you cannot the time machine was gone', '', 'at once like a lash across the face came the possibility of', 'losing my own age of being left helpless in this strange new world', 'the bare thought of it was an actual physical sensation i could', 'feel it grip me at the throat and stop my breathing in another', 'moment i was in a passion of fear and running with great leaping', 'strides down the slope once i fell headlong and cut my face i lost', 'no time in stanching the blood but jumped up and ran on with a', 'warm trickle down my cheek and chin all the time i ran i was saying', 'to myself they have moved it a little pushed it under the bushes', 'out of the way nevertheless i ran with all my might all the', 'time with the certainty that sometimes comes with excessive dread', 'i knew that such assurance was folly knew instinctively that the', 'machine was removed out of my reach my breath came with pain i', 'suppose i covered the whole distance from the hill crest to the', 'little lawn two miles perhaps in ten minutes and i am not a young', 'man i cursed aloud as i ran at my confident folly in leaving the', 'machine wasting good breath thereby i cried aloud and none', 'answered not a creature seemed to be stirring in that moonlit', 'world', '', 'when i reached the lawn my worst fears were realized not a trace', 'of the thing was to be seen i felt faint and cold when i faced the', 'empty space among the black tangle of bushes i ran round it', 'furiously as if the thing might be hidden in a corner and then', 'stopped abruptly with my hands clutching my hair above me towered', 'the sphinx upon the bronze pedestal white shining leprous in', 'the light of the rising moon it seemed to smile in mockery of my', 'dismay', '', 'i might have consoled myself by imagining the little people had put', 'the mechanism in some shelter for me had i not felt assured of', 'their physical and intellectual inadequacy that is what dismayed', 'me the sense of some hitherto unsuspected power through whose', 'intervention my invention had vanished yet for one thing i felt', 'assured unless some other age had produced its exact duplicate', 'the machine could not have moved in time the attachment of the', 'levers i will show you the method later prevented any one from', 'tampering with it in that way when they were removed it had moved', 'and was hid only in space but then where could it be', '', 'i think i must have had a kind of frenzy i remember running', 'violently in and out among the moonlit bushes all round the sphinx', 'and startling some white animal that in the dim light i took for a', 'small deer i remember too late that night beating the bushes', 'with my clenched fist until my knuckles were gashed and bleeding', 'from the broken twigs then sobbing and raving in my anguish of', 'mind i went down to the great building of stone the big hall was', 'dark silent and deserted i slipped on the uneven floor and fell', 'over one of the malachite tables almost breaking my shin i lit a', 'match and went on past the dusty curtains of which i have told you', '', 'there i found a second great hall covered with cushions upon', 'which perhaps a score or so of the little people were sleeping i', 'have no doubt they found my second appearance strange enough coming', 'suddenly out of the quiet darkness with inarticulate noises and the', 'splutter and flare of a match for they had forgotten about matches', 'where is my time machine i began bawling like an angry child', 'laying hands upon them and shaking them up together it must have', 'been very queer to them some laughed most of them looked sorely', 'frightened when i saw them standing round me it came into my head', 'that i was doing as foolish a thing as it was possible for me to do', 'under the circumstances in trying to revive the sensation of fear', 'for reasoning from their daylight behaviour i thought that fear', 'must be forgotten', '', 'abruptly i dashed down the match and knocking one of the people', 'over in my course went blundering across the big dining hall again', 'out under the moonlight i heard cries of terror and their little', 'feet running and stumbling this way and that i do not remember all', 'i did as the moon crept up the sky i suppose it was the unexpected', 'nature of my loss that maddened me i felt hopelessly cut off from', 'my own kind a strange animal in an unknown world i must have raved', 'to and fro screaming and crying upon god and fate i have a memory', 'of horrible fatigue as the long night of despair wore away of', 'looking in this impossible place and that of groping among moon lit', 'ruins and touching strange creatures in the black shadows at last', 'of lying on the ground near the sphinx and weeping with absolute', 'wretchedness i had nothing left but misery then i slept and when', 'i woke again it was full day and a couple of sparrows were hopping', 'round me on the turf within reach of my arm', '', 'i sat up in the freshness of the morning trying to remember how', 'i had got there and why i had such a profound sense of desertion', 'and despair then things came clear in my mind with the plain', 'reasonable daylight i could look my circumstances fairly in the', 'face i saw the wild folly of my frenzy overnight and i could', 'reason with myself suppose the worst i said suppose the', 'machine altogether lost perhaps destroyed it behoves me to be', 'calm and patient to learn the way of the people to get a clear', 'idea of the method of my loss and the means of getting materials', 'and tools so that in the end perhaps i may make another that', 'would be my only hope perhaps but better than despair and after', 'all it was a beautiful and curious world', '', 'but probably the machine had only been taken away still i must', 'be calm and patient find its hiding place and recover it by force', 'or cunning and with that i scrambled to my feet and looked about', 'me wondering where i could bathe i felt weary stiff and', 'travel soiled the freshness of the morning made me desire an equal', 'freshness i had exhausted my emotion indeed as i went about', 'my business i found myself wondering at my intense excitement', 'overnight i made a careful examination of the ground about the', 'little lawn i wasted some time in futile questionings conveyed as', 'well as i was able to such of the little people as came by they', 'all failed to understand my gestures some were simply stolid some', 'thought it was a jest and laughed at me i had the hardest task in', 'the world to keep my hands off their pretty laughing faces it was', 'a foolish impulse but the devil begotten of fear and blind anger', 'was ill curbed and still eager to take advantage of my perplexity', 'the turf gave better counsel i found a groove ripped in it about', 'midway between the pedestal of the sphinx and the marks of my feet', 'where on arrival i had struggled with the overturned machine', 'there were other signs of removal about with queer narrow', 'footprints like those i could imagine made by a sloth this directed', 'my closer attention to the pedestal it was as i think i have said', 'of bronze it was not a mere block but highly decorated with deep', 'framed panels on either side i went and rapped at these the', 'pedestal was hollow examining the panels with care i found them', 'discontinuous with the frames there were no handles or keyholes', 'but possibly the panels if they were doors as i supposed opened', 'from within one thing was clear enough to my mind it took no very', 'great mental effort to infer that my time machine was inside that', 'pedestal but how it got there was a different problem', '', 'i saw the heads of two orange clad people coming through the bushes', 'and under some blossom covered apple trees towards me i turned', 'smiling to them and beckoned them to me they came and then', 'pointing to the bronze pedestal i tried to intimate my wish to open', 'it but at my first gesture towards this they behaved very oddly i', 'don t know how to convey their expression to you suppose you were', 'to use a grossly improper gesture to a delicate minded woman it is', 'how she would look they went off as if they had received the last', 'possible insult i tried a sweet looking little chap in white next', 'with exactly the same result somehow his manner made me feel', 'ashamed of myself but as you know i wanted the time machine and', 'i tried him once more as he turned off like the others my temper', 'got the better of me in three strides i was after him had him by', 'the loose part of his robe round the neck and began dragging him', 'towards the sphinx then i saw the horror and repugnance of his', 'face and all of a sudden i let him go', '', 'but i was not beaten yet i banged with my fist at the bronze', 'panels i thought i heard something stir inside to be explicit', 'i thought i heard a sound like a chuckle but i must have been', 'mistaken then i got a big pebble from the river and came and', 'hammered till i had flattened a coil in the decorations and the', 'verdigris came off in powdery flakes the delicate little people', 'must have heard me hammering in gusty outbreaks a mile away on', 'either hand but nothing came of it i saw a crowd of them upon the', 'slopes looking furtively at me at last hot and tired i sat down', 'to watch the place but i was too restless to watch long i am too', 'occidental for a long vigil i could work at a problem for years', 'but to wait inactive for twenty four hours that is another matter', '', 'i got up after a time and began walking aimlessly through the', 'bushes towards the hill again patience said i to myself if you', 'want your machine again you must leave that sphinx alone if they', 'mean to take your machine away it s little good your wrecking their', 'bronze panels and if they don t you will get it back as soon as', 'you can ask for it to sit among all those unknown things before a', 'puzzle like that is hopeless that way lies monomania face this', 'world learn its ways watch it be careful of too hasty guesses', 'at its meaning in the end you will find clues to it all then', 'suddenly the humour of the situation came into my mind the thought', 'of the years i had spent in study and toil to get into the future', 'age and now my passion of anxiety to get out of it i had made', 'myself the most complicated and the most hopeless trap that ever a', 'man devised although it was at my own expense i could not help', 'myself i laughed aloud', '', 'going through the big palace it seemed to me that the little', 'people avoided me it may have been my fancy or it may have had', 'something to do with my hammering at the gates of bronze yet i felt', 'tolerably sure of the avoidance i was careful however to show no', 'concern and to abstain from any pursuit of them and in the course', 'of a day or two things got back to the old footing i made what', 'progress i could in the language and in addition i pushed my', 'explorations here and there either i missed some subtle point or', 'their language was excessively simple almost exclusively composed', 'of concrete substantives and verbs there seemed to be few if any', 'abstract terms or little use of figurative language their', 'sentences were usually simple and of two words and i failed to', 'convey or understand any but the simplest propositions i determined', 'to put the thought of my time machine and the mystery of the bronze', 'doors under the sphinx as much as possible in a corner of memory', 'until my growing knowledge would lead me back to them in a natural', 'way yet a certain feeling you may understand tethered me in a', 'circle of a few miles round the point of my arrival', '', 'so far as i could see all the world displayed the same exuberant', 'richness as the thames valley from every hill i climbed i saw the', 'same abundance of splendid buildings endlessly varied in material', 'and style the same clustering thickets of evergreens the same', 'blossom laden trees and tree ferns here and there water shone like', 'silver and beyond the land rose into blue undulating hills and', 'so faded into the serenity of the sky a peculiar feature which', 'presently attracted my attention was the presence of certain', 'circular wells several as it seemed to me of a very great depth', 'one lay by the path up the hill which i had followed during my', 'first walk like the others it was rimmed with bronze curiously', 'wrought and protected by a little cupola from the rain sitting by', 'the side of these wells and peering down into the shafted darkness', 'i could see no gleam of water nor could i start any reflection', 'with a lighted match but in all of them i heard a certain sound', 'a thud thud thud like the beating of some big engine and i', 'discovered from the flaring of my matches that a steady current of', 'air set down the shafts further i threw a scrap of paper into the', 'throat of one and instead of fluttering slowly down it was at', 'once sucked swiftly out of sight', '', 'after a time too i came to connect these wells with tall towers', 'standing here and there upon the slopes for above them there was', 'often just such a flicker in the air as one sees on a hot day above', 'a sun scorched beach putting things together i reached a strong', 'suggestion of an extensive system of subterranean ventilation whose', 'true import it was difficult to imagine i was at first inclined to', 'associate it with the sanitary apparatus of these people it was an', 'obvious conclusion but it was absolutely wrong', '', 'and here i must admit that i learned very little of drains and', 'bells and modes of conveyance and the like conveniences during my', 'time in this real future in some of these visions of utopias and', 'coming times which i have read there is a vast amount of detail', 'about building and social arrangements and so forth but while', 'such details are easy enough to obtain when the whole world is', 'contained in one s imagination they are altogether inaccessible to', 'a real traveller amid such realities as i found here conceive the', 'tale of london which a negro fresh from central africa would take', 'back to his tribe what would he know of railway companies of', 'social movements of telephone and telegraph wires of the parcels', 'delivery company and postal orders and the like yet we at least', 'should be willing enough to explain these things to him and even of', 'what he knew how much could he make his untravelled friend either', 'apprehend or believe then think how narrow the gap between a negro', 'and a white man of our own times and how wide the interval between', 'myself and these of the golden age i was sensible of much which was', 'unseen and which contributed to my comfort but save for a general', 'impression of automatic organization i fear i can convey very', 'little of the difference to your mind', '', 'in the matter of sepulture for instance i could see no signs of', 'crematoria nor anything suggestive of tombs but it occurred to me', 'that possibly there might be cemeteries or crematoria somewhere', 'beyond the range of my explorings this again was a question i', 'deliberately put to myself and my curiosity was at first entirely', 'defeated upon the point the thing puzzled me and i was led to make', 'a further remark which puzzled me still more that aged and infirm', 'among this people there were none', '', 'i must confess that my satisfaction with my first theories of an', 'automatic civilization and a decadent humanity did not long endure', 'yet i could think of no other let me put my difficulties the', 'several big palaces i had explored were mere living places great', 'dining halls and sleeping apartments i could find no machinery no', 'appliances of any kind yet these people were clothed in pleasant', 'fabrics that must at times need renewal and their sandals though', 'undecorated were fairly complex specimens of metalwork somehow', 'such things must be made and the little people displayed no vestige', 'of a creative tendency there were no shops no workshops no sign', 'of importations among them they spent all their time in playing', 'gently in bathing in the river in making love in a half playful', 'fashion in eating fruit and sleeping i could not see how things', 'were kept going', '', 'then again about the time machine something i knew not what', 'had taken it into the hollow pedestal of the white sphinx why for', 'the life of me i could not imagine those waterless wells too', 'those flickering pillars i felt i lacked a clue i felt how shall', 'i put it suppose you found an inscription with sentences here and', 'there in excellent plain english and interpolated therewith others', 'made up of words of letters even absolutely unknown to you well', 'on the third day of my visit that was how the world of eight', 'hundred and two thousand seven hundred and one presented itself to', 'me', '', 'that day too i made a friend of a sort it happened that as i', 'was watching some of the little people bathing in a shallow one of', 'them was seized with cramp and began drifting downstream the main', 'current ran rather swiftly but not too strongly for even a moderate', 'swimmer it will give you an idea therefore of the strange', 'deficiency in these creatures when i tell you that none made the', 'slightest attempt to rescue the weakly crying little thing which', 'was drowning before their eyes when i realized this i hurriedly', 'slipped off my clothes and wading in at a point lower down i', 'caught the poor mite and drew her safe to land a little rubbing of', 'the limbs soon brought her round and i had the satisfaction of', 'seeing she was all right before i left her i had got to such a low', 'estimate of her kind that i did not expect any gratitude from her', 'in that however i was wrong', '', 'this happened in the morning in the afternoon i met my little', 'woman as i believe it was as i was returning towards my centre', 'from an exploration and she received me with cries of delight and', 'presented me with a big garland of flowers evidently made for me', 'and me alone the thing took my imagination very possibly i had', 'been feeling desolate at any rate i did my best to display my', 'appreciation of the gift we were soon seated together in a little', 'stone arbour engaged in conversation chiefly of smiles the', 'creature s friendliness affected me exactly as a child s might have', 'done we passed each other flowers and she kissed my hands i did', 'the same to hers then i tried talk and found that her name was', 'weena which though i don t know what it meant somehow seemed', 'appropriate enough that was the beginning of a queer friendship', 'which lasted a week and ended as i will tell you', '', 'she was exactly like a child she wanted to be with me always she', 'tried to follow me everywhere and on my next journey out and about', 'it went to my heart to tire her down and leave her at last', 'exhausted and calling after me rather plaintively but the problems', 'of the world had to be mastered i had not i said to myself come', 'into the future to carry on a miniature flirtation yet her distress', 'when i left her was very great her expostulations at the parting', 'were sometimes frantic and i think altogether i had as much', 'trouble as comfort from her devotion nevertheless she was somehow', 'a very great comfort i thought it was mere childish affection that', 'made her cling to me until it was too late i did not clearly know', 'what i had inflicted upon her when i left her nor until it was too', 'late did i clearly understand what she was to me for by merely', 'seeming fond of me and showing in her weak futile way that she', 'cared for me the little doll of a creature presently gave my return', 'to the neighbourhood of the white sphinx almost the feeling of', 'coming home and i would watch for her tiny figure of white and gold', 'so soon as i came over the hill', '', 'it was from her too that i learned that fear had not yet left the', 'world she was fearless enough in the daylight and she had the', 'oddest confidence in me for once in a foolish moment i made', 'threatening grimaces at her and she simply laughed at them but she', 'dreaded the dark dreaded shadows dreaded black things darkness', 'to her was the one thing dreadful it was a singularly passionate', 'emotion and it set me thinking and observing i discovered then', 'among other things that these little people gathered into the great', 'houses after dark and slept in droves to enter upon them without a', 'light was to put them into a tumult of apprehension i never found', 'one out of doors or one sleeping alone within doors after dark', 'yet i was still such a blockhead that i missed the lesson of that', 'fear and in spite of weena s distress i insisted upon sleeping away', 'from these slumbering multitudes', '', 'it troubled her greatly but in the end her odd affection for me', 'triumphed and for five of the nights of our acquaintance including', 'the last night of all she slept with her head pillowed on my arm', 'but my story slips away from me as i speak of her it must have been', 'the night before her rescue that i was awakened about dawn i had', 'been restless dreaming most disagreeably that i was drowned and', 'that sea anemones were feeling over my face with their soft palps', 'i woke with a start and with an odd fancy that some greyish animal', 'had just rushed out of the chamber i tried to get to sleep again', 'but i felt restless and uncomfortable it was that dim grey hour', 'when things are just creeping out of darkness when everything is', 'colourless and clear cut and yet unreal i got up and went down', 'into the great hall and so out upon the flagstones in front of the', 'palace i thought i would make a virtue of necessity and see the', 'sunrise', '', 'the moon was setting and the dying moonlight and the first pallor', 'of dawn were mingled in a ghastly half light the bushes were inky', 'black the ground a sombre grey the sky colourless and cheerless', 'and up the hill i thought i could see ghosts there several times', 'as i scanned the slope i saw white figures twice i fancied i saw', 'a solitary white ape like creature running rather quickly up the', 'hill and once near the ruins i saw a leash of them carrying some', 'dark body they moved hastily i did not see what became of them', 'it seemed that they vanished among the bushes the dawn was still', 'indistinct you must understand i was feeling that chill', 'uncertain early morning feeling you may have known i doubted', 'my eyes', '', 'as the eastern sky grew brighter and the light of the day came on', 'and its vivid colouring returned upon the world once more i scanned', 'the view keenly but i saw no vestige of my white figures they were', 'mere creatures of the half light they must have been ghosts i', 'said i wonder whence they dated for a queer notion of grant', 'allen s came into my head and amused me if each generation die and', 'leave ghosts he argued the world at last will get overcrowded with', 'them on that theory they would have grown innumerable some eight', 'hundred thousand years hence and it was no great wonder to see four', 'at once but the jest was unsatisfying and i was thinking of these', 'figures all the morning until weena s rescue drove them out of my', 'head i associated them in some indefinite way with the white animal', 'i had startled in my first passionate search for the time machine', 'but weena was a pleasant substitute yet all the same they were', 'soon destined to take far deadlier possession of my mind', '', 'i think i have said how much hotter than our own was the weather', 'of this golden age i cannot account for it it may be that the sun', 'was hotter or the earth nearer the sun it is usual to assume that', 'the sun will go on cooling steadily in the future but people', 'unfamiliar with such speculations as those of the younger darwin', 'forget that the planets must ultimately fall back one by one into', 'the parent body as these catastrophes occur the sun will blaze', 'with renewed energy and it may be that some inner planet had', 'suffered this fate whatever the reason the fact remains that the', 'sun was very much hotter than we know it', '', 'well one very hot morning my fourth i think as i was seeking', 'shelter from the heat and glare in a colossal ruin near the great', 'house where i slept and fed there happened this strange thing', 'clambering among these heaps of masonry i found a narrow gallery', 'whose end and side windows were blocked by fallen masses of stone', 'by contrast with the brilliancy outside it seemed at first', 'impenetrably dark to me i entered it groping for the change from', 'light to blackness made spots of colour swim before me suddenly i', 'halted spellbound a pair of eyes luminous by reflection against', 'the daylight without was watching me out of the darkness', '', 'the old instinctive dread of wild beasts came upon me i clenched', 'my hands and steadfastly looked into the glaring eyeballs i was', 'afraid to turn then the thought of the absolute security in which', 'humanity appeared to be living came to my mind and then i', 'remembered that strange terror of the dark overcoming my fear to', 'some extent i advanced a step and spoke i will admit that my', 'voice was harsh and ill controlled i put out my hand and touched', 'something soft at once the eyes darted sideways and something', 'white ran past me i turned with my heart in my mouth and saw a', 'queer little ape like figure its head held down in a peculiar', 'manner running across the sunlit space behind me it blundered', 'against a block of granite staggered aside and in a moment was', 'hidden in a black shadow beneath another pile of ruined masonry', '', 'my impression of it is of course imperfect but i know it was a', 'dull white and had strange large greyish red eyes also that there', 'was flaxen hair on its head and down its back but as i say it', 'went too fast for me to see distinctly i cannot even say whether it', 'ran on all fours or only with its forearms held very low after an', 'instant s pause i followed it into the second heap of ruins i could', 'not find it at first but after a time in the profound obscurity i', 'came upon one of those round well like openings of which i have told', 'you half closed by a fallen pillar a sudden thought came to me', 'could this thing have vanished down the shaft i lit a match and', 'looking down i saw a small white moving creature with large', 'bright eyes which regarded me steadfastly as it retreated it made', 'me shudder it was so like a human spider it was clambering down', 'the wall and now i saw for the first time a number of metal foot', 'and hand rests forming a kind of ladder down the shaft then the', 'light burned my fingers and fell out of my hand going out as it', 'dropped and when i had lit another the little monster had', 'disappeared', '', 'i do not know how long i sat peering down that well it was not for', 'some time that i could succeed in persuading myself that the thing i', 'had seen was human but gradually the truth dawned on me that', 'man had not remained one species but had differentiated into two', 'distinct animals that my graceful children of the upper world were', 'not the sole descendants of our generation but that this bleached', 'obscene nocturnal thing which had flashed before me was also heir', 'to all the ages', '', 'i thought of the flickering pillars and of my theory of an', 'underground ventilation i began to suspect their true import and', 'what i wondered was this lemur doing in my scheme of a perfectly', 'balanced organization how was it related to the indolent serenity', 'of the beautiful upper worlders and what was hidden down there', 'at the foot of that shaft i sat upon the edge of the well telling', 'myself that at any rate there was nothing to fear and that there', 'i must descend for the solution of my difficulties and withal i', 'was absolutely afraid to go as i hesitated two of the beautiful', 'upper world people came running in their amorous sport across the', 'daylight in the shadow the male pursued the female flinging', 'flowers at her as he ran', '', 'they seemed distressed to find me my arm against the overturned', 'pillar peering down the well apparently it was considered bad form', 'to remark these apertures for when i pointed to this one and tried', 'to frame a question about it in their tongue they were still more', 'visibly distressed and turned away but they were interested by my', 'matches and i struck some to amuse them i tried them again about', 'the well and again i failed so presently i left them meaning to', 'go back to weena and see what i could get from her but my mind was', 'already in revolution my guesses and impressions were slipping and', 'sliding to a new adjustment i had now a clue to the import of these', 'wells to the ventilating towers to the mystery of the ghosts to', 'say nothing of a hint at the meaning of the bronze gates and the', 'fate of the time machine and very vaguely there came a suggestion', 'towards the solution of the economic problem that had puzzled me', '', 'here was the new view plainly this second species of man was', 'subterranean there were three circumstances in particular which', 'made me think that its rare emergence above ground was the outcome', 'of a long continued underground habit in the first place there was', 'the bleached look common in most animals that live largely in the', 'dark the white fish of the kentucky caves for instance then', 'those large eyes with that capacity for reflecting light are', 'common features of nocturnal things witness the owl and the cat', 'and last of all that evident confusion in the sunshine that hasty', 'yet fumbling awkward flight towards dark shadow and that peculiar', 'carriage of the head while in the light all reinforced the theory', 'of an extreme sensitiveness of the retina', '', 'beneath my feet then the earth must be tunnelled enormously and', 'these tunnellings were the habitat of the new race the presence of', 'ventilating shafts and wells along the hill slopes everywhere in', 'fact except along the river valley showed how universal were its', 'ramifications what so natural then as to assume that it was in', 'this artificial underworld that such work as was necessary to the', 'comfort of the daylight race was done the notion was so plausible', 'that i at once accepted it and went on to assume the how of this', 'splitting of the human species i dare say you will anticipate the', 'shape of my theory though for myself i very soon felt that it', 'fell far short of the truth', '', 'at first proceeding from the problems of our own age it seemed', 'clear as daylight to me that the gradual widening of the present', 'merely temporary and social difference between the capitalist and', 'the labourer was the key to the whole position no doubt it will', 'seem grotesque enough to you and wildly incredible and yet even', 'now there are existing circumstances to point that way there is', 'a tendency to utilize underground space for the less ornamental', 'purposes of civilization there is the metropolitan railway in', 'london for instance there are new electric railways there are', 'subways there are underground workrooms and restaurants and they', 'increase and multiply evidently i thought this tendency had', 'increased till industry had gradually lost its birthright in the', 'sky i mean that it had gone deeper and deeper into larger and ever', 'larger underground factories spending a still increasing amount of', 'its time therein till in the end even now does not an east end', 'worker live in such artificial conditions as practically to be cut', 'off from the natural surface of the earth', '', 'again the exclusive tendency of richer people due no doubt to', 'the increasing refinement of their education and the widening gulf', 'between them and the rude violence of the poor is already leading', 'to the closing in their interest of considerable portions of the', 'surface of the land about london for instance perhaps half the', 'prettier country is shut in against intrusion and this same', 'widening gulf which is due to the length and expense of the higher', 'educational process and the increased facilities for and temptations', 'towards refined habits on the part of the rich will make that', 'exchange between class and class that promotion by intermarriage', 'which at present retards the splitting of our species along lines', 'of social stratification less and less frequent so in the end', 'above ground you must have the haves pursuing pleasure and comfort', 'and beauty and below ground the have nots the workers getting', 'continually adapted to the conditions of their labour once they', 'were there they would no doubt have to pay rent and not a little', 'of it for the ventilation of their caverns and if they refused', 'they would starve or be suffocated for arrears such of them as were', 'so constituted as to be miserable and rebellious would die and in', 'the end the balance being permanent the survivors would become as', 'well adapted to the conditions of underground life and as happy in', 'their way as the upper world people were to theirs as it seemed to', 'me the refined beauty and the etiolated pallor followed naturally', 'enough', '', 'the great triumph of humanity i had dreamed of took a different', 'shape in my mind it had been no such triumph of moral education and', 'general co operation as i had imagined instead i saw a real', 'aristocracy armed with a perfected science and working to a logical', 'conclusion the industrial system of to day its triumph had not been', 'simply a triumph over nature but a triumph over nature and the', 'fellow man this i must warn you was my theory at the time i had', 'no convenient cicerone in the pattern of the utopian books my', 'explanation may be absolutely wrong i still think it is the', 'most plausible one but even on this supposition the balanced', 'civilization that was at last attained must have long since passed', 'its zenith and was now far fallen into decay the too perfect', 'security of the upper worlders had led them to a slow movement of', 'degeneration to a general dwindling in size strength and', 'intelligence that i could see clearly enough already what had', 'happened to the under grounders i did not yet suspect but from what', 'i had seen of the morlocks that by the by was the name by which', 'these creatures were called i could imagine that the modification', 'of the human type was even far more profound than among the eloi', 'the beautiful race that i already knew', '', 'then came troublesome doubts why had the morlocks taken my time', 'machine for i felt sure it was they who had taken it why too if', 'the eloi were masters could they not restore the machine to me and', 'why were they so terribly afraid of the dark i proceeded as i have', 'said to question weena about this under world but here again i was', 'disappointed at first she would not understand my questions and', 'presently she refused to answer them she shivered as though the', 'topic was unendurable and when i pressed her perhaps a little', 'harshly she burst into tears they were the only tears except my', 'own i ever saw in that golden age when i saw them i ceased', 'abruptly to trouble about the morlocks and was only concerned in', 'banishing these signs of the human inheritance from weena s eyes', 'and very soon she was smiling and clapping her hands while i', 'solemnly burned a match', '', '', '', '', 'vi', '', '', 'it may seem odd to you but it was two days before i could follow', 'up the new found clue in what was manifestly the proper way i felt', 'a peculiar shrinking from those pallid bodies they were just the', 'half bleached colour of the worms and things one sees preserved in', 'spirit in a zoological museum and they were filthily cold to the', 'touch probably my shrinking was largely due to the sympathetic', 'influence of the eloi whose disgust of the morlocks i now began', 'to appreciate', '', 'the next night i did not sleep well probably my health was a', 'little disordered i was oppressed with perplexity and doubt once', 'or twice i had a feeling of intense fear for which i could perceive', 'no definite reason i remember creeping noiselessly into the great', 'hall where the little people were sleeping in the moonlight that', 'night weena was among them and feeling reassured by their presence', 'it occurred to me even then that in the course of a few days the', 'moon must pass through its last quarter and the nights grow dark', 'when the appearances of these unpleasant creatures from below these', 'whitened lemurs this new vermin that had replaced the old might be', 'more abundant and on both these days i had the restless feeling of', 'one who shirks an inevitable duty i felt assured that the time', 'machine was only to be recovered by boldly penetrating these', 'underground mysteries yet i could not face the mystery if only i', 'had had a companion it would have been different but i was so', 'horribly alone and even to clamber down into the darkness of the', 'well appalled me i don t know if you will understand my feeling', 'but i never felt quite safe at my back', '', 'it was this restlessness this insecurity perhaps that drove me', 'further and further afield in my exploring expeditions going to the', 'south westward towards the rising country that is now called combe', 'wood i observed far off in the direction of nineteenth century', 'banstead a vast green structure different in character from any', 'i had hitherto seen it was larger than the largest of the palaces', 'or ruins i knew and the facade had an oriental look the face', 'of it having the lustre as well as the pale green tint a kind', 'of bluish green of a certain type of chinese porcelain this', 'difference in aspect suggested a difference in use and i was minded', 'to push on and explore but the day was growing late and i had come', 'upon the sight of the place after a long and tiring circuit so i', 'resolved to hold over the adventure for the following day and i', 'returned to the welcome and the caresses of little weena but next', 'morning i perceived clearly enough that my curiosity regarding the', 'palace of green porcelain was a piece of self deception to enable', 'me to shirk by another day an experience i dreaded i resolved i', 'would make the descent without further waste of time and started', 'out in the early morning towards a well near the ruins of granite', 'and aluminium', '', 'little weena ran with me she danced beside me to the well but', 'when she saw me lean over the mouth and look downward she seemed', 'strangely disconcerted good bye little weena i said kissing', 'her and then putting her down i began to feel over the parapet', 'for the climbing hooks rather hastily i may as well confess for', 'i feared my courage might leak away at first she watched me in', 'amazement then she gave a most piteous cry and running to me she', 'began to pull at me with her little hands i think her opposition', 'nerved me rather to proceed i shook her off perhaps a little', 'roughly and in another moment i was in the throat of the well i', 'saw her agonized face over the parapet and smiled to reassure her', 'then i had to look down at the unstable hooks to which i clung', '', 'i had to clamber down a shaft of perhaps two hundred yards the', 'descent was effected by means of metallic bars projecting from', 'the sides of the well and these being adapted to the needs of', 'a creature much smaller and lighter than myself i was speedily', 'cramped and fatigued by the descent and not simply fatigued one of', 'the bars bent suddenly under my weight and almost swung me off into', 'the blackness beneath for a moment i hung by one hand and after', 'that experience i did not dare to rest again though my arms and', 'back were presently acutely painful i went on clambering down the', 'sheer descent with as quick a motion as possible glancing upward', 'i saw the aperture a small blue disk in which a star was visible', 'while little weena s head showed as a round black projection the', 'thudding sound of a machine below grew louder and more oppressive', 'everything save that little disk above was profoundly dark and when', 'i looked up again weena had disappeared', '', 'i was in an agony of discomfort i had some thought of trying to go', 'up the shaft again and leave the under world alone but even while', 'i turned this over in my mind i continued to descend at last with', 'intense relief i saw dimly coming up a foot to the right of me a', 'slender loophole in the wall swinging myself in i found it was the', 'aperture of a narrow horizontal tunnel in which i could lie down and', 'rest it was not too soon my arms ached my back was cramped and i', 'was trembling with the prolonged terror of a fall besides this the', 'unbroken darkness had had a distressing effect upon my eyes the air', 'was full of the throb and hum of machinery pumping air down the', 'shaft', '', 'i do not know how long i lay i was roused by a soft hand touching', 'my face starting up in the darkness i snatched at my matches and', 'hastily striking one i saw three stooping white creatures similar', 'to the one i had seen above ground in the ruin hastily retreating', 'before the light living as they did in what appeared to me', 'impenetrable darkness their eyes were abnormally large and', 'sensitive just as are the pupils of the abysmal fishes and they', 'reflected the light in the same way i have no doubt they could see', 'me in that rayless obscurity and they did not seem to have any fear', 'of me apart from the light but so soon as i struck a match in', 'order to see them they fled incontinently vanishing into dark', 'gutters and tunnels from which their eyes glared at me in the', 'strangest fashion', '', 'i tried to call to them but the language they had was apparently', 'different from that of the over world people so that i was needs', 'left to my own unaided efforts and the thought of flight before', 'exploration was even then in my mind but i said to myself you are', 'in for it now and feeling my way along the tunnel i found the', 'noise of machinery grow louder presently the walls fell away from', 'me and i came to a large open space and striking another match', 'saw that i had entered a vast arched cavern which stretched into', 'utter darkness beyond the range of my light the view i had of it', 'was as much as one could see in the burning of a match', '', 'necessarily my memory is vague great shapes like big machines rose', 'out of the dimness and cast grotesque black shadows in which dim', 'spectral morlocks sheltered from the glare the place by the by', 'was very stuffy and oppressive and the faint halitus of freshly', 'shed blood was in the air some way down the central vista was a', 'little table of white metal laid with what seemed a meal the', 'morlocks at any rate were carnivorous even at the time i remember', 'wondering what large animal could have survived to furnish the red', 'joint i saw it was all very indistinct the heavy smell the big', 'unmeaning shapes the obscene figures lurking in the shadows and', 'only waiting for the darkness to come at me again then the match', 'burned down and stung my fingers and fell a wriggling red spot', 'in the blackness', '', 'i have thought since how particularly ill equipped i was for such', 'an experience when i had started with the time machine i had', 'started with the absurd assumption that the men of the future would', 'certainly be infinitely ahead of ourselves in all their appliances', 'i had come without arms without medicine without anything to', 'smoke at times i missed tobacco frightfully even without enough', 'matches if only i had thought of a kodak i could have flashed that', 'glimpse of the underworld in a second and examined it at leisure', 'but as it was i stood there with only the weapons and the powers', 'that nature had endowed me with hands feet and teeth these and', 'four safety matches that still remained to me', '', 'i was afraid to push my way in among all this machinery in the', 'dark and it was only with my last glimpse of light i discovered', 'that my store of matches had run low it had never occurred to me', 'until that moment that there was any need to economize them and i', 'had wasted almost half the box in astonishing the upper worlders to', 'whom fire was a novelty now as i say i had four left and while i', 'stood in the dark a hand touched mine lank fingers came feeling', 'over my face and i was sensible of a peculiar unpleasant odour i', 'fancied i heard the breathing of a crowd of those dreadful little', 'beings about me i felt the box of matches in my hand being gently', 'disengaged and other hands behind me plucking at my clothing the', 'sense of these unseen creatures examining me was indescribably', 'unpleasant the sudden realization of my ignorance of their ways of', 'thinking and doing came home to me very vividly in the darkness i', 'shouted at them as loudly as i could they started away and then', 'i could feel them approaching me again they clutched at me more', 'boldly whispering odd sounds to each other i shivered violently', 'and shouted again rather discordantly this time they were not so', 'seriously alarmed and they made a queer laughing noise as they came', 'back at me i will confess i was horribly frightened i determined', 'to strike another match and escape under the protection of its', 'glare i did so and eking out the flicker with a scrap of paper', 'from my pocket i made good my retreat to the narrow tunnel but i', 'had scarce entered this when my light was blown out and in the', 'blackness i could hear the morlocks rustling like wind among leaves', 'and pattering like the rain as they hurried after me', '', 'in a moment i was clutched by several hands and there was no', 'mistaking that they were trying to haul me back i struck another', 'light and waved it in their dazzled faces you can scarce imagine', 'how nauseatingly inhuman they looked those pale chinless faces', 'and great lidless pinkish grey eyes as they stared in their', 'blindness and bewilderment but i did not stay to look i promise', 'you i retreated again and when my second match had ended i struck', 'my third it had almost burned through when i reached the opening', 'into the shaft i lay down on the edge for the throb of the great', 'pump below made me giddy then i felt sideways for the projecting', 'hooks and as i did so my feet were grasped from behind and i', 'was violently tugged backward i lit my last match and it', 'incontinently went out but i had my hand on the climbing bars now', 'and kicking violently i disengaged myself from the clutches of the', 'morlocks and was speedily clambering up the shaft while they stayed', 'peering and blinking up at me all but one little wretch who', 'followed me for some way and well nigh secured my boot as a trophy', '', 'that climb seemed interminable to me with the last twenty or', 'thirty feet of it a deadly nausea came upon me i had the greatest', 'difficulty in keeping my hold the last few yards was a frightful', 'struggle against this faintness several times my head swam and i', 'felt all the sensations of falling at last however i got over the', 'well mouth somehow and staggered out of the ruin into the blinding', 'sunlight i fell upon my face even the soil smelt sweet and clean', 'then i remember weena kissing my hands and ears and the voices of', 'others among the eloi then for a time i was insensible', '', '', '', '', 'vii', '', '', 'now indeed i seemed in a worse case than before hitherto', 'except during my night s anguish at the loss of the time machine', 'i had felt a sustaining hope of ultimate escape but that hope was', 'staggered by these new discoveries hitherto i had merely thought', 'myself impeded by the childish simplicity of the little people and', 'by some unknown forces which i had only to understand to overcome', 'but there was an altogether new element in the sickening quality of', 'the morlocks a something inhuman and malign instinctively i', 'loathed them before i had felt as a man might feel who had fallen', 'into a pit my concern was with the pit and how to get out of it', 'now i felt like a beast in a trap whose enemy would come upon him', 'soon', '', 'the enemy i dreaded may surprise you it was the darkness of the', 'new moon weena had put this into my head by some at first', 'incomprehensible remarks about the dark nights it was not now', 'such a very difficult problem to guess what the coming dark nights', 'might mean the moon was on the wane each night there was a longer', 'interval of darkness and i now understood to some slight degree at', 'least the reason of the fear of the little upper world people for', 'the dark i wondered vaguely what foul villainy it might be that', 'the morlocks did under the new moon i felt pretty sure now that', 'my second hypothesis was all wrong the upper world people might', 'once have been the favoured aristocracy and the morlocks their', 'mechanical servants but that had long since passed away the two', 'species that had resulted from the evolution of man were sliding', 'down towards or had already arrived at an altogether new', 'relationship the eloi like the carolingian kings had decayed', 'to a mere beautiful futility they still possessed the earth on', 'sufferance since the morlocks subterranean for innumerable', 'generations had come at last to find the daylit surface', 'intolerable and the morlocks made their garments i inferred and', 'maintained them in their habitual needs perhaps through the', 'survival of an old habit of service they did it as a standing horse', 'paws with his foot or as a man enjoys killing animals in sport', 'because ancient and departed necessities had impressed it on the', 'organism but clearly the old order was already in part reversed', 'the nemesis of the delicate ones was creeping on apace ages ago', 'thousands of generations ago man had thrust his brother man out of', 'the ease and the sunshine and now that brother was coming back', 'changed already the eloi had begun to learn one old lesson anew', 'they were becoming reacquainted with fear and suddenly there came', 'into my head the memory of the meat i had seen in the under world', 'it seemed odd how it floated into my mind not stirred up as it', 'were by the current of my meditations but coming in almost like a', 'question from outside i tried to recall the form of it i had a', 'vague sense of something familiar but i could not tell what it was', 'at the time', '', 'still however helpless the little people in the presence of their', 'mysterious fear i was differently constituted i came out of this', 'age of ours this ripe prime of the human race when fear does not', 'paralyse and mystery has lost its terrors i at least would defend', 'myself without further delay i determined to make myself arms and a', 'fastness where i might sleep with that refuge as a base i could', 'face this strange world with some of that confidence i had lost in', 'realizing to what creatures night by night i lay exposed i felt', 'i could never sleep again until my bed was secure from them i', 'shuddered with horror to think how they must already have examined', 'me', '', 'i wandered during the afternoon along the valley of the thames but', 'found nothing that commended itself to my mind as inaccessible all', 'the buildings and trees seemed easily practicable to such dexterous', 'climbers as the morlocks to judge by their wells must be then the', 'tall pinnacles of the palace of green porcelain and the polished', 'gleam of its walls came back to my memory and in the evening', 'taking weena like a child upon my shoulder i went up the hills', 'towards the south west the distance i had reckoned was seven or', 'eight miles but it must have been nearer eighteen i had first seen', 'the place on a moist afternoon when distances are deceptively', 'diminished in addition the heel of one of my shoes was loose and', 'a nail was working through the sole they were comfortable old shoes', 'i wore about indoors so that i was lame and it was already long', 'past sunset when i came in sight of the palace silhouetted black', 'against the pale yellow of the sky', '', 'weena had been hugely delighted when i began to carry her but', 'after a while she desired me to let her down and ran along by the', 'side of me occasionally darting off on either hand to pick flowers', 'to stick in my pockets my pockets had always puzzled weena but at', 'the last she had concluded that they were an eccentric kind of vase', 'for floral decoration at least she utilized them for that purpose', 'and that reminds me in changing my jacket i found', '', 'the time traveller paused put his hand into his pocket and', 'silently placed two withered flowers not unlike very large white', 'mallows upon the little table then he resumed his narrative', '', 'as the hush of evening crept over the world and we proceeded over', 'the hill crest towards wimbledon weena grew tired and wanted to', 'return to the house of grey stone but i pointed out the distant', 'pinnacles of the palace of green porcelain to her and contrived to', 'make her understand that we were seeking a refuge there from her', 'fear you know that great pause that comes upon things before the', 'dusk even the breeze stops in the trees to me there is always an', 'air of expectation about that evening stillness the sky was clear', 'remote and empty save for a few horizontal bars far down in the', 'sunset well that night the expectation took the colour of my', 'fears in that darkling calm my senses seemed preternaturally', 'sharpened i fancied i could even feel the hollowness of the ground', 'beneath my feet could indeed almost see through it the morlocks', 'on their ant hill going hither and thither and waiting for the dark', 'in my excitement i fancied that they would receive my invasion of', 'their burrows as a declaration of war and why had they taken my', 'time machine', '', 'so we went on in the quiet and the twilight deepened into night', 'the clear blue of the distance faded and one star after another', 'came out the ground grew dim and the trees black weena s fears and', 'her fatigue grew upon her i took her in my arms and talked to her', 'and caressed her then as the darkness grew deeper she put her', 'arms round my neck and closing her eyes tightly pressed her face', 'against my shoulder so we went down a long slope into a valley and', 'there in the dimness i almost walked into a little river this i', 'waded and went up the opposite side of the valley past a number', 'of sleeping houses and by a statue a faun or some such figure', 'minus the head here too were acacias so far i had seen nothing of', 'the morlocks but it was yet early in the night and the darker hours', 'before the old moon rose were still to come', '', 'from the brow of the next hill i saw a thick wood spreading wide', 'and black before me i hesitated at this i could see no end to', 'it either to the right or the left feeling tired my feet in', 'particular were very sore i carefully lowered weena from my', 'shoulder as i halted and sat down upon the turf i could no', 'longer see the palace of green porcelain and i was in doubt of my', 'direction i looked into the thickness of the wood and thought of', 'what it might hide under that dense tangle of branches one would', 'be out of sight of the stars even were there no other lurking', 'danger a danger i did not care to let my imagination loose', 'upon there would still be all the roots to stumble over and the', 'tree boles to strike against', '', 'i was very tired too after the excitements of the day so i', 'decided that i would not face it but would pass the night upon the', 'open hill', '', 'weena i was glad to find was fast asleep i carefully wrapped her', 'in my jacket and sat down beside her to wait for the moonrise the', 'hill side was quiet and deserted but from the black of the wood', 'there came now and then a stir of living things above me shone the', 'stars for the night was very clear i felt a certain sense of', 'friendly comfort in their twinkling all the old constellations', 'had gone from the sky however that slow movement which is', 'imperceptible in a hundred human lifetimes had long since', 'rearranged them in unfamiliar groupings but the milky way it', 'seemed to me was still the same tattered streamer of star dust as', 'of yore southward as i judged it was a very bright red star that', 'was new to me it was even more splendid than our own green sirius', 'and amid all these scintillating points of light one bright planet', 'shone kindly and steadily like the face of an old friend', '', 'looking at these stars suddenly dwarfed my own troubles and all', 'the gravities of terrestrial life i thought of their unfathomable', 'distance and the slow inevitable drift of their movements out of', 'the unknown past into the unknown future i thought of the great', 'precessional cycle that the pole of the earth describes only forty', 'times had that silent revolution occurred during all the years that', 'i had traversed and during these few revolutions all the activity', 'all the traditions the complex organizations the nations', 'languages literatures aspirations even the mere memory of man as', 'i knew him had been swept out of existence instead were these', 'frail creatures who had forgotten their high ancestry and the white', 'things of which i went in terror then i thought of the great fear', 'that was between the two species and for the first time with a', 'sudden shiver came the clear knowledge of what the meat i had seen', 'might be yet it was too horrible i looked at little weena sleeping', 'beside me her face white and starlike under the stars and', 'forthwith dismissed the thought', '', 'through that long night i held my mind off the morlocks as well as', 'i could and whiled away the time by trying to fancy i could find', 'signs of the old constellations in the new confusion the sky kept', 'very clear except for a hazy cloud or so no doubt i dozed at', 'times then as my vigil wore on came a faintness in the eastward', 'sky like the reflection of some colourless fire and the old moon', 'rose thin and peaked and white and close behind and overtaking', 'it and overflowing it the dawn came pale at first and then', 'growing pink and warm no morlocks had approached us indeed i had', 'seen none upon the hill that night and in the confidence of renewed', 'day it almost seemed to me that my fear had been unreasonable i', 'stood up and found my foot with the loose heel swollen at the ankle', 'and painful under the heel so i sat down again took off my shoes', 'and flung them away', '', 'i awakened weena and we went down into the wood now green and', 'pleasant instead of black and forbidding we found some fruit', 'wherewith to break our fast we soon met others of the dainty ones', 'laughing and dancing in the sunlight as though there was no such', 'thing in nature as the night and then i thought once more of the', 'meat that i had seen i felt assured now of what it was and from', 'the bottom of my heart i pitied this last feeble rill from the great', 'flood of humanity clearly at some time in the long ago of human', 'decay the morlocks food had run short possibly they had lived on', 'rats and such like vermin even now man is far less discriminating', 'and exclusive in his food than he was far less than any monkey his', 'prejudice against human flesh is no deep seated instinct and so', 'these inhuman sons of men i tried to look at the thing in a', 'scientific spirit after all they were less human and more remote', 'than our cannibal ancestors of three or four thousand years ago', 'and the intelligence that would have made this state of things a', 'torment had gone why should i trouble myself these eloi were mere', 'fatted cattle which the ant like morlocks preserved and preyed', 'upon probably saw to the breeding of and there was weena dancing', 'at my side', '', 'then i tried to preserve myself from the horror that was coming', 'upon me by regarding it as a rigorous punishment of human', 'selfishness man had been content to live in ease and delight upon', 'the labours of his fellow man had taken necessity as his watchword', 'and excuse and in the fullness of time necessity had come home to', 'him i even tried a carlyle like scorn of this wretched aristocracy', 'in decay but this attitude of mind was impossible however great', 'their intellectual degradation the eloi had kept too much of the', 'human form not to claim my sympathy and to make me perforce a', 'sharer in their degradation and their fear', '', 'i had at that time very vague ideas as to the course i should', 'pursue my first was to secure some safe place of refuge and to', 'make myself such arms of metal or stone as i could contrive that', 'necessity was immediate in the next place i hoped to procure some', 'means of fire so that i should have the weapon of a torch at hand', 'for nothing i knew would be more efficient against these morlocks', 'then i wanted to arrange some contrivance to break open the doors of', 'bronze under the white sphinx i had in mind a battering ram i had', 'a persuasion that if i could enter those doors and carry a blaze of', 'light before me i should discover the time machine and escape i', 'could not imagine the morlocks were strong enough to move it far', 'away weena i had resolved to bring with me to our own time and', 'turning such schemes over in my mind i pursued our way towards the', 'building which my fancy had chosen as our dwelling', '', '', '', '', 'viii', '', '', 'i found the palace of green porcelain when we approached it about', 'noon deserted and falling into ruin only ragged vestiges of glass', 'remained in its windows and great sheets of the green facing had', 'fallen away from the corroded metallic framework it lay very high', 'upon a turfy down and looking north eastward before i entered it i', 'was surprised to see a large estuary or even creek where i judged', 'wandsworth and battersea must once have been i thought then though', 'i never followed up the thought of what might have happened or', 'might be happening to the living things in the sea', '', 'the material of the palace proved on examination to be indeed', 'porcelain and along the face of it i saw an inscription in some', 'unknown character i thought rather foolishly that weena might', 'help me to interpret this but i only learned that the bare idea of', 'writing had never entered her head she always seemed to me i', 'fancy more human than she was perhaps because her affection was so', 'human', '', 'within the big valves of the door which were open and broken we', 'found instead of the customary hall a long gallery lit by many', 'side windows at the first glance i was reminded of a museum', 'the tiled floor was thick with dust and a remarkable array of', 'miscellaneous objects was shrouded in the same grey covering then', 'i perceived standing strange and gaunt in the centre of the hall', 'what was clearly the lower part of a huge skeleton i recognized', 'by the oblique feet that it was some extinct creature after the', 'fashion of the megatherium the skull and the upper bones lay', 'beside it in the thick dust and in one place where rain water had', 'dropped through a leak in the roof the thing itself had been worn', 'away further in the gallery was the huge skeleton barrel of a', 'brontosaurus my museum hypothesis was confirmed going towards the', 'side i found what appeared to be sloping shelves and clearing away', 'the thick dust i found the old familiar glass cases of our own', 'time but they must have been air tight to judge from the fair', 'preservation of some of their contents', '', 'clearly we stood among the ruins of some latter day south', 'kensington here apparently was the palaeontological section', 'and a very splendid array of fossils it must have been though the', 'inevitable process of decay that had been staved off for a time and', 'had through the extinction of bacteria and fungi lost ninety nine', 'hundredths of its force was nevertheless with extreme sureness if', 'with extreme slowness at work again upon all its treasures here and', 'there i found traces of the little people in the shape of rare', 'fossils broken to pieces or threaded in strings upon reeds and the', 'cases had in some instances been bodily removed by the morlocks as', 'i judged the place was very silent the thick dust deadened our', 'footsteps weena who had been rolling a sea urchin down the sloping', 'glass of a case presently came as i stared about me and very', 'quietly took my hand and stood beside me', '', 'and at first i was so much surprised by this ancient monument of an', 'intellectual age that i gave no thought to the possibilities it', 'presented even my preoccupation about the time machine receded a', 'little from my mind', '', 'to judge from the size of the place this palace of green porcelain', 'had a great deal more in it than a gallery of palaeontology', 'possibly historical galleries it might be even a library to me', 'at least in my present circumstances these would be vastly more', 'interesting than this spectacle of oldtime geology in decay', 'exploring i found another short gallery running transversely to the', 'first this appeared to be devoted to minerals and the sight of a', 'block of sulphur set my mind running on gunpowder but i could find', 'no saltpeter indeed no nitrates of any kind doubtless they had', 'deliquesced ages ago yet the sulphur hung in my mind and set up a', 'train of thinking as for the rest of the contents of that gallery', 'though on the whole they were the best preserved of all i saw i had', 'little interest i am no specialist in mineralogy and i went on', 'down a very ruinous aisle running parallel to the first hall i had', 'entered apparently this section had been devoted to natural', 'history but everything had long since passed out of recognition a', 'few shrivelled and blackened vestiges of what had once been stuffed', 'animals desiccated mummies in jars that had once held spirit a', 'brown dust of departed plants that was all i was sorry for that', 'because i should have been glad to trace the patent readjustments by', 'which the conquest of animated nature had been attained then we', 'came to a gallery of simply colossal proportions but singularly', 'ill lit the floor of it running downward at a slight angle from the', 'end at which i entered at intervals white globes hung from the', 'ceiling many of them cracked and smashed which suggested that', 'originally the place had been artificially lit here i was more in', 'my element for rising on either side of me were the huge bulks of', 'big machines all greatly corroded and many broken down but some', 'still fairly complete you know i have a certain weakness for', 'mechanism and i was inclined to linger among these the more so as', 'for the most part they had the interest of puzzles and i could make', 'only the vaguest guesses at what they were for i fancied that if', 'i could solve their puzzles i should find myself in possession of', 'powers that might be of use against the morlocks', '', 'suddenly weena came very close to my side so suddenly that she', 'startled me had it not been for her i do not think i should have', 'noticed that the floor of the gallery sloped at all footnote it', 'may be of course that the floor did not slope but that the museum', 'was built into the side of a hill ed the end i had come in at', 'was quite above ground and was lit by rare slit like windows as', 'you went down the length the ground came up against these windows', 'until at last there was a pit like the area of a london house', 'before each and only a narrow line of daylight at the top i went', 'slowly along puzzling about the machines and had been too intent', 'upon them to notice the gradual diminution of the light until', 'weena s increasing apprehensions drew my attention then i saw that', 'the gallery ran down at last into a thick darkness i hesitated and', 'then as i looked round me i saw that the dust was less abundant', 'and its surface less even further away towards the dimness it', 'appeared to be broken by a number of small narrow footprints my', 'sense of the immediate presence of the morlocks revived at that', 'i felt that i was wasting my time in the academic examination of', 'machinery i called to mind that it was already far advanced in the', 'afternoon and that i had still no weapon no refuge and no means', 'of making a fire and then down in the remote blackness of the', 'gallery i heard a peculiar pattering and the same odd noises i had', 'heard down the well', '', 'i took weena s hand then struck with a sudden idea i left her', 'and turned to a machine from which projected a lever not unlike', 'those in a signal box clambering upon the stand and grasping this', 'lever in my hands i put all my weight upon it sideways suddenly', 'weena deserted in the central aisle began to whimper i had judged', 'the strength of the lever pretty correctly for it snapped after a', 'minute s strain and i rejoined her with a mace in my hand more than', 'sufficient i judged for any morlock skull i might encounter and i', 'longed very much to kill a morlock or so very inhuman you may', 'think to want to go killing one s own descendants but it was', 'impossible somehow to feel any humanity in the things only my', 'disinclination to leave weena and a persuasion that if i began to', 'slake my thirst for murder my time machine might suffer restrained', 'me from going straight down the gallery and killing the brutes i', 'heard', '', 'well mace in one hand and weena in the other i went out of that', 'gallery and into another and still larger one which at the first', 'glance reminded me of a military chapel hung with tattered flags', 'the brown and charred rags that hung from the sides of it i', 'presently recognized as the decaying vestiges of books they had', 'long since dropped to pieces and every semblance of print had left', 'them but here and there were warped boards and cracked metallic', 'clasps that told the tale well enough had i been a literary man i', 'might perhaps have moralized upon the futility of all ambition', 'but as it was the thing that struck me with keenest force was the', 'enormous waste of labour to which this sombre wilderness of rotting', 'paper testified at the time i will confess that i thought chiefly', 'of the philosophical transactions and my own seventeen papers upon', 'physical optics', '', 'then going up a broad staircase we came to what may once have', 'been a gallery of technical chemistry and here i had not a little', 'hope of useful discoveries except at one end where the roof had', 'collapsed this gallery was well preserved i went eagerly to every', 'unbroken case and at last in one of the really air tight cases', 'i found a box of matches very eagerly i tried them they were', 'perfectly good they were not even damp i turned to weena dance', 'i cried to her in her own tongue for now i had a weapon indeed', 'against the horrible creatures we feared and so in that derelict', 'museum upon the thick soft carpeting of dust to weena s huge', 'delight i solemnly performed a kind of composite dance whistling', 'the land of the leal as cheerfully as i could in part it was a', 'modest cancan in part a step dance in part a skirt dance so far', 'as my tail coat permitted and in part original for i am naturally', 'inventive as you know', '', 'now i still think that for this box of matches to have escaped', 'the wear of time for immemorial years was a most strange as for', 'me it was a most fortunate thing yet oddly enough i found a far', 'unlikelier substance and that was camphor i found it in a sealed', 'jar that by chance i suppose had been really hermetically sealed', 'i fancied at first that it was paraffin wax and smashed the glass', 'accordingly but the odour of camphor was unmistakable in the', 'universal decay this volatile substance had chanced to survive', 'perhaps through many thousands of centuries it reminded me of a', 'sepia painting i had once seen done from the ink of a fossil', 'belemnite that must have perished and become fossilized millions', 'of years ago i was about to throw it away but i remembered that', 'it was inflammable and burned with a good bright flame was in', 'fact an excellent candle and i put it in my pocket i found no', 'explosives however nor any means of breaking down the bronze', 'doors as yet my iron crowbar was the most helpful thing i had', 'chanced upon nevertheless i left that gallery greatly elated', '', 'i cannot tell you all the story of that long afternoon it would', 'require a great effort of memory to recall my explorations in at all', 'the proper order i remember a long gallery of rusting stands of', 'arms and how i hesitated between my crowbar and a hatchet or a', 'sword i could not carry both however and my bar of iron promised', 'best against the bronze gates there were numbers of guns pistols', 'and rifles the most were masses of rust but many were of some', 'new metal and still fairly sound but any cartridges or powder', 'there may once have been had rotted into dust one corner i saw was', 'charred and shattered perhaps i thought by an explosion among the', 'specimens in another place was a vast array of idols polynesian', 'mexican grecian phoenician every country on earth i should think', 'and here yielding to an irresistible impulse i wrote my name upon', 'the nose of a steatite monster from south america that particularly', 'took my fancy', '', 'as the evening drew on my interest waned i went through gallery', 'after gallery dusty silent often ruinous the exhibits sometimes', 'mere heaps of rust and lignite sometimes fresher in one place i', 'suddenly found myself near the model of a tin mine and then by the', 'merest accident i discovered in an air tight case two dynamite', 'cartridges i shouted eureka and smashed the case with joy then', 'came a doubt i hesitated then selecting a little side gallery', 'i made my essay i never felt such a disappointment as i did in', 'waiting five ten fifteen minutes for an explosion that never came', 'of course the things were dummies as i might have guessed from', 'their presence i really believe that had they not been so i should', 'have rushed off incontinently and blown sphinx bronze doors and', 'as it proved my chances of finding the time machine all together', 'into non existence', '', 'it was after that i think that we came to a little open court', 'within the palace it was turfed and had three fruit trees so we', 'rested and refreshed ourselves towards sunset i began to consider', 'our position night was creeping upon us and my inaccessible', 'hiding place had still to be found but that troubled me very little', 'now i had in my possession a thing that was perhaps the best of', 'all defences against the morlocks i had matches i had the camphor', 'in my pocket too if a blaze were needed it seemed to me that', 'the best thing we could do would be to pass the night in the open', 'protected by a fire in the morning there was the getting of the', 'time machine towards that as yet i had only my iron mace but', 'now with my growing knowledge i felt very differently towards', 'those bronze doors up to this i had refrained from forcing them', 'largely because of the mystery on the other side they had never', 'impressed me as being very strong and i hoped to find my bar of', 'iron not altogether inadequate for the work', '', '', '', '', 'ix', '', '', 'we emerged from the palace while the sun was still in part above', 'the horizon i was determined to reach the white sphinx early the', 'next morning and ere the dusk i purposed pushing through the woods', 'that had stopped me on the previous journey my plan was to go as', 'far as possible that night and then building a fire to sleep', 'in the protection of its glare accordingly as we went along i', 'gathered any sticks or dried grass i saw and presently had my arms', 'full of such litter thus loaded our progress was slower than i had', 'anticipated and besides weena was tired and i began to suffer from', 'sleepiness too so that it was full night before we reached the', 'wood upon the shrubby hill of its edge weena would have stopped', 'fearing the darkness before us but a singular sense of impending', 'calamity that should indeed have served me as a warning drove me', 'onward i had been without sleep for a night and two days and i was', 'feverish and irritable i felt sleep coming upon me and the', 'morlocks with it', '', 'while we hesitated among the black bushes behind us and dim', 'against their blackness i saw three crouching figures there was', 'scrub and long grass all about us and i did not feel safe from', 'their insidious approach the forest i calculated was rather', 'less than a mile across if we could get through it to the bare', 'hill side there as it seemed to me was an altogether safer', 'resting place i thought that with my matches and my camphor i could', 'contrive to keep my path illuminated through the woods yet it was', 'evident that if i was to flourish matches with my hands i should', 'have to abandon my firewood so rather reluctantly i put it down', 'and then it came into my head that i would amaze our friends behind', 'by lighting it i was to discover the atrocious folly of this', 'proceeding but it came to my mind as an ingenious move for covering', 'our retreat', '', 'i don t know if you have ever thought what a rare thing flame must', 'be in the absence of man and in a temperate climate the sun s', 'heat is rarely strong enough to burn even when it is focused by', 'dewdrops as is sometimes the case in more tropical districts', 'lightning may blast and blacken but it rarely gives rise to', 'widespread fire decaying vegetation may occasionally smoulder with', 'the heat of its fermentation but this rarely results in flame in', 'this decadence too the art of fire making had been forgotten on', 'the earth the red tongues that went licking up my heap of wood were', 'an altogether new and strange thing to weena', '', 'she wanted to run to it and play with it i believe she would have', 'cast herself into it had i not restrained her but i caught her up', 'and in spite of her struggles plunged boldly before me into the', 'wood for a little way the glare of my fire lit the path looking', 'back presently i could see through the crowded stems that from my', 'heap of sticks the blaze had spread to some bushes adjacent and a', 'curved line of fire was creeping up the grass of the hill i laughed', 'at that and turned again to the dark trees before me it was very', 'black and weena clung to me convulsively but there was still as', 'my eyes grew accustomed to the darkness sufficient light for me to', 'avoid the stems overhead it was simply black except where a gap of', 'remote blue sky shone down upon us here and there i struck none of', 'my matches because i had no hand free upon my left arm i carried my', 'little one in my right hand i had my iron bar', '', 'for some way i heard nothing but the crackling twigs under my feet', 'the faint rustle of the breeze above and my own breathing and the', 'throb of the blood vessels in my ears then i seemed to know of a', 'pattering about me i pushed on grimly the pattering grew more', 'distinct and then i caught the same queer sound and voices i had', 'heard in the under world there were evidently several of the', 'morlocks and they were closing in upon me indeed in another', 'minute i felt a tug at my coat then something at my arm and weena', 'shivered violently and became quite still', '', 'it was time for a match but to get one i must put her down i did', 'so and as i fumbled with my pocket a struggle began in the', 'darkness about my knees perfectly silent on her part and with the', 'same peculiar cooing sounds from the morlocks soft little hands', 'too were creeping over my coat and back touching even my neck', 'then the match scratched and fizzed i held it flaring and saw the', 'white backs of the morlocks in flight amid the trees i hastily took', 'a lump of camphor from my pocket and prepared to light it as soon', 'as the match should wane then i looked at weena she was lying', 'clutching my feet and quite motionless with her face to the ground', 'with a sudden fright i stooped to her she seemed scarcely to', 'breathe i lit the block of camphor and flung it to the ground', 'and as it split and flared up and drove back the morlocks and the', 'shadows i knelt down and lifted her the wood behind seemed full of', 'the stir and murmur of a great company', '', 'she seemed to have fainted i put her carefully upon my shoulder', 'and rose to push on and then there came a horrible realization in', 'manoeuvring with my matches and weena i had turned myself about', 'several times and now i had not the faintest idea in what direction', 'lay my path for all i knew i might be facing back towards the', 'palace of green porcelain i found myself in a cold sweat i had to', 'think rapidly what to do i determined to build a fire and encamp', 'where we were i put weena still motionless down upon a turfy', 'bole and very hastily as my first lump of camphor waned i began', 'collecting sticks and leaves here and there out of the darkness', 'round me the morlocks eyes shone like carbuncles', '', 'the camphor flickered and went out i lit a match and as i did so', 'two white forms that had been approaching weena dashed hastily away', 'one was so blinded by the light that he came straight for me and i', 'felt his bones grind under the blow of my fist he gave a whoop of', 'dismay staggered a little way and fell down i lit another piece', 'of camphor and went on gathering my bonfire presently i noticed', 'how dry was some of the foliage above me for since my arrival', 'on the time machine a matter of a week no rain had fallen so', 'instead of casting about among the trees for fallen twigs i began', 'leaping up and dragging down branches very soon i had a choking', 'smoky fire of green wood and dry sticks and could economize my', 'camphor then i turned to where weena lay beside my iron mace i', 'tried what i could to revive her but she lay like one dead i could', 'not even satisfy myself whether or not she breathed', '', 'now the smoke of the fire beat over towards me and it must have', 'made me heavy of a sudden moreover the vapour of camphor was in', 'the air my fire would not need replenishing for an hour or so i', 'felt very weary after my exertion and sat down the wood too was', 'full of a slumbrous murmur that i did not understand i seemed just', 'to nod and open my eyes but all was dark and the morlocks had', 'their hands upon me flinging off their clinging fingers i hastily', 'felt in my pocket for the match box and it had gone then they', 'gripped and closed with me again in a moment i knew what had', 'happened i had slept and my fire had gone out and the bitterness', 'of death came over my soul the forest seemed full of the smell of', 'burning wood i was caught by the neck by the hair by the arms', 'and pulled down it was indescribably horrible in the darkness to', 'feel all these soft creatures heaped upon me i felt as if i was in', 'a monstrous spider s web i was overpowered and went down i felt', 'little teeth nipping at my neck i rolled over and as i did so my', 'hand came against my iron lever it gave me strength i struggled', 'up shaking the human rats from me and holding the bar short', 'i thrust where i judged their faces might be i could feel the', 'succulent giving of flesh and bone under my blows and for a moment', 'i was free', '', 'the strange exultation that so often seems to accompany hard', 'fighting came upon me i knew that both i and weena were lost but i', 'determined to make the morlocks pay for their meat i stood with my', 'back to a tree swinging the iron bar before me the whole wood was', 'full of the stir and cries of them a minute passed their voices', 'seemed to rise to a higher pitch of excitement and their movements', 'grew faster yet none came within reach i stood glaring at the', 'blackness then suddenly came hope what if the morlocks were', 'afraid and close on the heels of that came a strange thing the', 'darkness seemed to grow luminous very dimly i began to see the', 'morlocks about me three battered at my feet and then i recognized', 'with incredulous surprise that the others were running in an', 'incessant stream as it seemed from behind me and away through the', 'wood in front and their backs seemed no longer white but reddish', 'as i stood agape i saw a little red spark go drifting across a gap', 'of starlight between the branches and vanish and at that i', 'understood the smell of burning wood the slumbrous murmur that was', 'growing now into a gusty roar the red glow and the morlocks', 'flight', '', 'stepping out from behind my tree and looking back i saw through', 'the black pillars of the nearer trees the flames of the burning', 'forest it was my first fire coming after me with that i looked for', 'weena but she was gone the hissing and crackling behind me the', 'explosive thud as each fresh tree burst into flame left little', 'time for reflection my iron bar still gripped i followed in the', 'morlocks path it was a close race once the flames crept forward', 'so swiftly on my right as i ran that i was outflanked and had to', 'strike off to the left but at last i emerged upon a small open', 'space and as i did so a morlock came blundering towards me and', 'past me and went on straight into the fire', '', 'and now i was to see the most weird and horrible thing i think of', 'all that i beheld in that future age this whole space was as bright', 'as day with the reflection of the fire in the centre was a hillock', 'or tumulus surmounted by a scorched hawthorn beyond this was', 'another arm of the burning forest with yellow tongues already', 'writhing from it completely encircling the space with a fence of', 'fire upon the hill side were some thirty or forty morlocks dazzled', 'by the light and heat and blundering hither and thither against', 'each other in their bewilderment at first i did not realize their', 'blindness and struck furiously at them with my bar in a frenzy of', 'fear as they approached me killing one and crippling several more', 'but when i had watched the gestures of one of them groping under the', 'hawthorn against the red sky and heard their moans i was assured', 'of their absolute helplessness and misery in the glare and i struck', 'no more of them', '', 'yet every now and then one would come straight towards me setting', 'loose a quivering horror that made me quick to elude him at one', 'time the flames died down somewhat and i feared the foul creatures', 'would presently be able to see me i was thinking of beginning the', 'fight by killing some of them before this should happen but the', 'fire burst out again brightly and i stayed my hand i walked about', 'the hill among them and avoided them looking for some trace of', 'weena but weena was gone', '', 'at last i sat down on the summit of the hillock and watched this', 'strange incredible company of blind things groping to and fro and', 'making uncanny noises to each other as the glare of the fire beat', 'on them the coiling uprush of smoke streamed across the sky and', 'through the rare tatters of that red canopy remote as though they', 'belonged to another universe shone the little stars two or three', 'morlocks came blundering into me and i drove them off with blows', 'of my fists trembling as i did so', '', 'for the most part of that night i was persuaded it was a nightmare', 'i bit myself and screamed in a passionate desire to awake i beat', 'the ground with my hands and got up and sat down again and', 'wandered here and there and again sat down then i would fall to', 'rubbing my eyes and calling upon god to let me awake thrice i saw', 'morlocks put their heads down in a kind of agony and rush into the', 'flames but at last above the subsiding red of the fire above the', 'streaming masses of black smoke and the whitening and blackening', 'tree stumps and the diminishing numbers of these dim creatures', 'came the white light of the day', '', 'i searched again for traces of weena but there were none it was', 'plain that they had left her poor little body in the forest i', 'cannot describe how it relieved me to think that it had escaped the', 'awful fate to which it seemed destined as i thought of that i was', 'almost moved to begin a massacre of the helpless abominations about', 'me but i contained myself the hillock as i have said was a kind', 'of island in the forest from its summit i could now make out', 'through a haze of smoke the palace of green porcelain and from that', 'i could get my bearings for the white sphinx and so leaving the', 'remnant of these damned souls still going hither and thither and', 'moaning as the day grew clearer i tied some grass about my feet', 'and limped on across smoking ashes and among black stems that still', 'pulsated internally with fire towards the hiding place of the time', 'machine i walked slowly for i was almost exhausted as well as', 'lame and i felt the intensest wretchedness for the horrible death', 'of little weena it seemed an overwhelming calamity now in this', 'old familiar room it is more like the sorrow of a dream than an', 'actual loss but that morning it left me absolutely lonely', 'again terribly alone i began to think of this house of mine of', 'this fireside of some of you and with such thoughts came a longing', 'that was pain', '', 'but as i walked over the smoking ashes under the bright morning', 'sky i made a discovery in my trouser pocket were still some loose', 'matches the box must have leaked before it was lost', '', '', '', '', 'x', '', '', 'about eight or nine in the morning i came to the same seat of', 'yellow metal from which i had viewed the world upon the evening of', 'my arrival i thought of my hasty conclusions upon that evening and', 'could not refrain from laughing bitterly at my confidence here', 'was the same beautiful scene the same abundant foliage the same', 'splendid palaces and magnificent ruins the same silver river', 'running between its fertile banks the gay robes of the beautiful', 'people moved hither and thither among the trees some were bathing', 'in exactly the place where i had saved weena and that suddenly gave', 'me a keen stab of pain and like blots upon the landscape rose the', 'cupolas above the ways to the under world i understood now what all', 'the beauty of the over world people covered very pleasant was their', 'day as pleasant as the day of the cattle in the field like the', 'cattle they knew of no enemies and provided against no needs and', 'their end was the same', '', 'i grieved to think how brief the dream of the human intellect had', 'been it had committed suicide it had set itself steadfastly', 'towards comfort and ease a balanced society with security and', 'permanency as its watchword it had attained its hopes to come', 'to this at last once life and property must have reached almost', 'absolute safety the rich had been assured of his wealth and', 'comfort the toiler assured of his life and work no doubt in that', 'perfect world there had been no unemployed problem no social', 'question left unsolved and a great quiet had followed', '', 'it is a law of nature we overlook that intellectual versatility', 'is the compensation for change danger and trouble an animal', 'perfectly in harmony with its environment is a perfect mechanism', 'nature never appeals to intelligence until habit and instinct are', 'useless there is no intelligence where there is no change and no', 'need of change only those animals partake of intelligence that have', 'to meet a huge variety of needs and dangers', '', 'so as i see it the upper world man had drifted towards his', 'feeble prettiness and the under world to mere mechanical industry', 'but that perfect state had lacked one thing even for mechanical', 'perfection absolute permanency apparently as time went on the', 'feeding of the under world however it was effected had become', 'disjointed mother necessity who had been staved off for a', 'few thousand years came back again and she began below the', 'under world being in contact with machinery which however perfect', 'still needs some little thought outside habit had probably retained', 'perforce rather more initiative if less of every other human', 'character than the upper and when other meat failed them they', 'turned to what old habit had hitherto forbidden so i say i saw it', 'in my last view of the world of eight hundred and two thousand seven', 'hundred and one it may be as wrong an explanation as mortal wit', 'could invent it is how the thing shaped itself to me and as that i', 'give it to you', '', 'after the fatigues excitements and terrors of the past days and', 'in spite of my grief this seat and the tranquil view and the warm', 'sunlight were very pleasant i was very tired and sleepy and soon', 'my theorizing passed into dozing catching myself at that i took my', 'own hint and spreading myself out upon the turf i had a long and', 'refreshing sleep', '', 'i awoke a little before sunsetting i now felt safe against being', 'caught napping by the morlocks and stretching myself i came on', 'down the hill towards the white sphinx i had my crowbar in one', 'hand and the other hand played with the matches in my pocket', '', 'and now came a most unexpected thing as i approached the pedestal', 'of the sphinx i found the bronze valves were open they had slid', 'down into grooves', '', 'at that i stopped short before them hesitating to enter', '', 'within was a small apartment and on a raised place in the corner', 'of this was the time machine i had the small levers in my pocket', 'so here after all my elaborate preparations for the siege of the', 'white sphinx was a meek surrender i threw my iron bar away almost', 'sorry not to use it', '', 'a sudden thought came into my head as i stooped towards the portal', 'for once at least i grasped the mental operations of the morlocks', 'suppressing a strong inclination to laugh i stepped through the', 'bronze frame and up to the time machine i was surprised to find it', 'had been carefully oiled and cleaned i have suspected since that', 'the morlocks had even partially taken it to pieces while trying in', 'their dim way to grasp its purpose', '', 'now as i stood and examined it finding a pleasure in the mere', 'touch of the contrivance the thing i had expected happened the', 'bronze panels suddenly slid up and struck the frame with a clang', 'i was in the dark trapped so the morlocks thought at that i', 'chuckled gleefully', '', 'i could already hear their murmuring laughter as they came towards', 'me very calmly i tried to strike the match i had only to fix on', 'the levers and depart then like a ghost but i had overlooked one', 'little thing the matches were of that abominable kind that light', 'only on the box', '', 'you may imagine how all my calm vanished the little brutes were', 'close upon me one touched me i made a sweeping blow in the dark at', 'them with the levers and began to scramble into the saddle of the', 'machine then came one hand upon me and then another then i had', 'simply to fight against their persistent fingers for my levers and', 'at the same time feel for the studs over which these fitted one', 'indeed they almost got away from me as it slipped from my hand', 'i had to butt in the dark with my head i could hear the morlock s', 'skull ring to recover it it was a nearer thing than the fight in', 'the forest i think this last scramble', '', 'but at last the lever was fitted and pulled over the clinging', 'hands slipped from me the darkness presently fell from my eyes', 'i found myself in the same grey light and tumult i have already', 'described', '', '', '', '', 'xi', '', '', 'i have already told you of the sickness and confusion that comes', 'with time travelling and this time i was not seated properly in the', 'saddle but sideways and in an unstable fashion for an indefinite', 'time i clung to the machine as it swayed and vibrated quite', 'unheeding how i went and when i brought myself to look at the dials', 'again i was amazed to find where i had arrived one dial records', 'days and another thousands of days another millions of days and', 'another thousands of millions now instead of reversing the levers', 'i had pulled them over so as to go forward with them and when i', 'came to look at these indicators i found that the thousands hand was', 'sweeping round as fast as the seconds hand of a watch into', 'futurity', '', 'as i drove on a peculiar change crept over the appearance of', 'things the palpitating greyness grew darker then though i was', 'still travelling with prodigious velocity the blinking succession', 'of day and night which was usually indicative of a slower pace', 'returned and grew more and more marked this puzzled me very much', 'at first the alternations of night and day grew slower and slower', 'and so did the passage of the sun across the sky until they seemed', 'to stretch through centuries at last a steady twilight brooded over', 'the earth a twilight only broken now and then when a comet glared', 'across the darkling sky the band of light that had indicated the', 'sun had long since disappeared for the sun had ceased to set it', 'simply rose and fell in the west and grew ever broader and more', 'red all trace of the moon had vanished the circling of the stars', 'growing slower and slower had given place to creeping points of', 'light at last some time before i stopped the sun red and very', 'large halted motionless upon the horizon a vast dome glowing with', 'a dull heat and now and then suffering a momentary extinction at', 'one time it had for a little while glowed more brilliantly again', 'but it speedily reverted to its sullen red heat i perceived by this', 'slowing down of its rising and setting that the work of the tidal', 'drag was done the earth had come to rest with one face to the sun', 'even as in our own time the moon faces the earth very cautiously', 'for i remembered my former headlong fall i began to reverse', 'my motion slower and slower went the circling hands until the', 'thousands one seemed motionless and the daily one was no longer a', 'mere mist upon its scale still slower until the dim outlines of a', 'desolate beach grew visible', '', 'i stopped very gently and sat upon the time machine looking round', 'the sky was no longer blue north eastward it was inky black', 'and out of the blackness shone brightly and steadily the pale', 'white stars overhead it was a deep indian red and starless and', 'south eastward it grew brighter to a glowing scarlet where cut by', 'the horizon lay the huge hull of the sun red and motionless the', 'rocks about me were of a harsh reddish colour and all the trace of', 'life that i could see at first was the intensely green vegetation', 'that covered every projecting point on their south eastern face it', 'was the same rich green that one sees on forest moss or on the', 'lichen in caves plants which like these grow in a perpetual', 'twilight', '', 'the machine was standing on a sloping beach the sea stretched away', 'to the south west to rise into a sharp bright horizon against the', 'wan sky there were no breakers and no waves for not a breath of', 'wind was stirring only a slight oily swell rose and fell like a', 'gentle breathing and showed that the eternal sea was still moving', 'and living and along the margin where the water sometimes broke was', 'a thick incrustation of salt pink under the lurid sky there was a', 'sense of oppression in my head and i noticed that i was breathing', 'very fast the sensation reminded me of my only experience of', 'mountaineering and from that i judged the air to be more rarefied', 'than it is now', '', 'far away up the desolate slope i heard a harsh scream and saw a', 'thing like a huge white butterfly go slanting and fluttering up into', 'the sky and circling disappear over some low hillocks beyond the', 'sound of its voice was so dismal that i shivered and seated myself', 'more firmly upon the machine looking round me again i saw that', 'quite near what i had taken to be a reddish mass of rock was moving', 'slowly towards me then i saw the thing was really a monstrous', 'crab like creature can you imagine a crab as large as yonder table', 'with its many legs moving slowly and uncertainly its big claws', 'swaying its long antennae like carters whips waving and feeling', 'and its stalked eyes gleaming at you on either side of its metallic', 'front its back was corrugated and ornamented with ungainly bosses', 'and a greenish incrustation blotched it here and there i could see', 'the many palps of its complicated mouth flickering and feeling as it', 'moved', '', 'as i stared at this sinister apparition crawling towards me i felt', 'a tickling on my cheek as though a fly had lighted there i tried to', 'brush it away with my hand but in a moment it returned and almost', 'immediately came another by my ear i struck at this and caught', 'something threadlike it was drawn swiftly out of my hand with a', 'frightful qualm i turned and i saw that i had grasped the antenna', 'of another monster crab that stood just behind me its evil eyes', 'were wriggling on their stalks its mouth was all alive with', 'appetite and its vast ungainly claws smeared with an algal slime', 'were descending upon me in a moment my hand was on the lever and', 'i had placed a month between myself and these monsters but i was', 'still on the same beach and i saw them distinctly now as soon as i', 'stopped dozens of them seemed to be crawling here and there in the', 'sombre light among the foliated sheets of intense green', '', 'i cannot convey the sense of abominable desolation that hung over', 'the world the red eastern sky the northward blackness the salt', 'dead sea the stony beach crawling with these foul slow stirring', 'monsters the uniform poisonous looking green of the lichenous', 'plants the thin air that hurts one s lungs all contributed to an', 'appalling effect i moved on a hundred years and there was the same', 'red sun a little larger a little duller the same dying sea the', 'same chill air and the same crowd of earthy crustacea creeping in', 'and out among the green weed and the red rocks and in the westward', 'sky i saw a curved pale line like a vast new moon', '', 'so i travelled stopping ever and again in great strides of a', 'thousand years or more drawn on by the mystery of the earth s fate', 'watching with a strange fascination the sun grow larger and duller', 'in the westward sky and the life of the old earth ebb away at', 'last more than thirty million years hence the huge red hot dome of', 'the sun had come to obscure nearly a tenth part of the darkling', 'heavens then i stopped once more for the crawling multitude of', 'crabs had disappeared and the red beach save for its livid green', 'liverworts and lichens seemed lifeless and now it was flecked with', 'white a bitter cold assailed me rare white flakes ever and again', 'came eddying down to the north eastward the glare of snow lay', 'under the starlight of the sable sky and i could see an undulating', 'crest of hillocks pinkish white there were fringes of ice along the', 'sea margin with drifting masses further out but the main expanse', 'of that salt ocean all bloody under the eternal sunset was still', 'unfrozen', '', 'i looked about me to see if any traces of animal life remained a', 'certain indefinable apprehension still kept me in the saddle of the', 'machine but i saw nothing moving in earth or sky or sea the green', 'slime on the rocks alone testified that life was not extinct a', 'shallow sandbank had appeared in the sea and the water had receded', 'from the beach i fancied i saw some black object flopping about', 'upon this bank but it became motionless as i looked at it and i', 'judged that my eye had been deceived and that the black object was', 'merely a rock the stars in the sky were intensely bright and seemed', 'to me to twinkle very little', '', 'suddenly i noticed that the circular westward outline of the sun', 'had changed that a concavity a bay had appeared in the curve i', 'saw this grow larger for a minute perhaps i stared aghast at this', 'blackness that was creeping over the day and then i realized that', 'an eclipse was beginning either the moon or the planet mercury was', 'passing across the sun s disk naturally at first i took it to be', 'the moon but there is much to incline me to believe that what i', 'really saw was the transit of an inner planet passing very near to', 'the earth', '', 'the darkness grew apace a cold wind began to blow in freshening', 'gusts from the east and the showering white flakes in the air', 'increased in number from the edge of the sea came a ripple and', 'whisper beyond these lifeless sounds the world was silent silent', 'it would be hard to convey the stillness of it all the sounds of', 'man the bleating of sheep the cries of birds the hum of insects', 'the stir that makes the background of our lives all that was over', 'as the darkness thickened the eddying flakes grew more abundant', 'dancing before my eyes and the cold of the air more intense at', 'last one by one swiftly one after the other the white peaks of', 'the distant hills vanished into blackness the breeze rose to a', 'moaning wind i saw the black central shadow of the eclipse sweeping', 'towards me in another moment the pale stars alone were visible all', 'else was rayless obscurity the sky was absolutely black', '', 'a horror of this great darkness came on me the cold that smote', 'to my marrow and the pain i felt in breathing overcame me i', 'shivered and a deadly nausea seized me then like a red hot bow', 'in the sky appeared the edge of the sun i got off the machine to', 'recover myself i felt giddy and incapable of facing the return', 'journey as i stood sick and confused i saw again the moving thing', 'upon the shoal there was no mistake now that it was a moving', 'thing against the red water of the sea it was a round thing the', 'size of a football perhaps or it may be bigger and tentacles', 'trailed down from it it seemed black against the weltering', 'blood red water and it was hopping fitfully about then i felt i', 'was fainting but a terrible dread of lying helpless in that remote', 'and awful twilight sustained me while i clambered upon the saddle', '', '', '', '', 'xii', '', '', 'so i came back for a long time i must have been insensible upon', 'the machine the blinking succession of the days and nights was', 'resumed the sun got golden again the sky blue i breathed with', 'greater freedom the fluctuating contours of the land ebbed and', 'flowed the hands spun backward upon the dials at last i saw again', 'the dim shadows of houses the evidences of decadent humanity', 'these too changed and passed and others came presently when the', 'million dial was at zero i slackened speed i began to recognize', 'our own pretty and familiar architecture the thousands hand ran back', 'to the starting point the night and day flapped slower and slower', 'then the old walls of the laboratory came round me very gently', 'now i slowed the mechanism down', '', 'i saw one little thing that seemed odd to me i think i have told', 'you that when i set out before my velocity became very high mrs', 'watchett had walked across the room travelling as it seemed to me', 'like a rocket as i returned i passed again across that minute when', 'she traversed the laboratory but now her every motion appeared to', 'be the exact inversion of her previous ones the door at the lower', 'end opened and she glided quietly up the laboratory back foremost', 'and disappeared behind the door by which she had previously entered', 'just before that i seemed to see hillyer for a moment but he passed', 'like a flash', '', 'then i stopped the machine and saw about me again the old familiar', 'laboratory my tools my appliances just as i had left them i got', 'off the thing very shakily and sat down upon my bench for several', 'minutes i trembled violently then i became calmer around me was', 'my old workshop again exactly as it had been i might have slept', 'there and the whole thing have been a dream', '', 'and yet not exactly the thing had started from the south east', 'corner of the laboratory it had come to rest again in the', 'north west against the wall where you saw it that gives you the', 'exact distance from my little lawn to the pedestal of the white', 'sphinx into which the morlocks had carried my machine', '', 'for a time my brain went stagnant presently i got up and came', 'through the passage here limping because my heel was still', 'painful and feeling sorely begrimed i saw the pall mall gazette', 'on the table by the door i found the date was indeed to day and', 'looking at the timepiece saw the hour was almost eight o clock i', 'heard your voices and the clatter of plates i hesitated i felt so', 'sick and weak then i sniffed good wholesome meat and opened the', 'door on you you know the rest i washed and dined and now i am', 'telling you the story', '', 'i know he said after a pause that all this will be absolutely', 'incredible to you to me the one incredible thing is that i am here', 'to night in this old familiar room looking into your friendly faces', 'and telling you these strange adventures', '', 'he looked at the medical man no i cannot expect you to believe', 'it take it as a lie or a prophecy say i dreamed it in the', 'workshop consider i have been speculating upon the destinies of our', 'race until i have hatched this fiction treat my assertion of its', 'truth as a mere stroke of art to enhance its interest and taking', 'it as a story what do you think of it', '', 'he took up his pipe and began in his old accustomed manner to tap', 'with it nervously upon the bars of the grate there was a momentary', 'stillness then chairs began to creak and shoes to scrape upon the', 'carpet i took my eyes off the time traveller s face and looked', 'round at his audience they were in the dark and little spots of', 'colour swam before them the medical man seemed absorbed in the', 'contemplation of our host the editor was looking hard at the end', 'of his cigar the sixth the journalist fumbled for his watch the', 'others as far as i remember were motionless', '', 'the editor stood up with a sigh what a pity it is you re not', 'a writer of stories he said putting his hand on the time', 'traveller s shoulder', '', 'you don t believe it', '', 'well', '', 'i thought not', '', 'the time traveller turned to us where are the matches he said', 'he lit one and spoke over his pipe puffing to tell you the truth', 'i hardly believe it myself and yet', '', 'his eye fell with a mute inquiry upon the withered white flowers', 'upon the little table then he turned over the hand holding his', 'pipe and i saw he was looking at some half healed scars on his', 'knuckles', '', 'the medical man rose came to the lamp and examined the flowers', 'the gynaeceum s odd he said the psychologist leant forward to', 'see holding out his hand for a specimen', '', 'i m hanged if it isn t a quarter to one said the journalist', 'how shall we get home', '', 'plenty of cabs at the station said the psychologist', '', 'it s a curious thing said the medical man but i certainly don t', 'know the natural order of these flowers may i have them', '', 'the time traveller hesitated then suddenly certainly not', '', 'where did you really get them said the medical man', '', 'the time traveller put his hand to his head he spoke like one who', 'was trying to keep hold of an idea that eluded him they were put', 'into my pocket by weena when i travelled into time he stared', 'round the room i m damned if it isn t all going this room and you', 'and the atmosphere of every day is too much for my memory did i', 'ever make a time machine or a model of a time machine or is it all', 'only a dream they say life is a dream a precious poor dream at', 'times but i can t stand another that won t fit it s madness and', 'where did the dream come from i must look at that machine if', 'there is one', '', 'he caught up the lamp swiftly and carried it flaring red through', 'the door into the corridor we followed him there in the flickering', 'light of the lamp was the machine sure enough squat ugly and', 'askew a thing of brass ebony ivory and translucent glimmering', 'quartz solid to the touch for i put out my hand and felt the rail', 'of it and with brown spots and smears upon the ivory and bits of', 'grass and moss upon the lower parts and one rail bent awry', '', 'the time traveller put the lamp down on the bench and ran his hand', 'along the damaged rail it s all right now he said the story i', 'told you was true i m sorry to have brought you out here in the', 'cold he took up the lamp and in an absolute silence we', 'returned to the smoking room', '', 'he came into the hall with us and helped the editor on with his', 'coat the medical man looked into his face and with a certain', 'hesitation told him he was suffering from overwork at which he', 'laughed hugely i remember him standing in the open doorway bawling', 'good night', '', 'i shared a cab with the editor he thought the tale a gaudy lie', 'for my own part i was unable to come to a conclusion the story was', 'so fantastic and incredible the telling so credible and sober i', 'lay awake most of the night thinking about it i determined to go', 'next day and see the time traveller again i was told he was in the', 'laboratory and being on easy terms in the house i went up to him', 'the laboratory however was empty i stared for a minute at the', 'time machine and put out my hand and touched the lever at that the', 'squat substantial looking mass swayed like a bough shaken by the', 'wind its instability startled me extremely and i had a queer', 'reminiscence of the childish days when i used to be forbidden to', 'meddle i came back through the corridor the time traveller met me', 'in the smoking room he was coming from the house he had a small', 'camera under one arm and a knapsack under the other he laughed when', 'he saw me and gave me an elbow to shake i m frightfully busy', 'said he with that thing in there', '', 'but is it not some hoax i said do you really travel through', 'time', '', 'really and truly i do and he looked frankly into my eyes he', 'hesitated his eye wandered about the room i only want half an', 'hour he said i know why you came and it s awfully good of you', 'there s some magazines here if you ll stop to lunch i ll prove you', 'this time travelling up to the hilt specimen and all if you ll', 'forgive my leaving you now', '', 'i consented hardly comprehending then the full import of his words', 'and he nodded and went on down the corridor i heard the door of', 'the laboratory slam seated myself in a chair and took up a daily', 'paper what was he going to do before lunch time then suddenly', 'i was reminded by an advertisement that i had promised to meet', 'richardson the publisher at two i looked at my watch and saw', 'that i could barely save that engagement i got up and went down the', 'passage to tell the time traveller', '', 'as i took hold of the handle of the door i heard an exclamation', 'oddly truncated at the end and a click and a thud a gust of air', 'whirled round me as i opened the door and from within came the', 'sound of broken glass falling on the floor the time traveller was', 'not there i seemed to see a ghostly indistinct figure sitting in', 'a whirling mass of black and brass for a moment a figure so', 'transparent that the bench behind with its sheets of drawings was', 'absolutely distinct but this phantasm vanished as i rubbed my eyes', 'the time machine had gone save for a subsiding stir of dust the', 'further end of the laboratory was empty a pane of the skylight had', 'apparently just been blown in', '', 'i felt an unreasonable amazement i knew that something strange had', 'happened and for the moment could not distinguish what the strange', 'thing might be as i stood staring the door into the garden opened', 'and the man servant appeared', '', 'we looked at each other then ideas began to come has mr', 'gone out that way said i', '', 'no sir no one has come out this way i was expecting to find him', 'here', '', 'at that i understood at the risk of disappointing richardson i', 'stayed on waiting for the time traveller waiting for the second', 'perhaps still stranger story and the specimens and photographs he', 'would bring with him but i am beginning now to fear that i must', 'wait a lifetime the time traveller vanished three years ago and', 'as everybody knows now he has never returned', '', '', '', '', 'epilogue', '', '', 'one cannot choose but wonder will he ever return it may be that he', 'swept back into the past and fell among the blood drinking hairy', 'savages of the age of unpolished stone into the abysses of the', 'cretaceous sea or among the grotesque saurians the huge reptilian', 'brutes of the jurassic times he may even now if i may use the', 'phrase be wandering on some plesiosaurus haunted oolitic coral', 'reef or beside the lonely saline lakes of the triassic age or did', 'he go forward into one of the nearer ages in which men are still', 'men but with the riddles of our own time answered and its wearisome', 'problems solved into the manhood of the race for i for my own', 'part cannot think that these latter days of weak experiment', 'fragmentary theory and mutual discord are indeed man s culminating', 'time i say for my own part he i know for the question had been', 'discussed among us long before the time machine was made thought', 'but cheerlessly of the advancement of mankind and saw in the', 'growing pile of civilization only a foolish heaping that must', 'inevitably fall back upon and destroy its makers in the end if that', 'is so it remains for us to live as though it were not so but to me', 'the future is still black and blank is a vast ignorance lit at a', 'few casual places by the memory of his story and i have by me for', 'my comfort two strange white flowers shrivelled now and brown and', 'flat and brittle to witness that even when mind and strength had', 'gone gratitude and a mutual tenderness still lived on in the heart', 'of man']\n",
      "# 文本总行数: 3221\n",
      "0: the time machine by h g wells\n",
      "1: \n",
      "2: \n",
      "3: \n",
      "4: \n",
      "5: i\n",
      "6: \n",
      "7: \n",
      "8: the time traveller for so it will be convenient to speak of him\n",
      "9: was expounding a recondite matter to us his grey eyes shone and\n",
      "10: twinkled and his usually pale face was flushed and animated the\n",
      "11: fire burned brightly and the soft radiance of the incandescent\n",
      "12: lights in the lilies of silver caught the bubbles that flashed and\n",
      "13: passed in our glasses our chairs being his patents embraced and\n",
      "14: caressed us rather than submitted to be sat upon and there was that\n",
      "15: luxurious after dinner atmosphere when thought roams gracefully\n",
      "16: free of the trammels of precision and he put it to us in this\n",
      "17: way marking the points with a lean forefinger as we sat and lazily\n",
      "18: admired his earnestness over this new paradox as we thought it\n",
      "19: and his fecundity\n",
      "20: \n",
      "21: you must follow me carefully i shall have to controvert one or two\n",
      "22: ideas that are almost universally accepted the geometry for\n",
      "23: instance they taught you at school is founded on a misconception\n",
      "24: \n",
      "25: is not that rather a large thing to expect us to begin upon\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "# from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "# 下载到../data/timemachine.txt\n",
    "# d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    # with open(d2l.download('time_machine'), 'r') as f:\n",
    "    with open('data/timemachine.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "\n",
    "# ['The machine is haha', '', '', ...]\n",
    "lines = read_time_machine()\n",
    "\n",
    "\n",
    "print('lines:', lines)\n",
    "print(f'# 文本总行数: {len(lines)}')\n",
    "for i, line in enumerate(lines):\n",
    "    print(f'{i}: {line}')\n",
    "    if i == 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.2. <a id='toc11_2_3_2_'></a>[词元化（Tokenization）](#toc0_)\n",
    "分词的方法\n",
    "  - 基于`规则`的分词：使用预定义的规则或词典进行分割，适用于规则明确的语言（如英语）。  \n",
    "  - `统计学`分词：基于词频和共现统计进行分割，适用于无明显分词标志的语言（如中文）。\n",
    "  - `机器学习`分词：利用监督学习模型进行分割，能够学习上下文信息进行更准确的分词。\n",
    "\n",
    "词元化的类型\n",
    "  * 基于`词`的分词（Word-Based Tokenization）：按照word拆分成列表格式\n",
    "  * `子词`分词（Subword Tokenization）：词根表示。\n",
    "    - BPE（Byte Pair Encoding）\n",
    "    - WordPiece，主要用于Google的模型，如BERT。\n",
    "    - Unigram，主要用于BERT。\n",
    "  * 基于`字符`的分词（Character-Based Tokenization）：按照char拆分成列表格式。\n",
    "  * `语素`分词（Morpheme-Based Tokenization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: [['the', 'time', 'machine', 'by', 'h', 'g', 'wells'], [], [], [], [], ['i'], [], [], ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him'], ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and'], ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the'], ['fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent'], ['lights', 'in', 'the', 'lilies', 'of', 'silver', 'caught', 'the', 'bubbles', 'that', 'flashed', 'and'], ['passed', 'in', 'our', 'glasses', 'our', 'chairs', 'being', 'his', 'patents', 'embraced', 'and'], ['caressed', 'us', 'rather', 'than', 'submitted', 'to', 'be', 'sat', 'upon', 'and', 'there', 'was', 'that'], ['luxurious', 'after', 'dinner', 'atmosphere', 'when', 'thought', 'roams', 'gracefully'], ['free', 'of', 'the', 'trammels', 'of', 'precision', 'and', 'he', 'put', 'it', 'to', 'us', 'in', 'this'], ['way', 'marking', 'the', 'points', 'with', 'a', 'lean', 'forefinger', 'as', 'we', 'sat', 'and', 'lazily'], ['admired', 'his', 'earnestness', 'over', 'this', 'new', 'paradox', 'as', 'we', 'thought', 'it'], ['and', 'his', 'fecundity'], [], ['you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two'], ['ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for'], ['instance', 'they', 'taught', 'you', 'at', 'school', 'is', 'founded', 'on', 'a', 'misconception'], [], ['is', 'not', 'that', 'rather', 'a', 'large', 'thing', 'to', 'expect', 'us', 'to', 'begin', 'upon'], ['said', 'filby', 'an', 'argumentative', 'person', 'with', 'red', 'hair'], [], ['i', 'do', 'not', 'mean', 'to', 'ask', 'you', 'to', 'accept', 'anything', 'without', 'reasonable'], ['ground', 'for', 'it', 'you', 'will', 'soon', 'admit', 'as', 'much', 'as', 'i', 'need', 'from', 'you', 'you'], ['know', 'of', 'course', 'that', 'a', 'mathematical', 'line', 'a', 'line', 'of', 'thickness', 'nil'], ['has', 'no', 'real', 'existence', 'they', 'taught', 'you', 'that', 'neither', 'has', 'a'], ['mathematical', 'plane', 'these', 'things', 'are', 'mere', 'abstractions'], [], ['that', 'is', 'all', 'right', 'said', 'the', 'psychologist'], [], ['nor', 'having', 'only', 'length', 'breadth', 'and', 'thickness', 'can', 'a', 'cube', 'have', 'a'], ['real', 'existence'], [], ['there', 'i', 'object', 'said', 'filby', 'of', 'course', 'a', 'solid', 'body', 'may', 'exist', 'all'], ['real', 'things'], [], ['so', 'most', 'people', 'think', 'but', 'wait', 'a', 'moment', 'can', 'an', 'instantaneous'], ['cube', 'exist'], [], ['don', 't', 'follow', 'you', 'said', 'filby'], [], ['can', 'a', 'cube', 'that', 'does', 'not', 'last', 'for', 'any', 'time', 'at', 'all', 'have', 'a', 'real'], ['existence'], [], ['filby', 'became', 'pensive', 'clearly', 'the', 'time', 'traveller', 'proceeded', 'any'], ['real', 'body', 'must', 'have', 'extension', 'in', 'four', 'directions', 'it', 'must', 'have'], ['length', 'breadth', 'thickness', 'and', 'duration', 'but', 'through', 'a', 'natural'], ['infirmity', 'of', 'the', 'flesh', 'which', 'i', 'will', 'explain', 'to', 'you', 'in', 'a', 'moment', 'we'], ['incline', 'to', 'overlook', 'this', 'fact', 'there', 'are', 'really', 'four', 'dimensions'], ['three', 'which', 'we', 'call', 'the', 'three', 'planes', 'of', 'space', 'and', 'a', 'fourth', 'time'], ['there', 'is', 'however', 'a', 'tendency', 'to', 'draw', 'an', 'unreal', 'distinction', 'between'], ['the', 'former', 'three', 'dimensions', 'and', 'the', 'latter', 'because', 'it', 'happens', 'that'], ['our', 'consciousness', 'moves', 'intermittently', 'in', 'one', 'direction', 'along', 'the'], ['latter', 'from', 'the', 'beginning', 'to', 'the', 'end', 'of', 'our', 'lives'], [], ['that', 'said', 'a', 'very', 'young', 'man', 'making', 'spasmodic', 'efforts', 'to', 'relight'], ['his', 'cigar', 'over', 'the', 'lamp', 'that', 'very', 'clear', 'indeed'], [], ['now', 'it', 'is', 'very', 'remarkable', 'that', 'this', 'is', 'so', 'extensively', 'overlooked'], ['continued', 'the', 'time', 'traveller', 'with', 'a', 'slight', 'accession', 'of'], ['cheerfulness', 'really', 'this', 'is', 'what', 'is', 'meant', 'by', 'the', 'fourth', 'dimension'], ['though', 'some', 'people', 'who', 'talk', 'about', 'the', 'fourth', 'dimension', 'do', 'not', 'know'], ['they', 'mean', 'it', 'it', 'is', 'only', 'another', 'way', 'of', 'looking', 'at', 'time', 'there', 'is'], ['no', 'difference', 'between', 'time', 'and', 'any', 'of', 'the', 'three', 'dimensions', 'of', 'space'], ['except', 'that', 'our', 'consciousness', 'moves', 'along', 'it', 'but', 'some', 'foolish'], ['people', 'have', 'got', 'hold', 'of', 'the', 'wrong', 'side', 'of', 'that', 'idea', 'you', 'have', 'all'], ['heard', 'what', 'they', 'have', 'to', 'say', 'about', 'this', 'fourth', 'dimension'], [], ['i', 'have', 'not', 'said', 'the', 'provincial', 'mayor'], [], ['it', 'is', 'simply', 'this', 'that', 'space', 'as', 'our', 'mathematicians', 'have', 'it', 'is'], ['spoken', 'of', 'as', 'having', 'three', 'dimensions', 'which', 'one', 'may', 'call', 'length'], ['breadth', 'and', 'thickness', 'and', 'is', 'always', 'definable', 'by', 'reference', 'to'], ['three', 'planes', 'each', 'at', 'right', 'angles', 'to', 'the', 'others', 'but', 'some'], ['philosophical', 'people', 'have', 'been', 'asking', 'why', 'three', 'dimensions'], ['particularly', 'why', 'not', 'another', 'direction', 'at', 'right', 'angles', 'to', 'the', 'other'], ['three', 'and', 'have', 'even', 'tried', 'to', 'construct', 'a', 'four', 'dimension', 'geometry'], ['professor', 'simon', 'newcomb', 'was', 'expounding', 'this', 'to', 'the', 'new', 'york'], ['mathematical', 'society', 'only', 'a', 'month', 'or', 'so', 'ago', 'you', 'know', 'how', 'on', 'a', 'flat'], ['surface', 'which', 'has', 'only', 'two', 'dimensions', 'we', 'can', 'represent', 'a', 'figure', 'of'], ['a', 'three', 'dimensional', 'solid', 'and', 'similarly', 'they', 'think', 'that', 'by', 'models'], ['of', 'three', 'dimensions', 'they', 'could', 'represent', 'one', 'of', 'four', 'if', 'they', 'could'], ['master', 'the', 'perspective', 'of', 'the', 'thing', 'see'], [], ['i', 'think', 'so', 'murmured', 'the', 'provincial', 'mayor', 'and', 'knitting', 'his'], ['brows', 'he', 'lapsed', 'into', 'an', 'introspective', 'state', 'his', 'lips', 'moving', 'as', 'one'], ['who', 'repeats', 'mystic', 'words', 'yes', 'i', 'think', 'i', 'see', 'it', 'now', 'he', 'said', 'after'], ['some', 'time', 'brightening', 'in', 'a', 'quite', 'transitory', 'manner'], [], ['well', 'i', 'do', 'not', 'mind', 'telling', 'you', 'i', 'have', 'been', 'at', 'work', 'upon', 'this'], ['geometry', 'of', 'four', 'dimensions', 'for', 'some', 'time', 'some', 'of', 'my', 'results'], ['are', 'curious', 'for', 'instance', 'here', 'is', 'a', 'portrait', 'of', 'a', 'man', 'at', 'eight'], ['years', 'old', 'another', 'at', 'fifteen', 'another', 'at', 'seventeen', 'another', 'at'], ['twenty', 'three', 'and', 'so', 'on', 'all', 'these', 'are', 'evidently', 'sections', 'as', 'it'], ['were', 'three', 'dimensional', 'representations', 'of', 'his', 'four', 'dimensioned'], ['being', 'which', 'is', 'a', 'fixed', 'and', 'unalterable', 'thing'], [], ['scientific', 'people', 'proceeded', 'the', 'time', 'traveller', 'after', 'the', 'pause'], ['required', 'for', 'the', 'proper', 'assimilation', 'of', 'this', 'know', 'very', 'well', 'that'], ['time', 'is', 'only', 'a', 'kind', 'of', 'space', 'here', 'is', 'a', 'popular', 'scientific', 'diagram'], ['a', 'weather', 'record', 'this', 'line', 'i', 'trace', 'with', 'my', 'finger', 'shows', 'the'], ['movement', 'of', 'the', 'barometer', 'yesterday', 'it', 'was', 'so', 'high', 'yesterday', 'night'], ['it', 'fell', 'then', 'this', 'morning', 'it', 'rose', 'again', 'and', 'so', 'gently', 'upward', 'to'], ['here', 'surely', 'the', 'mercury', 'did', 'not', 'trace', 'this', 'line', 'in', 'any', 'of', 'the'], ['dimensions', 'of', 'space', 'generally', 'recognized', 'but', 'certainly', 'it', 'traced'], ['such', 'a', 'line', 'and', 'that', 'line', 'therefore', 'we', 'must', 'conclude', 'was', 'along'], ['the', 'time', 'dimension'], [], ['but', 'said', 'the', 'medical', 'man', 'staring', 'hard', 'at', 'a', 'coal', 'in', 'the', 'fire', 'if'], ['time', 'is', 'really', 'only', 'a', 'fourth', 'dimension', 'of', 'space', 'why', 'is', 'it', 'and', 'why'], ['has', 'it', 'always', 'been', 'regarded', 'as', 'something', 'different', 'and', 'why', 'cannot'], ['we', 'move', 'in', 'time', 'as', 'we', 'move', 'about', 'in', 'the', 'other', 'dimensions', 'of', 'space'], [], ['the', 'time', 'traveller', 'smiled', 'are', 'you', 'sure', 'we', 'can', 'move', 'freely', 'in'], ['space', 'right', 'and', 'left', 'we', 'can', 'go', 'backward', 'and', 'forward', 'freely', 'enough'], ['and', 'men', 'always', 'have', 'done', 'so', 'i', 'admit', 'we', 'move', 'freely', 'in', 'two'], ['dimensions', 'but', 'how', 'about', 'up', 'and', 'down', 'gravitation', 'limits', 'us', 'there'], [], ['not', 'exactly', 'said', 'the', 'medical', 'man', 'there', 'are', 'balloons'], [], ['but', 'before', 'the', 'balloons', 'save', 'for', 'spasmodic', 'jumping', 'and', 'the'], ['inequalities', 'of', 'the', 'surface', 'man', 'had', 'no', 'freedom', 'of', 'vertical'], ['movement'], [], ['still', 'they', 'could', 'move', 'a', 'little', 'up', 'and', 'down', 'said', 'the', 'medical', 'man'], [], ['easier', 'far', 'easier', 'down', 'than', 'up'], [], ['and', 'you', 'cannot', 'move', 'at', 'all', 'in', 'time', 'you', 'cannot', 'get', 'away', 'from', 'the'], ['present', 'moment'], [], ['my', 'dear', 'sir', 'that', 'is', 'just', 'where', 'you', 'are', 'wrong', 'that', 'is', 'just', 'where'], ['the', 'whole', 'world', 'has', 'gone', 'wrong', 'we', 'are', 'always', 'getting', 'away', 'from', 'the'], ['present', 'moment', 'our', 'mental', 'existences', 'which', 'are', 'immaterial', 'and', 'have'], ['no', 'dimensions', 'are', 'passing', 'along', 'the', 'time', 'dimension', 'with', 'a', 'uniform'], ['velocity', 'from', 'the', 'cradle', 'to', 'the', 'grave', 'just', 'as', 'we', 'should', 'travel', 'down'], ['if', 'we', 'began', 'our', 'existence', 'fifty', 'miles', 'above', 'the', 'earth', 's', 'surface'], [], ['but', 'the', 'great', 'difficulty', 'is', 'this', 'interrupted', 'the', 'psychologist'], ['you', 'can', 'move', 'about', 'in', 'all', 'directions', 'of', 'space', 'but', 'you', 'cannot'], ['move', 'about', 'in', 'time'], [], ['that', 'is', 'the', 'germ', 'of', 'my', 'great', 'discovery', 'but', 'you', 'are', 'wrong', 'to', 'say'], ['that', 'we', 'cannot', 'move', 'about', 'in', 'time', 'for', 'instance', 'if', 'i', 'am', 'recalling'], ['an', 'incident', 'very', 'vividly', 'i', 'go', 'back', 'to', 'the', 'instant', 'of', 'its', 'occurrence'], ['i', 'become', 'absent', 'minded', 'as', 'you', 'say', 'i', 'jump', 'back', 'for', 'a', 'moment', 'of'], ['course', 'we', 'have', 'no', 'means', 'of', 'staying', 'back', 'for', 'any', 'length', 'of', 'time', 'any'], ['more', 'than', 'a', 'savage', 'or', 'an', 'animal', 'has', 'of', 'staying', 'six', 'feet', 'above', 'the'], ['ground', 'but', 'a', 'civilized', 'man', 'is', 'better', 'off', 'than', 'the', 'savage', 'in', 'this'], ['respect', 'he', 'can', 'go', 'up', 'against', 'gravitation', 'in', 'a', 'balloon', 'and', 'why'], ['should', 'he', 'not', 'hope', 'that', 'ultimately', 'he', 'may', 'be', 'able', 'to', 'stop', 'or'], ['accelerate', 'his', 'drift', 'along', 'the', 'time', 'dimension', 'or', 'even', 'turn', 'about'], ['and', 'travel', 'the', 'other', 'way'], [], ['oh', 'this', 'began', 'filby', 'is', 'all'], [], ['why', 'not', 'said', 'the', 'time', 'traveller'], [], ['it', 's', 'against', 'reason', 'said', 'filby'], [], ['what', 'reason', 'said', 'the', 'time', 'traveller'], [], ['you', 'can', 'show', 'black', 'is', 'white', 'by', 'argument', 'said', 'filby', 'but', 'you', 'will'], ['never', 'convince', 'me'], [], ['possibly', 'not', 'said', 'the', 'time', 'traveller', 'but', 'now', 'you', 'begin', 'to', 'see'], ['the', 'object', 'of', 'my', 'investigations', 'into', 'the', 'geometry', 'of', 'four'], ['dimensions', 'long', 'ago', 'i', 'had', 'a', 'vague', 'inkling', 'of', 'a', 'machine'], [], ['to', 'travel', 'through', 'time', 'exclaimed', 'the', 'very', 'young', 'man'], [], ['that', 'shall', 'travel', 'indifferently', 'in', 'any', 'direction', 'of', 'space', 'and', 'time'], ['as', 'the', 'driver', 'determines'], [], ['filby', 'contented', 'himself', 'with', 'laughter'], [], ['but', 'i', 'have', 'experimental', 'verification', 'said', 'the', 'time', 'traveller'], [], ['it', 'would', 'be', 'remarkably', 'convenient', 'for', 'the', 'historian', 'the'], ['psychologist', 'suggested', 'one', 'might', 'travel', 'back', 'and', 'verify', 'the'], ['accepted', 'account', 'of', 'the', 'battle', 'of', 'hastings', 'for', 'instance'], [], ['don', 't', 'you', 'think', 'you', 'would', 'attract', 'attention', 'said', 'the', 'medical', 'man'], ['our', 'ancestors', 'had', 'no', 'great', 'tolerance', 'for', 'anachronisms'], [], ['one', 'might', 'get', 'one', 's', 'greek', 'from', 'the', 'very', 'lips', 'of', 'homer', 'and', 'plato'], ['the', 'very', 'young', 'man', 'thought'], [], ['in', 'which', 'case', 'they', 'would', 'certainly', 'plough', 'you', 'for', 'the', 'little', 'go'], ['the', 'german', 'scholars', 'have', 'improved', 'greek', 'so', 'much'], [], ['then', 'there', 'is', 'the', 'future', 'said', 'the', 'very', 'young', 'man', 'just', 'think'], ['one', 'might', 'invest', 'all', 'one', 's', 'money', 'leave', 'it', 'to', 'accumulate', 'at'], ['interest', 'and', 'hurry', 'on', 'ahead'], [], ['to', 'discover', 'a', 'society', 'said', 'i', 'erected', 'on', 'a', 'strictly', 'communistic'], ['basis'], [], ['of', 'all', 'the', 'wild', 'extravagant', 'theories', 'began', 'the', 'psychologist'], [], ['yes', 'so', 'it', 'seemed', 'to', 'me', 'and', 'so', 'i', 'never', 'talked', 'of', 'it', 'until'], [], ['experimental', 'verification', 'cried', 'i', 'you', 'are', 'going', 'to', 'verify'], ['that'], [], ['the', 'experiment', 'cried', 'filby', 'who', 'was', 'getting', 'brain', 'weary'], [], ['let', 's', 'see', 'your', 'experiment', 'anyhow', 'said', 'the', 'psychologist', 'though'], ['it', 's', 'all', 'humbug', 'you', 'know'], [], ['the', 'time', 'traveller', 'smiled', 'round', 'at', 'us', 'then', 'still', 'smiling', 'faintly'], ['and', 'with', 'his', 'hands', 'deep', 'in', 'his', 'trousers', 'pockets', 'he', 'walked', 'slowly'], ['out', 'of', 'the', 'room', 'and', 'we', 'heard', 'his', 'slippers', 'shuffling', 'down', 'the', 'long'], ['passage', 'to', 'his', 'laboratory'], [], ['the', 'psychologist', 'looked', 'at', 'us', 'i', 'wonder', 'what', 'he', 's', 'got'], [], ['some', 'sleight', 'of', 'hand', 'trick', 'or', 'other', 'said', 'the', 'medical', 'man', 'and'], ['filby', 'tried', 'to', 'tell', 'us', 'about', 'a', 'conjurer', 'he', 'had', 'seen', 'at', 'burslem', 'but'], ['before', 'he', 'had', 'finished', 'his', 'preface', 'the', 'time', 'traveller', 'came', 'back', 'and'], ['filby', 's', 'anecdote', 'collapsed'], [], ['the', 'thing', 'the', 'time', 'traveller', 'held', 'in', 'his', 'hand', 'was', 'a', 'glittering'], ['metallic', 'framework', 'scarcely', 'larger', 'than', 'a', 'small', 'clock', 'and', 'very'], ['delicately', 'made', 'there', 'was', 'ivory', 'in', 'it', 'and', 'some', 'transparent'], ['crystalline', 'substance', 'and', 'now', 'i', 'must', 'be', 'explicit', 'for', 'this', 'that'], ['follows', 'unless', 'his', 'explanation', 'is', 'to', 'be', 'accepted', 'is', 'an', 'absolutely'], ['unaccountable', 'thing', 'he', 'took', 'one', 'of', 'the', 'small', 'octagonal', 'tables', 'that'], ['were', 'scattered', 'about', 'the', 'room', 'and', 'set', 'it', 'in', 'front', 'of', 'the', 'fire', 'with'], ['two', 'legs', 'on', 'the', 'hearthrug', 'on', 'this', 'table', 'he', 'placed', 'the', 'mechanism'], ['then', 'he', 'drew', 'up', 'a', 'chair', 'and', 'sat', 'down', 'the', 'only', 'other', 'object', 'on', 'the'], ['table', 'was', 'a', 'small', 'shaded', 'lamp', 'the', 'bright', 'light', 'of', 'which', 'fell', 'upon'], ['the', 'model', 'there', 'were', 'also', 'perhaps', 'a', 'dozen', 'candles', 'about', 'two', 'in'], ['brass', 'candlesticks', 'upon', 'the', 'mantel', 'and', 'several', 'in', 'sconces', 'so', 'that'], ['the', 'room', 'was', 'brilliantly', 'illuminated', 'i', 'sat', 'in', 'a', 'low', 'arm', 'chair'], ['nearest', 'the', 'fire', 'and', 'i', 'drew', 'this', 'forward', 'so', 'as', 'to', 'be', 'almost', 'between'], ['the', 'time', 'traveller', 'and', 'the', 'fireplace', 'filby', 'sat', 'behind', 'him', 'looking'], ['over', 'his', 'shoulder', 'the', 'medical', 'man', 'and', 'the', 'provincial', 'mayor', 'watched'], ['him', 'in', 'profile', 'from', 'the', 'right', 'the', 'psychologist', 'from', 'the', 'left', 'the'], ['very', 'young', 'man', 'stood', 'behind', 'the', 'psychologist', 'we', 'were', 'all', 'on', 'the'], ['alert', 'it', 'appears', 'incredible', 'to', 'me', 'that', 'any', 'kind', 'of', 'trick', 'however'], ['subtly', 'conceived', 'and', 'however', 'adroitly', 'done', 'could', 'have', 'been', 'played'], ['upon', 'us', 'under', 'these', 'conditions'], [], ['the', 'time', 'traveller', 'looked', 'at', 'us', 'and', 'then', 'at', 'the', 'mechanism', 'well'], ['said', 'the', 'psychologist'], [], ['this', 'little', 'affair', 'said', 'the', 'time', 'traveller', 'resting', 'his', 'elbows'], ['upon', 'the', 'table', 'and', 'pressing', 'his', 'hands', 'together', 'above', 'the', 'apparatus'], ['is', 'only', 'a', 'model', 'it', 'is', 'my', 'plan', 'for', 'a', 'machine', 'to', 'travel', 'through'], ['time', 'you', 'will', 'notice', 'that', 'it', 'looks', 'singularly', 'askew', 'and', 'that', 'there'], ['is', 'an', 'odd', 'twinkling', 'appearance', 'about', 'this', 'bar', 'as', 'though', 'it', 'was', 'in'], ['some', 'way', 'unreal', 'he', 'pointed', 'to', 'the', 'part', 'with', 'his', 'finger', 'also'], ['here', 'is', 'one', 'little', 'white', 'lever', 'and', 'here', 'is', 'another'], [], ['the', 'medical', 'man', 'got', 'up', 'out', 'of', 'his', 'chair', 'and', 'peered', 'into', 'the', 'thing'], ['it', 's', 'beautifully', 'made', 'he', 'said'], [], ['it', 'took', 'two', 'years', 'to', 'make', 'retorted', 'the', 'time', 'traveller', 'then', 'when'], ['we', 'had', 'all', 'imitated', 'the', 'action', 'of', 'the', 'medical', 'man', 'he', 'said', 'now', 'i'], ['want', 'you', 'clearly', 'to', 'understand', 'that', 'this', 'lever', 'being', 'pressed', 'over'], ['sends', 'the', 'machine', 'gliding', 'into', 'the', 'future', 'and', 'this', 'other', 'reverses'], ['the', 'motion', 'this', 'saddle', 'represents', 'the', 'seat', 'of', 'a', 'time', 'traveller'], ['presently', 'i', 'am', 'going', 'to', 'press', 'the', 'lever', 'and', 'off', 'the', 'machine', 'will'], ['go', 'it', 'will', 'vanish', 'pass', 'into', 'future', 'time', 'and', 'disappear', 'have', 'a'], ['good', 'look', 'at', 'the', 'thing', 'look', 'at', 'the', 'table', 'too', 'and', 'satisfy'], ['yourselves', 'there', 'is', 'no', 'trickery', 'i', 'don', 't', 'want', 'to', 'waste', 'this', 'model'], ['and', 'then', 'be', 'told', 'i', 'm', 'a', 'quack'], [], ['there', 'was', 'a', 'minute', 's', 'pause', 'perhaps', 'the', 'psychologist', 'seemed', 'about', 'to'], ['speak', 'to', 'me', 'but', 'changed', 'his', 'mind', 'then', 'the', 'time', 'traveller', 'put', 'forth'], ['his', 'finger', 'towards', 'the', 'lever', 'no', 'he', 'said', 'suddenly', 'lend', 'me', 'your'], ['hand', 'and', 'turning', 'to', 'the', 'psychologist', 'he', 'took', 'that', 'individual', 's'], ['hand', 'in', 'his', 'own', 'and', 'told', 'him', 'to', 'put', 'out', 'his', 'forefinger', 'so', 'that', 'it'], ['was', 'the', 'psychologist', 'himself', 'who', 'sent', 'forth', 'the', 'model', 'time', 'machine'], ['on', 'its', 'interminable', 'voyage', 'we', 'all', 'saw', 'the', 'lever', 'turn', 'i', 'am'], ['absolutely', 'certain', 'there', 'was', 'no', 'trickery', 'there', 'was', 'a', 'breath', 'of'], ['wind', 'and', 'the', 'lamp', 'flame', 'jumped', 'one', 'of', 'the', 'candles', 'on', 'the', 'mantel'], ['was', 'blown', 'out', 'and', 'the', 'little', 'machine', 'suddenly', 'swung', 'round', 'became'], ['indistinct', 'was', 'seen', 'as', 'a', 'ghost', 'for', 'a', 'second', 'perhaps', 'as', 'an', 'eddy', 'of'], ['faintly', 'glittering', 'brass', 'and', 'ivory', 'and', 'it', 'was', 'gone', 'vanished', 'save'], ['for', 'the', 'lamp', 'the', 'table', 'was', 'bare'], [], ['everyone', 'was', 'silent', 'for', 'a', 'minute', 'then', 'filby', 'said', 'he', 'was', 'damned'], [], ['the', 'psychologist', 'recovered', 'from', 'his', 'stupor', 'and', 'suddenly', 'looked'], ['under', 'the', 'table', 'at', 'that', 'the', 'time', 'traveller', 'laughed', 'cheerfully'], ['well', 'he', 'said', 'with', 'a', 'reminiscence', 'of', 'the', 'psychologist', 'then'], ['getting', 'up', 'he', 'went', 'to', 'the', 'tobacco', 'jar', 'on', 'the', 'mantel', 'and', 'with', 'his'], ['back', 'to', 'us', 'began', 'to', 'fill', 'his', 'pipe'], [], ['we', 'stared', 'at', 'each', 'other', 'look', 'here', 'said', 'the', 'medical', 'man', 'are', 'you'], ['in', 'earnest', 'about', 'this', 'do', 'you', 'seriously', 'believe', 'that', 'that', 'machine'], ['has', 'travelled', 'into', 'time'], [], ['certainly', 'said', 'the', 'time', 'traveller', 'stooping', 'to', 'light', 'a', 'spill', 'at'], ['the', 'fire', 'then', 'he', 'turned', 'lighting', 'his', 'pipe', 'to', 'look', 'at', 'the'], ['psychologist', 's', 'face', 'the', 'psychologist', 'to', 'show', 'that', 'he', 'was', 'not'], ['unhinged', 'helped', 'himself', 'to', 'a', 'cigar', 'and', 'tried', 'to', 'light', 'it', 'uncut'], ['what', 'is', 'more', 'i', 'have', 'a', 'big', 'machine', 'nearly', 'finished', 'in', 'there', 'he'], ['indicated', 'the', 'laboratory', 'and', 'when', 'that', 'is', 'put', 'together', 'i', 'mean', 'to'], ['have', 'a', 'journey', 'on', 'my', 'own', 'account'], [], ['you', 'mean', 'to', 'say', 'that', 'that', 'machine', 'has', 'travelled', 'into', 'the', 'future'], ['said', 'filby'], [], ['into', 'the', 'future', 'or', 'the', 'past', 'i', 'don', 't', 'for', 'certain', 'know', 'which'], [], ['after', 'an', 'interval', 'the', 'psychologist', 'had', 'an', 'inspiration', 'it', 'must', 'have'], ['gone', 'into', 'the', 'past', 'if', 'it', 'has', 'gone', 'anywhere', 'he', 'said'], [], ['why', 'said', 'the', 'time', 'traveller'], [], ['because', 'i', 'presume', 'that', 'it', 'has', 'not', 'moved', 'in', 'space', 'and', 'if', 'it'], ['travelled', 'into', 'the', 'future', 'it', 'would', 'still', 'be', 'here', 'all', 'this', 'time'], ['since', 'it', 'must', 'have', 'travelled', 'through', 'this', 'time'], [], ['but', 'i', 'said', 'if', 'it', 'travelled', 'into', 'the', 'past', 'it', 'would', 'have', 'been'], ['visible', 'when', 'we', 'came', 'first', 'into', 'this', 'room', 'and', 'last', 'thursday', 'when', 'we'], ['were', 'here', 'and', 'the', 'thursday', 'before', 'that', 'and', 'so', 'forth'], [], ['serious', 'objections', 'remarked', 'the', 'provincial', 'mayor', 'with', 'an', 'air', 'of'], ['impartiality', 'turning', 'towards', 'the', 'time', 'traveller'], [], ['not', 'a', 'bit', 'said', 'the', 'time', 'traveller', 'and', 'to', 'the', 'psychologist', 'you'], ['think', 'you', 'can', 'explain', 'that', 'it', 's', 'presentation', 'below', 'the', 'threshold'], ['you', 'know', 'diluted', 'presentation'], [], ['of', 'course', 'said', 'the', 'psychologist', 'and', 'reassured', 'us', 'that', 's', 'a'], ['simple', 'point', 'of', 'psychology', 'i', 'should', 'have', 'thought', 'of', 'it', 'it', 's', 'plain'], ['enough', 'and', 'helps', 'the', 'paradox', 'delightfully', 'we', 'cannot', 'see', 'it', 'nor'], ['can', 'we', 'appreciate', 'this', 'machine', 'any', 'more', 'than', 'we', 'can', 'the', 'spoke', 'of'], ['a', 'wheel', 'spinning', 'or', 'a', 'bullet', 'flying', 'through', 'the', 'air', 'if', 'it', 'is'], ['travelling', 'through', 'time', 'fifty', 'times', 'or', 'a', 'hundred', 'times', 'faster', 'than'], ['we', 'are', 'if', 'it', 'gets', 'through', 'a', 'minute', 'while', 'we', 'get', 'through', 'a', 'second'], ['the', 'impression', 'it', 'creates', 'will', 'of', 'course', 'be', 'only', 'one', 'fiftieth', 'or'], ['one', 'hundredth', 'of', 'what', 'it', 'would', 'make', 'if', 'it', 'were', 'not', 'travelling', 'in'], ['time', 'that', 's', 'plain', 'enough', 'he', 'passed', 'his', 'hand', 'through', 'the', 'space', 'in'], ['which', 'the', 'machine', 'had', 'been', 'you', 'see', 'he', 'said', 'laughing'], [], ['we', 'sat', 'and', 'stared', 'at', 'the', 'vacant', 'table', 'for', 'a', 'minute', 'or', 'so', 'then', 'the'], ['time', 'traveller', 'asked', 'us', 'what', 'we', 'thought', 'of', 'it', 'all'], [], ['it', 'sounds', 'plausible', 'enough', 'to', 'night', 'said', 'the', 'medical', 'man', 'but'], ['wait', 'until', 'to', 'morrow', 'wait', 'for', 'the', 'common', 'sense', 'of', 'the', 'morning'], [], ['would', 'you', 'like', 'to', 'see', 'the', 'time', 'machine', 'itself', 'asked', 'the', 'time'], ['traveller', 'and', 'therewith', 'taking', 'the', 'lamp', 'in', 'his', 'hand', 'he', 'led', 'the'], ['way', 'down', 'the', 'long', 'draughty', 'corridor', 'to', 'his', 'laboratory', 'i', 'remember'], ['vividly', 'the', 'flickering', 'light', 'his', 'queer', 'broad', 'head', 'in', 'silhouette'], ['the', 'dance', 'of', 'the', 'shadows', 'how', 'we', 'all', 'followed', 'him', 'puzzled', 'but'], ['incredulous', 'and', 'how', 'there', 'in', 'the', 'laboratory', 'we', 'beheld', 'a', 'larger'], ['edition', 'of', 'the', 'little', 'mechanism', 'which', 'we', 'had', 'seen', 'vanish', 'from', 'before'], ['our', 'eyes', 'parts', 'were', 'of', 'nickel', 'parts', 'of', 'ivory', 'parts', 'had', 'certainly'], ['been', 'filed', 'or', 'sawn', 'out', 'of', 'rock', 'crystal', 'the', 'thing', 'was', 'generally'], ['complete', 'but', 'the', 'twisted', 'crystalline', 'bars', 'lay', 'unfinished', 'upon', 'the'], ['bench', 'beside', 'some', 'sheets', 'of', 'drawings', 'and', 'i', 'took', 'one', 'up', 'for', 'a', 'better'], ['look', 'at', 'it', 'quartz', 'it', 'seemed', 'to', 'be'], [], ['look', 'here', 'said', 'the', 'medical', 'man', 'are', 'you', 'perfectly', 'serious'], ['or', 'is', 'this', 'a', 'trick', 'like', 'that', 'ghost', 'you', 'showed', 'us', 'last', 'christmas'], [], ['upon', 'that', 'machine', 'said', 'the', 'time', 'traveller', 'holding', 'the', 'lamp'], ['aloft', 'i', 'intend', 'to', 'explore', 'time', 'is', 'that', 'plain', 'i', 'was', 'never', 'more'], ['serious', 'in', 'my', 'life'], [], ['none', 'of', 'us', 'quite', 'knew', 'how', 'to', 'take', 'it'], [], ['i', 'caught', 'filby', 's', 'eye', 'over', 'the', 'shoulder', 'of', 'the', 'medical', 'man', 'and', 'he'], ['winked', 'at', 'me', 'solemnly'], [], [], [], [], ['ii'], [], [], ['i', 'think', 'that', 'at', 'that', 'time', 'none', 'of', 'us', 'quite', 'believed', 'in', 'the', 'time'], ['machine', 'the', 'fact', 'is', 'the', 'time', 'traveller', 'was', 'one', 'of', 'those', 'men', 'who'], ['are', 'too', 'clever', 'to', 'be', 'believed', 'you', 'never', 'felt', 'that', 'you', 'saw', 'all', 'round'], ['him', 'you', 'always', 'suspected', 'some', 'subtle', 'reserve', 'some', 'ingenuity', 'in'], ['ambush', 'behind', 'his', 'lucid', 'frankness', 'had', 'filby', 'shown', 'the', 'model', 'and'], ['explained', 'the', 'matter', 'in', 'the', 'time', 'traveller', 's', 'words', 'we', 'should', 'have'], ['shown', 'him', 'far', 'less', 'scepticism', 'for', 'we', 'should', 'have', 'perceived', 'his'], ['motives', 'a', 'pork', 'butcher', 'could', 'understand', 'filby', 'but', 'the', 'time'], ['traveller', 'had', 'more', 'than', 'a', 'touch', 'of', 'whim', 'among', 'his', 'elements', 'and', 'we'], ['distrusted', 'him', 'things', 'that', 'would', 'have', 'made', 'the', 'frame', 'of', 'a', 'less'], ['clever', 'man', 'seemed', 'tricks', 'in', 'his', 'hands', 'it', 'is', 'a', 'mistake', 'to', 'do', 'things'], ['too', 'easily', 'the', 'serious', 'people', 'who', 'took', 'him', 'seriously', 'never', 'felt'], ['quite', 'sure', 'of', 'his', 'deportment', 'they', 'were', 'somehow', 'aware', 'that', 'trusting'], ['their', 'reputations', 'for', 'judgment', 'with', 'him', 'was', 'like', 'furnishing', 'a'], ['nursery', 'with', 'egg', 'shell', 'china', 'so', 'i', 'don', 't', 'think', 'any', 'of', 'us', 'said', 'very'], ['much', 'about', 'time', 'travelling', 'in', 'the', 'interval', 'between', 'that', 'thursday', 'and'], ['the', 'next', 'though', 'its', 'odd', 'potentialities', 'ran', 'no', 'doubt', 'in', 'most', 'of'], ['our', 'minds', 'its', 'plausibility', 'that', 'is', 'its', 'practical', 'incredibleness'], ['the', 'curious', 'possibilities', 'of', 'anachronism', 'and', 'of', 'utter', 'confusion', 'it'], ['suggested', 'for', 'my', 'own', 'part', 'i', 'was', 'particularly', 'preoccupied', 'with', 'the'], ['trick', 'of', 'the', 'model', 'that', 'i', 'remember', 'discussing', 'with', 'the', 'medical', 'man'], ['whom', 'i', 'met', 'on', 'friday', 'at', 'the', 'linnaean', 'he', 'said', 'he', 'had', 'seen', 'a', 'similar'], ['thing', 'at', 'tubingen', 'and', 'laid', 'considerable', 'stress', 'on', 'the', 'blowing', 'out'], ['of', 'the', 'candle', 'but', 'how', 'the', 'trick', 'was', 'done', 'he', 'could', 'not', 'explain'], [], ['the', 'next', 'thursday', 'i', 'went', 'again', 'to', 'richmond', 'i', 'suppose', 'i', 'was', 'one', 'of'], ['the', 'time', 'traveller', 's', 'most', 'constant', 'guests', 'and', 'arriving', 'late', 'found'], ['four', 'or', 'five', 'men', 'already', 'assembled', 'in', 'his', 'drawing', 'room', 'the', 'medical'], ['man', 'was', 'standing', 'before', 'the', 'fire', 'with', 'a', 'sheet', 'of', 'paper', 'in', 'one', 'hand'], ['and', 'his', 'watch', 'in', 'the', 'other', 'i', 'looked', 'round', 'for', 'the', 'time', 'traveller'], ['and', 'it', 's', 'half', 'past', 'seven', 'now', 'said', 'the', 'medical', 'man', 'i', 'suppose'], ['we', 'd', 'better', 'have', 'dinner'], [], ['where', 's', 'said', 'i', 'naming', 'our', 'host'], [], ['you', 've', 'just', 'come', 'it', 's', 'rather', 'odd', 'he', 's', 'unavoidably', 'detained', 'he'], ['asks', 'me', 'in', 'this', 'note', 'to', 'lead', 'off', 'with', 'dinner', 'at', 'seven', 'if', 'he', 's', 'not'], ['back', 'says', 'he', 'll', 'explain', 'when', 'he', 'comes'], [], ['it', 'seems', 'a', 'pity', 'to', 'let', 'the', 'dinner', 'spoil', 'said', 'the', 'editor', 'of', 'a'], ['well', 'known', 'daily', 'paper', 'and', 'thereupon', 'the', 'doctor', 'rang', 'the', 'bell'], [], ['the', 'psychologist', 'was', 'the', 'only', 'person', 'besides', 'the', 'doctor', 'and', 'myself'], ['who', 'had', 'attended', 'the', 'previous', 'dinner', 'the', 'other', 'men', 'were', 'blank', 'the'], ['editor', 'aforementioned', 'a', 'certain', 'journalist', 'and', 'another', 'a', 'quiet'], ['shy', 'man', 'with', 'a', 'beard', 'whom', 'i', 'didn', 't', 'know', 'and', 'who', 'as', 'far', 'as', 'my'], ['observation', 'went', 'never', 'opened', 'his', 'mouth', 'all', 'the', 'evening', 'there', 'was'], ['some', 'speculation', 'at', 'the', 'dinner', 'table', 'about', 'the', 'time', 'traveller', 's'], ['absence', 'and', 'i', 'suggested', 'time', 'travelling', 'in', 'a', 'half', 'jocular', 'spirit'], ['the', 'editor', 'wanted', 'that', 'explained', 'to', 'him', 'and', 'the', 'psychologist'], ['volunteered', 'a', 'wooden', 'account', 'of', 'the', 'ingenious', 'paradox', 'and', 'trick', 'we'], ['had', 'witnessed', 'that', 'day', 'week', 'he', 'was', 'in', 'the', 'midst', 'of', 'his', 'exposition'], ['when', 'the', 'door', 'from', 'the', 'corridor', 'opened', 'slowly', 'and', 'without', 'noise', 'i'], ['was', 'facing', 'the', 'door', 'and', 'saw', 'it', 'first', 'hallo', 'i', 'said', 'at', 'last'], ['and', 'the', 'door', 'opened', 'wider', 'and', 'the', 'time', 'traveller', 'stood', 'before', 'us'], ['i', 'gave', 'a', 'cry', 'of', 'surprise', 'good', 'heavens', 'man', 'what', 's', 'the', 'matter'], ['cried', 'the', 'medical', 'man', 'who', 'saw', 'him', 'next', 'and', 'the', 'whole', 'tableful'], ['turned', 'towards', 'the', 'door'], [], ['he', 'was', 'in', 'an', 'amazing', 'plight', 'his', 'coat', 'was', 'dusty', 'and', 'dirty', 'and'], ['smeared', 'with', 'green', 'down', 'the', 'sleeves', 'his', 'hair', 'disordered', 'and', 'as', 'it'], ['seemed', 'to', 'me', 'greyer', 'either', 'with', 'dust', 'and', 'dirt', 'or', 'because', 'its', 'colour'], ['had', 'actually', 'faded', 'his', 'face', 'was', 'ghastly', 'pale', 'his', 'chin', 'had', 'a', 'brown'], ['cut', 'on', 'it', 'a', 'cut', 'half', 'healed', 'his', 'expression', 'was', 'haggard', 'and', 'drawn'], ['as', 'by', 'intense', 'suffering', 'for', 'a', 'moment', 'he', 'hesitated', 'in', 'the', 'doorway'], ['as', 'if', 'he', 'had', 'been', 'dazzled', 'by', 'the', 'light', 'then', 'he', 'came', 'into', 'the', 'room'], ['he', 'walked', 'with', 'just', 'such', 'a', 'limp', 'as', 'i', 'have', 'seen', 'in', 'footsore', 'tramps'], ['we', 'stared', 'at', 'him', 'in', 'silence', 'expecting', 'him', 'to', 'speak'], [], ['he', 'said', 'not', 'a', 'word', 'but', 'came', 'painfully', 'to', 'the', 'table', 'and', 'made', 'a'], ['motion', 'towards', 'the', 'wine', 'the', 'editor', 'filled', 'a', 'glass', 'of', 'champagne', 'and'], ['pushed', 'it', 'towards', 'him', 'he', 'drained', 'it', 'and', 'it', 'seemed', 'to', 'do', 'him', 'good'], ['for', 'he', 'looked', 'round', 'the', 'table', 'and', 'the', 'ghost', 'of', 'his', 'old', 'smile'], ['flickered', 'across', 'his', 'face', 'what', 'on', 'earth', 'have', 'you', 'been', 'up', 'to', 'man'], ['said', 'the', 'doctor', 'the', 'time', 'traveller', 'did', 'not', 'seem', 'to', 'hear', 'don', 't', 'let'], ['me', 'disturb', 'you', 'he', 'said', 'with', 'a', 'certain', 'faltering', 'articulation'], ['i', 'm', 'all', 'right', 'he', 'stopped', 'held', 'out', 'his', 'glass', 'for', 'more', 'and', 'took'], ['it', 'off', 'at', 'a', 'draught', 'that', 's', 'good', 'he', 'said', 'his', 'eyes', 'grew', 'brighter'], ['and', 'a', 'faint', 'colour', 'came', 'into', 'his', 'cheeks', 'his', 'glance', 'flickered', 'over'], ['our', 'faces', 'with', 'a', 'certain', 'dull', 'approval', 'and', 'then', 'went', 'round', 'the', 'warm'], ['and', 'comfortable', 'room', 'then', 'he', 'spoke', 'again', 'still', 'as', 'it', 'were', 'feeling'], ['his', 'way', 'among', 'his', 'words', 'i', 'm', 'going', 'to', 'wash', 'and', 'dress', 'and', 'then', 'i', 'll'], ['come', 'down', 'and', 'explain', 'things', 'save', 'me', 'some', 'of', 'that', 'mutton', 'i', 'm'], ['starving', 'for', 'a', 'bit', 'of', 'meat'], [], ['he', 'looked', 'across', 'at', 'the', 'editor', 'who', 'was', 'a', 'rare', 'visitor', 'and', 'hoped', 'he'], ['was', 'all', 'right', 'the', 'editor', 'began', 'a', 'question', 'tell', 'you', 'presently'], ['said', 'the', 'time', 'traveller', 'i', 'm', 'funny', 'be', 'all', 'right', 'in', 'a', 'minute'], [], ['he', 'put', 'down', 'his', 'glass', 'and', 'walked', 'towards', 'the', 'staircase', 'door', 'again'], ['i', 'remarked', 'his', 'lameness', 'and', 'the', 'soft', 'padding', 'sound', 'of', 'his', 'footfall'], ['and', 'standing', 'up', 'in', 'my', 'place', 'i', 'saw', 'his', 'feet', 'as', 'he', 'went', 'out', 'he', 'had'], ['nothing', 'on', 'them', 'but', 'a', 'pair', 'of', 'tattered', 'blood', 'stained', 'socks', 'then', 'the'], ['door', 'closed', 'upon', 'him', 'i', 'had', 'half', 'a', 'mind', 'to', 'follow', 'till', 'i', 'remembered'], ['how', 'he', 'detested', 'any', 'fuss', 'about', 'himself', 'for', 'a', 'minute', 'perhaps', 'my'], ['mind', 'was', 'wool', 'gathering', 'then', 'remarkable', 'behaviour', 'of', 'an', 'eminent'], ['scientist', 'i', 'heard', 'the', 'editor', 'say', 'thinking', 'after', 'his', 'wont', 'in'], ['headlines', 'and', 'this', 'brought', 'my', 'attention', 'back', 'to', 'the', 'bright'], ['dinner', 'table'], [], ['what', 's', 'the', 'game', 'said', 'the', 'journalist', 'has', 'he', 'been', 'doing', 'the'], ['amateur', 'cadger', 'i', 'don', 't', 'follow', 'i', 'met', 'the', 'eye', 'of', 'the', 'psychologist'], ['and', 'read', 'my', 'own', 'interpretation', 'in', 'his', 'face', 'i', 'thought', 'of', 'the', 'time'], ['traveller', 'limping', 'painfully', 'upstairs', 'i', 'don', 't', 'think', 'any', 'one', 'else', 'had'], ['noticed', 'his', 'lameness'], [], ['the', 'first', 'to', 'recover', 'completely', 'from', 'this', 'surprise', 'was', 'the', 'medical'], ['man', 'who', 'rang', 'the', 'bell', 'the', 'time', 'traveller', 'hated', 'to', 'have', 'servants'], ['waiting', 'at', 'dinner', 'for', 'a', 'hot', 'plate', 'at', 'that', 'the', 'editor', 'turned', 'to', 'his'], ['knife', 'and', 'fork', 'with', 'a', 'grunt', 'and', 'the', 'silent', 'man', 'followed', 'suit', 'the'], ['dinner', 'was', 'resumed', 'conversation', 'was', 'exclamatory', 'for', 'a', 'little', 'while'], ['with', 'gaps', 'of', 'wonderment', 'and', 'then', 'the', 'editor', 'got', 'fervent', 'in', 'his'], ['curiosity', 'does', 'our', 'friend', 'eke', 'out', 'his', 'modest', 'income', 'with', 'a'], ['crossing', 'or', 'has', 'he', 'his', 'nebuchadnezzar', 'phases', 'he', 'inquired', 'i', 'feel'], ['assured', 'it', 's', 'this', 'business', 'of', 'the', 'time', 'machine', 'i', 'said', 'and', 'took', 'up'], ['the', 'psychologist', 's', 'account', 'of', 'our', 'previous', 'meeting', 'the', 'new', 'guests'], ['were', 'frankly', 'incredulous', 'the', 'editor', 'raised', 'objections', 'what', 'was'], ['this', 'time', 'travelling', 'a', 'man', 'couldn', 't', 'cover', 'himself', 'with', 'dust', 'by'], ['rolling', 'in', 'a', 'paradox', 'could', 'he', 'and', 'then', 'as', 'the', 'idea', 'came', 'home', 'to'], ['him', 'he', 'resorted', 'to', 'caricature', 'hadn', 't', 'they', 'any', 'clothes', 'brushes', 'in'], ['the', 'future', 'the', 'journalist', 'too', 'would', 'not', 'believe', 'at', 'any', 'price', 'and'], ['joined', 'the', 'editor', 'in', 'the', 'easy', 'work', 'of', 'heaping', 'ridicule', 'on', 'the', 'whole'], ['thing', 'they', 'were', 'both', 'the', 'new', 'kind', 'of', 'journalist', 'very', 'joyous'], ['irreverent', 'young', 'men', 'our', 'special', 'correspondent', 'in', 'the', 'day'], ['after', 'to', 'morrow', 'reports', 'the', 'journalist', 'was', 'saying', 'or', 'rather'], ['shouting', 'when', 'the', 'time', 'traveller', 'came', 'back', 'he', 'was', 'dressed', 'in'], ['ordinary', 'evening', 'clothes', 'and', 'nothing', 'save', 'his', 'haggard', 'look', 'remained'], ['of', 'the', 'change', 'that', 'had', 'startled', 'me'], [], ['i', 'say', 'said', 'the', 'editor', 'hilariously', 'these', 'chaps', 'here', 'say', 'you', 'have'], ['been', 'travelling', 'into', 'the', 'middle', 'of', 'next', 'week', 'tell', 'us', 'all', 'about'], ['little', 'rosebery', 'will', 'you', 'what', 'will', 'you', 'take', 'for', 'the', 'lot'], [], ['the', 'time', 'traveller', 'came', 'to', 'the', 'place', 'reserved', 'for', 'him', 'without', 'a'], ['word', 'he', 'smiled', 'quietly', 'in', 'his', 'old', 'way', 'where', 's', 'my', 'mutton', 'he'], ['said', 'what', 'a', 'treat', 'it', 'is', 'to', 'stick', 'a', 'fork', 'into', 'meat', 'again'], [], ['story', 'cried', 'the', 'editor'], [], ['story', 'be', 'damned', 'said', 'the', 'time', 'traveller', 'i', 'want', 'something', 'to'], ['eat', 'i', 'won', 't', 'say', 'a', 'word', 'until', 'i', 'get', 'some', 'peptone', 'into', 'my', 'arteries'], ['thanks', 'and', 'the', 'salt'], [], ['one', 'word', 'said', 'i', 'have', 'you', 'been', 'time', 'travelling'], [], ['yes', 'said', 'the', 'time', 'traveller', 'with', 'his', 'mouth', 'full', 'nodding', 'his'], ['head'], [], ['i', 'd', 'give', 'a', 'shilling', 'a', 'line', 'for', 'a', 'verbatim', 'note', 'said', 'the', 'editor'], ['the', 'time', 'traveller', 'pushed', 'his', 'glass', 'towards', 'the', 'silent', 'man', 'and', 'rang'], ['it', 'with', 'his', 'fingernail', 'at', 'which', 'the', 'silent', 'man', 'who', 'had', 'been'], ['staring', 'at', 'his', 'face', 'started', 'convulsively', 'and', 'poured', 'him', 'wine'], ['the', 'rest', 'of', 'the', 'dinner', 'was', 'uncomfortable', 'for', 'my', 'own', 'part', 'sudden'], ['questions', 'kept', 'on', 'rising', 'to', 'my', 'lips', 'and', 'i', 'dare', 'say', 'it', 'was', 'the', 'same'], ['with', 'the', 'others', 'the', 'journalist', 'tried', 'to', 'relieve', 'the', 'tension', 'by'], ['telling', 'anecdotes', 'of', 'hettie', 'potter', 'the', 'time', 'traveller', 'devoted', 'his'], ['attention', 'to', 'his', 'dinner', 'and', 'displayed', 'the', 'appetite', 'of', 'a', 'tramp'], ['the', 'medical', 'man', 'smoked', 'a', 'cigarette', 'and', 'watched', 'the', 'time', 'traveller'], ['through', 'his', 'eyelashes', 'the', 'silent', 'man', 'seemed', 'even', 'more', 'clumsy', 'than'], ['usual', 'and', 'drank', 'champagne', 'with', 'regularity', 'and', 'determination', 'out', 'of'], ['sheer', 'nervousness', 'at', 'last', 'the', 'time', 'traveller', 'pushed', 'his', 'plate', 'away'], ['and', 'looked', 'round', 'us', 'i', 'suppose', 'i', 'must', 'apologize', 'he', 'said', 'i', 'was'], ['simply', 'starving', 'i', 've', 'had', 'a', 'most', 'amazing', 'time', 'he', 'reached', 'out', 'his'], ['hand', 'for', 'a', 'cigar', 'and', 'cut', 'the', 'end', 'but', 'come', 'into', 'the', 'smoking', 'room'], ['it', 's', 'too', 'long', 'a', 'story', 'to', 'tell', 'over', 'greasy', 'plates', 'and', 'ringing', 'the'], ['bell', 'in', 'passing', 'he', 'led', 'the', 'way', 'into', 'the', 'adjoining', 'room'], [], ['you', 'have', 'told', 'blank', 'and', 'dash', 'and', 'chose', 'about', 'the', 'machine', 'he'], ['said', 'to', 'me', 'leaning', 'back', 'in', 'his', 'easy', 'chair', 'and', 'naming', 'the', 'three', 'new'], ['guests'], [], ['but', 'the', 'thing', 's', 'a', 'mere', 'paradox', 'said', 'the', 'editor'], [], ['i', 'can', 't', 'argue', 'to', 'night', 'i', 'don', 't', 'mind', 'telling', 'you', 'the', 'story', 'but'], ['i', 'can', 't', 'argue', 'i', 'will', 'he', 'went', 'on', 'tell', 'you', 'the', 'story', 'of', 'what'], ['has', 'happened', 'to', 'me', 'if', 'you', 'like', 'but', 'you', 'must', 'refrain', 'from'], ['interruptions', 'i', 'want', 'to', 'tell', 'it', 'badly', 'most', 'of', 'it', 'will', 'sound', 'like'], ['lying', 'so', 'be', 'it', 'it', 's', 'true', 'every', 'word', 'of', 'it', 'all', 'the', 'same', 'i', 'was', 'in'], ['my', 'laboratory', 'at', 'four', 'o', 'clock', 'and', 'since', 'then', 'i', 've', 'lived', 'eight'], ['days', 'such', 'days', 'as', 'no', 'human', 'being', 'ever', 'lived', 'before', 'i', 'm', 'nearly'], ['worn', 'out', 'but', 'i', 'shan', 't', 'sleep', 'till', 'i', 've', 'told', 'this', 'thing', 'over', 'to', 'you'], ['then', 'i', 'shall', 'go', 'to', 'bed', 'but', 'no', 'interruptions', 'is', 'it', 'agreed'], [], ['agreed', 'said', 'the', 'editor', 'and', 'the', 'rest', 'of', 'us', 'echoed', 'agreed', 'and'], ['with', 'that', 'the', 'time', 'traveller', 'began', 'his', 'story', 'as', 'i', 'have', 'set', 'it', 'forth'], ['he', 'sat', 'back', 'in', 'his', 'chair', 'at', 'first', 'and', 'spoke', 'like', 'a', 'weary', 'man'], ['afterwards', 'he', 'got', 'more', 'animated', 'in', 'writing', 'it', 'down', 'i', 'feel', 'with', 'only'], ['too', 'much', 'keenness', 'the', 'inadequacy', 'of', 'pen', 'and', 'ink', 'and', 'above', 'all', 'my'], ['own', 'inadequacy', 'to', 'express', 'its', 'quality', 'you', 'read', 'i', 'will', 'suppose'], ['attentively', 'enough', 'but', 'you', 'cannot', 'see', 'the', 'speaker', 's', 'white'], ['sincere', 'face', 'in', 'the', 'bright', 'circle', 'of', 'the', 'little', 'lamp', 'nor', 'hear', 'the'], ['intonation', 'of', 'his', 'voice', 'you', 'cannot', 'know', 'how', 'his', 'expression', 'followed'], ['the', 'turns', 'of', 'his', 'story', 'most', 'of', 'us', 'hearers', 'were', 'in', 'shadow', 'for', 'the'], ['candles', 'in', 'the', 'smoking', 'room', 'had', 'not', 'been', 'lighted', 'and', 'only', 'the', 'face'], ['of', 'the', 'journalist', 'and', 'the', 'legs', 'of', 'the', 'silent', 'man', 'from', 'the', 'knees'], ['downward', 'were', 'illuminated', 'at', 'first', 'we', 'glanced', 'now', 'and', 'again', 'at', 'each'], ['other', 'after', 'a', 'time', 'we', 'ceased', 'to', 'do', 'that', 'and', 'looked', 'only', 'at', 'the'], ['time', 'traveller', 's', 'face'], [], [], [], [], ['iii'], [], [], ['i', 'told', 'some', 'of', 'you', 'last', 'thursday', 'of', 'the', 'principles', 'of', 'the', 'time'], ['machine', 'and', 'showed', 'you', 'the', 'actual', 'thing', 'itself', 'incomplete', 'in', 'the'], ['workshop', 'there', 'it', 'is', 'now', 'a', 'little', 'travel', 'worn', 'truly', 'and', 'one', 'of'], ['the', 'ivory', 'bars', 'is', 'cracked', 'and', 'a', 'brass', 'rail', 'bent', 'but', 'the', 'rest', 'of'], ['it', 's', 'sound', 'enough', 'i', 'expected', 'to', 'finish', 'it', 'on', 'friday', 'but', 'on', 'friday'], ['when', 'the', 'putting', 'together', 'was', 'nearly', 'done', 'i', 'found', 'that', 'one', 'of', 'the'], ['nickel', 'bars', 'was', 'exactly', 'one', 'inch', 'too', 'short', 'and', 'this', 'i', 'had', 'to', 'get'], ['remade', 'so', 'that', 'the', 'thing', 'was', 'not', 'complete', 'until', 'this', 'morning', 'it'], ['was', 'at', 'ten', 'o', 'clock', 'to', 'day', 'that', 'the', 'first', 'of', 'all', 'time', 'machines', 'began'], ['its', 'career', 'i', 'gave', 'it', 'a', 'last', 'tap', 'tried', 'all', 'the', 'screws', 'again', 'put'], ['one', 'more', 'drop', 'of', 'oil', 'on', 'the', 'quartz', 'rod', 'and', 'sat', 'myself', 'in', 'the'], ['saddle', 'i', 'suppose', 'a', 'suicide', 'who', 'holds', 'a', 'pistol', 'to', 'his', 'skull', 'feels'], ['much', 'the', 'same', 'wonder', 'at', 'what', 'will', 'come', 'next', 'as', 'i', 'felt', 'then', 'i', 'took'], ['the', 'starting', 'lever', 'in', 'one', 'hand', 'and', 'the', 'stopping', 'one', 'in', 'the', 'other'], ['pressed', 'the', 'first', 'and', 'almost', 'immediately', 'the', 'second', 'i', 'seemed', 'to'], ['reel', 'i', 'felt', 'a', 'nightmare', 'sensation', 'of', 'falling', 'and', 'looking', 'round'], ['i', 'saw', 'the', 'laboratory', 'exactly', 'as', 'before', 'had', 'anything', 'happened', 'for'], ['a', 'moment', 'i', 'suspected', 'that', 'my', 'intellect', 'had', 'tricked', 'me', 'then', 'i', 'noted'], ['the', 'clock', 'a', 'moment', 'before', 'as', 'it', 'seemed', 'it', 'had', 'stood', 'at', 'a', 'minute'], ['or', 'so', 'past', 'ten', 'now', 'it', 'was', 'nearly', 'half', 'past', 'three'], [], ['i', 'drew', 'a', 'breath', 'set', 'my', 'teeth', 'gripped', 'the', 'starting', 'lever', 'with', 'both'], ['hands', 'and', 'went', 'off', 'with', 'a', 'thud', 'the', 'laboratory', 'got', 'hazy', 'and', 'went'], ['dark', 'mrs', 'watchett', 'came', 'in', 'and', 'walked', 'apparently', 'without', 'seeing'], ['me', 'towards', 'the', 'garden', 'door', 'i', 'suppose', 'it', 'took', 'her', 'a', 'minute', 'or', 'so', 'to'], ['traverse', 'the', 'place', 'but', 'to', 'me', 'she', 'seemed', 'to', 'shoot', 'across', 'the', 'room'], ['like', 'a', 'rocket', 'i', 'pressed', 'the', 'lever', 'over', 'to', 'its', 'extreme', 'position', 'the'], ['night', 'came', 'like', 'the', 'turning', 'out', 'of', 'a', 'lamp', 'and', 'in', 'another', 'moment'], ['came', 'to', 'morrow', 'the', 'laboratory', 'grew', 'faint', 'and', 'hazy', 'then', 'fainter'], ['and', 'ever', 'fainter', 'to', 'morrow', 'night', 'came', 'black', 'then', 'day', 'again', 'night'], ['again', 'day', 'again', 'faster', 'and', 'faster', 'still', 'an', 'eddying', 'murmur', 'filled'], ['my', 'ears', 'and', 'a', 'strange', 'dumb', 'confusedness', 'descended', 'on', 'my', 'mind'], [], ['i', 'am', 'afraid', 'i', 'cannot', 'convey', 'the', 'peculiar', 'sensations', 'of', 'time'], ['travelling', 'they', 'are', 'excessively', 'unpleasant', 'there', 'is', 'a', 'feeling'], ['exactly', 'like', 'that', 'one', 'has', 'upon', 'a', 'switchback', 'of', 'a', 'helpless', 'headlong'], ['motion', 'i', 'felt', 'the', 'same', 'horrible', 'anticipation', 'too', 'of', 'an', 'imminent'], ['smash', 'as', 'i', 'put', 'on', 'pace', 'night', 'followed', 'day', 'like', 'the', 'flapping', 'of', 'a'], ['black', 'wing', 'the', 'dim', 'suggestion', 'of', 'the', 'laboratory', 'seemed', 'presently', 'to'], ['fall', 'away', 'from', 'me', 'and', 'i', 'saw', 'the', 'sun', 'hopping', 'swiftly', 'across', 'the', 'sky'], ['leaping', 'it', 'every', 'minute', 'and', 'every', 'minute', 'marking', 'a', 'day', 'i', 'supposed'], ['the', 'laboratory', 'had', 'been', 'destroyed', 'and', 'i', 'had', 'come', 'into', 'the', 'open', 'air'], ['i', 'had', 'a', 'dim', 'impression', 'of', 'scaffolding', 'but', 'i', 'was', 'already', 'going', 'too'], ['fast', 'to', 'be', 'conscious', 'of', 'any', 'moving', 'things', 'the', 'slowest', 'snail', 'that'], ['ever', 'crawled', 'dashed', 'by', 'too', 'fast', 'for', 'me', 'the', 'twinkling', 'succession', 'of'], ['darkness', 'and', 'light', 'was', 'excessively', 'painful', 'to', 'the', 'eye', 'then', 'in', 'the'], ['intermittent', 'darknesses', 'i', 'saw', 'the', 'moon', 'spinning', 'swiftly', 'through', 'her'], ['quarters', 'from', 'new', 'to', 'full', 'and', 'had', 'a', 'faint', 'glimpse', 'of', 'the', 'circling'], ['stars', 'presently', 'as', 'i', 'went', 'on', 'still', 'gaining', 'velocity', 'the'], ['palpitation', 'of', 'night', 'and', 'day', 'merged', 'into', 'one', 'continuous', 'greyness'], ['the', 'sky', 'took', 'on', 'a', 'wonderful', 'deepness', 'of', 'blue', 'a', 'splendid', 'luminous'], ['color', 'like', 'that', 'of', 'early', 'twilight', 'the', 'jerking', 'sun', 'became', 'a', 'streak'], ['of', 'fire', 'a', 'brilliant', 'arch', 'in', 'space', 'the', 'moon', 'a', 'fainter', 'fluctuating'], ['band', 'and', 'i', 'could', 'see', 'nothing', 'of', 'the', 'stars', 'save', 'now', 'and', 'then', 'a'], ['brighter', 'circle', 'flickering', 'in', 'the', 'blue'], [], ['the', 'landscape', 'was', 'misty', 'and', 'vague', 'i', 'was', 'still', 'on', 'the', 'hill', 'side'], ['upon', 'which', 'this', 'house', 'now', 'stands', 'and', 'the', 'shoulder', 'rose', 'above', 'me'], ['grey', 'and', 'dim', 'i', 'saw', 'trees', 'growing', 'and', 'changing', 'like', 'puffs', 'of', 'vapour'], ['now', 'brown', 'now', 'green', 'they', 'grew', 'spread', 'shivered', 'and', 'passed', 'away'], ['i', 'saw', 'huge', 'buildings', 'rise', 'up', 'faint', 'and', 'fair', 'and', 'pass', 'like', 'dreams'], ['the', 'whole', 'surface', 'of', 'the', 'earth', 'seemed', 'changed', 'melting', 'and', 'flowing'], ['under', 'my', 'eyes', 'the', 'little', 'hands', 'upon', 'the', 'dials', 'that', 'registered', 'my'], ['speed', 'raced', 'round', 'faster', 'and', 'faster', 'presently', 'i', 'noted', 'that', 'the', 'sun'], ['belt', 'swayed', 'up', 'and', 'down', 'from', 'solstice', 'to', 'solstice', 'in', 'a', 'minute', 'or'], ['less', 'and', 'that', 'consequently', 'my', 'pace', 'was', 'over', 'a', 'year', 'a', 'minute', 'and'], ['minute', 'by', 'minute', 'the', 'white', 'snow', 'flashed', 'across', 'the', 'world', 'and'], ['vanished', 'and', 'was', 'followed', 'by', 'the', 'bright', 'brief', 'green', 'of', 'spring'], [], ['the', 'unpleasant', 'sensations', 'of', 'the', 'start', 'were', 'less', 'poignant', 'now', 'they'], ['merged', 'at', 'last', 'into', 'a', 'kind', 'of', 'hysterical', 'exhilaration', 'i', 'remarked'], ['indeed', 'a', 'clumsy', 'swaying', 'of', 'the', 'machine', 'for', 'which', 'i', 'was', 'unable', 'to'], ['account', 'but', 'my', 'mind', 'was', 'too', 'confused', 'to', 'attend', 'to', 'it', 'so', 'with', 'a'], ['kind', 'of', 'madness', 'growing', 'upon', 'me', 'i', 'flung', 'myself', 'into', 'futurity', 'at'], ['first', 'i', 'scarce', 'thought', 'of', 'stopping', 'scarce', 'thought', 'of', 'anything', 'but'], ['these', 'new', 'sensations', 'but', 'presently', 'a', 'fresh', 'series', 'of', 'impressions'], ['grew', 'up', 'in', 'my', 'mind', 'a', 'certain', 'curiosity', 'and', 'therewith', 'a', 'certain'], ['dread', 'until', 'at', 'last', 'they', 'took', 'complete', 'possession', 'of', 'me', 'what'], ['strange', 'developments', 'of', 'humanity', 'what', 'wonderful', 'advances', 'upon', 'our'], ['rudimentary', 'civilization', 'i', 'thought', 'might', 'not', 'appear', 'when', 'i', 'came', 'to'], ['look', 'nearly', 'into', 'the', 'dim', 'elusive', 'world', 'that', 'raced', 'and', 'fluctuated'], ['before', 'my', 'eyes', 'i', 'saw', 'great', 'and', 'splendid', 'architecture', 'rising', 'about'], ['me', 'more', 'massive', 'than', 'any', 'buildings', 'of', 'our', 'own', 'time', 'and', 'yet', 'as', 'it'], ['seemed', 'built', 'of', 'glimmer', 'and', 'mist', 'i', 'saw', 'a', 'richer', 'green', 'flow', 'up', 'the'], ['hill', 'side', 'and', 'remain', 'there', 'without', 'any', 'wintry', 'intermission', 'even'], ['through', 'the', 'veil', 'of', 'my', 'confusion', 'the', 'earth', 'seemed', 'very', 'fair', 'and', 'so'], ['my', 'mind', 'came', 'round', 'to', 'the', 'business', 'of', 'stopping'], [], ['the', 'peculiar', 'risk', 'lay', 'in', 'the', 'possibility', 'of', 'my', 'finding', 'some'], ['substance', 'in', 'the', 'space', 'which', 'i', 'or', 'the', 'machine', 'occupied', 'so', 'long'], ['as', 'i', 'travelled', 'at', 'a', 'high', 'velocity', 'through', 'time', 'this', 'scarcely'], ['mattered', 'i', 'was', 'so', 'to', 'speak', 'attenuated', 'was', 'slipping', 'like', 'a', 'vapour'], ['through', 'the', 'interstices', 'of', 'intervening', 'substances', 'but', 'to', 'come', 'to'], ['a', 'stop', 'involved', 'the', 'jamming', 'of', 'myself', 'molecule', 'by', 'molecule', 'into'], ['whatever', 'lay', 'in', 'my', 'way', 'meant', 'bringing', 'my', 'atoms', 'into', 'such', 'intimate'], ['contact', 'with', 'those', 'of', 'the', 'obstacle', 'that', 'a', 'profound', 'chemical'], ['reaction', 'possibly', 'a', 'far', 'reaching', 'explosion', 'would', 'result', 'and', 'blow'], ['myself', 'and', 'my', 'apparatus', 'out', 'of', 'all', 'possible', 'dimensions', 'into', 'the'], ['unknown', 'this', 'possibility', 'had', 'occurred', 'to', 'me', 'again', 'and', 'again', 'while', 'i'], ['was', 'making', 'the', 'machine', 'but', 'then', 'i', 'had', 'cheerfully', 'accepted', 'it', 'as', 'an'], ['unavoidable', 'risk', 'one', 'of', 'the', 'risks', 'a', 'man', 'has', 'got', 'to', 'take', 'now', 'the'], ['risk', 'was', 'inevitable', 'i', 'no', 'longer', 'saw', 'it', 'in', 'the', 'same', 'cheerful', 'light'], ['the', 'fact', 'is', 'that', 'insensibly', 'the', 'absolute', 'strangeness', 'of', 'everything'], ['the', 'sickly', 'jarring', 'and', 'swaying', 'of', 'the', 'machine', 'above', 'all', 'the'], ['feeling', 'of', 'prolonged', 'falling', 'had', 'absolutely', 'upset', 'my', 'nerve', 'i', 'told'], ['myself', 'that', 'i', 'could', 'never', 'stop', 'and', 'with', 'a', 'gust', 'of', 'petulance', 'i'], ['resolved', 'to', 'stop', 'forthwith', 'like', 'an', 'impatient', 'fool', 'i', 'lugged', 'over'], ['the', 'lever', 'and', 'incontinently', 'the', 'thing', 'went', 'reeling', 'over', 'and', 'i', 'was'], ['flung', 'headlong', 'through', 'the', 'air'], [], ['there', 'was', 'the', 'sound', 'of', 'a', 'clap', 'of', 'thunder', 'in', 'my', 'ears', 'i', 'may', 'have'], ['been', 'stunned', 'for', 'a', 'moment', 'a', 'pitiless', 'hail', 'was', 'hissing', 'round', 'me'], ['and', 'i', 'was', 'sitting', 'on', 'soft', 'turf', 'in', 'front', 'of', 'the', 'overset', 'machine'], ['everything', 'still', 'seemed', 'grey', 'but', 'presently', 'i', 'remarked', 'that', 'the'], ['confusion', 'in', 'my', 'ears', 'was', 'gone', 'i', 'looked', 'round', 'me', 'i', 'was', 'on', 'what'], ['seemed', 'to', 'be', 'a', 'little', 'lawn', 'in', 'a', 'garden', 'surrounded', 'by', 'rhododendron'], ['bushes', 'and', 'i', 'noticed', 'that', 'their', 'mauve', 'and', 'purple', 'blossoms', 'were'], ['dropping', 'in', 'a', 'shower', 'under', 'the', 'beating', 'of', 'the', 'hail', 'stones', 'the'], ['rebounding', 'dancing', 'hail', 'hung', 'in', 'a', 'cloud', 'over', 'the', 'machine', 'and', 'drove'], ['along', 'the', 'ground', 'like', 'smoke', 'in', 'a', 'moment', 'i', 'was', 'wet', 'to', 'the', 'skin'], ['fine', 'hospitality', 'said', 'i', 'to', 'a', 'man', 'who', 'has', 'travelled', 'innumerable'], ['years', 'to', 'see', 'you'], [], ['presently', 'i', 'thought', 'what', 'a', 'fool', 'i', 'was', 'to', 'get', 'wet', 'i', 'stood', 'up', 'and'], ['looked', 'round', 'me', 'a', 'colossal', 'figure', 'carved', 'apparently', 'in', 'some', 'white'], ['stone', 'loomed', 'indistinctly', 'beyond', 'the', 'rhododendrons', 'through', 'the', 'hazy'], ['downpour', 'but', 'all', 'else', 'of', 'the', 'world', 'was', 'invisible'], [], ['my', 'sensations', 'would', 'be', 'hard', 'to', 'describe', 'as', 'the', 'columns', 'of', 'hail'], ['grew', 'thinner', 'i', 'saw', 'the', 'white', 'figure', 'more', 'distinctly', 'it', 'was', 'very'], ['large', 'for', 'a', 'silver', 'birch', 'tree', 'touched', 'its', 'shoulder', 'it', 'was', 'of', 'white'], ['marble', 'in', 'shape', 'something', 'like', 'a', 'winged', 'sphinx', 'but', 'the', 'wings'], ['instead', 'of', 'being', 'carried', 'vertically', 'at', 'the', 'sides', 'were', 'spread', 'so'], ['that', 'it', 'seemed', 'to', 'hover', 'the', 'pedestal', 'it', 'appeared', 'to', 'me', 'was', 'of'], ['bronze', 'and', 'was', 'thick', 'with', 'verdigris', 'it', 'chanced', 'that', 'the', 'face', 'was'], ['towards', 'me', 'the', 'sightless', 'eyes', 'seemed', 'to', 'watch', 'me', 'there', 'was', 'the'], ['faint', 'shadow', 'of', 'a', 'smile', 'on', 'the', 'lips', 'it', 'was', 'greatly', 'weather', 'worn'], ['and', 'that', 'imparted', 'an', 'unpleasant', 'suggestion', 'of', 'disease', 'i', 'stood'], ['looking', 'at', 'it', 'for', 'a', 'little', 'space', 'half', 'a', 'minute', 'perhaps', 'or', 'half', 'an'], ['hour', 'it', 'seemed', 'to', 'advance', 'and', 'to', 'recede', 'as', 'the', 'hail', 'drove', 'before', 'it'], ['denser', 'or', 'thinner', 'at', 'last', 'i', 'tore', 'my', 'eyes', 'from', 'it', 'for', 'a', 'moment', 'and'], ['saw', 'that', 'the', 'hail', 'curtain', 'had', 'worn', 'threadbare', 'and', 'that', 'the', 'sky', 'was'], ['lightening', 'with', 'the', 'promise', 'of', 'the', 'sun'], [], ['i', 'looked', 'up', 'again', 'at', 'the', 'crouching', 'white', 'shape', 'and', 'the', 'full'], ['temerity', 'of', 'my', 'voyage', 'came', 'suddenly', 'upon', 'me', 'what', 'might', 'appear', 'when'], ['that', 'hazy', 'curtain', 'was', 'altogether', 'withdrawn', 'what', 'might', 'not', 'have'], ['happened', 'to', 'men', 'what', 'if', 'cruelty', 'had', 'grown', 'into', 'a', 'common', 'passion'], ['what', 'if', 'in', 'this', 'interval', 'the', 'race', 'had', 'lost', 'its', 'manliness', 'and', 'had'], ['developed', 'into', 'something', 'inhuman', 'unsympathetic', 'and', 'overwhelmingly'], ['powerful', 'i', 'might', 'seem', 'some', 'old', 'world', 'savage', 'animal', 'only', 'the', 'more'], ['dreadful', 'and', 'disgusting', 'for', 'our', 'common', 'likeness', 'a', 'foul', 'creature', 'to'], ['be', 'incontinently', 'slain'], [], ['already', 'i', 'saw', 'other', 'vast', 'shapes', 'huge', 'buildings', 'with', 'intricate'], ['parapets', 'and', 'tall', 'columns', 'with', 'a', 'wooded', 'hill', 'side', 'dimly', 'creeping'], ['in', 'upon', 'me', 'through', 'the', 'lessening', 'storm', 'i', 'was', 'seized', 'with', 'a', 'panic'], ['fear', 'i', 'turned', 'frantically', 'to', 'the', 'time', 'machine', 'and', 'strove', 'hard', 'to'], ['readjust', 'it', 'as', 'i', 'did', 'so', 'the', 'shafts', 'of', 'the', 'sun', 'smote', 'through', 'the'], ['thunderstorm', 'the', 'grey', 'downpour', 'was', 'swept', 'aside', 'and', 'vanished', 'like'], ['the', 'trailing', 'garments', 'of', 'a', 'ghost', 'above', 'me', 'in', 'the', 'intense', 'blue'], ['of', 'the', 'summer', 'sky', 'some', 'faint', 'brown', 'shreds', 'of', 'cloud', 'whirled', 'into'], ['nothingness', 'the', 'great', 'buildings', 'about', 'me', 'stood', 'out', 'clear', 'and'], ['distinct', 'shining', 'with', 'the', 'wet', 'of', 'the', 'thunderstorm', 'and', 'picked', 'out'], ['in', 'white', 'by', 'the', 'unmelted', 'hailstones', 'piled', 'along', 'their', 'courses', 'i'], ['felt', 'naked', 'in', 'a', 'strange', 'world', 'i', 'felt', 'as', 'perhaps', 'a', 'bird', 'may', 'feel', 'in'], ['the', 'clear', 'air', 'knowing', 'the', 'hawk', 'wings', 'above', 'and', 'will', 'swoop', 'my', 'fear'], ['grew', 'to', 'frenzy', 'i', 'took', 'a', 'breathing', 'space', 'set', 'my', 'teeth', 'and', 'again'], ['grappled', 'fiercely', 'wrist', 'and', 'knee', 'with', 'the', 'machine', 'it', 'gave', 'under'], ['my', 'desperate', 'onset', 'and', 'turned', 'over', 'it', 'struck', 'my', 'chin', 'violently', 'one'], ['hand', 'on', 'the', 'saddle', 'the', 'other', 'on', 'the', 'lever', 'i', 'stood', 'panting', 'heavily'], ['in', 'attitude', 'to', 'mount', 'again'], [], ['but', 'with', 'this', 'recovery', 'of', 'a', 'prompt', 'retreat', 'my', 'courage', 'recovered', 'i'], ['looked', 'more', 'curiously', 'and', 'less', 'fearfully', 'at', 'this', 'world', 'of', 'the', 'remote'], ['future', 'in', 'a', 'circular', 'opening', 'high', 'up', 'in', 'the', 'wall', 'of', 'the', 'nearer'], ['house', 'i', 'saw', 'a', 'group', 'of', 'figures', 'clad', 'in', 'rich', 'soft', 'robes', 'they', 'had'], ['seen', 'me', 'and', 'their', 'faces', 'were', 'directed', 'towards', 'me'], [], ['then', 'i', 'heard', 'voices', 'approaching', 'me', 'coming', 'through', 'the', 'bushes', 'by'], ['the', 'white', 'sphinx', 'were', 'the', 'heads', 'and', 'shoulders', 'of', 'men', 'running', 'one', 'of'], ['these', 'emerged', 'in', 'a', 'pathway', 'leading', 'straight', 'to', 'the', 'little', 'lawn', 'upon'], ['which', 'i', 'stood', 'with', 'my', 'machine', 'he', 'was', 'a', 'slight', 'creature', 'perhaps'], ['four', 'feet', 'high', 'clad', 'in', 'a', 'purple', 'tunic', 'girdled', 'at', 'the', 'waist', 'with', 'a'], ['leather', 'belt', 'sandals', 'or', 'buskins', 'i', 'could', 'not', 'clearly', 'distinguish'], ['which', 'were', 'on', 'his', 'feet', 'his', 'legs', 'were', 'bare', 'to', 'the', 'knees', 'and', 'his'], ['head', 'was', 'bare', 'noticing', 'that', 'i', 'noticed', 'for', 'the', 'first', 'time', 'how', 'warm'], ['the', 'air', 'was'], [], ['he', 'struck', 'me', 'as', 'being', 'a', 'very', 'beautiful', 'and', 'graceful', 'creature', 'but'], ['indescribably', 'frail', 'his', 'flushed', 'face', 'reminded', 'me', 'of', 'the', 'more'], ['beautiful', 'kind', 'of', 'consumptive', 'that', 'hectic', 'beauty', 'of', 'which', 'we', 'used'], ['to', 'hear', 'so', 'much', 'at', 'the', 'sight', 'of', 'him', 'i', 'suddenly', 'regained', 'confidence'], ['i', 'took', 'my', 'hands', 'from', 'the', 'machine'], [], [], [], [], ['iv'], [], [], ['in', 'another', 'moment', 'we', 'were', 'standing', 'face', 'to', 'face', 'i', 'and', 'this', 'fragile'], ['thing', 'out', 'of', 'futurity', 'he', 'came', 'straight', 'up', 'to', 'me', 'and', 'laughed', 'into', 'my'], ['eyes', 'the', 'absence', 'from', 'his', 'bearing', 'of', 'any', 'sign', 'of', 'fear', 'struck', 'me', 'at'], ['once', 'then', 'he', 'turned', 'to', 'the', 'two', 'others', 'who', 'were', 'following', 'him', 'and'], ['spoke', 'to', 'them', 'in', 'a', 'strange', 'and', 'very', 'sweet', 'and', 'liquid', 'tongue'], [], ['there', 'were', 'others', 'coming', 'and', 'presently', 'a', 'little', 'group', 'of', 'perhaps'], ['eight', 'or', 'ten', 'of', 'these', 'exquisite', 'creatures', 'were', 'about', 'me', 'one', 'of', 'them'], ['addressed', 'me', 'it', 'came', 'into', 'my', 'head', 'oddly', 'enough', 'that', 'my', 'voice', 'was'], ['too', 'harsh', 'and', 'deep', 'for', 'them', 'so', 'i', 'shook', 'my', 'head', 'and', 'pointing', 'to', 'my'], ['ears', 'shook', 'it', 'again', 'he', 'came', 'a', 'step', 'forward', 'hesitated', 'and', 'then'], ['touched', 'my', 'hand', 'then', 'i', 'felt', 'other', 'soft', 'little', 'tentacles', 'upon', 'my'], ['back', 'and', 'shoulders', 'they', 'wanted', 'to', 'make', 'sure', 'i', 'was', 'real', 'there', 'was'], ['nothing', 'in', 'this', 'at', 'all', 'alarming', 'indeed', 'there', 'was', 'something', 'in'], ['these', 'pretty', 'little', 'people', 'that', 'inspired', 'confidence', 'a', 'graceful'], ['gentleness', 'a', 'certain', 'childlike', 'ease', 'and', 'besides', 'they', 'looked', 'so'], ['frail', 'that', 'i', 'could', 'fancy', 'myself', 'flinging', 'the', 'whole', 'dozen', 'of', 'them'], ['about', 'like', 'nine', 'pins', 'but', 'i', 'made', 'a', 'sudden', 'motion', 'to', 'warn', 'them', 'when', 'i'], ['saw', 'their', 'little', 'pink', 'hands', 'feeling', 'at', 'the', 'time', 'machine', 'happily'], ['then', 'when', 'it', 'was', 'not', 'too', 'late', 'i', 'thought', 'of', 'a', 'danger', 'i', 'had', 'hitherto'], ['forgotten', 'and', 'reaching', 'over', 'the', 'bars', 'of', 'the', 'machine', 'i', 'unscrewed', 'the'], ['little', 'levers', 'that', 'would', 'set', 'it', 'in', 'motion', 'and', 'put', 'these', 'in', 'my'], ['pocket', 'then', 'i', 'turned', 'again', 'to', 'see', 'what', 'i', 'could', 'do', 'in', 'the', 'way', 'of'], ['communication'], [], ['and', 'then', 'looking', 'more', 'nearly', 'into', 'their', 'features', 'i', 'saw', 'some'], ['further', 'peculiarities', 'in', 'their', 'dresden', 'china', 'type', 'of', 'prettiness'], ['their', 'hair', 'which', 'was', 'uniformly', 'curly', 'came', 'to', 'a', 'sharp', 'end', 'at', 'the'], ['neck', 'and', 'cheek', 'there', 'was', 'not', 'the', 'faintest', 'suggestion', 'of', 'it', 'on', 'the'], ['face', 'and', 'their', 'ears', 'were', 'singularly', 'minute', 'the', 'mouths', 'were', 'small'], ['with', 'bright', 'red', 'rather', 'thin', 'lips', 'and', 'the', 'little', 'chins', 'ran', 'to', 'a'], ['point', 'the', 'eyes', 'were', 'large', 'and', 'mild', 'and', 'this', 'may', 'seem', 'egotism', 'on'], ['my', 'part', 'i', 'fancied', 'even', 'that', 'there', 'was', 'a', 'certain', 'lack', 'of', 'the'], ['interest', 'i', 'might', 'have', 'expected', 'in', 'them'], [], ['as', 'they', 'made', 'no', 'effort', 'to', 'communicate', 'with', 'me', 'but', 'simply', 'stood'], ['round', 'me', 'smiling', 'and', 'speaking', 'in', 'soft', 'cooing', 'notes', 'to', 'each', 'other', 'i'], ['began', 'the', 'conversation', 'i', 'pointed', 'to', 'the', 'time', 'machine', 'and', 'to', 'myself'], ['then', 'hesitating', 'for', 'a', 'moment', 'how', 'to', 'express', 'time', 'i', 'pointed', 'to', 'the'], ['sun', 'at', 'once', 'a', 'quaintly', 'pretty', 'little', 'figure', 'in', 'chequered', 'purple', 'and'], ['white', 'followed', 'my', 'gesture', 'and', 'then', 'astonished', 'me', 'by', 'imitating', 'the'], ['sound', 'of', 'thunder'], [], ['for', 'a', 'moment', 'i', 'was', 'staggered', 'though', 'the', 'import', 'of', 'his', 'gesture', 'was'], ['plain', 'enough', 'the', 'question', 'had', 'come', 'into', 'my', 'mind', 'abruptly', 'were'], ['these', 'creatures', 'fools', 'you', 'may', 'hardly', 'understand', 'how', 'it', 'took', 'me'], ['you', 'see', 'i', 'had', 'always', 'anticipated', 'that', 'the', 'people', 'of', 'the', 'year', 'eight'], ['hundred', 'and', 'two', 'thousand', 'odd', 'would', 'be', 'incredibly', 'in', 'front', 'of', 'us', 'in'], ['knowledge', 'art', 'everything', 'then', 'one', 'of', 'them', 'suddenly', 'asked', 'me', 'a'], ['question', 'that', 'showed', 'him', 'to', 'be', 'on', 'the', 'intellectual', 'level', 'of', 'one', 'of'], ['our', 'five', 'year', 'old', 'children', 'asked', 'me', 'in', 'fact', 'if', 'i', 'had', 'come', 'from'], ['the', 'sun', 'in', 'a', 'thunderstorm', 'it', 'let', 'loose', 'the', 'judgment', 'i', 'had', 'suspended'], ['upon', 'their', 'clothes', 'their', 'frail', 'light', 'limbs', 'and', 'fragile', 'features'], ['a', 'flow', 'of', 'disappointment', 'rushed', 'across', 'my', 'mind', 'for', 'a', 'moment', 'i', 'felt'], ['that', 'i', 'had', 'built', 'the', 'time', 'machine', 'in', 'vain'], [], ['i', 'nodded', 'pointed', 'to', 'the', 'sun', 'and', 'gave', 'them', 'such', 'a', 'vivid', 'rendering'], ['of', 'a', 'thunderclap', 'as', 'startled', 'them', 'they', 'all', 'withdrew', 'a', 'pace', 'or', 'so'], ['and', 'bowed', 'then', 'came', 'one', 'laughing', 'towards', 'me', 'carrying', 'a', 'chain', 'of'], ['beautiful', 'flowers', 'altogether', 'new', 'to', 'me', 'and', 'put', 'it', 'about', 'my', 'neck'], ['the', 'idea', 'was', 'received', 'with', 'melodious', 'applause', 'and', 'presently', 'they'], ['were', 'all', 'running', 'to', 'and', 'fro', 'for', 'flowers', 'and', 'laughingly', 'flinging'], ['them', 'upon', 'me', 'until', 'i', 'was', 'almost', 'smothered', 'with', 'blossom', 'you', 'who'], ['have', 'never', 'seen', 'the', 'like', 'can', 'scarcely', 'imagine', 'what', 'delicate', 'and'], ['wonderful', 'flowers', 'countless', 'years', 'of', 'culture', 'had', 'created', 'then'], ['someone', 'suggested', 'that', 'their', 'plaything', 'should', 'be', 'exhibited', 'in', 'the'], ['nearest', 'building', 'and', 'so', 'i', 'was', 'led', 'past', 'the', 'sphinx', 'of', 'white', 'marble'], ['which', 'had', 'seemed', 'to', 'watch', 'me', 'all', 'the', 'while', 'with', 'a', 'smile', 'at', 'my'], ['astonishment', 'towards', 'a', 'vast', 'grey', 'edifice', 'of', 'fretted', 'stone', 'as', 'i'], ['went', 'with', 'them', 'the', 'memory', 'of', 'my', 'confident', 'anticipations', 'of', 'a'], ['profoundly', 'grave', 'and', 'intellectual', 'posterity', 'came', 'with', 'irresistible'], ['merriment', 'to', 'my', 'mind'], [], ['the', 'building', 'had', 'a', 'huge', 'entry', 'and', 'was', 'altogether', 'of', 'colossal'], ['dimensions', 'i', 'was', 'naturally', 'most', 'occupied', 'with', 'the', 'growing', 'crowd', 'of'], ['little', 'people', 'and', 'with', 'the', 'big', 'open', 'portals', 'that', 'yawned', 'before', 'me'], ['shadowy', 'and', 'mysterious', 'my', 'general', 'impression', 'of', 'the', 'world', 'i', 'saw'], ['over', 'their', 'heads', 'was', 'a', 'tangled', 'waste', 'of', 'beautiful', 'bushes', 'and'], ['flowers', 'a', 'long', 'neglected', 'and', 'yet', 'weedless', 'garden', 'i', 'saw', 'a', 'number'], ['of', 'tall', 'spikes', 'of', 'strange', 'white', 'flowers', 'measuring', 'a', 'foot', 'perhaps'], ['across', 'the', 'spread', 'of', 'the', 'waxen', 'petals', 'they', 'grew', 'scattered', 'as', 'if'], ['wild', 'among', 'the', 'variegated', 'shrubs', 'but', 'as', 'i', 'say', 'i', 'did', 'not', 'examine'], ['them', 'closely', 'at', 'this', 'time', 'the', 'time', 'machine', 'was', 'left', 'deserted', 'on', 'the'], ['turf', 'among', 'the', 'rhododendrons'], [], ['the', 'arch', 'of', 'the', 'doorway', 'was', 'richly', 'carved', 'but', 'naturally', 'i', 'did'], ['not', 'observe', 'the', 'carving', 'very', 'narrowly', 'though', 'i', 'fancied', 'i', 'saw'], ['suggestions', 'of', 'old', 'phoenician', 'decorations', 'as', 'i', 'passed', 'through', 'and'], ['it', 'struck', 'me', 'that', 'they', 'were', 'very', 'badly', 'broken', 'and', 'weather', 'worn'], ['several', 'more', 'brightly', 'clad', 'people', 'met', 'me', 'in', 'the', 'doorway', 'and', 'so', 'we'], ['entered', 'i', 'dressed', 'in', 'dingy', 'nineteenth', 'century', 'garments', 'looking'], ['grotesque', 'enough', 'garlanded', 'with', 'flowers', 'and', 'surrounded', 'by', 'an'], ['eddying', 'mass', 'of', 'bright', 'soft', 'colored', 'robes', 'and', 'shining', 'white', 'limbs'], ['in', 'a', 'melodious', 'whirl', 'of', 'laughter', 'and', 'laughing', 'speech'], [], ['the', 'big', 'doorway', 'opened', 'into', 'a', 'proportionately', 'great', 'hall', 'hung', 'with'], ['brown', 'the', 'roof', 'was', 'in', 'shadow', 'and', 'the', 'windows', 'partially', 'glazed'], ['with', 'coloured', 'glass', 'and', 'partially', 'unglazed', 'admitted', 'a', 'tempered'], ['light', 'the', 'floor', 'was', 'made', 'up', 'of', 'huge', 'blocks', 'of', 'some', 'very', 'hard', 'white'], ['metal', 'not', 'plates', 'nor', 'slabs', 'blocks', 'and', 'it', 'was', 'so', 'much', 'worn', 'as', 'i'], ['judged', 'by', 'the', 'going', 'to', 'and', 'fro', 'of', 'past', 'generations', 'as', 'to', 'be', 'deeply'], ['channelled', 'along', 'the', 'more', 'frequented', 'ways', 'transverse', 'to', 'the', 'length'], ['were', 'innumerable', 'tables', 'made', 'of', 'slabs', 'of', 'polished', 'stone', 'raised'], ['perhaps', 'a', 'foot', 'from', 'the', 'floor', 'and', 'upon', 'these', 'were', 'heaps', 'of', 'fruits'], ['some', 'i', 'recognized', 'as', 'a', 'kind', 'of', 'hypertrophied', 'raspberry', 'and', 'orange'], ['but', 'for', 'the', 'most', 'part', 'they', 'were', 'strange'], [], ['between', 'the', 'tables', 'was', 'scattered', 'a', 'great', 'number', 'of', 'cushions'], ['upon', 'these', 'my', 'conductors', 'seated', 'themselves', 'signing', 'for', 'me', 'to', 'do'], ['likewise', 'with', 'a', 'pretty', 'absence', 'of', 'ceremony', 'they', 'began', 'to', 'eat', 'the'], ['fruit', 'with', 'their', 'hands', 'flinging', 'peel', 'and', 'stalks', 'and', 'so', 'forth', 'into'], ['the', 'round', 'openings', 'in', 'the', 'sides', 'of', 'the', 'tables', 'i', 'was', 'not', 'loath', 'to'], ['follow', 'their', 'example', 'for', 'i', 'felt', 'thirsty', 'and', 'hungry', 'as', 'i', 'did', 'so', 'i'], ['surveyed', 'the', 'hall', 'at', 'my', 'leisure'], [], ['and', 'perhaps', 'the', 'thing', 'that', 'struck', 'me', 'most', 'was', 'its', 'dilapidated', 'look'], ['the', 'stained', 'glass', 'windows', 'which', 'displayed', 'only', 'a', 'geometrical'], ['pattern', 'were', 'broken', 'in', 'many', 'places', 'and', 'the', 'curtains', 'that', 'hung'], ['across', 'the', 'lower', 'end', 'were', 'thick', 'with', 'dust', 'and', 'it', 'caught', 'my', 'eye', 'that'], ['the', 'corner', 'of', 'the', 'marble', 'table', 'near', 'me', 'was', 'fractured', 'nevertheless'], ['the', 'general', 'effect', 'was', 'extremely', 'rich', 'and', 'picturesque', 'there', 'were'], ['perhaps', 'a', 'couple', 'of', 'hundred', 'people', 'dining', 'in', 'the', 'hall', 'and', 'most', 'of'], ['them', 'seated', 'as', 'near', 'to', 'me', 'as', 'they', 'could', 'come', 'were', 'watching', 'me', 'with'], ['interest', 'their', 'little', 'eyes', 'shining', 'over', 'the', 'fruit', 'they', 'were', 'eating'], ['all', 'were', 'clad', 'in', 'the', 'same', 'soft', 'and', 'yet', 'strong', 'silky', 'material'], [], ['fruit', 'by', 'the', 'by', 'was', 'all', 'their', 'diet', 'these', 'people', 'of', 'the', 'remote'], ['future', 'were', 'strict', 'vegetarians', 'and', 'while', 'i', 'was', 'with', 'them', 'in', 'spite'], ['of', 'some', 'carnal', 'cravings', 'i', 'had', 'to', 'be', 'frugivorous', 'also', 'indeed', 'i'], ['found', 'afterwards', 'that', 'horses', 'cattle', 'sheep', 'dogs', 'had', 'followed', 'the'], ['ichthyosaurus', 'into', 'extinction', 'but', 'the', 'fruits', 'were', 'very', 'delightful'], ['one', 'in', 'particular', 'that', 'seemed', 'to', 'be', 'in', 'season', 'all', 'the', 'time', 'i', 'was'], ['there', 'a', 'floury', 'thing', 'in', 'a', 'three', 'sided', 'husk', 'was', 'especially', 'good'], ['and', 'i', 'made', 'it', 'my', 'staple', 'at', 'first', 'i', 'was', 'puzzled', 'by', 'all', 'these', 'strange'], ['fruits', 'and', 'by', 'the', 'strange', 'flowers', 'i', 'saw', 'but', 'later', 'i', 'began', 'to'], ['perceive', 'their', 'import'], [], ['however', 'i', 'am', 'telling', 'you', 'of', 'my', 'fruit', 'dinner', 'in', 'the', 'distant', 'future'], ['now', 'so', 'soon', 'as', 'my', 'appetite', 'was', 'a', 'little', 'checked', 'i', 'determined', 'to'], ['make', 'a', 'resolute', 'attempt', 'to', 'learn', 'the', 'speech', 'of', 'these', 'new', 'men', 'of'], ['mine', 'clearly', 'that', 'was', 'the', 'next', 'thing', 'to', 'do', 'the', 'fruits', 'seemed', 'a'], ['convenient', 'thing', 'to', 'begin', 'upon', 'and', 'holding', 'one', 'of', 'these', 'up', 'i', 'began'], ['a', 'series', 'of', 'interrogative', 'sounds', 'and', 'gestures', 'i', 'had', 'some'], ['considerable', 'difficulty', 'in', 'conveying', 'my', 'meaning', 'at', 'first', 'my', 'efforts'], ['met', 'with', 'a', 'stare', 'of', 'surprise', 'or', 'inextinguishable', 'laughter', 'but'], ['presently', 'a', 'fair', 'haired', 'little', 'creature', 'seemed', 'to', 'grasp', 'my', 'intention'], ['and', 'repeated', 'a', 'name', 'they', 'had', 'to', 'chatter', 'and', 'explain', 'the', 'business'], ['at', 'great', 'length', 'to', 'each', 'other', 'and', 'my', 'first', 'attempts', 'to', 'make', 'the'], ['exquisite', 'little', 'sounds', 'of', 'their', 'language', 'caused', 'an', 'immense', 'amount'], ['of', 'amusement', 'however', 'i', 'felt', 'like', 'a', 'schoolmaster', 'amidst', 'children'], ['and', 'persisted', 'and', 'presently', 'i', 'had', 'a', 'score', 'of', 'noun', 'substantives', 'at'], ['least', 'at', 'my', 'command', 'and', 'then', 'i', 'got', 'to', 'demonstrative', 'pronouns', 'and'], ['even', 'the', 'verb', 'to', 'eat', 'but', 'it', 'was', 'slow', 'work', 'and', 'the', 'little', 'people'], ['soon', 'tired', 'and', 'wanted', 'to', 'get', 'away', 'from', 'my', 'interrogations', 'so', 'i'], ['determined', 'rather', 'of', 'necessity', 'to', 'let', 'them', 'give', 'their', 'lessons', 'in'], ['little', 'doses', 'when', 'they', 'felt', 'inclined', 'and', 'very', 'little', 'doses', 'i', 'found'], ['they', 'were', 'before', 'long', 'for', 'i', 'never', 'met', 'people', 'more', 'indolent', 'or', 'more'], ['easily', 'fatigued'], [], ['a', 'queer', 'thing', 'i', 'soon', 'discovered', 'about', 'my', 'little', 'hosts', 'and', 'that', 'was'], ['their', 'lack', 'of', 'interest', 'they', 'would', 'come', 'to', 'me', 'with', 'eager', 'cries', 'of'], ['astonishment', 'like', 'children', 'but', 'like', 'children', 'they', 'would', 'soon', 'stop'], ['examining', 'me', 'and', 'wander', 'away', 'after', 'some', 'other', 'toy', 'the', 'dinner', 'and', 'my'], ['conversational', 'beginnings', 'ended', 'i', 'noted', 'for', 'the', 'first', 'time', 'that'], ['almost', 'all', 'those', 'who', 'had', 'surrounded', 'me', 'at', 'first', 'were', 'gone', 'it', 'is'], ['odd', 'too', 'how', 'speedily', 'i', 'came', 'to', 'disregard', 'these', 'little', 'people', 'i'], ['went', 'out', 'through', 'the', 'portal', 'into', 'the', 'sunlit', 'world', 'again', 'as', 'soon', 'as'], ['my', 'hunger', 'was', 'satisfied', 'i', 'was', 'continually', 'meeting', 'more', 'of', 'these', 'men'], ['of', 'the', 'future', 'who', 'would', 'follow', 'me', 'a', 'little', 'distance', 'chatter', 'and'], ['laugh', 'about', 'me', 'and', 'having', 'smiled', 'and', 'gesticulated', 'in', 'a', 'friendly'], ['way', 'leave', 'me', 'again', 'to', 'my', 'own', 'devices'], [], ['the', 'calm', 'of', 'evening', 'was', 'upon', 'the', 'world', 'as', 'i', 'emerged', 'from', 'the', 'great'], ['hall', 'and', 'the', 'scene', 'was', 'lit', 'by', 'the', 'warm', 'glow', 'of', 'the', 'setting', 'sun'], ['at', 'first', 'things', 'were', 'very', 'confusing', 'everything', 'was', 'so', 'entirely'], ['different', 'from', 'the', 'world', 'i', 'had', 'known', 'even', 'the', 'flowers', 'the', 'big'], ['building', 'i', 'had', 'left', 'was', 'situated', 'on', 'the', 'slope', 'of', 'a', 'broad', 'river'], ['valley', 'but', 'the', 'thames', 'had', 'shifted', 'perhaps', 'a', 'mile', 'from', 'its', 'present'], ['position', 'i', 'resolved', 'to', 'mount', 'to', 'the', 'summit', 'of', 'a', 'crest', 'perhaps', 'a'], ['mile', 'and', 'a', 'half', 'away', 'from', 'which', 'i', 'could', 'get', 'a', 'wider', 'view', 'of', 'this'], ['our', 'planet', 'in', 'the', 'year', 'eight', 'hundred', 'and', 'two', 'thousand', 'seven', 'hundred'], ['and', 'one', 'a', 'd', 'for', 'that', 'i', 'should', 'explain', 'was', 'the', 'date', 'the', 'little'], ['dials', 'of', 'my', 'machine', 'recorded'], [], ['as', 'i', 'walked', 'i', 'was', 'watching', 'for', 'every', 'impression', 'that', 'could', 'possibly'], ['help', 'to', 'explain', 'the', 'condition', 'of', 'ruinous', 'splendour', 'in', 'which', 'i'], ['found', 'the', 'world', 'for', 'ruinous', 'it', 'was', 'a', 'little', 'way', 'up', 'the', 'hill', 'for'], ['instance', 'was', 'a', 'great', 'heap', 'of', 'granite', 'bound', 'together', 'by', 'masses', 'of'], ['aluminium', 'a', 'vast', 'labyrinth', 'of', 'precipitous', 'walls', 'and', 'crumpled'], ['heaps', 'amidst', 'which', 'were', 'thick', 'heaps', 'of', 'very', 'beautiful', 'pagoda', 'like'], ['plants', 'nettles', 'possibly', 'but', 'wonderfully', 'tinted', 'with', 'brown', 'about'], ['the', 'leaves', 'and', 'incapable', 'of', 'stinging', 'it', 'was', 'evidently', 'the', 'derelict'], ['remains', 'of', 'some', 'vast', 'structure', 'to', 'what', 'end', 'built', 'i', 'could', 'not'], ['determine', 'it', 'was', 'here', 'that', 'i', 'was', 'destined', 'at', 'a', 'later', 'date', 'to', 'have'], ['a', 'very', 'strange', 'experience', 'the', 'first', 'intimation', 'of', 'a', 'still', 'stranger'], ['discovery', 'but', 'of', 'that', 'i', 'will', 'speak', 'in', 'its', 'proper', 'place'], [], ['looking', 'round', 'with', 'a', 'sudden', 'thought', 'from', 'a', 'terrace', 'on', 'which', 'i'], ['rested', 'for', 'a', 'while', 'i', 'realized', 'that', 'there', 'were', 'no', 'small', 'houses', 'to', 'be'], ['seen', 'apparently', 'the', 'single', 'house', 'and', 'possibly', 'even', 'the', 'household'], ['had', 'vanished', 'here', 'and', 'there', 'among', 'the', 'greenery', 'were', 'palace', 'like'], ['buildings', 'but', 'the', 'house', 'and', 'the', 'cottage', 'which', 'form', 'such'], ['characteristic', 'features', 'of', 'our', 'own', 'english', 'landscape', 'had'], ['disappeared'], [], ['communism', 'said', 'i', 'to', 'myself'], [], ['and', 'on', 'the', 'heels', 'of', 'that', 'came', 'another', 'thought', 'i', 'looked', 'at', 'the'], ['half', 'dozen', 'little', 'figures', 'that', 'were', 'following', 'me', 'then', 'in', 'a', 'flash'], ['i', 'perceived', 'that', 'all', 'had', 'the', 'same', 'form', 'of', 'costume', 'the', 'same', 'soft'], ['hairless', 'visage', 'and', 'the', 'same', 'girlish', 'rotundity', 'of', 'limb', 'it', 'may', 'seem'], ['strange', 'perhaps', 'that', 'i', 'had', 'not', 'noticed', 'this', 'before', 'but', 'everything'], ['was', 'so', 'strange', 'now', 'i', 'saw', 'the', 'fact', 'plainly', 'enough', 'in', 'costume', 'and'], ['in', 'all', 'the', 'differences', 'of', 'texture', 'and', 'bearing', 'that', 'now', 'mark', 'off', 'the'], ['sexes', 'from', 'each', 'other', 'these', 'people', 'of', 'the', 'future', 'were', 'alike', 'and'], ['the', 'children', 'seemed', 'to', 'my', 'eyes', 'to', 'be', 'but', 'the', 'miniatures', 'of', 'their'], ['parents', 'i', 'judged', 'then', 'that', 'the', 'children', 'of', 'that', 'time', 'were'], ['extremely', 'precocious', 'physically', 'at', 'least', 'and', 'i', 'found', 'afterwards'], ['abundant', 'verification', 'of', 'my', 'opinion'], [], ['seeing', 'the', 'ease', 'and', 'security', 'in', 'which', 'these', 'people', 'were', 'living', 'i'], ['felt', 'that', 'this', 'close', 'resemblance', 'of', 'the', 'sexes', 'was', 'after', 'all', 'what'], ['one', 'would', 'expect', 'for', 'the', 'strength', 'of', 'a', 'man', 'and', 'the', 'softness', 'of', 'a'], ['woman', 'the', 'institution', 'of', 'the', 'family', 'and', 'the', 'differentiation', 'of'], ['occupations', 'are', 'mere', 'militant', 'necessities', 'of', 'an', 'age', 'of', 'physical'], ['force', 'where', 'population', 'is', 'balanced', 'and', 'abundant', 'much', 'childbearing'], ['becomes', 'an', 'evil', 'rather', 'than', 'a', 'blessing', 'to', 'the', 'state', 'where'], ['violence', 'comes', 'but', 'rarely', 'and', 'off', 'spring', 'are', 'secure', 'there', 'is', 'less'], ['necessity', 'indeed', 'there', 'is', 'no', 'necessity', 'for', 'an', 'efficient', 'family'], ['and', 'the', 'specialization', 'of', 'the', 'sexes', 'with', 'reference', 'to', 'their'], ['children', 's', 'needs', 'disappears', 'we', 'see', 'some', 'beginnings', 'of', 'this', 'even'], ['in', 'our', 'own', 'time', 'and', 'in', 'this', 'future', 'age', 'it', 'was', 'complete', 'this', 'i'], ['must', 'remind', 'you', 'was', 'my', 'speculation', 'at', 'the', 'time', 'later', 'i', 'was', 'to'], ['appreciate', 'how', 'far', 'it', 'fell', 'short', 'of', 'the', 'reality'], [], ['while', 'i', 'was', 'musing', 'upon', 'these', 'things', 'my', 'attention', 'was', 'attracted', 'by'], ['a', 'pretty', 'little', 'structure', 'like', 'a', 'well', 'under', 'a', 'cupola', 'i', 'thought', 'in'], ['a', 'transitory', 'way', 'of', 'the', 'oddness', 'of', 'wells', 'still', 'existing', 'and', 'then'], ['resumed', 'the', 'thread', 'of', 'my', 'speculations', 'there', 'were', 'no', 'large', 'buildings'], ['towards', 'the', 'top', 'of', 'the', 'hill', 'and', 'as', 'my', 'walking', 'powers', 'were', 'evidently'], ['miraculous', 'i', 'was', 'presently', 'left', 'alone', 'for', 'the', 'first', 'time', 'with', 'a'], ['strange', 'sense', 'of', 'freedom', 'and', 'adventure', 'i', 'pushed', 'on', 'up', 'to', 'the', 'crest'], [], ['there', 'i', 'found', 'a', 'seat', 'of', 'some', 'yellow', 'metal', 'that', 'i', 'did', 'not', 'recognize'], ['corroded', 'in', 'places', 'with', 'a', 'kind', 'of', 'pinkish', 'rust', 'and', 'half', 'smothered'], ['in', 'soft', 'moss', 'the', 'arm', 'rests', 'cast', 'and', 'filed', 'into', 'the', 'resemblance', 'of'], ['griffins', 'heads', 'i', 'sat', 'down', 'on', 'it', 'and', 'i', 'surveyed', 'the', 'broad', 'view', 'of'], ['our', 'old', 'world', 'under', 'the', 'sunset', 'of', 'that', 'long', 'day', 'it', 'was', 'as', 'sweet', 'and'], ['fair', 'a', 'view', 'as', 'i', 'have', 'ever', 'seen', 'the', 'sun', 'had', 'already', 'gone', 'below', 'the'], ['horizon', 'and', 'the', 'west', 'was', 'flaming', 'gold', 'touched', 'with', 'some', 'horizontal'], ['bars', 'of', 'purple', 'and', 'crimson', 'below', 'was', 'the', 'valley', 'of', 'the', 'thames', 'in'], ['which', 'the', 'river', 'lay', 'like', 'a', 'band', 'of', 'burnished', 'steel', 'i', 'have', 'already'], ['spoken', 'of', 'the', 'great', 'palaces', 'dotted', 'about', 'among', 'the', 'variegated'], ['greenery', 'some', 'in', 'ruins', 'and', 'some', 'still', 'occupied', 'here', 'and', 'there', 'rose'], ['a', 'white', 'or', 'silvery', 'figure', 'in', 'the', 'waste', 'garden', 'of', 'the', 'earth', 'here', 'and'], ['there', 'came', 'the', 'sharp', 'vertical', 'line', 'of', 'some', 'cupola', 'or', 'obelisk', 'there'], ['were', 'no', 'hedges', 'no', 'signs', 'of', 'proprietary', 'rights', 'no', 'evidences', 'of'], ['agriculture', 'the', 'whole', 'earth', 'had', 'become', 'a', 'garden'], [], ['so', 'watching', 'i', 'began', 'to', 'put', 'my', 'interpretation', 'upon', 'the', 'things', 'i', 'had'], ['seen', 'and', 'as', 'it', 'shaped', 'itself', 'to', 'me', 'that', 'evening', 'my', 'interpretation'], ['was', 'something', 'in', 'this', 'way', 'afterwards', 'i', 'found', 'i', 'had', 'got', 'only', 'a'], ['half', 'truth', 'or', 'only', 'a', 'glimpse', 'of', 'one', 'facet', 'of', 'the', 'truth'], [], ['it', 'seemed', 'to', 'me', 'that', 'i', 'had', 'happened', 'upon', 'humanity', 'upon', 'the', 'wane'], ['the', 'ruddy', 'sunset', 'set', 'me', 'thinking', 'of', 'the', 'sunset', 'of', 'mankind', 'for', 'the'], ['first', 'time', 'i', 'began', 'to', 'realize', 'an', 'odd', 'consequence', 'of', 'the', 'social'], ['effort', 'in', 'which', 'we', 'are', 'at', 'present', 'engaged', 'and', 'yet', 'come', 'to', 'think'], ['it', 'is', 'a', 'logical', 'consequence', 'enough', 'strength', 'is', 'the', 'outcome', 'of', 'need'], ['security', 'sets', 'a', 'premium', 'on', 'feebleness', 'the', 'work', 'of', 'ameliorating', 'the'], ['conditions', 'of', 'life', 'the', 'true', 'civilizing', 'process', 'that', 'makes', 'life', 'more'], ['and', 'more', 'secure', 'had', 'gone', 'steadily', 'on', 'to', 'a', 'climax', 'one', 'triumph', 'of', 'a'], ['united', 'humanity', 'over', 'nature', 'had', 'followed', 'another', 'things', 'that', 'are'], ['now', 'mere', 'dreams', 'had', 'become', 'projects', 'deliberately', 'put', 'in', 'hand', 'and'], ['carried', 'forward', 'and', 'the', 'harvest', 'was', 'what', 'i', 'saw'], [], ['after', 'all', 'the', 'sanitation', 'and', 'the', 'agriculture', 'of', 'to', 'day', 'are', 'still'], ['in', 'the', 'rudimentary', 'stage', 'the', 'science', 'of', 'our', 'time', 'has', 'attacked', 'but'], ['a', 'little', 'department', 'of', 'the', 'field', 'of', 'human', 'disease', 'but', 'even', 'so'], ['it', 'spreads', 'its', 'operations', 'very', 'steadily', 'and', 'persistently', 'our'], ['agriculture', 'and', 'horticulture', 'destroy', 'a', 'weed', 'just', 'here', 'and', 'there', 'and'], ['cultivate', 'perhaps', 'a', 'score', 'or', 'so', 'of', 'wholesome', 'plants', 'leaving', 'the'], ['greater', 'number', 'to', 'fight', 'out', 'a', 'balance', 'as', 'they', 'can', 'we', 'improve', 'our'], ['favourite', 'plants', 'and', 'animals', 'and', 'how', 'few', 'they', 'are', 'gradually', 'by'], ['selective', 'breeding', 'now', 'a', 'new', 'and', 'better', 'peach', 'now', 'a', 'seedless'], ['grape', 'now', 'a', 'sweeter', 'and', 'larger', 'flower', 'now', 'a', 'more', 'convenient', 'breed'], ['of', 'cattle', 'we', 'improve', 'them', 'gradually', 'because', 'our', 'ideals', 'are', 'vague'], ['and', 'tentative', 'and', 'our', 'knowledge', 'is', 'very', 'limited', 'because', 'nature'], ['too', 'is', 'shy', 'and', 'slow', 'in', 'our', 'clumsy', 'hands', 'some', 'day', 'all', 'this', 'will'], ['be', 'better', 'organized', 'and', 'still', 'better', 'that', 'is', 'the', 'drift', 'of', 'the'], ['current', 'in', 'spite', 'of', 'the', 'eddies', 'the', 'whole', 'world', 'will', 'be', 'intelligent'], ['educated', 'and', 'co', 'operating', 'things', 'will', 'move', 'faster', 'and', 'faster'], ['towards', 'the', 'subjugation', 'of', 'nature', 'in', 'the', 'end', 'wisely', 'and', 'carefully'], ['we', 'shall', 'readjust', 'the', 'balance', 'of', 'animal', 'and', 'vegetable', 'life', 'to', 'suit'], ['our', 'human', 'needs'], [], ['this', 'adjustment', 'i', 'say', 'must', 'have', 'been', 'done', 'and', 'done', 'well', 'done'], ['indeed', 'for', 'all', 'time', 'in', 'the', 'space', 'of', 'time', 'across', 'which', 'my', 'machine'], ['had', 'leaped', 'the', 'air', 'was', 'free', 'from', 'gnats', 'the', 'earth', 'from', 'weeds', 'or'], ['fungi', 'everywhere', 'were', 'fruits', 'and', 'sweet', 'and', 'delightful', 'flowers'], ['brilliant', 'butterflies', 'flew', 'hither', 'and', 'thither', 'the', 'ideal', 'of'], ['preventive', 'medicine', 'was', 'attained', 'diseases', 'had', 'been', 'stamped', 'out', 'i'], ['saw', 'no', 'evidence', 'of', 'any', 'contagious', 'diseases', 'during', 'all', 'my', 'stay', 'and', 'i'], ['shall', 'have', 'to', 'tell', 'you', 'later', 'that', 'even', 'the', 'processes', 'of', 'putrefaction'], ['and', 'decay', 'had', 'been', 'profoundly', 'affected', 'by', 'these', 'changes'], [], ['social', 'triumphs', 'too', 'had', 'been', 'effected', 'i', 'saw', 'mankind', 'housed', 'in'], ['splendid', 'shelters', 'gloriously', 'clothed', 'and', 'as', 'yet', 'i', 'had', 'found', 'them'], ['engaged', 'in', 'no', 'toil', 'there', 'were', 'no', 'signs', 'of', 'struggle', 'neither', 'social'], ['nor', 'economical', 'struggle', 'the', 'shop', 'the', 'advertisement', 'traffic', 'all'], ['that', 'commerce', 'which', 'constitutes', 'the', 'body', 'of', 'our', 'world', 'was', 'gone', 'it'], ['was', 'natural', 'on', 'that', 'golden', 'evening', 'that', 'i', 'should', 'jump', 'at', 'the', 'idea', 'of'], ['a', 'social', 'paradise', 'the', 'difficulty', 'of', 'increasing', 'population', 'had', 'been'], ['met', 'i', 'guessed', 'and', 'population', 'had', 'ceased', 'to', 'increase'], [], ['but', 'with', 'this', 'change', 'in', 'condition', 'comes', 'inevitably', 'adaptations', 'to'], ['the', 'change', 'what', 'unless', 'biological', 'science', 'is', 'a', 'mass', 'of', 'errors', 'is'], ['the', 'cause', 'of', 'human', 'intelligence', 'and', 'vigour', 'hardship', 'and', 'freedom'], ['conditions', 'under', 'which', 'the', 'active', 'strong', 'and', 'subtle', 'survive', 'and'], ['the', 'weaker', 'go', 'to', 'the', 'wall', 'conditions', 'that', 'put', 'a', 'premium', 'upon', 'the'], ['loyal', 'alliance', 'of', 'capable', 'men', 'upon', 'self', 'restraint', 'patience', 'and'], ['decision', 'and', 'the', 'institution', 'of', 'the', 'family', 'and', 'the', 'emotions', 'that'], ['arise', 'therein', 'the', 'fierce', 'jealousy', 'the', 'tenderness', 'for', 'offspring'], ['parental', 'self', 'devotion', 'all', 'found', 'their', 'justification', 'and', 'support', 'in'], ['the', 'imminent', 'dangers', 'of', 'the', 'young', 'now', 'where', 'are', 'these', 'imminent'], ['dangers', 'there', 'is', 'a', 'sentiment', 'arising', 'and', 'it', 'will', 'grow', 'against'], ['connubial', 'jealousy', 'against', 'fierce', 'maternity', 'against', 'passion'], ['of', 'all', 'sorts', 'unnecessary', 'things', 'now', 'and', 'things', 'that', 'make', 'us'], ['uncomfortable', 'savage', 'survivals', 'discords', 'in', 'a', 'refined', 'and', 'pleasant'], ['life'], [], ['i', 'thought', 'of', 'the', 'physical', 'slightness', 'of', 'the', 'people', 'their', 'lack', 'of'], ['intelligence', 'and', 'those', 'big', 'abundant', 'ruins', 'and', 'it', 'strengthened', 'my'], ['belief', 'in', 'a', 'perfect', 'conquest', 'of', 'nature', 'for', 'after', 'the', 'battle', 'comes'], ['quiet', 'humanity', 'had', 'been', 'strong', 'energetic', 'and', 'intelligent', 'and', 'had'], ['used', 'all', 'its', 'abundant', 'vitality', 'to', 'alter', 'the', 'conditions', 'under', 'which'], ['it', 'lived', 'and', 'now', 'came', 'the', 'reaction', 'of', 'the', 'altered', 'conditions'], [], ['under', 'the', 'new', 'conditions', 'of', 'perfect', 'comfort', 'and', 'security', 'that'], ['restless', 'energy', 'that', 'with', 'us', 'is', 'strength', 'would', 'become', 'weakness'], ['even', 'in', 'our', 'own', 'time', 'certain', 'tendencies', 'and', 'desires', 'once', 'necessary'], ['to', 'survival', 'are', 'a', 'constant', 'source', 'of', 'failure', 'physical', 'courage', 'and'], ['the', 'love', 'of', 'battle', 'for', 'instance', 'are', 'no', 'great', 'help', 'may', 'even', 'be'], ['hindrances', 'to', 'a', 'civilized', 'man', 'and', 'in', 'a', 'state', 'of', 'physical', 'balance'], ['and', 'security', 'power', 'intellectual', 'as', 'well', 'as', 'physical', 'would', 'be', 'out'], ['of', 'place', 'for', 'countless', 'years', 'i', 'judged', 'there', 'had', 'been', 'no', 'danger', 'of'], ['war', 'or', 'solitary', 'violence', 'no', 'danger', 'from', 'wild', 'beasts', 'no', 'wasting'], ['disease', 'to', 'require', 'strength', 'of', 'constitution', 'no', 'need', 'of', 'toil', 'for'], ['such', 'a', 'life', 'what', 'we', 'should', 'call', 'the', 'weak', 'are', 'as', 'well', 'equipped', 'as'], ['the', 'strong', 'are', 'indeed', 'no', 'longer', 'weak', 'better', 'equipped', 'indeed', 'they'], ['are', 'for', 'the', 'strong', 'would', 'be', 'fretted', 'by', 'an', 'energy', 'for', 'which', 'there'], ['was', 'no', 'outlet', 'no', 'doubt', 'the', 'exquisite', 'beauty', 'of', 'the', 'buildings', 'i', 'saw'], ['was', 'the', 'outcome', 'of', 'the', 'last', 'surgings', 'of', 'the', 'now', 'purposeless', 'energy'], ['of', 'mankind', 'before', 'it', 'settled', 'down', 'into', 'perfect', 'harmony', 'with', 'the'], ['conditions', 'under', 'which', 'it', 'lived', 'the', 'flourish', 'of', 'that', 'triumph', 'which'], ['began', 'the', 'last', 'great', 'peace', 'this', 'has', 'ever', 'been', 'the', 'fate', 'of', 'energy', 'in'], ['security', 'it', 'takes', 'to', 'art', 'and', 'to', 'eroticism', 'and', 'then', 'come', 'languor'], ['and', 'decay'], [], ['even', 'this', 'artistic', 'impetus', 'would', 'at', 'last', 'die', 'away', 'had', 'almost', 'died'], ['in', 'the', 'time', 'i', 'saw', 'to', 'adorn', 'themselves', 'with', 'flowers', 'to', 'dance', 'to'], ['sing', 'in', 'the', 'sunlight', 'so', 'much', 'was', 'left', 'of', 'the', 'artistic', 'spirit', 'and'], ['no', 'more', 'even', 'that', 'would', 'fade', 'in', 'the', 'end', 'into', 'a', 'contented'], ['inactivity', 'we', 'are', 'kept', 'keen', 'on', 'the', 'grindstone', 'of', 'pain', 'and'], ['necessity', 'and', 'it', 'seemed', 'to', 'me', 'that', 'here', 'was', 'that', 'hateful'], ['grindstone', 'broken', 'at', 'last'], [], ['as', 'i', 'stood', 'there', 'in', 'the', 'gathering', 'dark', 'i', 'thought', 'that', 'in', 'this'], ['simple', 'explanation', 'i', 'had', 'mastered', 'the', 'problem', 'of', 'the', 'world', 'mastered'], ['the', 'whole', 'secret', 'of', 'these', 'delicious', 'people', 'possibly', 'the', 'checks', 'they'], ['had', 'devised', 'for', 'the', 'increase', 'of', 'population', 'had', 'succeeded', 'too', 'well'], ['and', 'their', 'numbers', 'had', 'rather', 'diminished', 'than', 'kept', 'stationary'], ['that', 'would', 'account', 'for', 'the', 'abandoned', 'ruins', 'very', 'simple', 'was', 'my'], ['explanation', 'and', 'plausible', 'enough', 'as', 'most', 'wrong', 'theories', 'are'], [], [], [], [], ['v'], [], [], ['as', 'i', 'stood', 'there', 'musing', 'over', 'this', 'too', 'perfect', 'triumph', 'of', 'man', 'the'], ['full', 'moon', 'yellow', 'and', 'gibbous', 'came', 'up', 'out', 'of', 'an', 'overflow', 'of', 'silver'], ['light', 'in', 'the', 'north', 'east', 'the', 'bright', 'little', 'figures', 'ceased', 'to', 'move'], ['about', 'below', 'a', 'noiseless', 'owl', 'flitted', 'by', 'and', 'i', 'shivered', 'with', 'the'], ['chill', 'of', 'the', 'night', 'i', 'determined', 'to', 'descend', 'and', 'find', 'where', 'i', 'could'], ['sleep'], [], ['i', 'looked', 'for', 'the', 'building', 'i', 'knew', 'then', 'my', 'eye', 'travelled', 'along', 'to'], ['the', 'figure', 'of', 'the', 'white', 'sphinx', 'upon', 'the', 'pedestal', 'of', 'bronze', 'growing'], ['distinct', 'as', 'the', 'light', 'of', 'the', 'rising', 'moon', 'grew', 'brighter', 'i', 'could', 'see'], ['the', 'silver', 'birch', 'against', 'it', 'there', 'was', 'the', 'tangle', 'of', 'rhododendron'], ['bushes', 'black', 'in', 'the', 'pale', 'light', 'and', 'there', 'was', 'the', 'little', 'lawn'], ['i', 'looked', 'at', 'the', 'lawn', 'again', 'a', 'queer', 'doubt', 'chilled', 'my', 'complacency'], ['no', 'said', 'i', 'stoutly', 'to', 'myself', 'that', 'was', 'not', 'the', 'lawn'], [], ['but', 'it', 'was', 'the', 'lawn', 'for', 'the', 'white', 'leprous', 'face', 'of', 'the', 'sphinx', 'was'], ['towards', 'it', 'can', 'you', 'imagine', 'what', 'i', 'felt', 'as', 'this', 'conviction', 'came'], ['home', 'to', 'me', 'but', 'you', 'cannot', 'the', 'time', 'machine', 'was', 'gone'], [], ['at', 'once', 'like', 'a', 'lash', 'across', 'the', 'face', 'came', 'the', 'possibility', 'of'], ['losing', 'my', 'own', 'age', 'of', 'being', 'left', 'helpless', 'in', 'this', 'strange', 'new', 'world'], ['the', 'bare', 'thought', 'of', 'it', 'was', 'an', 'actual', 'physical', 'sensation', 'i', 'could'], ['feel', 'it', 'grip', 'me', 'at', 'the', 'throat', 'and', 'stop', 'my', 'breathing', 'in', 'another'], ['moment', 'i', 'was', 'in', 'a', 'passion', 'of', 'fear', 'and', 'running', 'with', 'great', 'leaping'], ['strides', 'down', 'the', 'slope', 'once', 'i', 'fell', 'headlong', 'and', 'cut', 'my', 'face', 'i', 'lost'], ['no', 'time', 'in', 'stanching', 'the', 'blood', 'but', 'jumped', 'up', 'and', 'ran', 'on', 'with', 'a'], ['warm', 'trickle', 'down', 'my', 'cheek', 'and', 'chin', 'all', 'the', 'time', 'i', 'ran', 'i', 'was', 'saying'], ['to', 'myself', 'they', 'have', 'moved', 'it', 'a', 'little', 'pushed', 'it', 'under', 'the', 'bushes'], ['out', 'of', 'the', 'way', 'nevertheless', 'i', 'ran', 'with', 'all', 'my', 'might', 'all', 'the'], ['time', 'with', 'the', 'certainty', 'that', 'sometimes', 'comes', 'with', 'excessive', 'dread'], ['i', 'knew', 'that', 'such', 'assurance', 'was', 'folly', 'knew', 'instinctively', 'that', 'the'], ['machine', 'was', 'removed', 'out', 'of', 'my', 'reach', 'my', 'breath', 'came', 'with', 'pain', 'i'], ['suppose', 'i', 'covered', 'the', 'whole', 'distance', 'from', 'the', 'hill', 'crest', 'to', 'the'], ['little', 'lawn', 'two', 'miles', 'perhaps', 'in', 'ten', 'minutes', 'and', 'i', 'am', 'not', 'a', 'young'], ['man', 'i', 'cursed', 'aloud', 'as', 'i', 'ran', 'at', 'my', 'confident', 'folly', 'in', 'leaving', 'the'], ['machine', 'wasting', 'good', 'breath', 'thereby', 'i', 'cried', 'aloud', 'and', 'none'], ['answered', 'not', 'a', 'creature', 'seemed', 'to', 'be', 'stirring', 'in', 'that', 'moonlit'], ['world'], [], ['when', 'i', 'reached', 'the', 'lawn', 'my', 'worst', 'fears', 'were', 'realized', 'not', 'a', 'trace'], ['of', 'the', 'thing', 'was', 'to', 'be', 'seen', 'i', 'felt', 'faint', 'and', 'cold', 'when', 'i', 'faced', 'the'], ['empty', 'space', 'among', 'the', 'black', 'tangle', 'of', 'bushes', 'i', 'ran', 'round', 'it'], ['furiously', 'as', 'if', 'the', 'thing', 'might', 'be', 'hidden', 'in', 'a', 'corner', 'and', 'then'], ['stopped', 'abruptly', 'with', 'my', 'hands', 'clutching', 'my', 'hair', 'above', 'me', 'towered'], ['the', 'sphinx', 'upon', 'the', 'bronze', 'pedestal', 'white', 'shining', 'leprous', 'in'], ['the', 'light', 'of', 'the', 'rising', 'moon', 'it', 'seemed', 'to', 'smile', 'in', 'mockery', 'of', 'my'], ['dismay'], [], ['i', 'might', 'have', 'consoled', 'myself', 'by', 'imagining', 'the', 'little', 'people', 'had', 'put'], ['the', 'mechanism', 'in', 'some', 'shelter', 'for', 'me', 'had', 'i', 'not', 'felt', 'assured', 'of'], ['their', 'physical', 'and', 'intellectual', 'inadequacy', 'that', 'is', 'what', 'dismayed'], ['me', 'the', 'sense', 'of', 'some', 'hitherto', 'unsuspected', 'power', 'through', 'whose'], ['intervention', 'my', 'invention', 'had', 'vanished', 'yet', 'for', 'one', 'thing', 'i', 'felt'], ['assured', 'unless', 'some', 'other', 'age', 'had', 'produced', 'its', 'exact', 'duplicate'], ['the', 'machine', 'could', 'not', 'have', 'moved', 'in', 'time', 'the', 'attachment', 'of', 'the'], ['levers', 'i', 'will', 'show', 'you', 'the', 'method', 'later', 'prevented', 'any', 'one', 'from'], ['tampering', 'with', 'it', 'in', 'that', 'way', 'when', 'they', 'were', 'removed', 'it', 'had', 'moved'], ['and', 'was', 'hid', 'only', 'in', 'space', 'but', 'then', 'where', 'could', 'it', 'be'], [], ['i', 'think', 'i', 'must', 'have', 'had', 'a', 'kind', 'of', 'frenzy', 'i', 'remember', 'running'], ['violently', 'in', 'and', 'out', 'among', 'the', 'moonlit', 'bushes', 'all', 'round', 'the', 'sphinx'], ['and', 'startling', 'some', 'white', 'animal', 'that', 'in', 'the', 'dim', 'light', 'i', 'took', 'for', 'a'], ['small', 'deer', 'i', 'remember', 'too', 'late', 'that', 'night', 'beating', 'the', 'bushes'], ['with', 'my', 'clenched', 'fist', 'until', 'my', 'knuckles', 'were', 'gashed', 'and', 'bleeding'], ['from', 'the', 'broken', 'twigs', 'then', 'sobbing', 'and', 'raving', 'in', 'my', 'anguish', 'of'], ['mind', 'i', 'went', 'down', 'to', 'the', 'great', 'building', 'of', 'stone', 'the', 'big', 'hall', 'was'], ['dark', 'silent', 'and', 'deserted', 'i', 'slipped', 'on', 'the', 'uneven', 'floor', 'and', 'fell'], ['over', 'one', 'of', 'the', 'malachite', 'tables', 'almost', 'breaking', 'my', 'shin', 'i', 'lit', 'a'], ['match', 'and', 'went', 'on', 'past', 'the', 'dusty', 'curtains', 'of', 'which', 'i', 'have', 'told', 'you'], [], ['there', 'i', 'found', 'a', 'second', 'great', 'hall', 'covered', 'with', 'cushions', 'upon'], ['which', 'perhaps', 'a', 'score', 'or', 'so', 'of', 'the', 'little', 'people', 'were', 'sleeping', 'i'], ['have', 'no', 'doubt', 'they', 'found', 'my', 'second', 'appearance', 'strange', 'enough', 'coming'], ['suddenly', 'out', 'of', 'the', 'quiet', 'darkness', 'with', 'inarticulate', 'noises', 'and', 'the'], ['splutter', 'and', 'flare', 'of', 'a', 'match', 'for', 'they', 'had', 'forgotten', 'about', 'matches'], ['where', 'is', 'my', 'time', 'machine', 'i', 'began', 'bawling', 'like', 'an', 'angry', 'child'], ['laying', 'hands', 'upon', 'them', 'and', 'shaking', 'them', 'up', 'together', 'it', 'must', 'have'], ['been', 'very', 'queer', 'to', 'them', 'some', 'laughed', 'most', 'of', 'them', 'looked', 'sorely'], ['frightened', 'when', 'i', 'saw', 'them', 'standing', 'round', 'me', 'it', 'came', 'into', 'my', 'head'], ['that', 'i', 'was', 'doing', 'as', 'foolish', 'a', 'thing', 'as', 'it', 'was', 'possible', 'for', 'me', 'to', 'do'], ['under', 'the', 'circumstances', 'in', 'trying', 'to', 'revive', 'the', 'sensation', 'of', 'fear'], ['for', 'reasoning', 'from', 'their', 'daylight', 'behaviour', 'i', 'thought', 'that', 'fear'], ['must', 'be', 'forgotten'], [], ['abruptly', 'i', 'dashed', 'down', 'the', 'match', 'and', 'knocking', 'one', 'of', 'the', 'people'], ['over', 'in', 'my', 'course', 'went', 'blundering', 'across', 'the', 'big', 'dining', 'hall', 'again'], ['out', 'under', 'the', 'moonlight', 'i', 'heard', 'cries', 'of', 'terror', 'and', 'their', 'little'], ['feet', 'running', 'and', 'stumbling', 'this', 'way', 'and', 'that', 'i', 'do', 'not', 'remember', 'all'], ['i', 'did', 'as', 'the', 'moon', 'crept', 'up', 'the', 'sky', 'i', 'suppose', 'it', 'was', 'the', 'unexpected'], ['nature', 'of', 'my', 'loss', 'that', 'maddened', 'me', 'i', 'felt', 'hopelessly', 'cut', 'off', 'from'], ['my', 'own', 'kind', 'a', 'strange', 'animal', 'in', 'an', 'unknown', 'world', 'i', 'must', 'have', 'raved'], ['to', 'and', 'fro', 'screaming', 'and', 'crying', 'upon', 'god', 'and', 'fate', 'i', 'have', 'a', 'memory'], ['of', 'horrible', 'fatigue', 'as', 'the', 'long', 'night', 'of', 'despair', 'wore', 'away', 'of'], ['looking', 'in', 'this', 'impossible', 'place', 'and', 'that', 'of', 'groping', 'among', 'moon', 'lit'], ['ruins', 'and', 'touching', 'strange', 'creatures', 'in', 'the', 'black', 'shadows', 'at', 'last'], ['of', 'lying', 'on', 'the', 'ground', 'near', 'the', 'sphinx', 'and', 'weeping', 'with', 'absolute'], ['wretchedness', 'i', 'had', 'nothing', 'left', 'but', 'misery', 'then', 'i', 'slept', 'and', 'when'], ['i', 'woke', 'again', 'it', 'was', 'full', 'day', 'and', 'a', 'couple', 'of', 'sparrows', 'were', 'hopping'], ['round', 'me', 'on', 'the', 'turf', 'within', 'reach', 'of', 'my', 'arm'], [], ['i', 'sat', 'up', 'in', 'the', 'freshness', 'of', 'the', 'morning', 'trying', 'to', 'remember', 'how'], ['i', 'had', 'got', 'there', 'and', 'why', 'i', 'had', 'such', 'a', 'profound', 'sense', 'of', 'desertion'], ['and', 'despair', 'then', 'things', 'came', 'clear', 'in', 'my', 'mind', 'with', 'the', 'plain'], ['reasonable', 'daylight', 'i', 'could', 'look', 'my', 'circumstances', 'fairly', 'in', 'the'], ['face', 'i', 'saw', 'the', 'wild', 'folly', 'of', 'my', 'frenzy', 'overnight', 'and', 'i', 'could'], ['reason', 'with', 'myself', 'suppose', 'the', 'worst', 'i', 'said', 'suppose', 'the'], ['machine', 'altogether', 'lost', 'perhaps', 'destroyed', 'it', 'behoves', 'me', 'to', 'be'], ['calm', 'and', 'patient', 'to', 'learn', 'the', 'way', 'of', 'the', 'people', 'to', 'get', 'a', 'clear'], ['idea', 'of', 'the', 'method', 'of', 'my', 'loss', 'and', 'the', 'means', 'of', 'getting', 'materials'], ['and', 'tools', 'so', 'that', 'in', 'the', 'end', 'perhaps', 'i', 'may', 'make', 'another', 'that'], ['would', 'be', 'my', 'only', 'hope', 'perhaps', 'but', 'better', 'than', 'despair', 'and', 'after'], ['all', 'it', 'was', 'a', 'beautiful', 'and', 'curious', 'world'], [], ['but', 'probably', 'the', 'machine', 'had', 'only', 'been', 'taken', 'away', 'still', 'i', 'must'], ['be', 'calm', 'and', 'patient', 'find', 'its', 'hiding', 'place', 'and', 'recover', 'it', 'by', 'force'], ['or', 'cunning', 'and', 'with', 'that', 'i', 'scrambled', 'to', 'my', 'feet', 'and', 'looked', 'about'], ['me', 'wondering', 'where', 'i', 'could', 'bathe', 'i', 'felt', 'weary', 'stiff', 'and'], ['travel', 'soiled', 'the', 'freshness', 'of', 'the', 'morning', 'made', 'me', 'desire', 'an', 'equal'], ['freshness', 'i', 'had', 'exhausted', 'my', 'emotion', 'indeed', 'as', 'i', 'went', 'about'], ['my', 'business', 'i', 'found', 'myself', 'wondering', 'at', 'my', 'intense', 'excitement'], ['overnight', 'i', 'made', 'a', 'careful', 'examination', 'of', 'the', 'ground', 'about', 'the'], ['little', 'lawn', 'i', 'wasted', 'some', 'time', 'in', 'futile', 'questionings', 'conveyed', 'as'], ['well', 'as', 'i', 'was', 'able', 'to', 'such', 'of', 'the', 'little', 'people', 'as', 'came', 'by', 'they'], ['all', 'failed', 'to', 'understand', 'my', 'gestures', 'some', 'were', 'simply', 'stolid', 'some'], ['thought', 'it', 'was', 'a', 'jest', 'and', 'laughed', 'at', 'me', 'i', 'had', 'the', 'hardest', 'task', 'in'], ['the', 'world', 'to', 'keep', 'my', 'hands', 'off', 'their', 'pretty', 'laughing', 'faces', 'it', 'was'], ['a', 'foolish', 'impulse', 'but', 'the', 'devil', 'begotten', 'of', 'fear', 'and', 'blind', 'anger'], ['was', 'ill', 'curbed', 'and', 'still', 'eager', 'to', 'take', 'advantage', 'of', 'my', 'perplexity'], ['the', 'turf', 'gave', 'better', 'counsel', 'i', 'found', 'a', 'groove', 'ripped', 'in', 'it', 'about'], ['midway', 'between', 'the', 'pedestal', 'of', 'the', 'sphinx', 'and', 'the', 'marks', 'of', 'my', 'feet'], ['where', 'on', 'arrival', 'i', 'had', 'struggled', 'with', 'the', 'overturned', 'machine'], ['there', 'were', 'other', 'signs', 'of', 'removal', 'about', 'with', 'queer', 'narrow'], ['footprints', 'like', 'those', 'i', 'could', 'imagine', 'made', 'by', 'a', 'sloth', 'this', 'directed'], ['my', 'closer', 'attention', 'to', 'the', 'pedestal', 'it', 'was', 'as', 'i', 'think', 'i', 'have', 'said'], ['of', 'bronze', 'it', 'was', 'not', 'a', 'mere', 'block', 'but', 'highly', 'decorated', 'with', 'deep'], ['framed', 'panels', 'on', 'either', 'side', 'i', 'went', 'and', 'rapped', 'at', 'these', 'the'], ['pedestal', 'was', 'hollow', 'examining', 'the', 'panels', 'with', 'care', 'i', 'found', 'them'], ['discontinuous', 'with', 'the', 'frames', 'there', 'were', 'no', 'handles', 'or', 'keyholes'], ['but', 'possibly', 'the', 'panels', 'if', 'they', 'were', 'doors', 'as', 'i', 'supposed', 'opened'], ['from', 'within', 'one', 'thing', 'was', 'clear', 'enough', 'to', 'my', 'mind', 'it', 'took', 'no', 'very'], ['great', 'mental', 'effort', 'to', 'infer', 'that', 'my', 'time', 'machine', 'was', 'inside', 'that'], ['pedestal', 'but', 'how', 'it', 'got', 'there', 'was', 'a', 'different', 'problem'], [], ['i', 'saw', 'the', 'heads', 'of', 'two', 'orange', 'clad', 'people', 'coming', 'through', 'the', 'bushes'], ['and', 'under', 'some', 'blossom', 'covered', 'apple', 'trees', 'towards', 'me', 'i', 'turned'], ['smiling', 'to', 'them', 'and', 'beckoned', 'them', 'to', 'me', 'they', 'came', 'and', 'then'], ['pointing', 'to', 'the', 'bronze', 'pedestal', 'i', 'tried', 'to', 'intimate', 'my', 'wish', 'to', 'open'], ['it', 'but', 'at', 'my', 'first', 'gesture', 'towards', 'this', 'they', 'behaved', 'very', 'oddly', 'i'], ['don', 't', 'know', 'how', 'to', 'convey', 'their', 'expression', 'to', 'you', 'suppose', 'you', 'were'], ['to', 'use', 'a', 'grossly', 'improper', 'gesture', 'to', 'a', 'delicate', 'minded', 'woman', 'it', 'is'], ['how', 'she', 'would', 'look', 'they', 'went', 'off', 'as', 'if', 'they', 'had', 'received', 'the', 'last'], ['possible', 'insult', 'i', 'tried', 'a', 'sweet', 'looking', 'little', 'chap', 'in', 'white', 'next'], ['with', 'exactly', 'the', 'same', 'result', 'somehow', 'his', 'manner', 'made', 'me', 'feel'], ['ashamed', 'of', 'myself', 'but', 'as', 'you', 'know', 'i', 'wanted', 'the', 'time', 'machine', 'and'], ['i', 'tried', 'him', 'once', 'more', 'as', 'he', 'turned', 'off', 'like', 'the', 'others', 'my', 'temper'], ['got', 'the', 'better', 'of', 'me', 'in', 'three', 'strides', 'i', 'was', 'after', 'him', 'had', 'him', 'by'], ['the', 'loose', 'part', 'of', 'his', 'robe', 'round', 'the', 'neck', 'and', 'began', 'dragging', 'him'], ['towards', 'the', 'sphinx', 'then', 'i', 'saw', 'the', 'horror', 'and', 'repugnance', 'of', 'his'], ['face', 'and', 'all', 'of', 'a', 'sudden', 'i', 'let', 'him', 'go'], [], ['but', 'i', 'was', 'not', 'beaten', 'yet', 'i', 'banged', 'with', 'my', 'fist', 'at', 'the', 'bronze'], ['panels', 'i', 'thought', 'i', 'heard', 'something', 'stir', 'inside', 'to', 'be', 'explicit'], ['i', 'thought', 'i', 'heard', 'a', 'sound', 'like', 'a', 'chuckle', 'but', 'i', 'must', 'have', 'been'], ['mistaken', 'then', 'i', 'got', 'a', 'big', 'pebble', 'from', 'the', 'river', 'and', 'came', 'and'], ['hammered', 'till', 'i', 'had', 'flattened', 'a', 'coil', 'in', 'the', 'decorations', 'and', 'the'], ['verdigris', 'came', 'off', 'in', 'powdery', 'flakes', 'the', 'delicate', 'little', 'people'], ['must', 'have', 'heard', 'me', 'hammering', 'in', 'gusty', 'outbreaks', 'a', 'mile', 'away', 'on'], ['either', 'hand', 'but', 'nothing', 'came', 'of', 'it', 'i', 'saw', 'a', 'crowd', 'of', 'them', 'upon', 'the'], ['slopes', 'looking', 'furtively', 'at', 'me', 'at', 'last', 'hot', 'and', 'tired', 'i', 'sat', 'down'], ['to', 'watch', 'the', 'place', 'but', 'i', 'was', 'too', 'restless', 'to', 'watch', 'long', 'i', 'am', 'too'], ['occidental', 'for', 'a', 'long', 'vigil', 'i', 'could', 'work', 'at', 'a', 'problem', 'for', 'years'], ['but', 'to', 'wait', 'inactive', 'for', 'twenty', 'four', 'hours', 'that', 'is', 'another', 'matter'], [], ['i', 'got', 'up', 'after', 'a', 'time', 'and', 'began', 'walking', 'aimlessly', 'through', 'the'], ['bushes', 'towards', 'the', 'hill', 'again', 'patience', 'said', 'i', 'to', 'myself', 'if', 'you'], ['want', 'your', 'machine', 'again', 'you', 'must', 'leave', 'that', 'sphinx', 'alone', 'if', 'they'], ['mean', 'to', 'take', 'your', 'machine', 'away', 'it', 's', 'little', 'good', 'your', 'wrecking', 'their'], ['bronze', 'panels', 'and', 'if', 'they', 'don', 't', 'you', 'will', 'get', 'it', 'back', 'as', 'soon', 'as'], ['you', 'can', 'ask', 'for', 'it', 'to', 'sit', 'among', 'all', 'those', 'unknown', 'things', 'before', 'a'], ['puzzle', 'like', 'that', 'is', 'hopeless', 'that', 'way', 'lies', 'monomania', 'face', 'this'], ['world', 'learn', 'its', 'ways', 'watch', 'it', 'be', 'careful', 'of', 'too', 'hasty', 'guesses'], ['at', 'its', 'meaning', 'in', 'the', 'end', 'you', 'will', 'find', 'clues', 'to', 'it', 'all', 'then'], ['suddenly', 'the', 'humour', 'of', 'the', 'situation', 'came', 'into', 'my', 'mind', 'the', 'thought'], ['of', 'the', 'years', 'i', 'had', 'spent', 'in', 'study', 'and', 'toil', 'to', 'get', 'into', 'the', 'future'], ['age', 'and', 'now', 'my', 'passion', 'of', 'anxiety', 'to', 'get', 'out', 'of', 'it', 'i', 'had', 'made'], ['myself', 'the', 'most', 'complicated', 'and', 'the', 'most', 'hopeless', 'trap', 'that', 'ever', 'a'], ['man', 'devised', 'although', 'it', 'was', 'at', 'my', 'own', 'expense', 'i', 'could', 'not', 'help'], ['myself', 'i', 'laughed', 'aloud'], [], ['going', 'through', 'the', 'big', 'palace', 'it', 'seemed', 'to', 'me', 'that', 'the', 'little'], ['people', 'avoided', 'me', 'it', 'may', 'have', 'been', 'my', 'fancy', 'or', 'it', 'may', 'have', 'had'], ['something', 'to', 'do', 'with', 'my', 'hammering', 'at', 'the', 'gates', 'of', 'bronze', 'yet', 'i', 'felt'], ['tolerably', 'sure', 'of', 'the', 'avoidance', 'i', 'was', 'careful', 'however', 'to', 'show', 'no'], ['concern', 'and', 'to', 'abstain', 'from', 'any', 'pursuit', 'of', 'them', 'and', 'in', 'the', 'course'], ['of', 'a', 'day', 'or', 'two', 'things', 'got', 'back', 'to', 'the', 'old', 'footing', 'i', 'made', 'what'], ['progress', 'i', 'could', 'in', 'the', 'language', 'and', 'in', 'addition', 'i', 'pushed', 'my'], ['explorations', 'here', 'and', 'there', 'either', 'i', 'missed', 'some', 'subtle', 'point', 'or'], ['their', 'language', 'was', 'excessively', 'simple', 'almost', 'exclusively', 'composed'], ['of', 'concrete', 'substantives', 'and', 'verbs', 'there', 'seemed', 'to', 'be', 'few', 'if', 'any'], ['abstract', 'terms', 'or', 'little', 'use', 'of', 'figurative', 'language', 'their'], ['sentences', 'were', 'usually', 'simple', 'and', 'of', 'two', 'words', 'and', 'i', 'failed', 'to'], ['convey', 'or', 'understand', 'any', 'but', 'the', 'simplest', 'propositions', 'i', 'determined'], ['to', 'put', 'the', 'thought', 'of', 'my', 'time', 'machine', 'and', 'the', 'mystery', 'of', 'the', 'bronze'], ['doors', 'under', 'the', 'sphinx', 'as', 'much', 'as', 'possible', 'in', 'a', 'corner', 'of', 'memory'], ['until', 'my', 'growing', 'knowledge', 'would', 'lead', 'me', 'back', 'to', 'them', 'in', 'a', 'natural'], ['way', 'yet', 'a', 'certain', 'feeling', 'you', 'may', 'understand', 'tethered', 'me', 'in', 'a'], ['circle', 'of', 'a', 'few', 'miles', 'round', 'the', 'point', 'of', 'my', 'arrival'], [], ['so', 'far', 'as', 'i', 'could', 'see', 'all', 'the', 'world', 'displayed', 'the', 'same', 'exuberant'], ['richness', 'as', 'the', 'thames', 'valley', 'from', 'every', 'hill', 'i', 'climbed', 'i', 'saw', 'the'], ['same', 'abundance', 'of', 'splendid', 'buildings', 'endlessly', 'varied', 'in', 'material'], ['and', 'style', 'the', 'same', 'clustering', 'thickets', 'of', 'evergreens', 'the', 'same'], ['blossom', 'laden', 'trees', 'and', 'tree', 'ferns', 'here', 'and', 'there', 'water', 'shone', 'like'], ['silver', 'and', 'beyond', 'the', 'land', 'rose', 'into', 'blue', 'undulating', 'hills', 'and'], ['so', 'faded', 'into', 'the', 'serenity', 'of', 'the', 'sky', 'a', 'peculiar', 'feature', 'which'], ['presently', 'attracted', 'my', 'attention', 'was', 'the', 'presence', 'of', 'certain'], ['circular', 'wells', 'several', 'as', 'it', 'seemed', 'to', 'me', 'of', 'a', 'very', 'great', 'depth'], ['one', 'lay', 'by', 'the', 'path', 'up', 'the', 'hill', 'which', 'i', 'had', 'followed', 'during', 'my'], ['first', 'walk', 'like', 'the', 'others', 'it', 'was', 'rimmed', 'with', 'bronze', 'curiously'], ['wrought', 'and', 'protected', 'by', 'a', 'little', 'cupola', 'from', 'the', 'rain', 'sitting', 'by'], ['the', 'side', 'of', 'these', 'wells', 'and', 'peering', 'down', 'into', 'the', 'shafted', 'darkness'], ['i', 'could', 'see', 'no', 'gleam', 'of', 'water', 'nor', 'could', 'i', 'start', 'any', 'reflection'], ['with', 'a', 'lighted', 'match', 'but', 'in', 'all', 'of', 'them', 'i', 'heard', 'a', 'certain', 'sound'], ['a', 'thud', 'thud', 'thud', 'like', 'the', 'beating', 'of', 'some', 'big', 'engine', 'and', 'i'], ['discovered', 'from', 'the', 'flaring', 'of', 'my', 'matches', 'that', 'a', 'steady', 'current', 'of'], ['air', 'set', 'down', 'the', 'shafts', 'further', 'i', 'threw', 'a', 'scrap', 'of', 'paper', 'into', 'the'], ['throat', 'of', 'one', 'and', 'instead', 'of', 'fluttering', 'slowly', 'down', 'it', 'was', 'at'], ['once', 'sucked', 'swiftly', 'out', 'of', 'sight'], [], ['after', 'a', 'time', 'too', 'i', 'came', 'to', 'connect', 'these', 'wells', 'with', 'tall', 'towers'], ['standing', 'here', 'and', 'there', 'upon', 'the', 'slopes', 'for', 'above', 'them', 'there', 'was'], ['often', 'just', 'such', 'a', 'flicker', 'in', 'the', 'air', 'as', 'one', 'sees', 'on', 'a', 'hot', 'day', 'above'], ['a', 'sun', 'scorched', 'beach', 'putting', 'things', 'together', 'i', 'reached', 'a', 'strong'], ['suggestion', 'of', 'an', 'extensive', 'system', 'of', 'subterranean', 'ventilation', 'whose'], ['true', 'import', 'it', 'was', 'difficult', 'to', 'imagine', 'i', 'was', 'at', 'first', 'inclined', 'to'], ['associate', 'it', 'with', 'the', 'sanitary', 'apparatus', 'of', 'these', 'people', 'it', 'was', 'an'], ['obvious', 'conclusion', 'but', 'it', 'was', 'absolutely', 'wrong'], [], ['and', 'here', 'i', 'must', 'admit', 'that', 'i', 'learned', 'very', 'little', 'of', 'drains', 'and'], ['bells', 'and', 'modes', 'of', 'conveyance', 'and', 'the', 'like', 'conveniences', 'during', 'my'], ['time', 'in', 'this', 'real', 'future', 'in', 'some', 'of', 'these', 'visions', 'of', 'utopias', 'and'], ['coming', 'times', 'which', 'i', 'have', 'read', 'there', 'is', 'a', 'vast', 'amount', 'of', 'detail'], ['about', 'building', 'and', 'social', 'arrangements', 'and', 'so', 'forth', 'but', 'while'], ['such', 'details', 'are', 'easy', 'enough', 'to', 'obtain', 'when', 'the', 'whole', 'world', 'is'], ['contained', 'in', 'one', 's', 'imagination', 'they', 'are', 'altogether', 'inaccessible', 'to'], ['a', 'real', 'traveller', 'amid', 'such', 'realities', 'as', 'i', 'found', 'here', 'conceive', 'the'], ['tale', 'of', 'london', 'which', 'a', 'negro', 'fresh', 'from', 'central', 'africa', 'would', 'take'], ['back', 'to', 'his', 'tribe', 'what', 'would', 'he', 'know', 'of', 'railway', 'companies', 'of'], ['social', 'movements', 'of', 'telephone', 'and', 'telegraph', 'wires', 'of', 'the', 'parcels'], ['delivery', 'company', 'and', 'postal', 'orders', 'and', 'the', 'like', 'yet', 'we', 'at', 'least'], ['should', 'be', 'willing', 'enough', 'to', 'explain', 'these', 'things', 'to', 'him', 'and', 'even', 'of'], ['what', 'he', 'knew', 'how', 'much', 'could', 'he', 'make', 'his', 'untravelled', 'friend', 'either'], ['apprehend', 'or', 'believe', 'then', 'think', 'how', 'narrow', 'the', 'gap', 'between', 'a', 'negro'], ['and', 'a', 'white', 'man', 'of', 'our', 'own', 'times', 'and', 'how', 'wide', 'the', 'interval', 'between'], ['myself', 'and', 'these', 'of', 'the', 'golden', 'age', 'i', 'was', 'sensible', 'of', 'much', 'which', 'was'], ['unseen', 'and', 'which', 'contributed', 'to', 'my', 'comfort', 'but', 'save', 'for', 'a', 'general'], ['impression', 'of', 'automatic', 'organization', 'i', 'fear', 'i', 'can', 'convey', 'very'], ['little', 'of', 'the', 'difference', 'to', 'your', 'mind'], [], ['in', 'the', 'matter', 'of', 'sepulture', 'for', 'instance', 'i', 'could', 'see', 'no', 'signs', 'of'], ['crematoria', 'nor', 'anything', 'suggestive', 'of', 'tombs', 'but', 'it', 'occurred', 'to', 'me'], ['that', 'possibly', 'there', 'might', 'be', 'cemeteries', 'or', 'crematoria', 'somewhere'], ['beyond', 'the', 'range', 'of', 'my', 'explorings', 'this', 'again', 'was', 'a', 'question', 'i'], ['deliberately', 'put', 'to', 'myself', 'and', 'my', 'curiosity', 'was', 'at', 'first', 'entirely'], ['defeated', 'upon', 'the', 'point', 'the', 'thing', 'puzzled', 'me', 'and', 'i', 'was', 'led', 'to', 'make'], ['a', 'further', 'remark', 'which', 'puzzled', 'me', 'still', 'more', 'that', 'aged', 'and', 'infirm'], ['among', 'this', 'people', 'there', 'were', 'none'], [], ['i', 'must', 'confess', 'that', 'my', 'satisfaction', 'with', 'my', 'first', 'theories', 'of', 'an'], ['automatic', 'civilization', 'and', 'a', 'decadent', 'humanity', 'did', 'not', 'long', 'endure'], ['yet', 'i', 'could', 'think', 'of', 'no', 'other', 'let', 'me', 'put', 'my', 'difficulties', 'the'], ['several', 'big', 'palaces', 'i', 'had', 'explored', 'were', 'mere', 'living', 'places', 'great'], ['dining', 'halls', 'and', 'sleeping', 'apartments', 'i', 'could', 'find', 'no', 'machinery', 'no'], ['appliances', 'of', 'any', 'kind', 'yet', 'these', 'people', 'were', 'clothed', 'in', 'pleasant'], ['fabrics', 'that', 'must', 'at', 'times', 'need', 'renewal', 'and', 'their', 'sandals', 'though'], ['undecorated', 'were', 'fairly', 'complex', 'specimens', 'of', 'metalwork', 'somehow'], ['such', 'things', 'must', 'be', 'made', 'and', 'the', 'little', 'people', 'displayed', 'no', 'vestige'], ['of', 'a', 'creative', 'tendency', 'there', 'were', 'no', 'shops', 'no', 'workshops', 'no', 'sign'], ['of', 'importations', 'among', 'them', 'they', 'spent', 'all', 'their', 'time', 'in', 'playing'], ['gently', 'in', 'bathing', 'in', 'the', 'river', 'in', 'making', 'love', 'in', 'a', 'half', 'playful'], ['fashion', 'in', 'eating', 'fruit', 'and', 'sleeping', 'i', 'could', 'not', 'see', 'how', 'things'], ['were', 'kept', 'going'], [], ['then', 'again', 'about', 'the', 'time', 'machine', 'something', 'i', 'knew', 'not', 'what'], ['had', 'taken', 'it', 'into', 'the', 'hollow', 'pedestal', 'of', 'the', 'white', 'sphinx', 'why', 'for'], ['the', 'life', 'of', 'me', 'i', 'could', 'not', 'imagine', 'those', 'waterless', 'wells', 'too'], ['those', 'flickering', 'pillars', 'i', 'felt', 'i', 'lacked', 'a', 'clue', 'i', 'felt', 'how', 'shall'], ['i', 'put', 'it', 'suppose', 'you', 'found', 'an', 'inscription', 'with', 'sentences', 'here', 'and'], ['there', 'in', 'excellent', 'plain', 'english', 'and', 'interpolated', 'therewith', 'others'], ['made', 'up', 'of', 'words', 'of', 'letters', 'even', 'absolutely', 'unknown', 'to', 'you', 'well'], ['on', 'the', 'third', 'day', 'of', 'my', 'visit', 'that', 'was', 'how', 'the', 'world', 'of', 'eight'], ['hundred', 'and', 'two', 'thousand', 'seven', 'hundred', 'and', 'one', 'presented', 'itself', 'to'], ['me'], [], ['that', 'day', 'too', 'i', 'made', 'a', 'friend', 'of', 'a', 'sort', 'it', 'happened', 'that', 'as', 'i'], ['was', 'watching', 'some', 'of', 'the', 'little', 'people', 'bathing', 'in', 'a', 'shallow', 'one', 'of'], ['them', 'was', 'seized', 'with', 'cramp', 'and', 'began', 'drifting', 'downstream', 'the', 'main'], ['current', 'ran', 'rather', 'swiftly', 'but', 'not', 'too', 'strongly', 'for', 'even', 'a', 'moderate'], ['swimmer', 'it', 'will', 'give', 'you', 'an', 'idea', 'therefore', 'of', 'the', 'strange'], ['deficiency', 'in', 'these', 'creatures', 'when', 'i', 'tell', 'you', 'that', 'none', 'made', 'the'], ['slightest', 'attempt', 'to', 'rescue', 'the', 'weakly', 'crying', 'little', 'thing', 'which'], ['was', 'drowning', 'before', 'their', 'eyes', 'when', 'i', 'realized', 'this', 'i', 'hurriedly'], ['slipped', 'off', 'my', 'clothes', 'and', 'wading', 'in', 'at', 'a', 'point', 'lower', 'down', 'i'], ['caught', 'the', 'poor', 'mite', 'and', 'drew', 'her', 'safe', 'to', 'land', 'a', 'little', 'rubbing', 'of'], ['the', 'limbs', 'soon', 'brought', 'her', 'round', 'and', 'i', 'had', 'the', 'satisfaction', 'of'], ['seeing', 'she', 'was', 'all', 'right', 'before', 'i', 'left', 'her', 'i', 'had', 'got', 'to', 'such', 'a', 'low'], ['estimate', 'of', 'her', 'kind', 'that', 'i', 'did', 'not', 'expect', 'any', 'gratitude', 'from', 'her'], ['in', 'that', 'however', 'i', 'was', 'wrong'], [], ['this', 'happened', 'in', 'the', 'morning', 'in', 'the', 'afternoon', 'i', 'met', 'my', 'little'], ['woman', 'as', 'i', 'believe', 'it', 'was', 'as', 'i', 'was', 'returning', 'towards', 'my', 'centre'], ['from', 'an', 'exploration', 'and', 'she', 'received', 'me', 'with', 'cries', 'of', 'delight', 'and'], ['presented', 'me', 'with', 'a', 'big', 'garland', 'of', 'flowers', 'evidently', 'made', 'for', 'me'], ['and', 'me', 'alone', 'the', 'thing', 'took', 'my', 'imagination', 'very', 'possibly', 'i', 'had'], ['been', 'feeling', 'desolate', 'at', 'any', 'rate', 'i', 'did', 'my', 'best', 'to', 'display', 'my'], ['appreciation', 'of', 'the', 'gift', 'we', 'were', 'soon', 'seated', 'together', 'in', 'a', 'little'], ['stone', 'arbour', 'engaged', 'in', 'conversation', 'chiefly', 'of', 'smiles', 'the'], ['creature', 's', 'friendliness', 'affected', 'me', 'exactly', 'as', 'a', 'child', 's', 'might', 'have'], ['done', 'we', 'passed', 'each', 'other', 'flowers', 'and', 'she', 'kissed', 'my', 'hands', 'i', 'did'], ['the', 'same', 'to', 'hers', 'then', 'i', 'tried', 'talk', 'and', 'found', 'that', 'her', 'name', 'was'], ['weena', 'which', 'though', 'i', 'don', 't', 'know', 'what', 'it', 'meant', 'somehow', 'seemed'], ['appropriate', 'enough', 'that', 'was', 'the', 'beginning', 'of', 'a', 'queer', 'friendship'], ['which', 'lasted', 'a', 'week', 'and', 'ended', 'as', 'i', 'will', 'tell', 'you'], [], ['she', 'was', 'exactly', 'like', 'a', 'child', 'she', 'wanted', 'to', 'be', 'with', 'me', 'always', 'she'], ['tried', 'to', 'follow', 'me', 'everywhere', 'and', 'on', 'my', 'next', 'journey', 'out', 'and', 'about'], ['it', 'went', 'to', 'my', 'heart', 'to', 'tire', 'her', 'down', 'and', 'leave', 'her', 'at', 'last'], ['exhausted', 'and', 'calling', 'after', 'me', 'rather', 'plaintively', 'but', 'the', 'problems'], ['of', 'the', 'world', 'had', 'to', 'be', 'mastered', 'i', 'had', 'not', 'i', 'said', 'to', 'myself', 'come'], ['into', 'the', 'future', 'to', 'carry', 'on', 'a', 'miniature', 'flirtation', 'yet', 'her', 'distress'], ['when', 'i', 'left', 'her', 'was', 'very', 'great', 'her', 'expostulations', 'at', 'the', 'parting'], ['were', 'sometimes', 'frantic', 'and', 'i', 'think', 'altogether', 'i', 'had', 'as', 'much'], ['trouble', 'as', 'comfort', 'from', 'her', 'devotion', 'nevertheless', 'she', 'was', 'somehow'], ['a', 'very', 'great', 'comfort', 'i', 'thought', 'it', 'was', 'mere', 'childish', 'affection', 'that'], ['made', 'her', 'cling', 'to', 'me', 'until', 'it', 'was', 'too', 'late', 'i', 'did', 'not', 'clearly', 'know'], ['what', 'i', 'had', 'inflicted', 'upon', 'her', 'when', 'i', 'left', 'her', 'nor', 'until', 'it', 'was', 'too'], ['late', 'did', 'i', 'clearly', 'understand', 'what', 'she', 'was', 'to', 'me', 'for', 'by', 'merely'], ['seeming', 'fond', 'of', 'me', 'and', 'showing', 'in', 'her', 'weak', 'futile', 'way', 'that', 'she'], ['cared', 'for', 'me', 'the', 'little', 'doll', 'of', 'a', 'creature', 'presently', 'gave', 'my', 'return'], ['to', 'the', 'neighbourhood', 'of', 'the', 'white', 'sphinx', 'almost', 'the', 'feeling', 'of'], ['coming', 'home', 'and', 'i', 'would', 'watch', 'for', 'her', 'tiny', 'figure', 'of', 'white', 'and', 'gold'], ['so', 'soon', 'as', 'i', 'came', 'over', 'the', 'hill'], [], ['it', 'was', 'from', 'her', 'too', 'that', 'i', 'learned', 'that', 'fear', 'had', 'not', 'yet', 'left', 'the'], ['world', 'she', 'was', 'fearless', 'enough', 'in', 'the', 'daylight', 'and', 'she', 'had', 'the'], ['oddest', 'confidence', 'in', 'me', 'for', 'once', 'in', 'a', 'foolish', 'moment', 'i', 'made'], ['threatening', 'grimaces', 'at', 'her', 'and', 'she', 'simply', 'laughed', 'at', 'them', 'but', 'she'], ['dreaded', 'the', 'dark', 'dreaded', 'shadows', 'dreaded', 'black', 'things', 'darkness'], ['to', 'her', 'was', 'the', 'one', 'thing', 'dreadful', 'it', 'was', 'a', 'singularly', 'passionate'], ['emotion', 'and', 'it', 'set', 'me', 'thinking', 'and', 'observing', 'i', 'discovered', 'then'], ['among', 'other', 'things', 'that', 'these', 'little', 'people', 'gathered', 'into', 'the', 'great'], ['houses', 'after', 'dark', 'and', 'slept', 'in', 'droves', 'to', 'enter', 'upon', 'them', 'without', 'a'], ['light', 'was', 'to', 'put', 'them', 'into', 'a', 'tumult', 'of', 'apprehension', 'i', 'never', 'found'], ['one', 'out', 'of', 'doors', 'or', 'one', 'sleeping', 'alone', 'within', 'doors', 'after', 'dark'], ['yet', 'i', 'was', 'still', 'such', 'a', 'blockhead', 'that', 'i', 'missed', 'the', 'lesson', 'of', 'that'], ['fear', 'and', 'in', 'spite', 'of', 'weena', 's', 'distress', 'i', 'insisted', 'upon', 'sleeping', 'away'], ['from', 'these', 'slumbering', 'multitudes'], [], ['it', 'troubled', 'her', 'greatly', 'but', 'in', 'the', 'end', 'her', 'odd', 'affection', 'for', 'me'], ['triumphed', 'and', 'for', 'five', 'of', 'the', 'nights', 'of', 'our', 'acquaintance', 'including'], ['the', 'last', 'night', 'of', 'all', 'she', 'slept', 'with', 'her', 'head', 'pillowed', 'on', 'my', 'arm'], ['but', 'my', 'story', 'slips', 'away', 'from', 'me', 'as', 'i', 'speak', 'of', 'her', 'it', 'must', 'have', 'been'], ['the', 'night', 'before', 'her', 'rescue', 'that', 'i', 'was', 'awakened', 'about', 'dawn', 'i', 'had'], ['been', 'restless', 'dreaming', 'most', 'disagreeably', 'that', 'i', 'was', 'drowned', 'and'], ['that', 'sea', 'anemones', 'were', 'feeling', 'over', 'my', 'face', 'with', 'their', 'soft', 'palps'], ['i', 'woke', 'with', 'a', 'start', 'and', 'with', 'an', 'odd', 'fancy', 'that', 'some', 'greyish', 'animal'], ['had', 'just', 'rushed', 'out', 'of', 'the', 'chamber', 'i', 'tried', 'to', 'get', 'to', 'sleep', 'again'], ['but', 'i', 'felt', 'restless', 'and', 'uncomfortable', 'it', 'was', 'that', 'dim', 'grey', 'hour'], ['when', 'things', 'are', 'just', 'creeping', 'out', 'of', 'darkness', 'when', 'everything', 'is'], ['colourless', 'and', 'clear', 'cut', 'and', 'yet', 'unreal', 'i', 'got', 'up', 'and', 'went', 'down'], ['into', 'the', 'great', 'hall', 'and', 'so', 'out', 'upon', 'the', 'flagstones', 'in', 'front', 'of', 'the'], ['palace', 'i', 'thought', 'i', 'would', 'make', 'a', 'virtue', 'of', 'necessity', 'and', 'see', 'the'], ['sunrise'], [], ['the', 'moon', 'was', 'setting', 'and', 'the', 'dying', 'moonlight', 'and', 'the', 'first', 'pallor'], ['of', 'dawn', 'were', 'mingled', 'in', 'a', 'ghastly', 'half', 'light', 'the', 'bushes', 'were', 'inky'], ['black', 'the', 'ground', 'a', 'sombre', 'grey', 'the', 'sky', 'colourless', 'and', 'cheerless'], ['and', 'up', 'the', 'hill', 'i', 'thought', 'i', 'could', 'see', 'ghosts', 'there', 'several', 'times'], ['as', 'i', 'scanned', 'the', 'slope', 'i', 'saw', 'white', 'figures', 'twice', 'i', 'fancied', 'i', 'saw'], ['a', 'solitary', 'white', 'ape', 'like', 'creature', 'running', 'rather', 'quickly', 'up', 'the'], ['hill', 'and', 'once', 'near', 'the', 'ruins', 'i', 'saw', 'a', 'leash', 'of', 'them', 'carrying', 'some'], ['dark', 'body', 'they', 'moved', 'hastily', 'i', 'did', 'not', 'see', 'what', 'became', 'of', 'them'], ['it', 'seemed', 'that', 'they', 'vanished', 'among', 'the', 'bushes', 'the', 'dawn', 'was', 'still'], ['indistinct', 'you', 'must', 'understand', 'i', 'was', 'feeling', 'that', 'chill'], ['uncertain', 'early', 'morning', 'feeling', 'you', 'may', 'have', 'known', 'i', 'doubted'], ['my', 'eyes'], [], ['as', 'the', 'eastern', 'sky', 'grew', 'brighter', 'and', 'the', 'light', 'of', 'the', 'day', 'came', 'on'], ['and', 'its', 'vivid', 'colouring', 'returned', 'upon', 'the', 'world', 'once', 'more', 'i', 'scanned'], ['the', 'view', 'keenly', 'but', 'i', 'saw', 'no', 'vestige', 'of', 'my', 'white', 'figures', 'they', 'were'], ['mere', 'creatures', 'of', 'the', 'half', 'light', 'they', 'must', 'have', 'been', 'ghosts', 'i'], ['said', 'i', 'wonder', 'whence', 'they', 'dated', 'for', 'a', 'queer', 'notion', 'of', 'grant'], ['allen', 's', 'came', 'into', 'my', 'head', 'and', 'amused', 'me', 'if', 'each', 'generation', 'die', 'and'], ['leave', 'ghosts', 'he', 'argued', 'the', 'world', 'at', 'last', 'will', 'get', 'overcrowded', 'with'], ['them', 'on', 'that', 'theory', 'they', 'would', 'have', 'grown', 'innumerable', 'some', 'eight'], ['hundred', 'thousand', 'years', 'hence', 'and', 'it', 'was', 'no', 'great', 'wonder', 'to', 'see', 'four'], ['at', 'once', 'but', 'the', 'jest', 'was', 'unsatisfying', 'and', 'i', 'was', 'thinking', 'of', 'these'], ['figures', 'all', 'the', 'morning', 'until', 'weena', 's', 'rescue', 'drove', 'them', 'out', 'of', 'my'], ['head', 'i', 'associated', 'them', 'in', 'some', 'indefinite', 'way', 'with', 'the', 'white', 'animal'], ['i', 'had', 'startled', 'in', 'my', 'first', 'passionate', 'search', 'for', 'the', 'time', 'machine'], ['but', 'weena', 'was', 'a', 'pleasant', 'substitute', 'yet', 'all', 'the', 'same', 'they', 'were'], ['soon', 'destined', 'to', 'take', 'far', 'deadlier', 'possession', 'of', 'my', 'mind'], [], ['i', 'think', 'i', 'have', 'said', 'how', 'much', 'hotter', 'than', 'our', 'own', 'was', 'the', 'weather'], ['of', 'this', 'golden', 'age', 'i', 'cannot', 'account', 'for', 'it', 'it', 'may', 'be', 'that', 'the', 'sun'], ['was', 'hotter', 'or', 'the', 'earth', 'nearer', 'the', 'sun', 'it', 'is', 'usual', 'to', 'assume', 'that'], ['the', 'sun', 'will', 'go', 'on', 'cooling', 'steadily', 'in', 'the', 'future', 'but', 'people'], ['unfamiliar', 'with', 'such', 'speculations', 'as', 'those', 'of', 'the', 'younger', 'darwin'], ['forget', 'that', 'the', 'planets', 'must', 'ultimately', 'fall', 'back', 'one', 'by', 'one', 'into'], ['the', 'parent', 'body', 'as', 'these', 'catastrophes', 'occur', 'the', 'sun', 'will', 'blaze'], ['with', 'renewed', 'energy', 'and', 'it', 'may', 'be', 'that', 'some', 'inner', 'planet', 'had'], ['suffered', 'this', 'fate', 'whatever', 'the', 'reason', 'the', 'fact', 'remains', 'that', 'the'], ['sun', 'was', 'very', 'much', 'hotter', 'than', 'we', 'know', 'it'], [], ['well', 'one', 'very', 'hot', 'morning', 'my', 'fourth', 'i', 'think', 'as', 'i', 'was', 'seeking'], ['shelter', 'from', 'the', 'heat', 'and', 'glare', 'in', 'a', 'colossal', 'ruin', 'near', 'the', 'great'], ['house', 'where', 'i', 'slept', 'and', 'fed', 'there', 'happened', 'this', 'strange', 'thing'], ['clambering', 'among', 'these', 'heaps', 'of', 'masonry', 'i', 'found', 'a', 'narrow', 'gallery'], ['whose', 'end', 'and', 'side', 'windows', 'were', 'blocked', 'by', 'fallen', 'masses', 'of', 'stone'], ['by', 'contrast', 'with', 'the', 'brilliancy', 'outside', 'it', 'seemed', 'at', 'first'], ['impenetrably', 'dark', 'to', 'me', 'i', 'entered', 'it', 'groping', 'for', 'the', 'change', 'from'], ['light', 'to', 'blackness', 'made', 'spots', 'of', 'colour', 'swim', 'before', 'me', 'suddenly', 'i'], ['halted', 'spellbound', 'a', 'pair', 'of', 'eyes', 'luminous', 'by', 'reflection', 'against'], ['the', 'daylight', 'without', 'was', 'watching', 'me', 'out', 'of', 'the', 'darkness'], [], ['the', 'old', 'instinctive', 'dread', 'of', 'wild', 'beasts', 'came', 'upon', 'me', 'i', 'clenched'], ['my', 'hands', 'and', 'steadfastly', 'looked', 'into', 'the', 'glaring', 'eyeballs', 'i', 'was'], ['afraid', 'to', 'turn', 'then', 'the', 'thought', 'of', 'the', 'absolute', 'security', 'in', 'which'], ['humanity', 'appeared', 'to', 'be', 'living', 'came', 'to', 'my', 'mind', 'and', 'then', 'i'], ['remembered', 'that', 'strange', 'terror', 'of', 'the', 'dark', 'overcoming', 'my', 'fear', 'to'], ['some', 'extent', 'i', 'advanced', 'a', 'step', 'and', 'spoke', 'i', 'will', 'admit', 'that', 'my'], ['voice', 'was', 'harsh', 'and', 'ill', 'controlled', 'i', 'put', 'out', 'my', 'hand', 'and', 'touched'], ['something', 'soft', 'at', 'once', 'the', 'eyes', 'darted', 'sideways', 'and', 'something'], ['white', 'ran', 'past', 'me', 'i', 'turned', 'with', 'my', 'heart', 'in', 'my', 'mouth', 'and', 'saw', 'a'], ['queer', 'little', 'ape', 'like', 'figure', 'its', 'head', 'held', 'down', 'in', 'a', 'peculiar'], ['manner', 'running', 'across', 'the', 'sunlit', 'space', 'behind', 'me', 'it', 'blundered'], ['against', 'a', 'block', 'of', 'granite', 'staggered', 'aside', 'and', 'in', 'a', 'moment', 'was'], ['hidden', 'in', 'a', 'black', 'shadow', 'beneath', 'another', 'pile', 'of', 'ruined', 'masonry'], [], ['my', 'impression', 'of', 'it', 'is', 'of', 'course', 'imperfect', 'but', 'i', 'know', 'it', 'was', 'a'], ['dull', 'white', 'and', 'had', 'strange', 'large', 'greyish', 'red', 'eyes', 'also', 'that', 'there'], ['was', 'flaxen', 'hair', 'on', 'its', 'head', 'and', 'down', 'its', 'back', 'but', 'as', 'i', 'say', 'it'], ['went', 'too', 'fast', 'for', 'me', 'to', 'see', 'distinctly', 'i', 'cannot', 'even', 'say', 'whether', 'it'], ['ran', 'on', 'all', 'fours', 'or', 'only', 'with', 'its', 'forearms', 'held', 'very', 'low', 'after', 'an'], ['instant', 's', 'pause', 'i', 'followed', 'it', 'into', 'the', 'second', 'heap', 'of', 'ruins', 'i', 'could'], ['not', 'find', 'it', 'at', 'first', 'but', 'after', 'a', 'time', 'in', 'the', 'profound', 'obscurity', 'i'], ['came', 'upon', 'one', 'of', 'those', 'round', 'well', 'like', 'openings', 'of', 'which', 'i', 'have', 'told'], ['you', 'half', 'closed', 'by', 'a', 'fallen', 'pillar', 'a', 'sudden', 'thought', 'came', 'to', 'me'], ['could', 'this', 'thing', 'have', 'vanished', 'down', 'the', 'shaft', 'i', 'lit', 'a', 'match', 'and'], ['looking', 'down', 'i', 'saw', 'a', 'small', 'white', 'moving', 'creature', 'with', 'large'], ['bright', 'eyes', 'which', 'regarded', 'me', 'steadfastly', 'as', 'it', 'retreated', 'it', 'made'], ['me', 'shudder', 'it', 'was', 'so', 'like', 'a', 'human', 'spider', 'it', 'was', 'clambering', 'down'], ['the', 'wall', 'and', 'now', 'i', 'saw', 'for', 'the', 'first', 'time', 'a', 'number', 'of', 'metal', 'foot'], ['and', 'hand', 'rests', 'forming', 'a', 'kind', 'of', 'ladder', 'down', 'the', 'shaft', 'then', 'the'], ['light', 'burned', 'my', 'fingers', 'and', 'fell', 'out', 'of', 'my', 'hand', 'going', 'out', 'as', 'it'], ['dropped', 'and', 'when', 'i', 'had', 'lit', 'another', 'the', 'little', 'monster', 'had'], ['disappeared'], [], ['i', 'do', 'not', 'know', 'how', 'long', 'i', 'sat', 'peering', 'down', 'that', 'well', 'it', 'was', 'not', 'for'], ['some', 'time', 'that', 'i', 'could', 'succeed', 'in', 'persuading', 'myself', 'that', 'the', 'thing', 'i'], ['had', 'seen', 'was', 'human', 'but', 'gradually', 'the', 'truth', 'dawned', 'on', 'me', 'that'], ['man', 'had', 'not', 'remained', 'one', 'species', 'but', 'had', 'differentiated', 'into', 'two'], ['distinct', 'animals', 'that', 'my', 'graceful', 'children', 'of', 'the', 'upper', 'world', 'were'], ['not', 'the', 'sole', 'descendants', 'of', 'our', 'generation', 'but', 'that', 'this', 'bleached'], ['obscene', 'nocturnal', 'thing', 'which', 'had', 'flashed', 'before', 'me', 'was', 'also', 'heir'], ['to', 'all', 'the', 'ages'], [], ['i', 'thought', 'of', 'the', 'flickering', 'pillars', 'and', 'of', 'my', 'theory', 'of', 'an'], ['underground', 'ventilation', 'i', 'began', 'to', 'suspect', 'their', 'true', 'import', 'and'], ['what', 'i', 'wondered', 'was', 'this', 'lemur', 'doing', 'in', 'my', 'scheme', 'of', 'a', 'perfectly'], ['balanced', 'organization', 'how', 'was', 'it', 'related', 'to', 'the', 'indolent', 'serenity'], ['of', 'the', 'beautiful', 'upper', 'worlders', 'and', 'what', 'was', 'hidden', 'down', 'there'], ['at', 'the', 'foot', 'of', 'that', 'shaft', 'i', 'sat', 'upon', 'the', 'edge', 'of', 'the', 'well', 'telling'], ['myself', 'that', 'at', 'any', 'rate', 'there', 'was', 'nothing', 'to', 'fear', 'and', 'that', 'there'], ['i', 'must', 'descend', 'for', 'the', 'solution', 'of', 'my', 'difficulties', 'and', 'withal', 'i'], ['was', 'absolutely', 'afraid', 'to', 'go', 'as', 'i', 'hesitated', 'two', 'of', 'the', 'beautiful'], ['upper', 'world', 'people', 'came', 'running', 'in', 'their', 'amorous', 'sport', 'across', 'the'], ['daylight', 'in', 'the', 'shadow', 'the', 'male', 'pursued', 'the', 'female', 'flinging'], ['flowers', 'at', 'her', 'as', 'he', 'ran'], [], ['they', 'seemed', 'distressed', 'to', 'find', 'me', 'my', 'arm', 'against', 'the', 'overturned'], ['pillar', 'peering', 'down', 'the', 'well', 'apparently', 'it', 'was', 'considered', 'bad', 'form'], ['to', 'remark', 'these', 'apertures', 'for', 'when', 'i', 'pointed', 'to', 'this', 'one', 'and', 'tried'], ['to', 'frame', 'a', 'question', 'about', 'it', 'in', 'their', 'tongue', 'they', 'were', 'still', 'more'], ['visibly', 'distressed', 'and', 'turned', 'away', 'but', 'they', 'were', 'interested', 'by', 'my'], ['matches', 'and', 'i', 'struck', 'some', 'to', 'amuse', 'them', 'i', 'tried', 'them', 'again', 'about'], ['the', 'well', 'and', 'again', 'i', 'failed', 'so', 'presently', 'i', 'left', 'them', 'meaning', 'to'], ['go', 'back', 'to', 'weena', 'and', 'see', 'what', 'i', 'could', 'get', 'from', 'her', 'but', 'my', 'mind', 'was'], ['already', 'in', 'revolution', 'my', 'guesses', 'and', 'impressions', 'were', 'slipping', 'and'], ['sliding', 'to', 'a', 'new', 'adjustment', 'i', 'had', 'now', 'a', 'clue', 'to', 'the', 'import', 'of', 'these'], ['wells', 'to', 'the', 'ventilating', 'towers', 'to', 'the', 'mystery', 'of', 'the', 'ghosts', 'to'], ['say', 'nothing', 'of', 'a', 'hint', 'at', 'the', 'meaning', 'of', 'the', 'bronze', 'gates', 'and', 'the'], ['fate', 'of', 'the', 'time', 'machine', 'and', 'very', 'vaguely', 'there', 'came', 'a', 'suggestion'], ['towards', 'the', 'solution', 'of', 'the', 'economic', 'problem', 'that', 'had', 'puzzled', 'me'], [], ['here', 'was', 'the', 'new', 'view', 'plainly', 'this', 'second', 'species', 'of', 'man', 'was'], ['subterranean', 'there', 'were', 'three', 'circumstances', 'in', 'particular', 'which'], ['made', 'me', 'think', 'that', 'its', 'rare', 'emergence', 'above', 'ground', 'was', 'the', 'outcome'], ['of', 'a', 'long', 'continued', 'underground', 'habit', 'in', 'the', 'first', 'place', 'there', 'was'], ['the', 'bleached', 'look', 'common', 'in', 'most', 'animals', 'that', 'live', 'largely', 'in', 'the'], ['dark', 'the', 'white', 'fish', 'of', 'the', 'kentucky', 'caves', 'for', 'instance', 'then'], ['those', 'large', 'eyes', 'with', 'that', 'capacity', 'for', 'reflecting', 'light', 'are'], ['common', 'features', 'of', 'nocturnal', 'things', 'witness', 'the', 'owl', 'and', 'the', 'cat'], ['and', 'last', 'of', 'all', 'that', 'evident', 'confusion', 'in', 'the', 'sunshine', 'that', 'hasty'], ['yet', 'fumbling', 'awkward', 'flight', 'towards', 'dark', 'shadow', 'and', 'that', 'peculiar'], ['carriage', 'of', 'the', 'head', 'while', 'in', 'the', 'light', 'all', 'reinforced', 'the', 'theory'], ['of', 'an', 'extreme', 'sensitiveness', 'of', 'the', 'retina'], [], ['beneath', 'my', 'feet', 'then', 'the', 'earth', 'must', 'be', 'tunnelled', 'enormously', 'and'], ['these', 'tunnellings', 'were', 'the', 'habitat', 'of', 'the', 'new', 'race', 'the', 'presence', 'of'], ['ventilating', 'shafts', 'and', 'wells', 'along', 'the', 'hill', 'slopes', 'everywhere', 'in'], ['fact', 'except', 'along', 'the', 'river', 'valley', 'showed', 'how', 'universal', 'were', 'its'], ['ramifications', 'what', 'so', 'natural', 'then', 'as', 'to', 'assume', 'that', 'it', 'was', 'in'], ['this', 'artificial', 'underworld', 'that', 'such', 'work', 'as', 'was', 'necessary', 'to', 'the'], ['comfort', 'of', 'the', 'daylight', 'race', 'was', 'done', 'the', 'notion', 'was', 'so', 'plausible'], ['that', 'i', 'at', 'once', 'accepted', 'it', 'and', 'went', 'on', 'to', 'assume', 'the', 'how', 'of', 'this'], ['splitting', 'of', 'the', 'human', 'species', 'i', 'dare', 'say', 'you', 'will', 'anticipate', 'the'], ['shape', 'of', 'my', 'theory', 'though', 'for', 'myself', 'i', 'very', 'soon', 'felt', 'that', 'it'], ['fell', 'far', 'short', 'of', 'the', 'truth'], [], ['at', 'first', 'proceeding', 'from', 'the', 'problems', 'of', 'our', 'own', 'age', 'it', 'seemed'], ['clear', 'as', 'daylight', 'to', 'me', 'that', 'the', 'gradual', 'widening', 'of', 'the', 'present'], ['merely', 'temporary', 'and', 'social', 'difference', 'between', 'the', 'capitalist', 'and'], ['the', 'labourer', 'was', 'the', 'key', 'to', 'the', 'whole', 'position', 'no', 'doubt', 'it', 'will'], ['seem', 'grotesque', 'enough', 'to', 'you', 'and', 'wildly', 'incredible', 'and', 'yet', 'even'], ['now', 'there', 'are', 'existing', 'circumstances', 'to', 'point', 'that', 'way', 'there', 'is'], ['a', 'tendency', 'to', 'utilize', 'underground', 'space', 'for', 'the', 'less', 'ornamental'], ['purposes', 'of', 'civilization', 'there', 'is', 'the', 'metropolitan', 'railway', 'in'], ['london', 'for', 'instance', 'there', 'are', 'new', 'electric', 'railways', 'there', 'are'], ['subways', 'there', 'are', 'underground', 'workrooms', 'and', 'restaurants', 'and', 'they'], ['increase', 'and', 'multiply', 'evidently', 'i', 'thought', 'this', 'tendency', 'had'], ['increased', 'till', 'industry', 'had', 'gradually', 'lost', 'its', 'birthright', 'in', 'the'], ['sky', 'i', 'mean', 'that', 'it', 'had', 'gone', 'deeper', 'and', 'deeper', 'into', 'larger', 'and', 'ever'], ['larger', 'underground', 'factories', 'spending', 'a', 'still', 'increasing', 'amount', 'of'], ['its', 'time', 'therein', 'till', 'in', 'the', 'end', 'even', 'now', 'does', 'not', 'an', 'east', 'end'], ['worker', 'live', 'in', 'such', 'artificial', 'conditions', 'as', 'practically', 'to', 'be', 'cut'], ['off', 'from', 'the', 'natural', 'surface', 'of', 'the', 'earth'], [], ['again', 'the', 'exclusive', 'tendency', 'of', 'richer', 'people', 'due', 'no', 'doubt', 'to'], ['the', 'increasing', 'refinement', 'of', 'their', 'education', 'and', 'the', 'widening', 'gulf'], ['between', 'them', 'and', 'the', 'rude', 'violence', 'of', 'the', 'poor', 'is', 'already', 'leading'], ['to', 'the', 'closing', 'in', 'their', 'interest', 'of', 'considerable', 'portions', 'of', 'the'], ['surface', 'of', 'the', 'land', 'about', 'london', 'for', 'instance', 'perhaps', 'half', 'the'], ['prettier', 'country', 'is', 'shut', 'in', 'against', 'intrusion', 'and', 'this', 'same'], ['widening', 'gulf', 'which', 'is', 'due', 'to', 'the', 'length', 'and', 'expense', 'of', 'the', 'higher'], ['educational', 'process', 'and', 'the', 'increased', 'facilities', 'for', 'and', 'temptations'], ['towards', 'refined', 'habits', 'on', 'the', 'part', 'of', 'the', 'rich', 'will', 'make', 'that'], ['exchange', 'between', 'class', 'and', 'class', 'that', 'promotion', 'by', 'intermarriage'], ['which', 'at', 'present', 'retards', 'the', 'splitting', 'of', 'our', 'species', 'along', 'lines'], ['of', 'social', 'stratification', 'less', 'and', 'less', 'frequent', 'so', 'in', 'the', 'end'], ['above', 'ground', 'you', 'must', 'have', 'the', 'haves', 'pursuing', 'pleasure', 'and', 'comfort'], ['and', 'beauty', 'and', 'below', 'ground', 'the', 'have', 'nots', 'the', 'workers', 'getting'], ['continually', 'adapted', 'to', 'the', 'conditions', 'of', 'their', 'labour', 'once', 'they'], ['were', 'there', 'they', 'would', 'no', 'doubt', 'have', 'to', 'pay', 'rent', 'and', 'not', 'a', 'little'], ['of', 'it', 'for', 'the', 'ventilation', 'of', 'their', 'caverns', 'and', 'if', 'they', 'refused'], ['they', 'would', 'starve', 'or', 'be', 'suffocated', 'for', 'arrears', 'such', 'of', 'them', 'as', 'were'], ['so', 'constituted', 'as', 'to', 'be', 'miserable', 'and', 'rebellious', 'would', 'die', 'and', 'in'], ['the', 'end', 'the', 'balance', 'being', 'permanent', 'the', 'survivors', 'would', 'become', 'as'], ['well', 'adapted', 'to', 'the', 'conditions', 'of', 'underground', 'life', 'and', 'as', 'happy', 'in'], ['their', 'way', 'as', 'the', 'upper', 'world', 'people', 'were', 'to', 'theirs', 'as', 'it', 'seemed', 'to'], ['me', 'the', 'refined', 'beauty', 'and', 'the', 'etiolated', 'pallor', 'followed', 'naturally'], ['enough'], [], ['the', 'great', 'triumph', 'of', 'humanity', 'i', 'had', 'dreamed', 'of', 'took', 'a', 'different'], ['shape', 'in', 'my', 'mind', 'it', 'had', 'been', 'no', 'such', 'triumph', 'of', 'moral', 'education', 'and'], ['general', 'co', 'operation', 'as', 'i', 'had', 'imagined', 'instead', 'i', 'saw', 'a', 'real'], ['aristocracy', 'armed', 'with', 'a', 'perfected', 'science', 'and', 'working', 'to', 'a', 'logical'], ['conclusion', 'the', 'industrial', 'system', 'of', 'to', 'day', 'its', 'triumph', 'had', 'not', 'been'], ['simply', 'a', 'triumph', 'over', 'nature', 'but', 'a', 'triumph', 'over', 'nature', 'and', 'the'], ['fellow', 'man', 'this', 'i', 'must', 'warn', 'you', 'was', 'my', 'theory', 'at', 'the', 'time', 'i', 'had'], ['no', 'convenient', 'cicerone', 'in', 'the', 'pattern', 'of', 'the', 'utopian', 'books', 'my'], ['explanation', 'may', 'be', 'absolutely', 'wrong', 'i', 'still', 'think', 'it', 'is', 'the'], ['most', 'plausible', 'one', 'but', 'even', 'on', 'this', 'supposition', 'the', 'balanced'], ['civilization', 'that', 'was', 'at', 'last', 'attained', 'must', 'have', 'long', 'since', 'passed'], ['its', 'zenith', 'and', 'was', 'now', 'far', 'fallen', 'into', 'decay', 'the', 'too', 'perfect'], ['security', 'of', 'the', 'upper', 'worlders', 'had', 'led', 'them', 'to', 'a', 'slow', 'movement', 'of'], ['degeneration', 'to', 'a', 'general', 'dwindling', 'in', 'size', 'strength', 'and'], ['intelligence', 'that', 'i', 'could', 'see', 'clearly', 'enough', 'already', 'what', 'had'], ['happened', 'to', 'the', 'under', 'grounders', 'i', 'did', 'not', 'yet', 'suspect', 'but', 'from', 'what'], ['i', 'had', 'seen', 'of', 'the', 'morlocks', 'that', 'by', 'the', 'by', 'was', 'the', 'name', 'by', 'which'], ['these', 'creatures', 'were', 'called', 'i', 'could', 'imagine', 'that', 'the', 'modification'], ['of', 'the', 'human', 'type', 'was', 'even', 'far', 'more', 'profound', 'than', 'among', 'the', 'eloi'], ['the', 'beautiful', 'race', 'that', 'i', 'already', 'knew'], [], ['then', 'came', 'troublesome', 'doubts', 'why', 'had', 'the', 'morlocks', 'taken', 'my', 'time'], ['machine', 'for', 'i', 'felt', 'sure', 'it', 'was', 'they', 'who', 'had', 'taken', 'it', 'why', 'too', 'if'], ['the', 'eloi', 'were', 'masters', 'could', 'they', 'not', 'restore', 'the', 'machine', 'to', 'me', 'and'], ['why', 'were', 'they', 'so', 'terribly', 'afraid', 'of', 'the', 'dark', 'i', 'proceeded', 'as', 'i', 'have'], ['said', 'to', 'question', 'weena', 'about', 'this', 'under', 'world', 'but', 'here', 'again', 'i', 'was'], ['disappointed', 'at', 'first', 'she', 'would', 'not', 'understand', 'my', 'questions', 'and'], ['presently', 'she', 'refused', 'to', 'answer', 'them', 'she', 'shivered', 'as', 'though', 'the'], ['topic', 'was', 'unendurable', 'and', 'when', 'i', 'pressed', 'her', 'perhaps', 'a', 'little'], ['harshly', 'she', 'burst', 'into', 'tears', 'they', 'were', 'the', 'only', 'tears', 'except', 'my'], ['own', 'i', 'ever', 'saw', 'in', 'that', 'golden', 'age', 'when', 'i', 'saw', 'them', 'i', 'ceased'], ['abruptly', 'to', 'trouble', 'about', 'the', 'morlocks', 'and', 'was', 'only', 'concerned', 'in'], ['banishing', 'these', 'signs', 'of', 'the', 'human', 'inheritance', 'from', 'weena', 's', 'eyes'], ['and', 'very', 'soon', 'she', 'was', 'smiling', 'and', 'clapping', 'her', 'hands', 'while', 'i'], ['solemnly', 'burned', 'a', 'match'], [], [], [], [], ['vi'], [], [], ['it', 'may', 'seem', 'odd', 'to', 'you', 'but', 'it', 'was', 'two', 'days', 'before', 'i', 'could', 'follow'], ['up', 'the', 'new', 'found', 'clue', 'in', 'what', 'was', 'manifestly', 'the', 'proper', 'way', 'i', 'felt'], ['a', 'peculiar', 'shrinking', 'from', 'those', 'pallid', 'bodies', 'they', 'were', 'just', 'the'], ['half', 'bleached', 'colour', 'of', 'the', 'worms', 'and', 'things', 'one', 'sees', 'preserved', 'in'], ['spirit', 'in', 'a', 'zoological', 'museum', 'and', 'they', 'were', 'filthily', 'cold', 'to', 'the'], ['touch', 'probably', 'my', 'shrinking', 'was', 'largely', 'due', 'to', 'the', 'sympathetic'], ['influence', 'of', 'the', 'eloi', 'whose', 'disgust', 'of', 'the', 'morlocks', 'i', 'now', 'began'], ['to', 'appreciate'], [], ['the', 'next', 'night', 'i', 'did', 'not', 'sleep', 'well', 'probably', 'my', 'health', 'was', 'a'], ['little', 'disordered', 'i', 'was', 'oppressed', 'with', 'perplexity', 'and', 'doubt', 'once'], ['or', 'twice', 'i', 'had', 'a', 'feeling', 'of', 'intense', 'fear', 'for', 'which', 'i', 'could', 'perceive'], ['no', 'definite', 'reason', 'i', 'remember', 'creeping', 'noiselessly', 'into', 'the', 'great'], ['hall', 'where', 'the', 'little', 'people', 'were', 'sleeping', 'in', 'the', 'moonlight', 'that'], ['night', 'weena', 'was', 'among', 'them', 'and', 'feeling', 'reassured', 'by', 'their', 'presence'], ['it', 'occurred', 'to', 'me', 'even', 'then', 'that', 'in', 'the', 'course', 'of', 'a', 'few', 'days', 'the'], ['moon', 'must', 'pass', 'through', 'its', 'last', 'quarter', 'and', 'the', 'nights', 'grow', 'dark'], ['when', 'the', 'appearances', 'of', 'these', 'unpleasant', 'creatures', 'from', 'below', 'these'], ['whitened', 'lemurs', 'this', 'new', 'vermin', 'that', 'had', 'replaced', 'the', 'old', 'might', 'be'], ['more', 'abundant', 'and', 'on', 'both', 'these', 'days', 'i', 'had', 'the', 'restless', 'feeling', 'of'], ['one', 'who', 'shirks', 'an', 'inevitable', 'duty', 'i', 'felt', 'assured', 'that', 'the', 'time'], ['machine', 'was', 'only', 'to', 'be', 'recovered', 'by', 'boldly', 'penetrating', 'these'], ['underground', 'mysteries', 'yet', 'i', 'could', 'not', 'face', 'the', 'mystery', 'if', 'only', 'i'], ['had', 'had', 'a', 'companion', 'it', 'would', 'have', 'been', 'different', 'but', 'i', 'was', 'so'], ['horribly', 'alone', 'and', 'even', 'to', 'clamber', 'down', 'into', 'the', 'darkness', 'of', 'the'], ['well', 'appalled', 'me', 'i', 'don', 't', 'know', 'if', 'you', 'will', 'understand', 'my', 'feeling'], ['but', 'i', 'never', 'felt', 'quite', 'safe', 'at', 'my', 'back'], [], ['it', 'was', 'this', 'restlessness', 'this', 'insecurity', 'perhaps', 'that', 'drove', 'me'], ['further', 'and', 'further', 'afield', 'in', 'my', 'exploring', 'expeditions', 'going', 'to', 'the'], ['south', 'westward', 'towards', 'the', 'rising', 'country', 'that', 'is', 'now', 'called', 'combe'], ['wood', 'i', 'observed', 'far', 'off', 'in', 'the', 'direction', 'of', 'nineteenth', 'century'], ['banstead', 'a', 'vast', 'green', 'structure', 'different', 'in', 'character', 'from', 'any'], ['i', 'had', 'hitherto', 'seen', 'it', 'was', 'larger', 'than', 'the', 'largest', 'of', 'the', 'palaces'], ['or', 'ruins', 'i', 'knew', 'and', 'the', 'facade', 'had', 'an', 'oriental', 'look', 'the', 'face'], ['of', 'it', 'having', 'the', 'lustre', 'as', 'well', 'as', 'the', 'pale', 'green', 'tint', 'a', 'kind'], ['of', 'bluish', 'green', 'of', 'a', 'certain', 'type', 'of', 'chinese', 'porcelain', 'this'], ['difference', 'in', 'aspect', 'suggested', 'a', 'difference', 'in', 'use', 'and', 'i', 'was', 'minded'], ['to', 'push', 'on', 'and', 'explore', 'but', 'the', 'day', 'was', 'growing', 'late', 'and', 'i', 'had', 'come'], ['upon', 'the', 'sight', 'of', 'the', 'place', 'after', 'a', 'long', 'and', 'tiring', 'circuit', 'so', 'i'], ['resolved', 'to', 'hold', 'over', 'the', 'adventure', 'for', 'the', 'following', 'day', 'and', 'i'], ['returned', 'to', 'the', 'welcome', 'and', 'the', 'caresses', 'of', 'little', 'weena', 'but', 'next'], ['morning', 'i', 'perceived', 'clearly', 'enough', 'that', 'my', 'curiosity', 'regarding', 'the'], ['palace', 'of', 'green', 'porcelain', 'was', 'a', 'piece', 'of', 'self', 'deception', 'to', 'enable'], ['me', 'to', 'shirk', 'by', 'another', 'day', 'an', 'experience', 'i', 'dreaded', 'i', 'resolved', 'i'], ['would', 'make', 'the', 'descent', 'without', 'further', 'waste', 'of', 'time', 'and', 'started'], ['out', 'in', 'the', 'early', 'morning', 'towards', 'a', 'well', 'near', 'the', 'ruins', 'of', 'granite'], ['and', 'aluminium'], [], ['little', 'weena', 'ran', 'with', 'me', 'she', 'danced', 'beside', 'me', 'to', 'the', 'well', 'but'], ['when', 'she', 'saw', 'me', 'lean', 'over', 'the', 'mouth', 'and', 'look', 'downward', 'she', 'seemed'], ['strangely', 'disconcerted', 'good', 'bye', 'little', 'weena', 'i', 'said', 'kissing'], ['her', 'and', 'then', 'putting', 'her', 'down', 'i', 'began', 'to', 'feel', 'over', 'the', 'parapet'], ['for', 'the', 'climbing', 'hooks', 'rather', 'hastily', 'i', 'may', 'as', 'well', 'confess', 'for'], ['i', 'feared', 'my', 'courage', 'might', 'leak', 'away', 'at', 'first', 'she', 'watched', 'me', 'in'], ['amazement', 'then', 'she', 'gave', 'a', 'most', 'piteous', 'cry', 'and', 'running', 'to', 'me', 'she'], ['began', 'to', 'pull', 'at', 'me', 'with', 'her', 'little', 'hands', 'i', 'think', 'her', 'opposition'], ['nerved', 'me', 'rather', 'to', 'proceed', 'i', 'shook', 'her', 'off', 'perhaps', 'a', 'little'], ['roughly', 'and', 'in', 'another', 'moment', 'i', 'was', 'in', 'the', 'throat', 'of', 'the', 'well', 'i'], ['saw', 'her', 'agonized', 'face', 'over', 'the', 'parapet', 'and', 'smiled', 'to', 'reassure', 'her'], ['then', 'i', 'had', 'to', 'look', 'down', 'at', 'the', 'unstable', 'hooks', 'to', 'which', 'i', 'clung'], [], ['i', 'had', 'to', 'clamber', 'down', 'a', 'shaft', 'of', 'perhaps', 'two', 'hundred', 'yards', 'the'], ['descent', 'was', 'effected', 'by', 'means', 'of', 'metallic', 'bars', 'projecting', 'from'], ['the', 'sides', 'of', 'the', 'well', 'and', 'these', 'being', 'adapted', 'to', 'the', 'needs', 'of'], ['a', 'creature', 'much', 'smaller', 'and', 'lighter', 'than', 'myself', 'i', 'was', 'speedily'], ['cramped', 'and', 'fatigued', 'by', 'the', 'descent', 'and', 'not', 'simply', 'fatigued', 'one', 'of'], ['the', 'bars', 'bent', 'suddenly', 'under', 'my', 'weight', 'and', 'almost', 'swung', 'me', 'off', 'into'], ['the', 'blackness', 'beneath', 'for', 'a', 'moment', 'i', 'hung', 'by', 'one', 'hand', 'and', 'after'], ['that', 'experience', 'i', 'did', 'not', 'dare', 'to', 'rest', 'again', 'though', 'my', 'arms', 'and'], ['back', 'were', 'presently', 'acutely', 'painful', 'i', 'went', 'on', 'clambering', 'down', 'the'], ['sheer', 'descent', 'with', 'as', 'quick', 'a', 'motion', 'as', 'possible', 'glancing', 'upward'], ['i', 'saw', 'the', 'aperture', 'a', 'small', 'blue', 'disk', 'in', 'which', 'a', 'star', 'was', 'visible'], ['while', 'little', 'weena', 's', 'head', 'showed', 'as', 'a', 'round', 'black', 'projection', 'the'], ['thudding', 'sound', 'of', 'a', 'machine', 'below', 'grew', 'louder', 'and', 'more', 'oppressive'], ['everything', 'save', 'that', 'little', 'disk', 'above', 'was', 'profoundly', 'dark', 'and', 'when'], ['i', 'looked', 'up', 'again', 'weena', 'had', 'disappeared'], [], ['i', 'was', 'in', 'an', 'agony', 'of', 'discomfort', 'i', 'had', 'some', 'thought', 'of', 'trying', 'to', 'go'], ['up', 'the', 'shaft', 'again', 'and', 'leave', 'the', 'under', 'world', 'alone', 'but', 'even', 'while'], ['i', 'turned', 'this', 'over', 'in', 'my', 'mind', 'i', 'continued', 'to', 'descend', 'at', 'last', 'with'], ['intense', 'relief', 'i', 'saw', 'dimly', 'coming', 'up', 'a', 'foot', 'to', 'the', 'right', 'of', 'me', 'a'], ['slender', 'loophole', 'in', 'the', 'wall', 'swinging', 'myself', 'in', 'i', 'found', 'it', 'was', 'the'], ['aperture', 'of', 'a', 'narrow', 'horizontal', 'tunnel', 'in', 'which', 'i', 'could', 'lie', 'down', 'and'], ['rest', 'it', 'was', 'not', 'too', 'soon', 'my', 'arms', 'ached', 'my', 'back', 'was', 'cramped', 'and', 'i'], ['was', 'trembling', 'with', 'the', 'prolonged', 'terror', 'of', 'a', 'fall', 'besides', 'this', 'the'], ['unbroken', 'darkness', 'had', 'had', 'a', 'distressing', 'effect', 'upon', 'my', 'eyes', 'the', 'air'], ['was', 'full', 'of', 'the', 'throb', 'and', 'hum', 'of', 'machinery', 'pumping', 'air', 'down', 'the'], ['shaft'], [], ['i', 'do', 'not', 'know', 'how', 'long', 'i', 'lay', 'i', 'was', 'roused', 'by', 'a', 'soft', 'hand', 'touching'], ['my', 'face', 'starting', 'up', 'in', 'the', 'darkness', 'i', 'snatched', 'at', 'my', 'matches', 'and'], ['hastily', 'striking', 'one', 'i', 'saw', 'three', 'stooping', 'white', 'creatures', 'similar'], ['to', 'the', 'one', 'i', 'had', 'seen', 'above', 'ground', 'in', 'the', 'ruin', 'hastily', 'retreating'], ['before', 'the', 'light', 'living', 'as', 'they', 'did', 'in', 'what', 'appeared', 'to', 'me'], ['impenetrable', 'darkness', 'their', 'eyes', 'were', 'abnormally', 'large', 'and'], ['sensitive', 'just', 'as', 'are', 'the', 'pupils', 'of', 'the', 'abysmal', 'fishes', 'and', 'they'], ['reflected', 'the', 'light', 'in', 'the', 'same', 'way', 'i', 'have', 'no', 'doubt', 'they', 'could', 'see'], ['me', 'in', 'that', 'rayless', 'obscurity', 'and', 'they', 'did', 'not', 'seem', 'to', 'have', 'any', 'fear'], ['of', 'me', 'apart', 'from', 'the', 'light', 'but', 'so', 'soon', 'as', 'i', 'struck', 'a', 'match', 'in'], ['order', 'to', 'see', 'them', 'they', 'fled', 'incontinently', 'vanishing', 'into', 'dark'], ['gutters', 'and', 'tunnels', 'from', 'which', 'their', 'eyes', 'glared', 'at', 'me', 'in', 'the'], ['strangest', 'fashion'], [], ['i', 'tried', 'to', 'call', 'to', 'them', 'but', 'the', 'language', 'they', 'had', 'was', 'apparently'], ['different', 'from', 'that', 'of', 'the', 'over', 'world', 'people', 'so', 'that', 'i', 'was', 'needs'], ['left', 'to', 'my', 'own', 'unaided', 'efforts', 'and', 'the', 'thought', 'of', 'flight', 'before'], ['exploration', 'was', 'even', 'then', 'in', 'my', 'mind', 'but', 'i', 'said', 'to', 'myself', 'you', 'are'], ['in', 'for', 'it', 'now', 'and', 'feeling', 'my', 'way', 'along', 'the', 'tunnel', 'i', 'found', 'the'], ['noise', 'of', 'machinery', 'grow', 'louder', 'presently', 'the', 'walls', 'fell', 'away', 'from'], ['me', 'and', 'i', 'came', 'to', 'a', 'large', 'open', 'space', 'and', 'striking', 'another', 'match'], ['saw', 'that', 'i', 'had', 'entered', 'a', 'vast', 'arched', 'cavern', 'which', 'stretched', 'into'], ['utter', 'darkness', 'beyond', 'the', 'range', 'of', 'my', 'light', 'the', 'view', 'i', 'had', 'of', 'it'], ['was', 'as', 'much', 'as', 'one', 'could', 'see', 'in', 'the', 'burning', 'of', 'a', 'match'], [], ['necessarily', 'my', 'memory', 'is', 'vague', 'great', 'shapes', 'like', 'big', 'machines', 'rose'], ['out', 'of', 'the', 'dimness', 'and', 'cast', 'grotesque', 'black', 'shadows', 'in', 'which', 'dim'], ['spectral', 'morlocks', 'sheltered', 'from', 'the', 'glare', 'the', 'place', 'by', 'the', 'by'], ['was', 'very', 'stuffy', 'and', 'oppressive', 'and', 'the', 'faint', 'halitus', 'of', 'freshly'], ['shed', 'blood', 'was', 'in', 'the', 'air', 'some', 'way', 'down', 'the', 'central', 'vista', 'was', 'a'], ['little', 'table', 'of', 'white', 'metal', 'laid', 'with', 'what', 'seemed', 'a', 'meal', 'the'], ['morlocks', 'at', 'any', 'rate', 'were', 'carnivorous', 'even', 'at', 'the', 'time', 'i', 'remember'], ['wondering', 'what', 'large', 'animal', 'could', 'have', 'survived', 'to', 'furnish', 'the', 'red'], ['joint', 'i', 'saw', 'it', 'was', 'all', 'very', 'indistinct', 'the', 'heavy', 'smell', 'the', 'big'], ['unmeaning', 'shapes', 'the', 'obscene', 'figures', 'lurking', 'in', 'the', 'shadows', 'and'], ['only', 'waiting', 'for', 'the', 'darkness', 'to', 'come', 'at', 'me', 'again', 'then', 'the', 'match'], ['burned', 'down', 'and', 'stung', 'my', 'fingers', 'and', 'fell', 'a', 'wriggling', 'red', 'spot'], ['in', 'the', 'blackness'], [], ['i', 'have', 'thought', 'since', 'how', 'particularly', 'ill', 'equipped', 'i', 'was', 'for', 'such'], ['an', 'experience', 'when', 'i', 'had', 'started', 'with', 'the', 'time', 'machine', 'i', 'had'], ['started', 'with', 'the', 'absurd', 'assumption', 'that', 'the', 'men', 'of', 'the', 'future', 'would'], ['certainly', 'be', 'infinitely', 'ahead', 'of', 'ourselves', 'in', 'all', 'their', 'appliances'], ['i', 'had', 'come', 'without', 'arms', 'without', 'medicine', 'without', 'anything', 'to'], ['smoke', 'at', 'times', 'i', 'missed', 'tobacco', 'frightfully', 'even', 'without', 'enough'], ['matches', 'if', 'only', 'i', 'had', 'thought', 'of', 'a', 'kodak', 'i', 'could', 'have', 'flashed', 'that'], ['glimpse', 'of', 'the', 'underworld', 'in', 'a', 'second', 'and', 'examined', 'it', 'at', 'leisure'], ['but', 'as', 'it', 'was', 'i', 'stood', 'there', 'with', 'only', 'the', 'weapons', 'and', 'the', 'powers'], ['that', 'nature', 'had', 'endowed', 'me', 'with', 'hands', 'feet', 'and', 'teeth', 'these', 'and'], ['four', 'safety', 'matches', 'that', 'still', 'remained', 'to', 'me'], [], ['i', 'was', 'afraid', 'to', 'push', 'my', 'way', 'in', 'among', 'all', 'this', 'machinery', 'in', 'the'], ['dark', 'and', 'it', 'was', 'only', 'with', 'my', 'last', 'glimpse', 'of', 'light', 'i', 'discovered'], ['that', 'my', 'store', 'of', 'matches', 'had', 'run', 'low', 'it', 'had', 'never', 'occurred', 'to', 'me'], ['until', 'that', 'moment', 'that', 'there', 'was', 'any', 'need', 'to', 'economize', 'them', 'and', 'i'], ['had', 'wasted', 'almost', 'half', 'the', 'box', 'in', 'astonishing', 'the', 'upper', 'worlders', 'to'], ['whom', 'fire', 'was', 'a', 'novelty', 'now', 'as', 'i', 'say', 'i', 'had', 'four', 'left', 'and', 'while', 'i'], ['stood', 'in', 'the', 'dark', 'a', 'hand', 'touched', 'mine', 'lank', 'fingers', 'came', 'feeling'], ['over', 'my', 'face', 'and', 'i', 'was', 'sensible', 'of', 'a', 'peculiar', 'unpleasant', 'odour', 'i'], ['fancied', 'i', 'heard', 'the', 'breathing', 'of', 'a', 'crowd', 'of', 'those', 'dreadful', 'little'], ['beings', 'about', 'me', 'i', 'felt', 'the', 'box', 'of', 'matches', 'in', 'my', 'hand', 'being', 'gently'], ['disengaged', 'and', 'other', 'hands', 'behind', 'me', 'plucking', 'at', 'my', 'clothing', 'the'], ['sense', 'of', 'these', 'unseen', 'creatures', 'examining', 'me', 'was', 'indescribably'], ['unpleasant', 'the', 'sudden', 'realization', 'of', 'my', 'ignorance', 'of', 'their', 'ways', 'of'], ['thinking', 'and', 'doing', 'came', 'home', 'to', 'me', 'very', 'vividly', 'in', 'the', 'darkness', 'i'], ['shouted', 'at', 'them', 'as', 'loudly', 'as', 'i', 'could', 'they', 'started', 'away', 'and', 'then'], ['i', 'could', 'feel', 'them', 'approaching', 'me', 'again', 'they', 'clutched', 'at', 'me', 'more'], ['boldly', 'whispering', 'odd', 'sounds', 'to', 'each', 'other', 'i', 'shivered', 'violently'], ['and', 'shouted', 'again', 'rather', 'discordantly', 'this', 'time', 'they', 'were', 'not', 'so'], ['seriously', 'alarmed', 'and', 'they', 'made', 'a', 'queer', 'laughing', 'noise', 'as', 'they', 'came'], ['back', 'at', 'me', 'i', 'will', 'confess', 'i', 'was', 'horribly', 'frightened', 'i', 'determined'], ['to', 'strike', 'another', 'match', 'and', 'escape', 'under', 'the', 'protection', 'of', 'its'], ['glare', 'i', 'did', 'so', 'and', 'eking', 'out', 'the', 'flicker', 'with', 'a', 'scrap', 'of', 'paper'], ['from', 'my', 'pocket', 'i', 'made', 'good', 'my', 'retreat', 'to', 'the', 'narrow', 'tunnel', 'but', 'i'], ['had', 'scarce', 'entered', 'this', 'when', 'my', 'light', 'was', 'blown', 'out', 'and', 'in', 'the'], ['blackness', 'i', 'could', 'hear', 'the', 'morlocks', 'rustling', 'like', 'wind', 'among', 'leaves'], ['and', 'pattering', 'like', 'the', 'rain', 'as', 'they', 'hurried', 'after', 'me'], [], ['in', 'a', 'moment', 'i', 'was', 'clutched', 'by', 'several', 'hands', 'and', 'there', 'was', 'no'], ['mistaking', 'that', 'they', 'were', 'trying', 'to', 'haul', 'me', 'back', 'i', 'struck', 'another'], ['light', 'and', 'waved', 'it', 'in', 'their', 'dazzled', 'faces', 'you', 'can', 'scarce', 'imagine'], ['how', 'nauseatingly', 'inhuman', 'they', 'looked', 'those', 'pale', 'chinless', 'faces'], ['and', 'great', 'lidless', 'pinkish', 'grey', 'eyes', 'as', 'they', 'stared', 'in', 'their'], ['blindness', 'and', 'bewilderment', 'but', 'i', 'did', 'not', 'stay', 'to', 'look', 'i', 'promise'], ['you', 'i', 'retreated', 'again', 'and', 'when', 'my', 'second', 'match', 'had', 'ended', 'i', 'struck'], ['my', 'third', 'it', 'had', 'almost', 'burned', 'through', 'when', 'i', 'reached', 'the', 'opening'], ['into', 'the', 'shaft', 'i', 'lay', 'down', 'on', 'the', 'edge', 'for', 'the', 'throb', 'of', 'the', 'great'], ['pump', 'below', 'made', 'me', 'giddy', 'then', 'i', 'felt', 'sideways', 'for', 'the', 'projecting'], ['hooks', 'and', 'as', 'i', 'did', 'so', 'my', 'feet', 'were', 'grasped', 'from', 'behind', 'and', 'i'], ['was', 'violently', 'tugged', 'backward', 'i', 'lit', 'my', 'last', 'match', 'and', 'it'], ['incontinently', 'went', 'out', 'but', 'i', 'had', 'my', 'hand', 'on', 'the', 'climbing', 'bars', 'now'], ['and', 'kicking', 'violently', 'i', 'disengaged', 'myself', 'from', 'the', 'clutches', 'of', 'the'], ['morlocks', 'and', 'was', 'speedily', 'clambering', 'up', 'the', 'shaft', 'while', 'they', 'stayed'], ['peering', 'and', 'blinking', 'up', 'at', 'me', 'all', 'but', 'one', 'little', 'wretch', 'who'], ['followed', 'me', 'for', 'some', 'way', 'and', 'well', 'nigh', 'secured', 'my', 'boot', 'as', 'a', 'trophy'], [], ['that', 'climb', 'seemed', 'interminable', 'to', 'me', 'with', 'the', 'last', 'twenty', 'or'], ['thirty', 'feet', 'of', 'it', 'a', 'deadly', 'nausea', 'came', 'upon', 'me', 'i', 'had', 'the', 'greatest'], ['difficulty', 'in', 'keeping', 'my', 'hold', 'the', 'last', 'few', 'yards', 'was', 'a', 'frightful'], ['struggle', 'against', 'this', 'faintness', 'several', 'times', 'my', 'head', 'swam', 'and', 'i'], ['felt', 'all', 'the', 'sensations', 'of', 'falling', 'at', 'last', 'however', 'i', 'got', 'over', 'the'], ['well', 'mouth', 'somehow', 'and', 'staggered', 'out', 'of', 'the', 'ruin', 'into', 'the', 'blinding'], ['sunlight', 'i', 'fell', 'upon', 'my', 'face', 'even', 'the', 'soil', 'smelt', 'sweet', 'and', 'clean'], ['then', 'i', 'remember', 'weena', 'kissing', 'my', 'hands', 'and', 'ears', 'and', 'the', 'voices', 'of'], ['others', 'among', 'the', 'eloi', 'then', 'for', 'a', 'time', 'i', 'was', 'insensible'], [], [], [], [], ['vii'], [], [], ['now', 'indeed', 'i', 'seemed', 'in', 'a', 'worse', 'case', 'than', 'before', 'hitherto'], ['except', 'during', 'my', 'night', 's', 'anguish', 'at', 'the', 'loss', 'of', 'the', 'time', 'machine'], ['i', 'had', 'felt', 'a', 'sustaining', 'hope', 'of', 'ultimate', 'escape', 'but', 'that', 'hope', 'was'], ['staggered', 'by', 'these', 'new', 'discoveries', 'hitherto', 'i', 'had', 'merely', 'thought'], ['myself', 'impeded', 'by', 'the', 'childish', 'simplicity', 'of', 'the', 'little', 'people', 'and'], ['by', 'some', 'unknown', 'forces', 'which', 'i', 'had', 'only', 'to', 'understand', 'to', 'overcome'], ['but', 'there', 'was', 'an', 'altogether', 'new', 'element', 'in', 'the', 'sickening', 'quality', 'of'], ['the', 'morlocks', 'a', 'something', 'inhuman', 'and', 'malign', 'instinctively', 'i'], ['loathed', 'them', 'before', 'i', 'had', 'felt', 'as', 'a', 'man', 'might', 'feel', 'who', 'had', 'fallen'], ['into', 'a', 'pit', 'my', 'concern', 'was', 'with', 'the', 'pit', 'and', 'how', 'to', 'get', 'out', 'of', 'it'], ['now', 'i', 'felt', 'like', 'a', 'beast', 'in', 'a', 'trap', 'whose', 'enemy', 'would', 'come', 'upon', 'him'], ['soon'], [], ['the', 'enemy', 'i', 'dreaded', 'may', 'surprise', 'you', 'it', 'was', 'the', 'darkness', 'of', 'the'], ['new', 'moon', 'weena', 'had', 'put', 'this', 'into', 'my', 'head', 'by', 'some', 'at', 'first'], ['incomprehensible', 'remarks', 'about', 'the', 'dark', 'nights', 'it', 'was', 'not', 'now'], ['such', 'a', 'very', 'difficult', 'problem', 'to', 'guess', 'what', 'the', 'coming', 'dark', 'nights'], ['might', 'mean', 'the', 'moon', 'was', 'on', 'the', 'wane', 'each', 'night', 'there', 'was', 'a', 'longer'], ['interval', 'of', 'darkness', 'and', 'i', 'now', 'understood', 'to', 'some', 'slight', 'degree', 'at'], ['least', 'the', 'reason', 'of', 'the', 'fear', 'of', 'the', 'little', 'upper', 'world', 'people', 'for'], ['the', 'dark', 'i', 'wondered', 'vaguely', 'what', 'foul', 'villainy', 'it', 'might', 'be', 'that'], ['the', 'morlocks', 'did', 'under', 'the', 'new', 'moon', 'i', 'felt', 'pretty', 'sure', 'now', 'that'], ['my', 'second', 'hypothesis', 'was', 'all', 'wrong', 'the', 'upper', 'world', 'people', 'might'], ['once', 'have', 'been', 'the', 'favoured', 'aristocracy', 'and', 'the', 'morlocks', 'their'], ['mechanical', 'servants', 'but', 'that', 'had', 'long', 'since', 'passed', 'away', 'the', 'two'], ['species', 'that', 'had', 'resulted', 'from', 'the', 'evolution', 'of', 'man', 'were', 'sliding'], ['down', 'towards', 'or', 'had', 'already', 'arrived', 'at', 'an', 'altogether', 'new'], ['relationship', 'the', 'eloi', 'like', 'the', 'carolingian', 'kings', 'had', 'decayed'], ['to', 'a', 'mere', 'beautiful', 'futility', 'they', 'still', 'possessed', 'the', 'earth', 'on'], ['sufferance', 'since', 'the', 'morlocks', 'subterranean', 'for', 'innumerable'], ['generations', 'had', 'come', 'at', 'last', 'to', 'find', 'the', 'daylit', 'surface'], ['intolerable', 'and', 'the', 'morlocks', 'made', 'their', 'garments', 'i', 'inferred', 'and'], ['maintained', 'them', 'in', 'their', 'habitual', 'needs', 'perhaps', 'through', 'the'], ['survival', 'of', 'an', 'old', 'habit', 'of', 'service', 'they', 'did', 'it', 'as', 'a', 'standing', 'horse'], ['paws', 'with', 'his', 'foot', 'or', 'as', 'a', 'man', 'enjoys', 'killing', 'animals', 'in', 'sport'], ['because', 'ancient', 'and', 'departed', 'necessities', 'had', 'impressed', 'it', 'on', 'the'], ['organism', 'but', 'clearly', 'the', 'old', 'order', 'was', 'already', 'in', 'part', 'reversed'], ['the', 'nemesis', 'of', 'the', 'delicate', 'ones', 'was', 'creeping', 'on', 'apace', 'ages', 'ago'], ['thousands', 'of', 'generations', 'ago', 'man', 'had', 'thrust', 'his', 'brother', 'man', 'out', 'of'], ['the', 'ease', 'and', 'the', 'sunshine', 'and', 'now', 'that', 'brother', 'was', 'coming', 'back'], ['changed', 'already', 'the', 'eloi', 'had', 'begun', 'to', 'learn', 'one', 'old', 'lesson', 'anew'], ['they', 'were', 'becoming', 'reacquainted', 'with', 'fear', 'and', 'suddenly', 'there', 'came'], ['into', 'my', 'head', 'the', 'memory', 'of', 'the', 'meat', 'i', 'had', 'seen', 'in', 'the', 'under', 'world'], ['it', 'seemed', 'odd', 'how', 'it', 'floated', 'into', 'my', 'mind', 'not', 'stirred', 'up', 'as', 'it'], ['were', 'by', 'the', 'current', 'of', 'my', 'meditations', 'but', 'coming', 'in', 'almost', 'like', 'a'], ['question', 'from', 'outside', 'i', 'tried', 'to', 'recall', 'the', 'form', 'of', 'it', 'i', 'had', 'a'], ['vague', 'sense', 'of', 'something', 'familiar', 'but', 'i', 'could', 'not', 'tell', 'what', 'it', 'was'], ['at', 'the', 'time'], [], ['still', 'however', 'helpless', 'the', 'little', 'people', 'in', 'the', 'presence', 'of', 'their'], ['mysterious', 'fear', 'i', 'was', 'differently', 'constituted', 'i', 'came', 'out', 'of', 'this'], ['age', 'of', 'ours', 'this', 'ripe', 'prime', 'of', 'the', 'human', 'race', 'when', 'fear', 'does', 'not'], ['paralyse', 'and', 'mystery', 'has', 'lost', 'its', 'terrors', 'i', 'at', 'least', 'would', 'defend'], ['myself', 'without', 'further', 'delay', 'i', 'determined', 'to', 'make', 'myself', 'arms', 'and', 'a'], ['fastness', 'where', 'i', 'might', 'sleep', 'with', 'that', 'refuge', 'as', 'a', 'base', 'i', 'could'], ['face', 'this', 'strange', 'world', 'with', 'some', 'of', 'that', 'confidence', 'i', 'had', 'lost', 'in'], ['realizing', 'to', 'what', 'creatures', 'night', 'by', 'night', 'i', 'lay', 'exposed', 'i', 'felt'], ['i', 'could', 'never', 'sleep', 'again', 'until', 'my', 'bed', 'was', 'secure', 'from', 'them', 'i'], ['shuddered', 'with', 'horror', 'to', 'think', 'how', 'they', 'must', 'already', 'have', 'examined'], ['me'], [], ['i', 'wandered', 'during', 'the', 'afternoon', 'along', 'the', 'valley', 'of', 'the', 'thames', 'but'], ['found', 'nothing', 'that', 'commended', 'itself', 'to', 'my', 'mind', 'as', 'inaccessible', 'all'], ['the', 'buildings', 'and', 'trees', 'seemed', 'easily', 'practicable', 'to', 'such', 'dexterous'], ['climbers', 'as', 'the', 'morlocks', 'to', 'judge', 'by', 'their', 'wells', 'must', 'be', 'then', 'the'], ['tall', 'pinnacles', 'of', 'the', 'palace', 'of', 'green', 'porcelain', 'and', 'the', 'polished'], ['gleam', 'of', 'its', 'walls', 'came', 'back', 'to', 'my', 'memory', 'and', 'in', 'the', 'evening'], ['taking', 'weena', 'like', 'a', 'child', 'upon', 'my', 'shoulder', 'i', 'went', 'up', 'the', 'hills'], ['towards', 'the', 'south', 'west', 'the', 'distance', 'i', 'had', 'reckoned', 'was', 'seven', 'or'], ['eight', 'miles', 'but', 'it', 'must', 'have', 'been', 'nearer', 'eighteen', 'i', 'had', 'first', 'seen'], ['the', 'place', 'on', 'a', 'moist', 'afternoon', 'when', 'distances', 'are', 'deceptively'], ['diminished', 'in', 'addition', 'the', 'heel', 'of', 'one', 'of', 'my', 'shoes', 'was', 'loose', 'and'], ['a', 'nail', 'was', 'working', 'through', 'the', 'sole', 'they', 'were', 'comfortable', 'old', 'shoes'], ['i', 'wore', 'about', 'indoors', 'so', 'that', 'i', 'was', 'lame', 'and', 'it', 'was', 'already', 'long'], ['past', 'sunset', 'when', 'i', 'came', 'in', 'sight', 'of', 'the', 'palace', 'silhouetted', 'black'], ['against', 'the', 'pale', 'yellow', 'of', 'the', 'sky'], [], ['weena', 'had', 'been', 'hugely', 'delighted', 'when', 'i', 'began', 'to', 'carry', 'her', 'but'], ['after', 'a', 'while', 'she', 'desired', 'me', 'to', 'let', 'her', 'down', 'and', 'ran', 'along', 'by', 'the'], ['side', 'of', 'me', 'occasionally', 'darting', 'off', 'on', 'either', 'hand', 'to', 'pick', 'flowers'], ['to', 'stick', 'in', 'my', 'pockets', 'my', 'pockets', 'had', 'always', 'puzzled', 'weena', 'but', 'at'], ['the', 'last', 'she', 'had', 'concluded', 'that', 'they', 'were', 'an', 'eccentric', 'kind', 'of', 'vase'], ['for', 'floral', 'decoration', 'at', 'least', 'she', 'utilized', 'them', 'for', 'that', 'purpose'], ['and', 'that', 'reminds', 'me', 'in', 'changing', 'my', 'jacket', 'i', 'found'], [], ['the', 'time', 'traveller', 'paused', 'put', 'his', 'hand', 'into', 'his', 'pocket', 'and'], ['silently', 'placed', 'two', 'withered', 'flowers', 'not', 'unlike', 'very', 'large', 'white'], ['mallows', 'upon', 'the', 'little', 'table', 'then', 'he', 'resumed', 'his', 'narrative'], [], ['as', 'the', 'hush', 'of', 'evening', 'crept', 'over', 'the', 'world', 'and', 'we', 'proceeded', 'over'], ['the', 'hill', 'crest', 'towards', 'wimbledon', 'weena', 'grew', 'tired', 'and', 'wanted', 'to'], ['return', 'to', 'the', 'house', 'of', 'grey', 'stone', 'but', 'i', 'pointed', 'out', 'the', 'distant'], ['pinnacles', 'of', 'the', 'palace', 'of', 'green', 'porcelain', 'to', 'her', 'and', 'contrived', 'to'], ['make', 'her', 'understand', 'that', 'we', 'were', 'seeking', 'a', 'refuge', 'there', 'from', 'her'], ['fear', 'you', 'know', 'that', 'great', 'pause', 'that', 'comes', 'upon', 'things', 'before', 'the'], ['dusk', 'even', 'the', 'breeze', 'stops', 'in', 'the', 'trees', 'to', 'me', 'there', 'is', 'always', 'an'], ['air', 'of', 'expectation', 'about', 'that', 'evening', 'stillness', 'the', 'sky', 'was', 'clear'], ['remote', 'and', 'empty', 'save', 'for', 'a', 'few', 'horizontal', 'bars', 'far', 'down', 'in', 'the'], ['sunset', 'well', 'that', 'night', 'the', 'expectation', 'took', 'the', 'colour', 'of', 'my'], ['fears', 'in', 'that', 'darkling', 'calm', 'my', 'senses', 'seemed', 'preternaturally'], ['sharpened', 'i', 'fancied', 'i', 'could', 'even', 'feel', 'the', 'hollowness', 'of', 'the', 'ground'], ['beneath', 'my', 'feet', 'could', 'indeed', 'almost', 'see', 'through', 'it', 'the', 'morlocks'], ['on', 'their', 'ant', 'hill', 'going', 'hither', 'and', 'thither', 'and', 'waiting', 'for', 'the', 'dark'], ['in', 'my', 'excitement', 'i', 'fancied', 'that', 'they', 'would', 'receive', 'my', 'invasion', 'of'], ['their', 'burrows', 'as', 'a', 'declaration', 'of', 'war', 'and', 'why', 'had', 'they', 'taken', 'my'], ['time', 'machine'], [], ['so', 'we', 'went', 'on', 'in', 'the', 'quiet', 'and', 'the', 'twilight', 'deepened', 'into', 'night'], ['the', 'clear', 'blue', 'of', 'the', 'distance', 'faded', 'and', 'one', 'star', 'after', 'another'], ['came', 'out', 'the', 'ground', 'grew', 'dim', 'and', 'the', 'trees', 'black', 'weena', 's', 'fears', 'and'], ['her', 'fatigue', 'grew', 'upon', 'her', 'i', 'took', 'her', 'in', 'my', 'arms', 'and', 'talked', 'to', 'her'], ['and', 'caressed', 'her', 'then', 'as', 'the', 'darkness', 'grew', 'deeper', 'she', 'put', 'her'], ['arms', 'round', 'my', 'neck', 'and', 'closing', 'her', 'eyes', 'tightly', 'pressed', 'her', 'face'], ['against', 'my', 'shoulder', 'so', 'we', 'went', 'down', 'a', 'long', 'slope', 'into', 'a', 'valley', 'and'], ['there', 'in', 'the', 'dimness', 'i', 'almost', 'walked', 'into', 'a', 'little', 'river', 'this', 'i'], ['waded', 'and', 'went', 'up', 'the', 'opposite', 'side', 'of', 'the', 'valley', 'past', 'a', 'number'], ['of', 'sleeping', 'houses', 'and', 'by', 'a', 'statue', 'a', 'faun', 'or', 'some', 'such', 'figure'], ['minus', 'the', 'head', 'here', 'too', 'were', 'acacias', 'so', 'far', 'i', 'had', 'seen', 'nothing', 'of'], ['the', 'morlocks', 'but', 'it', 'was', 'yet', 'early', 'in', 'the', 'night', 'and', 'the', 'darker', 'hours'], ['before', 'the', 'old', 'moon', 'rose', 'were', 'still', 'to', 'come'], [], ['from', 'the', 'brow', 'of', 'the', 'next', 'hill', 'i', 'saw', 'a', 'thick', 'wood', 'spreading', 'wide'], ['and', 'black', 'before', 'me', 'i', 'hesitated', 'at', 'this', 'i', 'could', 'see', 'no', 'end', 'to'], ['it', 'either', 'to', 'the', 'right', 'or', 'the', 'left', 'feeling', 'tired', 'my', 'feet', 'in'], ['particular', 'were', 'very', 'sore', 'i', 'carefully', 'lowered', 'weena', 'from', 'my'], ['shoulder', 'as', 'i', 'halted', 'and', 'sat', 'down', 'upon', 'the', 'turf', 'i', 'could', 'no'], ['longer', 'see', 'the', 'palace', 'of', 'green', 'porcelain', 'and', 'i', 'was', 'in', 'doubt', 'of', 'my'], ['direction', 'i', 'looked', 'into', 'the', 'thickness', 'of', 'the', 'wood', 'and', 'thought', 'of'], ['what', 'it', 'might', 'hide', 'under', 'that', 'dense', 'tangle', 'of', 'branches', 'one', 'would'], ['be', 'out', 'of', 'sight', 'of', 'the', 'stars', 'even', 'were', 'there', 'no', 'other', 'lurking'], ['danger', 'a', 'danger', 'i', 'did', 'not', 'care', 'to', 'let', 'my', 'imagination', 'loose'], ['upon', 'there', 'would', 'still', 'be', 'all', 'the', 'roots', 'to', 'stumble', 'over', 'and', 'the'], ['tree', 'boles', 'to', 'strike', 'against'], [], ['i', 'was', 'very', 'tired', 'too', 'after', 'the', 'excitements', 'of', 'the', 'day', 'so', 'i'], ['decided', 'that', 'i', 'would', 'not', 'face', 'it', 'but', 'would', 'pass', 'the', 'night', 'upon', 'the'], ['open', 'hill'], [], ['weena', 'i', 'was', 'glad', 'to', 'find', 'was', 'fast', 'asleep', 'i', 'carefully', 'wrapped', 'her'], ['in', 'my', 'jacket', 'and', 'sat', 'down', 'beside', 'her', 'to', 'wait', 'for', 'the', 'moonrise', 'the'], ['hill', 'side', 'was', 'quiet', 'and', 'deserted', 'but', 'from', 'the', 'black', 'of', 'the', 'wood'], ['there', 'came', 'now', 'and', 'then', 'a', 'stir', 'of', 'living', 'things', 'above', 'me', 'shone', 'the'], ['stars', 'for', 'the', 'night', 'was', 'very', 'clear', 'i', 'felt', 'a', 'certain', 'sense', 'of'], ['friendly', 'comfort', 'in', 'their', 'twinkling', 'all', 'the', 'old', 'constellations'], ['had', 'gone', 'from', 'the', 'sky', 'however', 'that', 'slow', 'movement', 'which', 'is'], ['imperceptible', 'in', 'a', 'hundred', 'human', 'lifetimes', 'had', 'long', 'since'], ['rearranged', 'them', 'in', 'unfamiliar', 'groupings', 'but', 'the', 'milky', 'way', 'it'], ['seemed', 'to', 'me', 'was', 'still', 'the', 'same', 'tattered', 'streamer', 'of', 'star', 'dust', 'as'], ['of', 'yore', 'southward', 'as', 'i', 'judged', 'it', 'was', 'a', 'very', 'bright', 'red', 'star', 'that'], ['was', 'new', 'to', 'me', 'it', 'was', 'even', 'more', 'splendid', 'than', 'our', 'own', 'green', 'sirius'], ['and', 'amid', 'all', 'these', 'scintillating', 'points', 'of', 'light', 'one', 'bright', 'planet'], ['shone', 'kindly', 'and', 'steadily', 'like', 'the', 'face', 'of', 'an', 'old', 'friend'], [], ['looking', 'at', 'these', 'stars', 'suddenly', 'dwarfed', 'my', 'own', 'troubles', 'and', 'all'], ['the', 'gravities', 'of', 'terrestrial', 'life', 'i', 'thought', 'of', 'their', 'unfathomable'], ['distance', 'and', 'the', 'slow', 'inevitable', 'drift', 'of', 'their', 'movements', 'out', 'of'], ['the', 'unknown', 'past', 'into', 'the', 'unknown', 'future', 'i', 'thought', 'of', 'the', 'great'], ['precessional', 'cycle', 'that', 'the', 'pole', 'of', 'the', 'earth', 'describes', 'only', 'forty'], ['times', 'had', 'that', 'silent', 'revolution', 'occurred', 'during', 'all', 'the', 'years', 'that'], ['i', 'had', 'traversed', 'and', 'during', 'these', 'few', 'revolutions', 'all', 'the', 'activity'], ['all', 'the', 'traditions', 'the', 'complex', 'organizations', 'the', 'nations'], ['languages', 'literatures', 'aspirations', 'even', 'the', 'mere', 'memory', 'of', 'man', 'as'], ['i', 'knew', 'him', 'had', 'been', 'swept', 'out', 'of', 'existence', 'instead', 'were', 'these'], ['frail', 'creatures', 'who', 'had', 'forgotten', 'their', 'high', 'ancestry', 'and', 'the', 'white'], ['things', 'of', 'which', 'i', 'went', 'in', 'terror', 'then', 'i', 'thought', 'of', 'the', 'great', 'fear'], ['that', 'was', 'between', 'the', 'two', 'species', 'and', 'for', 'the', 'first', 'time', 'with', 'a'], ['sudden', 'shiver', 'came', 'the', 'clear', 'knowledge', 'of', 'what', 'the', 'meat', 'i', 'had', 'seen'], ['might', 'be', 'yet', 'it', 'was', 'too', 'horrible', 'i', 'looked', 'at', 'little', 'weena', 'sleeping'], ['beside', 'me', 'her', 'face', 'white', 'and', 'starlike', 'under', 'the', 'stars', 'and'], ['forthwith', 'dismissed', 'the', 'thought'], [], ['through', 'that', 'long', 'night', 'i', 'held', 'my', 'mind', 'off', 'the', 'morlocks', 'as', 'well', 'as'], ['i', 'could', 'and', 'whiled', 'away', 'the', 'time', 'by', 'trying', 'to', 'fancy', 'i', 'could', 'find'], ['signs', 'of', 'the', 'old', 'constellations', 'in', 'the', 'new', 'confusion', 'the', 'sky', 'kept'], ['very', 'clear', 'except', 'for', 'a', 'hazy', 'cloud', 'or', 'so', 'no', 'doubt', 'i', 'dozed', 'at'], ['times', 'then', 'as', 'my', 'vigil', 'wore', 'on', 'came', 'a', 'faintness', 'in', 'the', 'eastward'], ['sky', 'like', 'the', 'reflection', 'of', 'some', 'colourless', 'fire', 'and', 'the', 'old', 'moon'], ['rose', 'thin', 'and', 'peaked', 'and', 'white', 'and', 'close', 'behind', 'and', 'overtaking'], ['it', 'and', 'overflowing', 'it', 'the', 'dawn', 'came', 'pale', 'at', 'first', 'and', 'then'], ['growing', 'pink', 'and', 'warm', 'no', 'morlocks', 'had', 'approached', 'us', 'indeed', 'i', 'had'], ['seen', 'none', 'upon', 'the', 'hill', 'that', 'night', 'and', 'in', 'the', 'confidence', 'of', 'renewed'], ['day', 'it', 'almost', 'seemed', 'to', 'me', 'that', 'my', 'fear', 'had', 'been', 'unreasonable', 'i'], ['stood', 'up', 'and', 'found', 'my', 'foot', 'with', 'the', 'loose', 'heel', 'swollen', 'at', 'the', 'ankle'], ['and', 'painful', 'under', 'the', 'heel', 'so', 'i', 'sat', 'down', 'again', 'took', 'off', 'my', 'shoes'], ['and', 'flung', 'them', 'away'], [], ['i', 'awakened', 'weena', 'and', 'we', 'went', 'down', 'into', 'the', 'wood', 'now', 'green', 'and'], ['pleasant', 'instead', 'of', 'black', 'and', 'forbidding', 'we', 'found', 'some', 'fruit'], ['wherewith', 'to', 'break', 'our', 'fast', 'we', 'soon', 'met', 'others', 'of', 'the', 'dainty', 'ones'], ['laughing', 'and', 'dancing', 'in', 'the', 'sunlight', 'as', 'though', 'there', 'was', 'no', 'such'], ['thing', 'in', 'nature', 'as', 'the', 'night', 'and', 'then', 'i', 'thought', 'once', 'more', 'of', 'the'], ['meat', 'that', 'i', 'had', 'seen', 'i', 'felt', 'assured', 'now', 'of', 'what', 'it', 'was', 'and', 'from'], ['the', 'bottom', 'of', 'my', 'heart', 'i', 'pitied', 'this', 'last', 'feeble', 'rill', 'from', 'the', 'great'], ['flood', 'of', 'humanity', 'clearly', 'at', 'some', 'time', 'in', 'the', 'long', 'ago', 'of', 'human'], ['decay', 'the', 'morlocks', 'food', 'had', 'run', 'short', 'possibly', 'they', 'had', 'lived', 'on'], ['rats', 'and', 'such', 'like', 'vermin', 'even', 'now', 'man', 'is', 'far', 'less', 'discriminating'], ['and', 'exclusive', 'in', 'his', 'food', 'than', 'he', 'was', 'far', 'less', 'than', 'any', 'monkey', 'his'], ['prejudice', 'against', 'human', 'flesh', 'is', 'no', 'deep', 'seated', 'instinct', 'and', 'so'], ['these', 'inhuman', 'sons', 'of', 'men', 'i', 'tried', 'to', 'look', 'at', 'the', 'thing', 'in', 'a'], ['scientific', 'spirit', 'after', 'all', 'they', 'were', 'less', 'human', 'and', 'more', 'remote'], ['than', 'our', 'cannibal', 'ancestors', 'of', 'three', 'or', 'four', 'thousand', 'years', 'ago'], ['and', 'the', 'intelligence', 'that', 'would', 'have', 'made', 'this', 'state', 'of', 'things', 'a'], ['torment', 'had', 'gone', 'why', 'should', 'i', 'trouble', 'myself', 'these', 'eloi', 'were', 'mere'], ['fatted', 'cattle', 'which', 'the', 'ant', 'like', 'morlocks', 'preserved', 'and', 'preyed'], ['upon', 'probably', 'saw', 'to', 'the', 'breeding', 'of', 'and', 'there', 'was', 'weena', 'dancing'], ['at', 'my', 'side'], [], ['then', 'i', 'tried', 'to', 'preserve', 'myself', 'from', 'the', 'horror', 'that', 'was', 'coming'], ['upon', 'me', 'by', 'regarding', 'it', 'as', 'a', 'rigorous', 'punishment', 'of', 'human'], ['selfishness', 'man', 'had', 'been', 'content', 'to', 'live', 'in', 'ease', 'and', 'delight', 'upon'], ['the', 'labours', 'of', 'his', 'fellow', 'man', 'had', 'taken', 'necessity', 'as', 'his', 'watchword'], ['and', 'excuse', 'and', 'in', 'the', 'fullness', 'of', 'time', 'necessity', 'had', 'come', 'home', 'to'], ['him', 'i', 'even', 'tried', 'a', 'carlyle', 'like', 'scorn', 'of', 'this', 'wretched', 'aristocracy'], ['in', 'decay', 'but', 'this', 'attitude', 'of', 'mind', 'was', 'impossible', 'however', 'great'], ['their', 'intellectual', 'degradation', 'the', 'eloi', 'had', 'kept', 'too', 'much', 'of', 'the'], ['human', 'form', 'not', 'to', 'claim', 'my', 'sympathy', 'and', 'to', 'make', 'me', 'perforce', 'a'], ['sharer', 'in', 'their', 'degradation', 'and', 'their', 'fear'], [], ['i', 'had', 'at', 'that', 'time', 'very', 'vague', 'ideas', 'as', 'to', 'the', 'course', 'i', 'should'], ['pursue', 'my', 'first', 'was', 'to', 'secure', 'some', 'safe', 'place', 'of', 'refuge', 'and', 'to'], ['make', 'myself', 'such', 'arms', 'of', 'metal', 'or', 'stone', 'as', 'i', 'could', 'contrive', 'that'], ['necessity', 'was', 'immediate', 'in', 'the', 'next', 'place', 'i', 'hoped', 'to', 'procure', 'some'], ['means', 'of', 'fire', 'so', 'that', 'i', 'should', 'have', 'the', 'weapon', 'of', 'a', 'torch', 'at', 'hand'], ['for', 'nothing', 'i', 'knew', 'would', 'be', 'more', 'efficient', 'against', 'these', 'morlocks'], ['then', 'i', 'wanted', 'to', 'arrange', 'some', 'contrivance', 'to', 'break', 'open', 'the', 'doors', 'of'], ['bronze', 'under', 'the', 'white', 'sphinx', 'i', 'had', 'in', 'mind', 'a', 'battering', 'ram', 'i', 'had'], ['a', 'persuasion', 'that', 'if', 'i', 'could', 'enter', 'those', 'doors', 'and', 'carry', 'a', 'blaze', 'of'], ['light', 'before', 'me', 'i', 'should', 'discover', 'the', 'time', 'machine', 'and', 'escape', 'i'], ['could', 'not', 'imagine', 'the', 'morlocks', 'were', 'strong', 'enough', 'to', 'move', 'it', 'far'], ['away', 'weena', 'i', 'had', 'resolved', 'to', 'bring', 'with', 'me', 'to', 'our', 'own', 'time', 'and'], ['turning', 'such', 'schemes', 'over', 'in', 'my', 'mind', 'i', 'pursued', 'our', 'way', 'towards', 'the'], ['building', 'which', 'my', 'fancy', 'had', 'chosen', 'as', 'our', 'dwelling'], [], [], [], [], ['viii'], [], [], ['i', 'found', 'the', 'palace', 'of', 'green', 'porcelain', 'when', 'we', 'approached', 'it', 'about'], ['noon', 'deserted', 'and', 'falling', 'into', 'ruin', 'only', 'ragged', 'vestiges', 'of', 'glass'], ['remained', 'in', 'its', 'windows', 'and', 'great', 'sheets', 'of', 'the', 'green', 'facing', 'had'], ['fallen', 'away', 'from', 'the', 'corroded', 'metallic', 'framework', 'it', 'lay', 'very', 'high'], ['upon', 'a', 'turfy', 'down', 'and', 'looking', 'north', 'eastward', 'before', 'i', 'entered', 'it', 'i'], ['was', 'surprised', 'to', 'see', 'a', 'large', 'estuary', 'or', 'even', 'creek', 'where', 'i', 'judged'], ['wandsworth', 'and', 'battersea', 'must', 'once', 'have', 'been', 'i', 'thought', 'then', 'though'], ['i', 'never', 'followed', 'up', 'the', 'thought', 'of', 'what', 'might', 'have', 'happened', 'or'], ['might', 'be', 'happening', 'to', 'the', 'living', 'things', 'in', 'the', 'sea'], [], ['the', 'material', 'of', 'the', 'palace', 'proved', 'on', 'examination', 'to', 'be', 'indeed'], ['porcelain', 'and', 'along', 'the', 'face', 'of', 'it', 'i', 'saw', 'an', 'inscription', 'in', 'some'], ['unknown', 'character', 'i', 'thought', 'rather', 'foolishly', 'that', 'weena', 'might'], ['help', 'me', 'to', 'interpret', 'this', 'but', 'i', 'only', 'learned', 'that', 'the', 'bare', 'idea', 'of'], ['writing', 'had', 'never', 'entered', 'her', 'head', 'she', 'always', 'seemed', 'to', 'me', 'i'], ['fancy', 'more', 'human', 'than', 'she', 'was', 'perhaps', 'because', 'her', 'affection', 'was', 'so'], ['human'], [], ['within', 'the', 'big', 'valves', 'of', 'the', 'door', 'which', 'were', 'open', 'and', 'broken', 'we'], ['found', 'instead', 'of', 'the', 'customary', 'hall', 'a', 'long', 'gallery', 'lit', 'by', 'many'], ['side', 'windows', 'at', 'the', 'first', 'glance', 'i', 'was', 'reminded', 'of', 'a', 'museum'], ['the', 'tiled', 'floor', 'was', 'thick', 'with', 'dust', 'and', 'a', 'remarkable', 'array', 'of'], ['miscellaneous', 'objects', 'was', 'shrouded', 'in', 'the', 'same', 'grey', 'covering', 'then'], ['i', 'perceived', 'standing', 'strange', 'and', 'gaunt', 'in', 'the', 'centre', 'of', 'the', 'hall'], ['what', 'was', 'clearly', 'the', 'lower', 'part', 'of', 'a', 'huge', 'skeleton', 'i', 'recognized'], ['by', 'the', 'oblique', 'feet', 'that', 'it', 'was', 'some', 'extinct', 'creature', 'after', 'the'], ['fashion', 'of', 'the', 'megatherium', 'the', 'skull', 'and', 'the', 'upper', 'bones', 'lay'], ['beside', 'it', 'in', 'the', 'thick', 'dust', 'and', 'in', 'one', 'place', 'where', 'rain', 'water', 'had'], ['dropped', 'through', 'a', 'leak', 'in', 'the', 'roof', 'the', 'thing', 'itself', 'had', 'been', 'worn'], ['away', 'further', 'in', 'the', 'gallery', 'was', 'the', 'huge', 'skeleton', 'barrel', 'of', 'a'], ['brontosaurus', 'my', 'museum', 'hypothesis', 'was', 'confirmed', 'going', 'towards', 'the'], ['side', 'i', 'found', 'what', 'appeared', 'to', 'be', 'sloping', 'shelves', 'and', 'clearing', 'away'], ['the', 'thick', 'dust', 'i', 'found', 'the', 'old', 'familiar', 'glass', 'cases', 'of', 'our', 'own'], ['time', 'but', 'they', 'must', 'have', 'been', 'air', 'tight', 'to', 'judge', 'from', 'the', 'fair'], ['preservation', 'of', 'some', 'of', 'their', 'contents'], [], ['clearly', 'we', 'stood', 'among', 'the', 'ruins', 'of', 'some', 'latter', 'day', 'south'], ['kensington', 'here', 'apparently', 'was', 'the', 'palaeontological', 'section'], ['and', 'a', 'very', 'splendid', 'array', 'of', 'fossils', 'it', 'must', 'have', 'been', 'though', 'the'], ['inevitable', 'process', 'of', 'decay', 'that', 'had', 'been', 'staved', 'off', 'for', 'a', 'time', 'and'], ['had', 'through', 'the', 'extinction', 'of', 'bacteria', 'and', 'fungi', 'lost', 'ninety', 'nine'], ['hundredths', 'of', 'its', 'force', 'was', 'nevertheless', 'with', 'extreme', 'sureness', 'if'], ['with', 'extreme', 'slowness', 'at', 'work', 'again', 'upon', 'all', 'its', 'treasures', 'here', 'and'], ['there', 'i', 'found', 'traces', 'of', 'the', 'little', 'people', 'in', 'the', 'shape', 'of', 'rare'], ['fossils', 'broken', 'to', 'pieces', 'or', 'threaded', 'in', 'strings', 'upon', 'reeds', 'and', 'the'], ['cases', 'had', 'in', 'some', 'instances', 'been', 'bodily', 'removed', 'by', 'the', 'morlocks', 'as'], ['i', 'judged', 'the', 'place', 'was', 'very', 'silent', 'the', 'thick', 'dust', 'deadened', 'our'], ['footsteps', 'weena', 'who', 'had', 'been', 'rolling', 'a', 'sea', 'urchin', 'down', 'the', 'sloping'], ['glass', 'of', 'a', 'case', 'presently', 'came', 'as', 'i', 'stared', 'about', 'me', 'and', 'very'], ['quietly', 'took', 'my', 'hand', 'and', 'stood', 'beside', 'me'], [], ['and', 'at', 'first', 'i', 'was', 'so', 'much', 'surprised', 'by', 'this', 'ancient', 'monument', 'of', 'an'], ['intellectual', 'age', 'that', 'i', 'gave', 'no', 'thought', 'to', 'the', 'possibilities', 'it'], ['presented', 'even', 'my', 'preoccupation', 'about', 'the', 'time', 'machine', 'receded', 'a'], ['little', 'from', 'my', 'mind'], [], ['to', 'judge', 'from', 'the', 'size', 'of', 'the', 'place', 'this', 'palace', 'of', 'green', 'porcelain'], ['had', 'a', 'great', 'deal', 'more', 'in', 'it', 'than', 'a', 'gallery', 'of', 'palaeontology'], ['possibly', 'historical', 'galleries', 'it', 'might', 'be', 'even', 'a', 'library', 'to', 'me'], ['at', 'least', 'in', 'my', 'present', 'circumstances', 'these', 'would', 'be', 'vastly', 'more'], ['interesting', 'than', 'this', 'spectacle', 'of', 'oldtime', 'geology', 'in', 'decay'], ['exploring', 'i', 'found', 'another', 'short', 'gallery', 'running', 'transversely', 'to', 'the'], ['first', 'this', 'appeared', 'to', 'be', 'devoted', 'to', 'minerals', 'and', 'the', 'sight', 'of', 'a'], ['block', 'of', 'sulphur', 'set', 'my', 'mind', 'running', 'on', 'gunpowder', 'but', 'i', 'could', 'find'], ['no', 'saltpeter', 'indeed', 'no', 'nitrates', 'of', 'any', 'kind', 'doubtless', 'they', 'had'], ['deliquesced', 'ages', 'ago', 'yet', 'the', 'sulphur', 'hung', 'in', 'my', 'mind', 'and', 'set', 'up', 'a'], ['train', 'of', 'thinking', 'as', 'for', 'the', 'rest', 'of', 'the', 'contents', 'of', 'that', 'gallery'], ['though', 'on', 'the', 'whole', 'they', 'were', 'the', 'best', 'preserved', 'of', 'all', 'i', 'saw', 'i', 'had'], ['little', 'interest', 'i', 'am', 'no', 'specialist', 'in', 'mineralogy', 'and', 'i', 'went', 'on'], ['down', 'a', 'very', 'ruinous', 'aisle', 'running', 'parallel', 'to', 'the', 'first', 'hall', 'i', 'had'], ['entered', 'apparently', 'this', 'section', 'had', 'been', 'devoted', 'to', 'natural'], ['history', 'but', 'everything', 'had', 'long', 'since', 'passed', 'out', 'of', 'recognition', 'a'], ['few', 'shrivelled', 'and', 'blackened', 'vestiges', 'of', 'what', 'had', 'once', 'been', 'stuffed'], ['animals', 'desiccated', 'mummies', 'in', 'jars', 'that', 'had', 'once', 'held', 'spirit', 'a'], ['brown', 'dust', 'of', 'departed', 'plants', 'that', 'was', 'all', 'i', 'was', 'sorry', 'for', 'that'], ['because', 'i', 'should', 'have', 'been', 'glad', 'to', 'trace', 'the', 'patent', 'readjustments', 'by'], ['which', 'the', 'conquest', 'of', 'animated', 'nature', 'had', 'been', 'attained', 'then', 'we'], ['came', 'to', 'a', 'gallery', 'of', 'simply', 'colossal', 'proportions', 'but', 'singularly'], ['ill', 'lit', 'the', 'floor', 'of', 'it', 'running', 'downward', 'at', 'a', 'slight', 'angle', 'from', 'the'], ['end', 'at', 'which', 'i', 'entered', 'at', 'intervals', 'white', 'globes', 'hung', 'from', 'the'], ['ceiling', 'many', 'of', 'them', 'cracked', 'and', 'smashed', 'which', 'suggested', 'that'], ['originally', 'the', 'place', 'had', 'been', 'artificially', 'lit', 'here', 'i', 'was', 'more', 'in'], ['my', 'element', 'for', 'rising', 'on', 'either', 'side', 'of', 'me', 'were', 'the', 'huge', 'bulks', 'of'], ['big', 'machines', 'all', 'greatly', 'corroded', 'and', 'many', 'broken', 'down', 'but', 'some'], ['still', 'fairly', 'complete', 'you', 'know', 'i', 'have', 'a', 'certain', 'weakness', 'for'], ['mechanism', 'and', 'i', 'was', 'inclined', 'to', 'linger', 'among', 'these', 'the', 'more', 'so', 'as'], ['for', 'the', 'most', 'part', 'they', 'had', 'the', 'interest', 'of', 'puzzles', 'and', 'i', 'could', 'make'], ['only', 'the', 'vaguest', 'guesses', 'at', 'what', 'they', 'were', 'for', 'i', 'fancied', 'that', 'if'], ['i', 'could', 'solve', 'their', 'puzzles', 'i', 'should', 'find', 'myself', 'in', 'possession', 'of'], ['powers', 'that', 'might', 'be', 'of', 'use', 'against', 'the', 'morlocks'], [], ['suddenly', 'weena', 'came', 'very', 'close', 'to', 'my', 'side', 'so', 'suddenly', 'that', 'she'], ['startled', 'me', 'had', 'it', 'not', 'been', 'for', 'her', 'i', 'do', 'not', 'think', 'i', 'should', 'have'], ['noticed', 'that', 'the', 'floor', 'of', 'the', 'gallery', 'sloped', 'at', 'all', 'footnote', 'it'], ['may', 'be', 'of', 'course', 'that', 'the', 'floor', 'did', 'not', 'slope', 'but', 'that', 'the', 'museum'], ['was', 'built', 'into', 'the', 'side', 'of', 'a', 'hill', 'ed', 'the', 'end', 'i', 'had', 'come', 'in', 'at'], ['was', 'quite', 'above', 'ground', 'and', 'was', 'lit', 'by', 'rare', 'slit', 'like', 'windows', 'as'], ['you', 'went', 'down', 'the', 'length', 'the', 'ground', 'came', 'up', 'against', 'these', 'windows'], ['until', 'at', 'last', 'there', 'was', 'a', 'pit', 'like', 'the', 'area', 'of', 'a', 'london', 'house'], ['before', 'each', 'and', 'only', 'a', 'narrow', 'line', 'of', 'daylight', 'at', 'the', 'top', 'i', 'went'], ['slowly', 'along', 'puzzling', 'about', 'the', 'machines', 'and', 'had', 'been', 'too', 'intent'], ['upon', 'them', 'to', 'notice', 'the', 'gradual', 'diminution', 'of', 'the', 'light', 'until'], ['weena', 's', 'increasing', 'apprehensions', 'drew', 'my', 'attention', 'then', 'i', 'saw', 'that'], ['the', 'gallery', 'ran', 'down', 'at', 'last', 'into', 'a', 'thick', 'darkness', 'i', 'hesitated', 'and'], ['then', 'as', 'i', 'looked', 'round', 'me', 'i', 'saw', 'that', 'the', 'dust', 'was', 'less', 'abundant'], ['and', 'its', 'surface', 'less', 'even', 'further', 'away', 'towards', 'the', 'dimness', 'it'], ['appeared', 'to', 'be', 'broken', 'by', 'a', 'number', 'of', 'small', 'narrow', 'footprints', 'my'], ['sense', 'of', 'the', 'immediate', 'presence', 'of', 'the', 'morlocks', 'revived', 'at', 'that'], ['i', 'felt', 'that', 'i', 'was', 'wasting', 'my', 'time', 'in', 'the', 'academic', 'examination', 'of'], ['machinery', 'i', 'called', 'to', 'mind', 'that', 'it', 'was', 'already', 'far', 'advanced', 'in', 'the'], ['afternoon', 'and', 'that', 'i', 'had', 'still', 'no', 'weapon', 'no', 'refuge', 'and', 'no', 'means'], ['of', 'making', 'a', 'fire', 'and', 'then', 'down', 'in', 'the', 'remote', 'blackness', 'of', 'the'], ['gallery', 'i', 'heard', 'a', 'peculiar', 'pattering', 'and', 'the', 'same', 'odd', 'noises', 'i', 'had'], ['heard', 'down', 'the', 'well'], [], ['i', 'took', 'weena', 's', 'hand', 'then', 'struck', 'with', 'a', 'sudden', 'idea', 'i', 'left', 'her'], ['and', 'turned', 'to', 'a', 'machine', 'from', 'which', 'projected', 'a', 'lever', 'not', 'unlike'], ['those', 'in', 'a', 'signal', 'box', 'clambering', 'upon', 'the', 'stand', 'and', 'grasping', 'this'], ['lever', 'in', 'my', 'hands', 'i', 'put', 'all', 'my', 'weight', 'upon', 'it', 'sideways', 'suddenly'], ['weena', 'deserted', 'in', 'the', 'central', 'aisle', 'began', 'to', 'whimper', 'i', 'had', 'judged'], ['the', 'strength', 'of', 'the', 'lever', 'pretty', 'correctly', 'for', 'it', 'snapped', 'after', 'a'], ['minute', 's', 'strain', 'and', 'i', 'rejoined', 'her', 'with', 'a', 'mace', 'in', 'my', 'hand', 'more', 'than'], ['sufficient', 'i', 'judged', 'for', 'any', 'morlock', 'skull', 'i', 'might', 'encounter', 'and', 'i'], ['longed', 'very', 'much', 'to', 'kill', 'a', 'morlock', 'or', 'so', 'very', 'inhuman', 'you', 'may'], ['think', 'to', 'want', 'to', 'go', 'killing', 'one', 's', 'own', 'descendants', 'but', 'it', 'was'], ['impossible', 'somehow', 'to', 'feel', 'any', 'humanity', 'in', 'the', 'things', 'only', 'my'], ['disinclination', 'to', 'leave', 'weena', 'and', 'a', 'persuasion', 'that', 'if', 'i', 'began', 'to'], ['slake', 'my', 'thirst', 'for', 'murder', 'my', 'time', 'machine', 'might', 'suffer', 'restrained'], ['me', 'from', 'going', 'straight', 'down', 'the', 'gallery', 'and', 'killing', 'the', 'brutes', 'i'], ['heard'], [], ['well', 'mace', 'in', 'one', 'hand', 'and', 'weena', 'in', 'the', 'other', 'i', 'went', 'out', 'of', 'that'], ['gallery', 'and', 'into', 'another', 'and', 'still', 'larger', 'one', 'which', 'at', 'the', 'first'], ['glance', 'reminded', 'me', 'of', 'a', 'military', 'chapel', 'hung', 'with', 'tattered', 'flags'], ['the', 'brown', 'and', 'charred', 'rags', 'that', 'hung', 'from', 'the', 'sides', 'of', 'it', 'i'], ['presently', 'recognized', 'as', 'the', 'decaying', 'vestiges', 'of', 'books', 'they', 'had'], ['long', 'since', 'dropped', 'to', 'pieces', 'and', 'every', 'semblance', 'of', 'print', 'had', 'left'], ['them', 'but', 'here', 'and', 'there', 'were', 'warped', 'boards', 'and', 'cracked', 'metallic'], ['clasps', 'that', 'told', 'the', 'tale', 'well', 'enough', 'had', 'i', 'been', 'a', 'literary', 'man', 'i'], ['might', 'perhaps', 'have', 'moralized', 'upon', 'the', 'futility', 'of', 'all', 'ambition'], ['but', 'as', 'it', 'was', 'the', 'thing', 'that', 'struck', 'me', 'with', 'keenest', 'force', 'was', 'the'], ['enormous', 'waste', 'of', 'labour', 'to', 'which', 'this', 'sombre', 'wilderness', 'of', 'rotting'], ['paper', 'testified', 'at', 'the', 'time', 'i', 'will', 'confess', 'that', 'i', 'thought', 'chiefly'], ['of', 'the', 'philosophical', 'transactions', 'and', 'my', 'own', 'seventeen', 'papers', 'upon'], ['physical', 'optics'], [], ['then', 'going', 'up', 'a', 'broad', 'staircase', 'we', 'came', 'to', 'what', 'may', 'once', 'have'], ['been', 'a', 'gallery', 'of', 'technical', 'chemistry', 'and', 'here', 'i', 'had', 'not', 'a', 'little'], ['hope', 'of', 'useful', 'discoveries', 'except', 'at', 'one', 'end', 'where', 'the', 'roof', 'had'], ['collapsed', 'this', 'gallery', 'was', 'well', 'preserved', 'i', 'went', 'eagerly', 'to', 'every'], ['unbroken', 'case', 'and', 'at', 'last', 'in', 'one', 'of', 'the', 'really', 'air', 'tight', 'cases'], ['i', 'found', 'a', 'box', 'of', 'matches', 'very', 'eagerly', 'i', 'tried', 'them', 'they', 'were'], ['perfectly', 'good', 'they', 'were', 'not', 'even', 'damp', 'i', 'turned', 'to', 'weena', 'dance'], ['i', 'cried', 'to', 'her', 'in', 'her', 'own', 'tongue', 'for', 'now', 'i', 'had', 'a', 'weapon', 'indeed'], ['against', 'the', 'horrible', 'creatures', 'we', 'feared', 'and', 'so', 'in', 'that', 'derelict'], ['museum', 'upon', 'the', 'thick', 'soft', 'carpeting', 'of', 'dust', 'to', 'weena', 's', 'huge'], ['delight', 'i', 'solemnly', 'performed', 'a', 'kind', 'of', 'composite', 'dance', 'whistling'], ['the', 'land', 'of', 'the', 'leal', 'as', 'cheerfully', 'as', 'i', 'could', 'in', 'part', 'it', 'was', 'a'], ['modest', 'cancan', 'in', 'part', 'a', 'step', 'dance', 'in', 'part', 'a', 'skirt', 'dance', 'so', 'far'], ['as', 'my', 'tail', 'coat', 'permitted', 'and', 'in', 'part', 'original', 'for', 'i', 'am', 'naturally'], ['inventive', 'as', 'you', 'know'], [], ['now', 'i', 'still', 'think', 'that', 'for', 'this', 'box', 'of', 'matches', 'to', 'have', 'escaped'], ['the', 'wear', 'of', 'time', 'for', 'immemorial', 'years', 'was', 'a', 'most', 'strange', 'as', 'for'], ['me', 'it', 'was', 'a', 'most', 'fortunate', 'thing', 'yet', 'oddly', 'enough', 'i', 'found', 'a', 'far'], ['unlikelier', 'substance', 'and', 'that', 'was', 'camphor', 'i', 'found', 'it', 'in', 'a', 'sealed'], ['jar', 'that', 'by', 'chance', 'i', 'suppose', 'had', 'been', 'really', 'hermetically', 'sealed'], ['i', 'fancied', 'at', 'first', 'that', 'it', 'was', 'paraffin', 'wax', 'and', 'smashed', 'the', 'glass'], ['accordingly', 'but', 'the', 'odour', 'of', 'camphor', 'was', 'unmistakable', 'in', 'the'], ['universal', 'decay', 'this', 'volatile', 'substance', 'had', 'chanced', 'to', 'survive'], ['perhaps', 'through', 'many', 'thousands', 'of', 'centuries', 'it', 'reminded', 'me', 'of', 'a'], ['sepia', 'painting', 'i', 'had', 'once', 'seen', 'done', 'from', 'the', 'ink', 'of', 'a', 'fossil'], ['belemnite', 'that', 'must', 'have', 'perished', 'and', 'become', 'fossilized', 'millions'], ['of', 'years', 'ago', 'i', 'was', 'about', 'to', 'throw', 'it', 'away', 'but', 'i', 'remembered', 'that'], ['it', 'was', 'inflammable', 'and', 'burned', 'with', 'a', 'good', 'bright', 'flame', 'was', 'in'], ['fact', 'an', 'excellent', 'candle', 'and', 'i', 'put', 'it', 'in', 'my', 'pocket', 'i', 'found', 'no'], ['explosives', 'however', 'nor', 'any', 'means', 'of', 'breaking', 'down', 'the', 'bronze'], ['doors', 'as', 'yet', 'my', 'iron', 'crowbar', 'was', 'the', 'most', 'helpful', 'thing', 'i', 'had'], ['chanced', 'upon', 'nevertheless', 'i', 'left', 'that', 'gallery', 'greatly', 'elated'], [], ['i', 'cannot', 'tell', 'you', 'all', 'the', 'story', 'of', 'that', 'long', 'afternoon', 'it', 'would'], ['require', 'a', 'great', 'effort', 'of', 'memory', 'to', 'recall', 'my', 'explorations', 'in', 'at', 'all'], ['the', 'proper', 'order', 'i', 'remember', 'a', 'long', 'gallery', 'of', 'rusting', 'stands', 'of'], ['arms', 'and', 'how', 'i', 'hesitated', 'between', 'my', 'crowbar', 'and', 'a', 'hatchet', 'or', 'a'], ['sword', 'i', 'could', 'not', 'carry', 'both', 'however', 'and', 'my', 'bar', 'of', 'iron', 'promised'], ['best', 'against', 'the', 'bronze', 'gates', 'there', 'were', 'numbers', 'of', 'guns', 'pistols'], ['and', 'rifles', 'the', 'most', 'were', 'masses', 'of', 'rust', 'but', 'many', 'were', 'of', 'some'], ['new', 'metal', 'and', 'still', 'fairly', 'sound', 'but', 'any', 'cartridges', 'or', 'powder'], ['there', 'may', 'once', 'have', 'been', 'had', 'rotted', 'into', 'dust', 'one', 'corner', 'i', 'saw', 'was'], ['charred', 'and', 'shattered', 'perhaps', 'i', 'thought', 'by', 'an', 'explosion', 'among', 'the'], ['specimens', 'in', 'another', 'place', 'was', 'a', 'vast', 'array', 'of', 'idols', 'polynesian'], ['mexican', 'grecian', 'phoenician', 'every', 'country', 'on', 'earth', 'i', 'should', 'think'], ['and', 'here', 'yielding', 'to', 'an', 'irresistible', 'impulse', 'i', 'wrote', 'my', 'name', 'upon'], ['the', 'nose', 'of', 'a', 'steatite', 'monster', 'from', 'south', 'america', 'that', 'particularly'], ['took', 'my', 'fancy'], [], ['as', 'the', 'evening', 'drew', 'on', 'my', 'interest', 'waned', 'i', 'went', 'through', 'gallery'], ['after', 'gallery', 'dusty', 'silent', 'often', 'ruinous', 'the', 'exhibits', 'sometimes'], ['mere', 'heaps', 'of', 'rust', 'and', 'lignite', 'sometimes', 'fresher', 'in', 'one', 'place', 'i'], ['suddenly', 'found', 'myself', 'near', 'the', 'model', 'of', 'a', 'tin', 'mine', 'and', 'then', 'by', 'the'], ['merest', 'accident', 'i', 'discovered', 'in', 'an', 'air', 'tight', 'case', 'two', 'dynamite'], ['cartridges', 'i', 'shouted', 'eureka', 'and', 'smashed', 'the', 'case', 'with', 'joy', 'then'], ['came', 'a', 'doubt', 'i', 'hesitated', 'then', 'selecting', 'a', 'little', 'side', 'gallery'], ['i', 'made', 'my', 'essay', 'i', 'never', 'felt', 'such', 'a', 'disappointment', 'as', 'i', 'did', 'in'], ['waiting', 'five', 'ten', 'fifteen', 'minutes', 'for', 'an', 'explosion', 'that', 'never', 'came'], ['of', 'course', 'the', 'things', 'were', 'dummies', 'as', 'i', 'might', 'have', 'guessed', 'from'], ['their', 'presence', 'i', 'really', 'believe', 'that', 'had', 'they', 'not', 'been', 'so', 'i', 'should'], ['have', 'rushed', 'off', 'incontinently', 'and', 'blown', 'sphinx', 'bronze', 'doors', 'and'], ['as', 'it', 'proved', 'my', 'chances', 'of', 'finding', 'the', 'time', 'machine', 'all', 'together'], ['into', 'non', 'existence'], [], ['it', 'was', 'after', 'that', 'i', 'think', 'that', 'we', 'came', 'to', 'a', 'little', 'open', 'court'], ['within', 'the', 'palace', 'it', 'was', 'turfed', 'and', 'had', 'three', 'fruit', 'trees', 'so', 'we'], ['rested', 'and', 'refreshed', 'ourselves', 'towards', 'sunset', 'i', 'began', 'to', 'consider'], ['our', 'position', 'night', 'was', 'creeping', 'upon', 'us', 'and', 'my', 'inaccessible'], ['hiding', 'place', 'had', 'still', 'to', 'be', 'found', 'but', 'that', 'troubled', 'me', 'very', 'little'], ['now', 'i', 'had', 'in', 'my', 'possession', 'a', 'thing', 'that', 'was', 'perhaps', 'the', 'best', 'of'], ['all', 'defences', 'against', 'the', 'morlocks', 'i', 'had', 'matches', 'i', 'had', 'the', 'camphor'], ['in', 'my', 'pocket', 'too', 'if', 'a', 'blaze', 'were', 'needed', 'it', 'seemed', 'to', 'me', 'that'], ['the', 'best', 'thing', 'we', 'could', 'do', 'would', 'be', 'to', 'pass', 'the', 'night', 'in', 'the', 'open'], ['protected', 'by', 'a', 'fire', 'in', 'the', 'morning', 'there', 'was', 'the', 'getting', 'of', 'the'], ['time', 'machine', 'towards', 'that', 'as', 'yet', 'i', 'had', 'only', 'my', 'iron', 'mace', 'but'], ['now', 'with', 'my', 'growing', 'knowledge', 'i', 'felt', 'very', 'differently', 'towards'], ['those', 'bronze', 'doors', 'up', 'to', 'this', 'i', 'had', 'refrained', 'from', 'forcing', 'them'], ['largely', 'because', 'of', 'the', 'mystery', 'on', 'the', 'other', 'side', 'they', 'had', 'never'], ['impressed', 'me', 'as', 'being', 'very', 'strong', 'and', 'i', 'hoped', 'to', 'find', 'my', 'bar', 'of'], ['iron', 'not', 'altogether', 'inadequate', 'for', 'the', 'work'], [], [], [], [], ['ix'], [], [], ['we', 'emerged', 'from', 'the', 'palace', 'while', 'the', 'sun', 'was', 'still', 'in', 'part', 'above'], ['the', 'horizon', 'i', 'was', 'determined', 'to', 'reach', 'the', 'white', 'sphinx', 'early', 'the'], ['next', 'morning', 'and', 'ere', 'the', 'dusk', 'i', 'purposed', 'pushing', 'through', 'the', 'woods'], ['that', 'had', 'stopped', 'me', 'on', 'the', 'previous', 'journey', 'my', 'plan', 'was', 'to', 'go', 'as'], ['far', 'as', 'possible', 'that', 'night', 'and', 'then', 'building', 'a', 'fire', 'to', 'sleep'], ['in', 'the', 'protection', 'of', 'its', 'glare', 'accordingly', 'as', 'we', 'went', 'along', 'i'], ['gathered', 'any', 'sticks', 'or', 'dried', 'grass', 'i', 'saw', 'and', 'presently', 'had', 'my', 'arms'], ['full', 'of', 'such', 'litter', 'thus', 'loaded', 'our', 'progress', 'was', 'slower', 'than', 'i', 'had'], ['anticipated', 'and', 'besides', 'weena', 'was', 'tired', 'and', 'i', 'began', 'to', 'suffer', 'from'], ['sleepiness', 'too', 'so', 'that', 'it', 'was', 'full', 'night', 'before', 'we', 'reached', 'the'], ['wood', 'upon', 'the', 'shrubby', 'hill', 'of', 'its', 'edge', 'weena', 'would', 'have', 'stopped'], ['fearing', 'the', 'darkness', 'before', 'us', 'but', 'a', 'singular', 'sense', 'of', 'impending'], ['calamity', 'that', 'should', 'indeed', 'have', 'served', 'me', 'as', 'a', 'warning', 'drove', 'me'], ['onward', 'i', 'had', 'been', 'without', 'sleep', 'for', 'a', 'night', 'and', 'two', 'days', 'and', 'i', 'was'], ['feverish', 'and', 'irritable', 'i', 'felt', 'sleep', 'coming', 'upon', 'me', 'and', 'the'], ['morlocks', 'with', 'it'], [], ['while', 'we', 'hesitated', 'among', 'the', 'black', 'bushes', 'behind', 'us', 'and', 'dim'], ['against', 'their', 'blackness', 'i', 'saw', 'three', 'crouching', 'figures', 'there', 'was'], ['scrub', 'and', 'long', 'grass', 'all', 'about', 'us', 'and', 'i', 'did', 'not', 'feel', 'safe', 'from'], ['their', 'insidious', 'approach', 'the', 'forest', 'i', 'calculated', 'was', 'rather'], ['less', 'than', 'a', 'mile', 'across', 'if', 'we', 'could', 'get', 'through', 'it', 'to', 'the', 'bare'], ['hill', 'side', 'there', 'as', 'it', 'seemed', 'to', 'me', 'was', 'an', 'altogether', 'safer'], ['resting', 'place', 'i', 'thought', 'that', 'with', 'my', 'matches', 'and', 'my', 'camphor', 'i', 'could'], ['contrive', 'to', 'keep', 'my', 'path', 'illuminated', 'through', 'the', 'woods', 'yet', 'it', 'was'], ['evident', 'that', 'if', 'i', 'was', 'to', 'flourish', 'matches', 'with', 'my', 'hands', 'i', 'should'], ['have', 'to', 'abandon', 'my', 'firewood', 'so', 'rather', 'reluctantly', 'i', 'put', 'it', 'down'], ['and', 'then', 'it', 'came', 'into', 'my', 'head', 'that', 'i', 'would', 'amaze', 'our', 'friends', 'behind'], ['by', 'lighting', 'it', 'i', 'was', 'to', 'discover', 'the', 'atrocious', 'folly', 'of', 'this'], ['proceeding', 'but', 'it', 'came', 'to', 'my', 'mind', 'as', 'an', 'ingenious', 'move', 'for', 'covering'], ['our', 'retreat'], [], ['i', 'don', 't', 'know', 'if', 'you', 'have', 'ever', 'thought', 'what', 'a', 'rare', 'thing', 'flame', 'must'], ['be', 'in', 'the', 'absence', 'of', 'man', 'and', 'in', 'a', 'temperate', 'climate', 'the', 'sun', 's'], ['heat', 'is', 'rarely', 'strong', 'enough', 'to', 'burn', 'even', 'when', 'it', 'is', 'focused', 'by'], ['dewdrops', 'as', 'is', 'sometimes', 'the', 'case', 'in', 'more', 'tropical', 'districts'], ['lightning', 'may', 'blast', 'and', 'blacken', 'but', 'it', 'rarely', 'gives', 'rise', 'to'], ['widespread', 'fire', 'decaying', 'vegetation', 'may', 'occasionally', 'smoulder', 'with'], ['the', 'heat', 'of', 'its', 'fermentation', 'but', 'this', 'rarely', 'results', 'in', 'flame', 'in'], ['this', 'decadence', 'too', 'the', 'art', 'of', 'fire', 'making', 'had', 'been', 'forgotten', 'on'], ['the', 'earth', 'the', 'red', 'tongues', 'that', 'went', 'licking', 'up', 'my', 'heap', 'of', 'wood', 'were'], ['an', 'altogether', 'new', 'and', 'strange', 'thing', 'to', 'weena'], [], ['she', 'wanted', 'to', 'run', 'to', 'it', 'and', 'play', 'with', 'it', 'i', 'believe', 'she', 'would', 'have'], ['cast', 'herself', 'into', 'it', 'had', 'i', 'not', 'restrained', 'her', 'but', 'i', 'caught', 'her', 'up'], ['and', 'in', 'spite', 'of', 'her', 'struggles', 'plunged', 'boldly', 'before', 'me', 'into', 'the'], ['wood', 'for', 'a', 'little', 'way', 'the', 'glare', 'of', 'my', 'fire', 'lit', 'the', 'path', 'looking'], ['back', 'presently', 'i', 'could', 'see', 'through', 'the', 'crowded', 'stems', 'that', 'from', 'my'], ['heap', 'of', 'sticks', 'the', 'blaze', 'had', 'spread', 'to', 'some', 'bushes', 'adjacent', 'and', 'a'], ['curved', 'line', 'of', 'fire', 'was', 'creeping', 'up', 'the', 'grass', 'of', 'the', 'hill', 'i', 'laughed'], ['at', 'that', 'and', 'turned', 'again', 'to', 'the', 'dark', 'trees', 'before', 'me', 'it', 'was', 'very'], ['black', 'and', 'weena', 'clung', 'to', 'me', 'convulsively', 'but', 'there', 'was', 'still', 'as'], ['my', 'eyes', 'grew', 'accustomed', 'to', 'the', 'darkness', 'sufficient', 'light', 'for', 'me', 'to'], ['avoid', 'the', 'stems', 'overhead', 'it', 'was', 'simply', 'black', 'except', 'where', 'a', 'gap', 'of'], ['remote', 'blue', 'sky', 'shone', 'down', 'upon', 'us', 'here', 'and', 'there', 'i', 'struck', 'none', 'of'], ['my', 'matches', 'because', 'i', 'had', 'no', 'hand', 'free', 'upon', 'my', 'left', 'arm', 'i', 'carried', 'my'], ['little', 'one', 'in', 'my', 'right', 'hand', 'i', 'had', 'my', 'iron', 'bar'], [], ['for', 'some', 'way', 'i', 'heard', 'nothing', 'but', 'the', 'crackling', 'twigs', 'under', 'my', 'feet'], ['the', 'faint', 'rustle', 'of', 'the', 'breeze', 'above', 'and', 'my', 'own', 'breathing', 'and', 'the'], ['throb', 'of', 'the', 'blood', 'vessels', 'in', 'my', 'ears', 'then', 'i', 'seemed', 'to', 'know', 'of', 'a'], ['pattering', 'about', 'me', 'i', 'pushed', 'on', 'grimly', 'the', 'pattering', 'grew', 'more'], ['distinct', 'and', 'then', 'i', 'caught', 'the', 'same', 'queer', 'sound', 'and', 'voices', 'i', 'had'], ['heard', 'in', 'the', 'under', 'world', 'there', 'were', 'evidently', 'several', 'of', 'the'], ['morlocks', 'and', 'they', 'were', 'closing', 'in', 'upon', 'me', 'indeed', 'in', 'another'], ['minute', 'i', 'felt', 'a', 'tug', 'at', 'my', 'coat', 'then', 'something', 'at', 'my', 'arm', 'and', 'weena'], ['shivered', 'violently', 'and', 'became', 'quite', 'still'], [], ['it', 'was', 'time', 'for', 'a', 'match', 'but', 'to', 'get', 'one', 'i', 'must', 'put', 'her', 'down', 'i', 'did'], ['so', 'and', 'as', 'i', 'fumbled', 'with', 'my', 'pocket', 'a', 'struggle', 'began', 'in', 'the'], ['darkness', 'about', 'my', 'knees', 'perfectly', 'silent', 'on', 'her', 'part', 'and', 'with', 'the'], ['same', 'peculiar', 'cooing', 'sounds', 'from', 'the', 'morlocks', 'soft', 'little', 'hands'], ['too', 'were', 'creeping', 'over', 'my', 'coat', 'and', 'back', 'touching', 'even', 'my', 'neck'], ['then', 'the', 'match', 'scratched', 'and', 'fizzed', 'i', 'held', 'it', 'flaring', 'and', 'saw', 'the'], ['white', 'backs', 'of', 'the', 'morlocks', 'in', 'flight', 'amid', 'the', 'trees', 'i', 'hastily', 'took'], ['a', 'lump', 'of', 'camphor', 'from', 'my', 'pocket', 'and', 'prepared', 'to', 'light', 'it', 'as', 'soon'], ['as', 'the', 'match', 'should', 'wane', 'then', 'i', 'looked', 'at', 'weena', 'she', 'was', 'lying'], ['clutching', 'my', 'feet', 'and', 'quite', 'motionless', 'with', 'her', 'face', 'to', 'the', 'ground'], ['with', 'a', 'sudden', 'fright', 'i', 'stooped', 'to', 'her', 'she', 'seemed', 'scarcely', 'to'], ['breathe', 'i', 'lit', 'the', 'block', 'of', 'camphor', 'and', 'flung', 'it', 'to', 'the', 'ground'], ['and', 'as', 'it', 'split', 'and', 'flared', 'up', 'and', 'drove', 'back', 'the', 'morlocks', 'and', 'the'], ['shadows', 'i', 'knelt', 'down', 'and', 'lifted', 'her', 'the', 'wood', 'behind', 'seemed', 'full', 'of'], ['the', 'stir', 'and', 'murmur', 'of', 'a', 'great', 'company'], [], ['she', 'seemed', 'to', 'have', 'fainted', 'i', 'put', 'her', 'carefully', 'upon', 'my', 'shoulder'], ['and', 'rose', 'to', 'push', 'on', 'and', 'then', 'there', 'came', 'a', 'horrible', 'realization', 'in'], ['manoeuvring', 'with', 'my', 'matches', 'and', 'weena', 'i', 'had', 'turned', 'myself', 'about'], ['several', 'times', 'and', 'now', 'i', 'had', 'not', 'the', 'faintest', 'idea', 'in', 'what', 'direction'], ['lay', 'my', 'path', 'for', 'all', 'i', 'knew', 'i', 'might', 'be', 'facing', 'back', 'towards', 'the'], ['palace', 'of', 'green', 'porcelain', 'i', 'found', 'myself', 'in', 'a', 'cold', 'sweat', 'i', 'had', 'to'], ['think', 'rapidly', 'what', 'to', 'do', 'i', 'determined', 'to', 'build', 'a', 'fire', 'and', 'encamp'], ['where', 'we', 'were', 'i', 'put', 'weena', 'still', 'motionless', 'down', 'upon', 'a', 'turfy'], ['bole', 'and', 'very', 'hastily', 'as', 'my', 'first', 'lump', 'of', 'camphor', 'waned', 'i', 'began'], ['collecting', 'sticks', 'and', 'leaves', 'here', 'and', 'there', 'out', 'of', 'the', 'darkness'], ['round', 'me', 'the', 'morlocks', 'eyes', 'shone', 'like', 'carbuncles'], [], ['the', 'camphor', 'flickered', 'and', 'went', 'out', 'i', 'lit', 'a', 'match', 'and', 'as', 'i', 'did', 'so'], ['two', 'white', 'forms', 'that', 'had', 'been', 'approaching', 'weena', 'dashed', 'hastily', 'away'], ['one', 'was', 'so', 'blinded', 'by', 'the', 'light', 'that', 'he', 'came', 'straight', 'for', 'me', 'and', 'i'], ['felt', 'his', 'bones', 'grind', 'under', 'the', 'blow', 'of', 'my', 'fist', 'he', 'gave', 'a', 'whoop', 'of'], ['dismay', 'staggered', 'a', 'little', 'way', 'and', 'fell', 'down', 'i', 'lit', 'another', 'piece'], ['of', 'camphor', 'and', 'went', 'on', 'gathering', 'my', 'bonfire', 'presently', 'i', 'noticed'], ['how', 'dry', 'was', 'some', 'of', 'the', 'foliage', 'above', 'me', 'for', 'since', 'my', 'arrival'], ['on', 'the', 'time', 'machine', 'a', 'matter', 'of', 'a', 'week', 'no', 'rain', 'had', 'fallen', 'so'], ['instead', 'of', 'casting', 'about', 'among', 'the', 'trees', 'for', 'fallen', 'twigs', 'i', 'began'], ['leaping', 'up', 'and', 'dragging', 'down', 'branches', 'very', 'soon', 'i', 'had', 'a', 'choking'], ['smoky', 'fire', 'of', 'green', 'wood', 'and', 'dry', 'sticks', 'and', 'could', 'economize', 'my'], ['camphor', 'then', 'i', 'turned', 'to', 'where', 'weena', 'lay', 'beside', 'my', 'iron', 'mace', 'i'], ['tried', 'what', 'i', 'could', 'to', 'revive', 'her', 'but', 'she', 'lay', 'like', 'one', 'dead', 'i', 'could'], ['not', 'even', 'satisfy', 'myself', 'whether', 'or', 'not', 'she', 'breathed'], [], ['now', 'the', 'smoke', 'of', 'the', 'fire', 'beat', 'over', 'towards', 'me', 'and', 'it', 'must', 'have'], ['made', 'me', 'heavy', 'of', 'a', 'sudden', 'moreover', 'the', 'vapour', 'of', 'camphor', 'was', 'in'], ['the', 'air', 'my', 'fire', 'would', 'not', 'need', 'replenishing', 'for', 'an', 'hour', 'or', 'so', 'i'], ['felt', 'very', 'weary', 'after', 'my', 'exertion', 'and', 'sat', 'down', 'the', 'wood', 'too', 'was'], ['full', 'of', 'a', 'slumbrous', 'murmur', 'that', 'i', 'did', 'not', 'understand', 'i', 'seemed', 'just'], ['to', 'nod', 'and', 'open', 'my', 'eyes', 'but', 'all', 'was', 'dark', 'and', 'the', 'morlocks', 'had'], ['their', 'hands', 'upon', 'me', 'flinging', 'off', 'their', 'clinging', 'fingers', 'i', 'hastily'], ['felt', 'in', 'my', 'pocket', 'for', 'the', 'match', 'box', 'and', 'it', 'had', 'gone', 'then', 'they'], ['gripped', 'and', 'closed', 'with', 'me', 'again', 'in', 'a', 'moment', 'i', 'knew', 'what', 'had'], ['happened', 'i', 'had', 'slept', 'and', 'my', 'fire', 'had', 'gone', 'out', 'and', 'the', 'bitterness'], ['of', 'death', 'came', 'over', 'my', 'soul', 'the', 'forest', 'seemed', 'full', 'of', 'the', 'smell', 'of'], ['burning', 'wood', 'i', 'was', 'caught', 'by', 'the', 'neck', 'by', 'the', 'hair', 'by', 'the', 'arms'], ['and', 'pulled', 'down', 'it', 'was', 'indescribably', 'horrible', 'in', 'the', 'darkness', 'to'], ['feel', 'all', 'these', 'soft', 'creatures', 'heaped', 'upon', 'me', 'i', 'felt', 'as', 'if', 'i', 'was', 'in'], ['a', 'monstrous', 'spider', 's', 'web', 'i', 'was', 'overpowered', 'and', 'went', 'down', 'i', 'felt'], ['little', 'teeth', 'nipping', 'at', 'my', 'neck', 'i', 'rolled', 'over', 'and', 'as', 'i', 'did', 'so', 'my'], ['hand', 'came', 'against', 'my', 'iron', 'lever', 'it', 'gave', 'me', 'strength', 'i', 'struggled'], ['up', 'shaking', 'the', 'human', 'rats', 'from', 'me', 'and', 'holding', 'the', 'bar', 'short'], ['i', 'thrust', 'where', 'i', 'judged', 'their', 'faces', 'might', 'be', 'i', 'could', 'feel', 'the'], ['succulent', 'giving', 'of', 'flesh', 'and', 'bone', 'under', 'my', 'blows', 'and', 'for', 'a', 'moment'], ['i', 'was', 'free'], [], ['the', 'strange', 'exultation', 'that', 'so', 'often', 'seems', 'to', 'accompany', 'hard'], ['fighting', 'came', 'upon', 'me', 'i', 'knew', 'that', 'both', 'i', 'and', 'weena', 'were', 'lost', 'but', 'i'], ['determined', 'to', 'make', 'the', 'morlocks', 'pay', 'for', 'their', 'meat', 'i', 'stood', 'with', 'my'], ['back', 'to', 'a', 'tree', 'swinging', 'the', 'iron', 'bar', 'before', 'me', 'the', 'whole', 'wood', 'was'], ['full', 'of', 'the', 'stir', 'and', 'cries', 'of', 'them', 'a', 'minute', 'passed', 'their', 'voices'], ['seemed', 'to', 'rise', 'to', 'a', 'higher', 'pitch', 'of', 'excitement', 'and', 'their', 'movements'], ['grew', 'faster', 'yet', 'none', 'came', 'within', 'reach', 'i', 'stood', 'glaring', 'at', 'the'], ['blackness', 'then', 'suddenly', 'came', 'hope', 'what', 'if', 'the', 'morlocks', 'were'], ['afraid', 'and', 'close', 'on', 'the', 'heels', 'of', 'that', 'came', 'a', 'strange', 'thing', 'the'], ['darkness', 'seemed', 'to', 'grow', 'luminous', 'very', 'dimly', 'i', 'began', 'to', 'see', 'the'], ['morlocks', 'about', 'me', 'three', 'battered', 'at', 'my', 'feet', 'and', 'then', 'i', 'recognized'], ['with', 'incredulous', 'surprise', 'that', 'the', 'others', 'were', 'running', 'in', 'an'], ['incessant', 'stream', 'as', 'it', 'seemed', 'from', 'behind', 'me', 'and', 'away', 'through', 'the'], ['wood', 'in', 'front', 'and', 'their', 'backs', 'seemed', 'no', 'longer', 'white', 'but', 'reddish'], ['as', 'i', 'stood', 'agape', 'i', 'saw', 'a', 'little', 'red', 'spark', 'go', 'drifting', 'across', 'a', 'gap'], ['of', 'starlight', 'between', 'the', 'branches', 'and', 'vanish', 'and', 'at', 'that', 'i'], ['understood', 'the', 'smell', 'of', 'burning', 'wood', 'the', 'slumbrous', 'murmur', 'that', 'was'], ['growing', 'now', 'into', 'a', 'gusty', 'roar', 'the', 'red', 'glow', 'and', 'the', 'morlocks'], ['flight'], [], ['stepping', 'out', 'from', 'behind', 'my', 'tree', 'and', 'looking', 'back', 'i', 'saw', 'through'], ['the', 'black', 'pillars', 'of', 'the', 'nearer', 'trees', 'the', 'flames', 'of', 'the', 'burning'], ['forest', 'it', 'was', 'my', 'first', 'fire', 'coming', 'after', 'me', 'with', 'that', 'i', 'looked', 'for'], ['weena', 'but', 'she', 'was', 'gone', 'the', 'hissing', 'and', 'crackling', 'behind', 'me', 'the'], ['explosive', 'thud', 'as', 'each', 'fresh', 'tree', 'burst', 'into', 'flame', 'left', 'little'], ['time', 'for', 'reflection', 'my', 'iron', 'bar', 'still', 'gripped', 'i', 'followed', 'in', 'the'], ['morlocks', 'path', 'it', 'was', 'a', 'close', 'race', 'once', 'the', 'flames', 'crept', 'forward'], ['so', 'swiftly', 'on', 'my', 'right', 'as', 'i', 'ran', 'that', 'i', 'was', 'outflanked', 'and', 'had', 'to'], ['strike', 'off', 'to', 'the', 'left', 'but', 'at', 'last', 'i', 'emerged', 'upon', 'a', 'small', 'open'], ['space', 'and', 'as', 'i', 'did', 'so', 'a', 'morlock', 'came', 'blundering', 'towards', 'me', 'and'], ['past', 'me', 'and', 'went', 'on', 'straight', 'into', 'the', 'fire'], [], ['and', 'now', 'i', 'was', 'to', 'see', 'the', 'most', 'weird', 'and', 'horrible', 'thing', 'i', 'think', 'of'], ['all', 'that', 'i', 'beheld', 'in', 'that', 'future', 'age', 'this', 'whole', 'space', 'was', 'as', 'bright'], ['as', 'day', 'with', 'the', 'reflection', 'of', 'the', 'fire', 'in', 'the', 'centre', 'was', 'a', 'hillock'], ['or', 'tumulus', 'surmounted', 'by', 'a', 'scorched', 'hawthorn', 'beyond', 'this', 'was'], ['another', 'arm', 'of', 'the', 'burning', 'forest', 'with', 'yellow', 'tongues', 'already'], ['writhing', 'from', 'it', 'completely', 'encircling', 'the', 'space', 'with', 'a', 'fence', 'of'], ['fire', 'upon', 'the', 'hill', 'side', 'were', 'some', 'thirty', 'or', 'forty', 'morlocks', 'dazzled'], ['by', 'the', 'light', 'and', 'heat', 'and', 'blundering', 'hither', 'and', 'thither', 'against'], ['each', 'other', 'in', 'their', 'bewilderment', 'at', 'first', 'i', 'did', 'not', 'realize', 'their'], ['blindness', 'and', 'struck', 'furiously', 'at', 'them', 'with', 'my', 'bar', 'in', 'a', 'frenzy', 'of'], ['fear', 'as', 'they', 'approached', 'me', 'killing', 'one', 'and', 'crippling', 'several', 'more'], ['but', 'when', 'i', 'had', 'watched', 'the', 'gestures', 'of', 'one', 'of', 'them', 'groping', 'under', 'the'], ['hawthorn', 'against', 'the', 'red', 'sky', 'and', 'heard', 'their', 'moans', 'i', 'was', 'assured'], ['of', 'their', 'absolute', 'helplessness', 'and', 'misery', 'in', 'the', 'glare', 'and', 'i', 'struck'], ['no', 'more', 'of', 'them'], [], ['yet', 'every', 'now', 'and', 'then', 'one', 'would', 'come', 'straight', 'towards', 'me', 'setting'], ['loose', 'a', 'quivering', 'horror', 'that', 'made', 'me', 'quick', 'to', 'elude', 'him', 'at', 'one'], ['time', 'the', 'flames', 'died', 'down', 'somewhat', 'and', 'i', 'feared', 'the', 'foul', 'creatures'], ['would', 'presently', 'be', 'able', 'to', 'see', 'me', 'i', 'was', 'thinking', 'of', 'beginning', 'the'], ['fight', 'by', 'killing', 'some', 'of', 'them', 'before', 'this', 'should', 'happen', 'but', 'the'], ['fire', 'burst', 'out', 'again', 'brightly', 'and', 'i', 'stayed', 'my', 'hand', 'i', 'walked', 'about'], ['the', 'hill', 'among', 'them', 'and', 'avoided', 'them', 'looking', 'for', 'some', 'trace', 'of'], ['weena', 'but', 'weena', 'was', 'gone'], [], ['at', 'last', 'i', 'sat', 'down', 'on', 'the', 'summit', 'of', 'the', 'hillock', 'and', 'watched', 'this'], ['strange', 'incredible', 'company', 'of', 'blind', 'things', 'groping', 'to', 'and', 'fro', 'and'], ['making', 'uncanny', 'noises', 'to', 'each', 'other', 'as', 'the', 'glare', 'of', 'the', 'fire', 'beat'], ['on', 'them', 'the', 'coiling', 'uprush', 'of', 'smoke', 'streamed', 'across', 'the', 'sky', 'and'], ['through', 'the', 'rare', 'tatters', 'of', 'that', 'red', 'canopy', 'remote', 'as', 'though', 'they'], ['belonged', 'to', 'another', 'universe', 'shone', 'the', 'little', 'stars', 'two', 'or', 'three'], ['morlocks', 'came', 'blundering', 'into', 'me', 'and', 'i', 'drove', 'them', 'off', 'with', 'blows'], ['of', 'my', 'fists', 'trembling', 'as', 'i', 'did', 'so'], [], ['for', 'the', 'most', 'part', 'of', 'that', 'night', 'i', 'was', 'persuaded', 'it', 'was', 'a', 'nightmare'], ['i', 'bit', 'myself', 'and', 'screamed', 'in', 'a', 'passionate', 'desire', 'to', 'awake', 'i', 'beat'], ['the', 'ground', 'with', 'my', 'hands', 'and', 'got', 'up', 'and', 'sat', 'down', 'again', 'and'], ['wandered', 'here', 'and', 'there', 'and', 'again', 'sat', 'down', 'then', 'i', 'would', 'fall', 'to'], ['rubbing', 'my', 'eyes', 'and', 'calling', 'upon', 'god', 'to', 'let', 'me', 'awake', 'thrice', 'i', 'saw'], ['morlocks', 'put', 'their', 'heads', 'down', 'in', 'a', 'kind', 'of', 'agony', 'and', 'rush', 'into', 'the'], ['flames', 'but', 'at', 'last', 'above', 'the', 'subsiding', 'red', 'of', 'the', 'fire', 'above', 'the'], ['streaming', 'masses', 'of', 'black', 'smoke', 'and', 'the', 'whitening', 'and', 'blackening'], ['tree', 'stumps', 'and', 'the', 'diminishing', 'numbers', 'of', 'these', 'dim', 'creatures'], ['came', 'the', 'white', 'light', 'of', 'the', 'day'], [], ['i', 'searched', 'again', 'for', 'traces', 'of', 'weena', 'but', 'there', 'were', 'none', 'it', 'was'], ['plain', 'that', 'they', 'had', 'left', 'her', 'poor', 'little', 'body', 'in', 'the', 'forest', 'i'], ['cannot', 'describe', 'how', 'it', 'relieved', 'me', 'to', 'think', 'that', 'it', 'had', 'escaped', 'the'], ['awful', 'fate', 'to', 'which', 'it', 'seemed', 'destined', 'as', 'i', 'thought', 'of', 'that', 'i', 'was'], ['almost', 'moved', 'to', 'begin', 'a', 'massacre', 'of', 'the', 'helpless', 'abominations', 'about'], ['me', 'but', 'i', 'contained', 'myself', 'the', 'hillock', 'as', 'i', 'have', 'said', 'was', 'a', 'kind'], ['of', 'island', 'in', 'the', 'forest', 'from', 'its', 'summit', 'i', 'could', 'now', 'make', 'out'], ['through', 'a', 'haze', 'of', 'smoke', 'the', 'palace', 'of', 'green', 'porcelain', 'and', 'from', 'that'], ['i', 'could', 'get', 'my', 'bearings', 'for', 'the', 'white', 'sphinx', 'and', 'so', 'leaving', 'the'], ['remnant', 'of', 'these', 'damned', 'souls', 'still', 'going', 'hither', 'and', 'thither', 'and'], ['moaning', 'as', 'the', 'day', 'grew', 'clearer', 'i', 'tied', 'some', 'grass', 'about', 'my', 'feet'], ['and', 'limped', 'on', 'across', 'smoking', 'ashes', 'and', 'among', 'black', 'stems', 'that', 'still'], ['pulsated', 'internally', 'with', 'fire', 'towards', 'the', 'hiding', 'place', 'of', 'the', 'time'], ['machine', 'i', 'walked', 'slowly', 'for', 'i', 'was', 'almost', 'exhausted', 'as', 'well', 'as'], ['lame', 'and', 'i', 'felt', 'the', 'intensest', 'wretchedness', 'for', 'the', 'horrible', 'death'], ['of', 'little', 'weena', 'it', 'seemed', 'an', 'overwhelming', 'calamity', 'now', 'in', 'this'], ['old', 'familiar', 'room', 'it', 'is', 'more', 'like', 'the', 'sorrow', 'of', 'a', 'dream', 'than', 'an'], ['actual', 'loss', 'but', 'that', 'morning', 'it', 'left', 'me', 'absolutely', 'lonely'], ['again', 'terribly', 'alone', 'i', 'began', 'to', 'think', 'of', 'this', 'house', 'of', 'mine', 'of'], ['this', 'fireside', 'of', 'some', 'of', 'you', 'and', 'with', 'such', 'thoughts', 'came', 'a', 'longing'], ['that', 'was', 'pain'], [], ['but', 'as', 'i', 'walked', 'over', 'the', 'smoking', 'ashes', 'under', 'the', 'bright', 'morning'], ['sky', 'i', 'made', 'a', 'discovery', 'in', 'my', 'trouser', 'pocket', 'were', 'still', 'some', 'loose'], ['matches', 'the', 'box', 'must', 'have', 'leaked', 'before', 'it', 'was', 'lost'], [], [], [], [], ['x'], [], [], ['about', 'eight', 'or', 'nine', 'in', 'the', 'morning', 'i', 'came', 'to', 'the', 'same', 'seat', 'of'], ['yellow', 'metal', 'from', 'which', 'i', 'had', 'viewed', 'the', 'world', 'upon', 'the', 'evening', 'of'], ['my', 'arrival', 'i', 'thought', 'of', 'my', 'hasty', 'conclusions', 'upon', 'that', 'evening', 'and'], ['could', 'not', 'refrain', 'from', 'laughing', 'bitterly', 'at', 'my', 'confidence', 'here'], ['was', 'the', 'same', 'beautiful', 'scene', 'the', 'same', 'abundant', 'foliage', 'the', 'same'], ['splendid', 'palaces', 'and', 'magnificent', 'ruins', 'the', 'same', 'silver', 'river'], ['running', 'between', 'its', 'fertile', 'banks', 'the', 'gay', 'robes', 'of', 'the', 'beautiful'], ['people', 'moved', 'hither', 'and', 'thither', 'among', 'the', 'trees', 'some', 'were', 'bathing'], ['in', 'exactly', 'the', 'place', 'where', 'i', 'had', 'saved', 'weena', 'and', 'that', 'suddenly', 'gave'], ['me', 'a', 'keen', 'stab', 'of', 'pain', 'and', 'like', 'blots', 'upon', 'the', 'landscape', 'rose', 'the'], ['cupolas', 'above', 'the', 'ways', 'to', 'the', 'under', 'world', 'i', 'understood', 'now', 'what', 'all'], ['the', 'beauty', 'of', 'the', 'over', 'world', 'people', 'covered', 'very', 'pleasant', 'was', 'their'], ['day', 'as', 'pleasant', 'as', 'the', 'day', 'of', 'the', 'cattle', 'in', 'the', 'field', 'like', 'the'], ['cattle', 'they', 'knew', 'of', 'no', 'enemies', 'and', 'provided', 'against', 'no', 'needs', 'and'], ['their', 'end', 'was', 'the', 'same'], [], ['i', 'grieved', 'to', 'think', 'how', 'brief', 'the', 'dream', 'of', 'the', 'human', 'intellect', 'had'], ['been', 'it', 'had', 'committed', 'suicide', 'it', 'had', 'set', 'itself', 'steadfastly'], ['towards', 'comfort', 'and', 'ease', 'a', 'balanced', 'society', 'with', 'security', 'and'], ['permanency', 'as', 'its', 'watchword', 'it', 'had', 'attained', 'its', 'hopes', 'to', 'come'], ['to', 'this', 'at', 'last', 'once', 'life', 'and', 'property', 'must', 'have', 'reached', 'almost'], ['absolute', 'safety', 'the', 'rich', 'had', 'been', 'assured', 'of', 'his', 'wealth', 'and'], ['comfort', 'the', 'toiler', 'assured', 'of', 'his', 'life', 'and', 'work', 'no', 'doubt', 'in', 'that'], ['perfect', 'world', 'there', 'had', 'been', 'no', 'unemployed', 'problem', 'no', 'social'], ['question', 'left', 'unsolved', 'and', 'a', 'great', 'quiet', 'had', 'followed'], [], ['it', 'is', 'a', 'law', 'of', 'nature', 'we', 'overlook', 'that', 'intellectual', 'versatility'], ['is', 'the', 'compensation', 'for', 'change', 'danger', 'and', 'trouble', 'an', 'animal'], ['perfectly', 'in', 'harmony', 'with', 'its', 'environment', 'is', 'a', 'perfect', 'mechanism'], ['nature', 'never', 'appeals', 'to', 'intelligence', 'until', 'habit', 'and', 'instinct', 'are'], ['useless', 'there', 'is', 'no', 'intelligence', 'where', 'there', 'is', 'no', 'change', 'and', 'no'], ['need', 'of', 'change', 'only', 'those', 'animals', 'partake', 'of', 'intelligence', 'that', 'have'], ['to', 'meet', 'a', 'huge', 'variety', 'of', 'needs', 'and', 'dangers'], [], ['so', 'as', 'i', 'see', 'it', 'the', 'upper', 'world', 'man', 'had', 'drifted', 'towards', 'his'], ['feeble', 'prettiness', 'and', 'the', 'under', 'world', 'to', 'mere', 'mechanical', 'industry'], ['but', 'that', 'perfect', 'state', 'had', 'lacked', 'one', 'thing', 'even', 'for', 'mechanical'], ['perfection', 'absolute', 'permanency', 'apparently', 'as', 'time', 'went', 'on', 'the'], ['feeding', 'of', 'the', 'under', 'world', 'however', 'it', 'was', 'effected', 'had', 'become'], ['disjointed', 'mother', 'necessity', 'who', 'had', 'been', 'staved', 'off', 'for', 'a'], ['few', 'thousand', 'years', 'came', 'back', 'again', 'and', 'she', 'began', 'below', 'the'], ['under', 'world', 'being', 'in', 'contact', 'with', 'machinery', 'which', 'however', 'perfect'], ['still', 'needs', 'some', 'little', 'thought', 'outside', 'habit', 'had', 'probably', 'retained'], ['perforce', 'rather', 'more', 'initiative', 'if', 'less', 'of', 'every', 'other', 'human'], ['character', 'than', 'the', 'upper', 'and', 'when', 'other', 'meat', 'failed', 'them', 'they'], ['turned', 'to', 'what', 'old', 'habit', 'had', 'hitherto', 'forbidden', 'so', 'i', 'say', 'i', 'saw', 'it'], ['in', 'my', 'last', 'view', 'of', 'the', 'world', 'of', 'eight', 'hundred', 'and', 'two', 'thousand', 'seven'], ['hundred', 'and', 'one', 'it', 'may', 'be', 'as', 'wrong', 'an', 'explanation', 'as', 'mortal', 'wit'], ['could', 'invent', 'it', 'is', 'how', 'the', 'thing', 'shaped', 'itself', 'to', 'me', 'and', 'as', 'that', 'i'], ['give', 'it', 'to', 'you'], [], ['after', 'the', 'fatigues', 'excitements', 'and', 'terrors', 'of', 'the', 'past', 'days', 'and'], ['in', 'spite', 'of', 'my', 'grief', 'this', 'seat', 'and', 'the', 'tranquil', 'view', 'and', 'the', 'warm'], ['sunlight', 'were', 'very', 'pleasant', 'i', 'was', 'very', 'tired', 'and', 'sleepy', 'and', 'soon'], ['my', 'theorizing', 'passed', 'into', 'dozing', 'catching', 'myself', 'at', 'that', 'i', 'took', 'my'], ['own', 'hint', 'and', 'spreading', 'myself', 'out', 'upon', 'the', 'turf', 'i', 'had', 'a', 'long', 'and'], ['refreshing', 'sleep'], [], ['i', 'awoke', 'a', 'little', 'before', 'sunsetting', 'i', 'now', 'felt', 'safe', 'against', 'being'], ['caught', 'napping', 'by', 'the', 'morlocks', 'and', 'stretching', 'myself', 'i', 'came', 'on'], ['down', 'the', 'hill', 'towards', 'the', 'white', 'sphinx', 'i', 'had', 'my', 'crowbar', 'in', 'one'], ['hand', 'and', 'the', 'other', 'hand', 'played', 'with', 'the', 'matches', 'in', 'my', 'pocket'], [], ['and', 'now', 'came', 'a', 'most', 'unexpected', 'thing', 'as', 'i', 'approached', 'the', 'pedestal'], ['of', 'the', 'sphinx', 'i', 'found', 'the', 'bronze', 'valves', 'were', 'open', 'they', 'had', 'slid'], ['down', 'into', 'grooves'], [], ['at', 'that', 'i', 'stopped', 'short', 'before', 'them', 'hesitating', 'to', 'enter'], [], ['within', 'was', 'a', 'small', 'apartment', 'and', 'on', 'a', 'raised', 'place', 'in', 'the', 'corner'], ['of', 'this', 'was', 'the', 'time', 'machine', 'i', 'had', 'the', 'small', 'levers', 'in', 'my', 'pocket'], ['so', 'here', 'after', 'all', 'my', 'elaborate', 'preparations', 'for', 'the', 'siege', 'of', 'the'], ['white', 'sphinx', 'was', 'a', 'meek', 'surrender', 'i', 'threw', 'my', 'iron', 'bar', 'away', 'almost'], ['sorry', 'not', 'to', 'use', 'it'], [], ['a', 'sudden', 'thought', 'came', 'into', 'my', 'head', 'as', 'i', 'stooped', 'towards', 'the', 'portal'], ['for', 'once', 'at', 'least', 'i', 'grasped', 'the', 'mental', 'operations', 'of', 'the', 'morlocks'], ['suppressing', 'a', 'strong', 'inclination', 'to', 'laugh', 'i', 'stepped', 'through', 'the'], ['bronze', 'frame', 'and', 'up', 'to', 'the', 'time', 'machine', 'i', 'was', 'surprised', 'to', 'find', 'it'], ['had', 'been', 'carefully', 'oiled', 'and', 'cleaned', 'i', 'have', 'suspected', 'since', 'that'], ['the', 'morlocks', 'had', 'even', 'partially', 'taken', 'it', 'to', 'pieces', 'while', 'trying', 'in'], ['their', 'dim', 'way', 'to', 'grasp', 'its', 'purpose'], [], ['now', 'as', 'i', 'stood', 'and', 'examined', 'it', 'finding', 'a', 'pleasure', 'in', 'the', 'mere'], ['touch', 'of', 'the', 'contrivance', 'the', 'thing', 'i', 'had', 'expected', 'happened', 'the'], ['bronze', 'panels', 'suddenly', 'slid', 'up', 'and', 'struck', 'the', 'frame', 'with', 'a', 'clang'], ['i', 'was', 'in', 'the', 'dark', 'trapped', 'so', 'the', 'morlocks', 'thought', 'at', 'that', 'i'], ['chuckled', 'gleefully'], [], ['i', 'could', 'already', 'hear', 'their', 'murmuring', 'laughter', 'as', 'they', 'came', 'towards'], ['me', 'very', 'calmly', 'i', 'tried', 'to', 'strike', 'the', 'match', 'i', 'had', 'only', 'to', 'fix', 'on'], ['the', 'levers', 'and', 'depart', 'then', 'like', 'a', 'ghost', 'but', 'i', 'had', 'overlooked', 'one'], ['little', 'thing', 'the', 'matches', 'were', 'of', 'that', 'abominable', 'kind', 'that', 'light'], ['only', 'on', 'the', 'box'], [], ['you', 'may', 'imagine', 'how', 'all', 'my', 'calm', 'vanished', 'the', 'little', 'brutes', 'were'], ['close', 'upon', 'me', 'one', 'touched', 'me', 'i', 'made', 'a', 'sweeping', 'blow', 'in', 'the', 'dark', 'at'], ['them', 'with', 'the', 'levers', 'and', 'began', 'to', 'scramble', 'into', 'the', 'saddle', 'of', 'the'], ['machine', 'then', 'came', 'one', 'hand', 'upon', 'me', 'and', 'then', 'another', 'then', 'i', 'had'], ['simply', 'to', 'fight', 'against', 'their', 'persistent', 'fingers', 'for', 'my', 'levers', 'and'], ['at', 'the', 'same', 'time', 'feel', 'for', 'the', 'studs', 'over', 'which', 'these', 'fitted', 'one'], ['indeed', 'they', 'almost', 'got', 'away', 'from', 'me', 'as', 'it', 'slipped', 'from', 'my', 'hand'], ['i', 'had', 'to', 'butt', 'in', 'the', 'dark', 'with', 'my', 'head', 'i', 'could', 'hear', 'the', 'morlock', 's'], ['skull', 'ring', 'to', 'recover', 'it', 'it', 'was', 'a', 'nearer', 'thing', 'than', 'the', 'fight', 'in'], ['the', 'forest', 'i', 'think', 'this', 'last', 'scramble'], [], ['but', 'at', 'last', 'the', 'lever', 'was', 'fitted', 'and', 'pulled', 'over', 'the', 'clinging'], ['hands', 'slipped', 'from', 'me', 'the', 'darkness', 'presently', 'fell', 'from', 'my', 'eyes'], ['i', 'found', 'myself', 'in', 'the', 'same', 'grey', 'light', 'and', 'tumult', 'i', 'have', 'already'], ['described'], [], [], [], [], ['xi'], [], [], ['i', 'have', 'already', 'told', 'you', 'of', 'the', 'sickness', 'and', 'confusion', 'that', 'comes'], ['with', 'time', 'travelling', 'and', 'this', 'time', 'i', 'was', 'not', 'seated', 'properly', 'in', 'the'], ['saddle', 'but', 'sideways', 'and', 'in', 'an', 'unstable', 'fashion', 'for', 'an', 'indefinite'], ['time', 'i', 'clung', 'to', 'the', 'machine', 'as', 'it', 'swayed', 'and', 'vibrated', 'quite'], ['unheeding', 'how', 'i', 'went', 'and', 'when', 'i', 'brought', 'myself', 'to', 'look', 'at', 'the', 'dials'], ['again', 'i', 'was', 'amazed', 'to', 'find', 'where', 'i', 'had', 'arrived', 'one', 'dial', 'records'], ['days', 'and', 'another', 'thousands', 'of', 'days', 'another', 'millions', 'of', 'days', 'and'], ['another', 'thousands', 'of', 'millions', 'now', 'instead', 'of', 'reversing', 'the', 'levers'], ['i', 'had', 'pulled', 'them', 'over', 'so', 'as', 'to', 'go', 'forward', 'with', 'them', 'and', 'when', 'i'], ['came', 'to', 'look', 'at', 'these', 'indicators', 'i', 'found', 'that', 'the', 'thousands', 'hand', 'was'], ['sweeping', 'round', 'as', 'fast', 'as', 'the', 'seconds', 'hand', 'of', 'a', 'watch', 'into'], ['futurity'], [], ['as', 'i', 'drove', 'on', 'a', 'peculiar', 'change', 'crept', 'over', 'the', 'appearance', 'of'], ['things', 'the', 'palpitating', 'greyness', 'grew', 'darker', 'then', 'though', 'i', 'was'], ['still', 'travelling', 'with', 'prodigious', 'velocity', 'the', 'blinking', 'succession'], ['of', 'day', 'and', 'night', 'which', 'was', 'usually', 'indicative', 'of', 'a', 'slower', 'pace'], ['returned', 'and', 'grew', 'more', 'and', 'more', 'marked', 'this', 'puzzled', 'me', 'very', 'much'], ['at', 'first', 'the', 'alternations', 'of', 'night', 'and', 'day', 'grew', 'slower', 'and', 'slower'], ['and', 'so', 'did', 'the', 'passage', 'of', 'the', 'sun', 'across', 'the', 'sky', 'until', 'they', 'seemed'], ['to', 'stretch', 'through', 'centuries', 'at', 'last', 'a', 'steady', 'twilight', 'brooded', 'over'], ['the', 'earth', 'a', 'twilight', 'only', 'broken', 'now', 'and', 'then', 'when', 'a', 'comet', 'glared'], ['across', 'the', 'darkling', 'sky', 'the', 'band', 'of', 'light', 'that', 'had', 'indicated', 'the'], ['sun', 'had', 'long', 'since', 'disappeared', 'for', 'the', 'sun', 'had', 'ceased', 'to', 'set', 'it'], ['simply', 'rose', 'and', 'fell', 'in', 'the', 'west', 'and', 'grew', 'ever', 'broader', 'and', 'more'], ['red', 'all', 'trace', 'of', 'the', 'moon', 'had', 'vanished', 'the', 'circling', 'of', 'the', 'stars'], ['growing', 'slower', 'and', 'slower', 'had', 'given', 'place', 'to', 'creeping', 'points', 'of'], ['light', 'at', 'last', 'some', 'time', 'before', 'i', 'stopped', 'the', 'sun', 'red', 'and', 'very'], ['large', 'halted', 'motionless', 'upon', 'the', 'horizon', 'a', 'vast', 'dome', 'glowing', 'with'], ['a', 'dull', 'heat', 'and', 'now', 'and', 'then', 'suffering', 'a', 'momentary', 'extinction', 'at'], ['one', 'time', 'it', 'had', 'for', 'a', 'little', 'while', 'glowed', 'more', 'brilliantly', 'again'], ['but', 'it', 'speedily', 'reverted', 'to', 'its', 'sullen', 'red', 'heat', 'i', 'perceived', 'by', 'this'], ['slowing', 'down', 'of', 'its', 'rising', 'and', 'setting', 'that', 'the', 'work', 'of', 'the', 'tidal'], ['drag', 'was', 'done', 'the', 'earth', 'had', 'come', 'to', 'rest', 'with', 'one', 'face', 'to', 'the', 'sun'], ['even', 'as', 'in', 'our', 'own', 'time', 'the', 'moon', 'faces', 'the', 'earth', 'very', 'cautiously'], ['for', 'i', 'remembered', 'my', 'former', 'headlong', 'fall', 'i', 'began', 'to', 'reverse'], ['my', 'motion', 'slower', 'and', 'slower', 'went', 'the', 'circling', 'hands', 'until', 'the'], ['thousands', 'one', 'seemed', 'motionless', 'and', 'the', 'daily', 'one', 'was', 'no', 'longer', 'a'], ['mere', 'mist', 'upon', 'its', 'scale', 'still', 'slower', 'until', 'the', 'dim', 'outlines', 'of', 'a'], ['desolate', 'beach', 'grew', 'visible'], [], ['i', 'stopped', 'very', 'gently', 'and', 'sat', 'upon', 'the', 'time', 'machine', 'looking', 'round'], ['the', 'sky', 'was', 'no', 'longer', 'blue', 'north', 'eastward', 'it', 'was', 'inky', 'black'], ['and', 'out', 'of', 'the', 'blackness', 'shone', 'brightly', 'and', 'steadily', 'the', 'pale'], ['white', 'stars', 'overhead', 'it', 'was', 'a', 'deep', 'indian', 'red', 'and', 'starless', 'and'], ['south', 'eastward', 'it', 'grew', 'brighter', 'to', 'a', 'glowing', 'scarlet', 'where', 'cut', 'by'], ['the', 'horizon', 'lay', 'the', 'huge', 'hull', 'of', 'the', 'sun', 'red', 'and', 'motionless', 'the'], ['rocks', 'about', 'me', 'were', 'of', 'a', 'harsh', 'reddish', 'colour', 'and', 'all', 'the', 'trace', 'of'], ['life', 'that', 'i', 'could', 'see', 'at', 'first', 'was', 'the', 'intensely', 'green', 'vegetation'], ['that', 'covered', 'every', 'projecting', 'point', 'on', 'their', 'south', 'eastern', 'face', 'it'], ['was', 'the', 'same', 'rich', 'green', 'that', 'one', 'sees', 'on', 'forest', 'moss', 'or', 'on', 'the'], ['lichen', 'in', 'caves', 'plants', 'which', 'like', 'these', 'grow', 'in', 'a', 'perpetual'], ['twilight'], [], ['the', 'machine', 'was', 'standing', 'on', 'a', 'sloping', 'beach', 'the', 'sea', 'stretched', 'away'], ['to', 'the', 'south', 'west', 'to', 'rise', 'into', 'a', 'sharp', 'bright', 'horizon', 'against', 'the'], ['wan', 'sky', 'there', 'were', 'no', 'breakers', 'and', 'no', 'waves', 'for', 'not', 'a', 'breath', 'of'], ['wind', 'was', 'stirring', 'only', 'a', 'slight', 'oily', 'swell', 'rose', 'and', 'fell', 'like', 'a'], ['gentle', 'breathing', 'and', 'showed', 'that', 'the', 'eternal', 'sea', 'was', 'still', 'moving'], ['and', 'living', 'and', 'along', 'the', 'margin', 'where', 'the', 'water', 'sometimes', 'broke', 'was'], ['a', 'thick', 'incrustation', 'of', 'salt', 'pink', 'under', 'the', 'lurid', 'sky', 'there', 'was', 'a'], ['sense', 'of', 'oppression', 'in', 'my', 'head', 'and', 'i', 'noticed', 'that', 'i', 'was', 'breathing'], ['very', 'fast', 'the', 'sensation', 'reminded', 'me', 'of', 'my', 'only', 'experience', 'of'], ['mountaineering', 'and', 'from', 'that', 'i', 'judged', 'the', 'air', 'to', 'be', 'more', 'rarefied'], ['than', 'it', 'is', 'now'], [], ['far', 'away', 'up', 'the', 'desolate', 'slope', 'i', 'heard', 'a', 'harsh', 'scream', 'and', 'saw', 'a'], ['thing', 'like', 'a', 'huge', 'white', 'butterfly', 'go', 'slanting', 'and', 'fluttering', 'up', 'into'], ['the', 'sky', 'and', 'circling', 'disappear', 'over', 'some', 'low', 'hillocks', 'beyond', 'the'], ['sound', 'of', 'its', 'voice', 'was', 'so', 'dismal', 'that', 'i', 'shivered', 'and', 'seated', 'myself'], ['more', 'firmly', 'upon', 'the', 'machine', 'looking', 'round', 'me', 'again', 'i', 'saw', 'that'], ['quite', 'near', 'what', 'i', 'had', 'taken', 'to', 'be', 'a', 'reddish', 'mass', 'of', 'rock', 'was', 'moving'], ['slowly', 'towards', 'me', 'then', 'i', 'saw', 'the', 'thing', 'was', 'really', 'a', 'monstrous'], ['crab', 'like', 'creature', 'can', 'you', 'imagine', 'a', 'crab', 'as', 'large', 'as', 'yonder', 'table'], ['with', 'its', 'many', 'legs', 'moving', 'slowly', 'and', 'uncertainly', 'its', 'big', 'claws'], ['swaying', 'its', 'long', 'antennae', 'like', 'carters', 'whips', 'waving', 'and', 'feeling'], ['and', 'its', 'stalked', 'eyes', 'gleaming', 'at', 'you', 'on', 'either', 'side', 'of', 'its', 'metallic'], ['front', 'its', 'back', 'was', 'corrugated', 'and', 'ornamented', 'with', 'ungainly', 'bosses'], ['and', 'a', 'greenish', 'incrustation', 'blotched', 'it', 'here', 'and', 'there', 'i', 'could', 'see'], ['the', 'many', 'palps', 'of', 'its', 'complicated', 'mouth', 'flickering', 'and', 'feeling', 'as', 'it'], ['moved'], [], ['as', 'i', 'stared', 'at', 'this', 'sinister', 'apparition', 'crawling', 'towards', 'me', 'i', 'felt'], ['a', 'tickling', 'on', 'my', 'cheek', 'as', 'though', 'a', 'fly', 'had', 'lighted', 'there', 'i', 'tried', 'to'], ['brush', 'it', 'away', 'with', 'my', 'hand', 'but', 'in', 'a', 'moment', 'it', 'returned', 'and', 'almost'], ['immediately', 'came', 'another', 'by', 'my', 'ear', 'i', 'struck', 'at', 'this', 'and', 'caught'], ['something', 'threadlike', 'it', 'was', 'drawn', 'swiftly', 'out', 'of', 'my', 'hand', 'with', 'a'], ['frightful', 'qualm', 'i', 'turned', 'and', 'i', 'saw', 'that', 'i', 'had', 'grasped', 'the', 'antenna'], ['of', 'another', 'monster', 'crab', 'that', 'stood', 'just', 'behind', 'me', 'its', 'evil', 'eyes'], ['were', 'wriggling', 'on', 'their', 'stalks', 'its', 'mouth', 'was', 'all', 'alive', 'with'], ['appetite', 'and', 'its', 'vast', 'ungainly', 'claws', 'smeared', 'with', 'an', 'algal', 'slime'], ['were', 'descending', 'upon', 'me', 'in', 'a', 'moment', 'my', 'hand', 'was', 'on', 'the', 'lever', 'and'], ['i', 'had', 'placed', 'a', 'month', 'between', 'myself', 'and', 'these', 'monsters', 'but', 'i', 'was'], ['still', 'on', 'the', 'same', 'beach', 'and', 'i', 'saw', 'them', 'distinctly', 'now', 'as', 'soon', 'as', 'i'], ['stopped', 'dozens', 'of', 'them', 'seemed', 'to', 'be', 'crawling', 'here', 'and', 'there', 'in', 'the'], ['sombre', 'light', 'among', 'the', 'foliated', 'sheets', 'of', 'intense', 'green'], [], ['i', 'cannot', 'convey', 'the', 'sense', 'of', 'abominable', 'desolation', 'that', 'hung', 'over'], ['the', 'world', 'the', 'red', 'eastern', 'sky', 'the', 'northward', 'blackness', 'the', 'salt'], ['dead', 'sea', 'the', 'stony', 'beach', 'crawling', 'with', 'these', 'foul', 'slow', 'stirring'], ['monsters', 'the', 'uniform', 'poisonous', 'looking', 'green', 'of', 'the', 'lichenous'], ['plants', 'the', 'thin', 'air', 'that', 'hurts', 'one', 's', 'lungs', 'all', 'contributed', 'to', 'an'], ['appalling', 'effect', 'i', 'moved', 'on', 'a', 'hundred', 'years', 'and', 'there', 'was', 'the', 'same'], ['red', 'sun', 'a', 'little', 'larger', 'a', 'little', 'duller', 'the', 'same', 'dying', 'sea', 'the'], ['same', 'chill', 'air', 'and', 'the', 'same', 'crowd', 'of', 'earthy', 'crustacea', 'creeping', 'in'], ['and', 'out', 'among', 'the', 'green', 'weed', 'and', 'the', 'red', 'rocks', 'and', 'in', 'the', 'westward'], ['sky', 'i', 'saw', 'a', 'curved', 'pale', 'line', 'like', 'a', 'vast', 'new', 'moon'], [], ['so', 'i', 'travelled', 'stopping', 'ever', 'and', 'again', 'in', 'great', 'strides', 'of', 'a'], ['thousand', 'years', 'or', 'more', 'drawn', 'on', 'by', 'the', 'mystery', 'of', 'the', 'earth', 's', 'fate'], ['watching', 'with', 'a', 'strange', 'fascination', 'the', 'sun', 'grow', 'larger', 'and', 'duller'], ['in', 'the', 'westward', 'sky', 'and', 'the', 'life', 'of', 'the', 'old', 'earth', 'ebb', 'away', 'at'], ['last', 'more', 'than', 'thirty', 'million', 'years', 'hence', 'the', 'huge', 'red', 'hot', 'dome', 'of'], ['the', 'sun', 'had', 'come', 'to', 'obscure', 'nearly', 'a', 'tenth', 'part', 'of', 'the', 'darkling'], ['heavens', 'then', 'i', 'stopped', 'once', 'more', 'for', 'the', 'crawling', 'multitude', 'of'], ['crabs', 'had', 'disappeared', 'and', 'the', 'red', 'beach', 'save', 'for', 'its', 'livid', 'green'], ['liverworts', 'and', 'lichens', 'seemed', 'lifeless', 'and', 'now', 'it', 'was', 'flecked', 'with'], ['white', 'a', 'bitter', 'cold', 'assailed', 'me', 'rare', 'white', 'flakes', 'ever', 'and', 'again'], ['came', 'eddying', 'down', 'to', 'the', 'north', 'eastward', 'the', 'glare', 'of', 'snow', 'lay'], ['under', 'the', 'starlight', 'of', 'the', 'sable', 'sky', 'and', 'i', 'could', 'see', 'an', 'undulating'], ['crest', 'of', 'hillocks', 'pinkish', 'white', 'there', 'were', 'fringes', 'of', 'ice', 'along', 'the'], ['sea', 'margin', 'with', 'drifting', 'masses', 'further', 'out', 'but', 'the', 'main', 'expanse'], ['of', 'that', 'salt', 'ocean', 'all', 'bloody', 'under', 'the', 'eternal', 'sunset', 'was', 'still'], ['unfrozen'], [], ['i', 'looked', 'about', 'me', 'to', 'see', 'if', 'any', 'traces', 'of', 'animal', 'life', 'remained', 'a'], ['certain', 'indefinable', 'apprehension', 'still', 'kept', 'me', 'in', 'the', 'saddle', 'of', 'the'], ['machine', 'but', 'i', 'saw', 'nothing', 'moving', 'in', 'earth', 'or', 'sky', 'or', 'sea', 'the', 'green'], ['slime', 'on', 'the', 'rocks', 'alone', 'testified', 'that', 'life', 'was', 'not', 'extinct', 'a'], ['shallow', 'sandbank', 'had', 'appeared', 'in', 'the', 'sea', 'and', 'the', 'water', 'had', 'receded'], ['from', 'the', 'beach', 'i', 'fancied', 'i', 'saw', 'some', 'black', 'object', 'flopping', 'about'], ['upon', 'this', 'bank', 'but', 'it', 'became', 'motionless', 'as', 'i', 'looked', 'at', 'it', 'and', 'i'], ['judged', 'that', 'my', 'eye', 'had', 'been', 'deceived', 'and', 'that', 'the', 'black', 'object', 'was'], ['merely', 'a', 'rock', 'the', 'stars', 'in', 'the', 'sky', 'were', 'intensely', 'bright', 'and', 'seemed'], ['to', 'me', 'to', 'twinkle', 'very', 'little'], [], ['suddenly', 'i', 'noticed', 'that', 'the', 'circular', 'westward', 'outline', 'of', 'the', 'sun'], ['had', 'changed', 'that', 'a', 'concavity', 'a', 'bay', 'had', 'appeared', 'in', 'the', 'curve', 'i'], ['saw', 'this', 'grow', 'larger', 'for', 'a', 'minute', 'perhaps', 'i', 'stared', 'aghast', 'at', 'this'], ['blackness', 'that', 'was', 'creeping', 'over', 'the', 'day', 'and', 'then', 'i', 'realized', 'that'], ['an', 'eclipse', 'was', 'beginning', 'either', 'the', 'moon', 'or', 'the', 'planet', 'mercury', 'was'], ['passing', 'across', 'the', 'sun', 's', 'disk', 'naturally', 'at', 'first', 'i', 'took', 'it', 'to', 'be'], ['the', 'moon', 'but', 'there', 'is', 'much', 'to', 'incline', 'me', 'to', 'believe', 'that', 'what', 'i'], ['really', 'saw', 'was', 'the', 'transit', 'of', 'an', 'inner', 'planet', 'passing', 'very', 'near', 'to'], ['the', 'earth'], [], ['the', 'darkness', 'grew', 'apace', 'a', 'cold', 'wind', 'began', 'to', 'blow', 'in', 'freshening'], ['gusts', 'from', 'the', 'east', 'and', 'the', 'showering', 'white', 'flakes', 'in', 'the', 'air'], ['increased', 'in', 'number', 'from', 'the', 'edge', 'of', 'the', 'sea', 'came', 'a', 'ripple', 'and'], ['whisper', 'beyond', 'these', 'lifeless', 'sounds', 'the', 'world', 'was', 'silent', 'silent'], ['it', 'would', 'be', 'hard', 'to', 'convey', 'the', 'stillness', 'of', 'it', 'all', 'the', 'sounds', 'of'], ['man', 'the', 'bleating', 'of', 'sheep', 'the', 'cries', 'of', 'birds', 'the', 'hum', 'of', 'insects'], ['the', 'stir', 'that', 'makes', 'the', 'background', 'of', 'our', 'lives', 'all', 'that', 'was', 'over'], ['as', 'the', 'darkness', 'thickened', 'the', 'eddying', 'flakes', 'grew', 'more', 'abundant'], ['dancing', 'before', 'my', 'eyes', 'and', 'the', 'cold', 'of', 'the', 'air', 'more', 'intense', 'at'], ['last', 'one', 'by', 'one', 'swiftly', 'one', 'after', 'the', 'other', 'the', 'white', 'peaks', 'of'], ['the', 'distant', 'hills', 'vanished', 'into', 'blackness', 'the', 'breeze', 'rose', 'to', 'a'], ['moaning', 'wind', 'i', 'saw', 'the', 'black', 'central', 'shadow', 'of', 'the', 'eclipse', 'sweeping'], ['towards', 'me', 'in', 'another', 'moment', 'the', 'pale', 'stars', 'alone', 'were', 'visible', 'all'], ['else', 'was', 'rayless', 'obscurity', 'the', 'sky', 'was', 'absolutely', 'black'], [], ['a', 'horror', 'of', 'this', 'great', 'darkness', 'came', 'on', 'me', 'the', 'cold', 'that', 'smote'], ['to', 'my', 'marrow', 'and', 'the', 'pain', 'i', 'felt', 'in', 'breathing', 'overcame', 'me', 'i'], ['shivered', 'and', 'a', 'deadly', 'nausea', 'seized', 'me', 'then', 'like', 'a', 'red', 'hot', 'bow'], ['in', 'the', 'sky', 'appeared', 'the', 'edge', 'of', 'the', 'sun', 'i', 'got', 'off', 'the', 'machine', 'to'], ['recover', 'myself', 'i', 'felt', 'giddy', 'and', 'incapable', 'of', 'facing', 'the', 'return'], ['journey', 'as', 'i', 'stood', 'sick', 'and', 'confused', 'i', 'saw', 'again', 'the', 'moving', 'thing'], ['upon', 'the', 'shoal', 'there', 'was', 'no', 'mistake', 'now', 'that', 'it', 'was', 'a', 'moving'], ['thing', 'against', 'the', 'red', 'water', 'of', 'the', 'sea', 'it', 'was', 'a', 'round', 'thing', 'the'], ['size', 'of', 'a', 'football', 'perhaps', 'or', 'it', 'may', 'be', 'bigger', 'and', 'tentacles'], ['trailed', 'down', 'from', 'it', 'it', 'seemed', 'black', 'against', 'the', 'weltering'], ['blood', 'red', 'water', 'and', 'it', 'was', 'hopping', 'fitfully', 'about', 'then', 'i', 'felt', 'i'], ['was', 'fainting', 'but', 'a', 'terrible', 'dread', 'of', 'lying', 'helpless', 'in', 'that', 'remote'], ['and', 'awful', 'twilight', 'sustained', 'me', 'while', 'i', 'clambered', 'upon', 'the', 'saddle'], [], [], [], [], ['xii'], [], [], ['so', 'i', 'came', 'back', 'for', 'a', 'long', 'time', 'i', 'must', 'have', 'been', 'insensible', 'upon'], ['the', 'machine', 'the', 'blinking', 'succession', 'of', 'the', 'days', 'and', 'nights', 'was'], ['resumed', 'the', 'sun', 'got', 'golden', 'again', 'the', 'sky', 'blue', 'i', 'breathed', 'with'], ['greater', 'freedom', 'the', 'fluctuating', 'contours', 'of', 'the', 'land', 'ebbed', 'and'], ['flowed', 'the', 'hands', 'spun', 'backward', 'upon', 'the', 'dials', 'at', 'last', 'i', 'saw', 'again'], ['the', 'dim', 'shadows', 'of', 'houses', 'the', 'evidences', 'of', 'decadent', 'humanity'], ['these', 'too', 'changed', 'and', 'passed', 'and', 'others', 'came', 'presently', 'when', 'the'], ['million', 'dial', 'was', 'at', 'zero', 'i', 'slackened', 'speed', 'i', 'began', 'to', 'recognize'], ['our', 'own', 'pretty', 'and', 'familiar', 'architecture', 'the', 'thousands', 'hand', 'ran', 'back'], ['to', 'the', 'starting', 'point', 'the', 'night', 'and', 'day', 'flapped', 'slower', 'and', 'slower'], ['then', 'the', 'old', 'walls', 'of', 'the', 'laboratory', 'came', 'round', 'me', 'very', 'gently'], ['now', 'i', 'slowed', 'the', 'mechanism', 'down'], [], ['i', 'saw', 'one', 'little', 'thing', 'that', 'seemed', 'odd', 'to', 'me', 'i', 'think', 'i', 'have', 'told'], ['you', 'that', 'when', 'i', 'set', 'out', 'before', 'my', 'velocity', 'became', 'very', 'high', 'mrs'], ['watchett', 'had', 'walked', 'across', 'the', 'room', 'travelling', 'as', 'it', 'seemed', 'to', 'me'], ['like', 'a', 'rocket', 'as', 'i', 'returned', 'i', 'passed', 'again', 'across', 'that', 'minute', 'when'], ['she', 'traversed', 'the', 'laboratory', 'but', 'now', 'her', 'every', 'motion', 'appeared', 'to'], ['be', 'the', 'exact', 'inversion', 'of', 'her', 'previous', 'ones', 'the', 'door', 'at', 'the', 'lower'], ['end', 'opened', 'and', 'she', 'glided', 'quietly', 'up', 'the', 'laboratory', 'back', 'foremost'], ['and', 'disappeared', 'behind', 'the', 'door', 'by', 'which', 'she', 'had', 'previously', 'entered'], ['just', 'before', 'that', 'i', 'seemed', 'to', 'see', 'hillyer', 'for', 'a', 'moment', 'but', 'he', 'passed'], ['like', 'a', 'flash'], [], ['then', 'i', 'stopped', 'the', 'machine', 'and', 'saw', 'about', 'me', 'again', 'the', 'old', 'familiar'], ['laboratory', 'my', 'tools', 'my', 'appliances', 'just', 'as', 'i', 'had', 'left', 'them', 'i', 'got'], ['off', 'the', 'thing', 'very', 'shakily', 'and', 'sat', 'down', 'upon', 'my', 'bench', 'for', 'several'], ['minutes', 'i', 'trembled', 'violently', 'then', 'i', 'became', 'calmer', 'around', 'me', 'was'], ['my', 'old', 'workshop', 'again', 'exactly', 'as', 'it', 'had', 'been', 'i', 'might', 'have', 'slept'], ['there', 'and', 'the', 'whole', 'thing', 'have', 'been', 'a', 'dream'], [], ['and', 'yet', 'not', 'exactly', 'the', 'thing', 'had', 'started', 'from', 'the', 'south', 'east'], ['corner', 'of', 'the', 'laboratory', 'it', 'had', 'come', 'to', 'rest', 'again', 'in', 'the'], ['north', 'west', 'against', 'the', 'wall', 'where', 'you', 'saw', 'it', 'that', 'gives', 'you', 'the'], ['exact', 'distance', 'from', 'my', 'little', 'lawn', 'to', 'the', 'pedestal', 'of', 'the', 'white'], ['sphinx', 'into', 'which', 'the', 'morlocks', 'had', 'carried', 'my', 'machine'], [], ['for', 'a', 'time', 'my', 'brain', 'went', 'stagnant', 'presently', 'i', 'got', 'up', 'and', 'came'], ['through', 'the', 'passage', 'here', 'limping', 'because', 'my', 'heel', 'was', 'still'], ['painful', 'and', 'feeling', 'sorely', 'begrimed', 'i', 'saw', 'the', 'pall', 'mall', 'gazette'], ['on', 'the', 'table', 'by', 'the', 'door', 'i', 'found', 'the', 'date', 'was', 'indeed', 'to', 'day', 'and'], ['looking', 'at', 'the', 'timepiece', 'saw', 'the', 'hour', 'was', 'almost', 'eight', 'o', 'clock', 'i'], ['heard', 'your', 'voices', 'and', 'the', 'clatter', 'of', 'plates', 'i', 'hesitated', 'i', 'felt', 'so'], ['sick', 'and', 'weak', 'then', 'i', 'sniffed', 'good', 'wholesome', 'meat', 'and', 'opened', 'the'], ['door', 'on', 'you', 'you', 'know', 'the', 'rest', 'i', 'washed', 'and', 'dined', 'and', 'now', 'i', 'am'], ['telling', 'you', 'the', 'story'], [], ['i', 'know', 'he', 'said', 'after', 'a', 'pause', 'that', 'all', 'this', 'will', 'be', 'absolutely'], ['incredible', 'to', 'you', 'to', 'me', 'the', 'one', 'incredible', 'thing', 'is', 'that', 'i', 'am', 'here'], ['to', 'night', 'in', 'this', 'old', 'familiar', 'room', 'looking', 'into', 'your', 'friendly', 'faces'], ['and', 'telling', 'you', 'these', 'strange', 'adventures'], [], ['he', 'looked', 'at', 'the', 'medical', 'man', 'no', 'i', 'cannot', 'expect', 'you', 'to', 'believe'], ['it', 'take', 'it', 'as', 'a', 'lie', 'or', 'a', 'prophecy', 'say', 'i', 'dreamed', 'it', 'in', 'the'], ['workshop', 'consider', 'i', 'have', 'been', 'speculating', 'upon', 'the', 'destinies', 'of', 'our'], ['race', 'until', 'i', 'have', 'hatched', 'this', 'fiction', 'treat', 'my', 'assertion', 'of', 'its'], ['truth', 'as', 'a', 'mere', 'stroke', 'of', 'art', 'to', 'enhance', 'its', 'interest', 'and', 'taking'], ['it', 'as', 'a', 'story', 'what', 'do', 'you', 'think', 'of', 'it'], [], ['he', 'took', 'up', 'his', 'pipe', 'and', 'began', 'in', 'his', 'old', 'accustomed', 'manner', 'to', 'tap'], ['with', 'it', 'nervously', 'upon', 'the', 'bars', 'of', 'the', 'grate', 'there', 'was', 'a', 'momentary'], ['stillness', 'then', 'chairs', 'began', 'to', 'creak', 'and', 'shoes', 'to', 'scrape', 'upon', 'the'], ['carpet', 'i', 'took', 'my', 'eyes', 'off', 'the', 'time', 'traveller', 's', 'face', 'and', 'looked'], ['round', 'at', 'his', 'audience', 'they', 'were', 'in', 'the', 'dark', 'and', 'little', 'spots', 'of'], ['colour', 'swam', 'before', 'them', 'the', 'medical', 'man', 'seemed', 'absorbed', 'in', 'the'], ['contemplation', 'of', 'our', 'host', 'the', 'editor', 'was', 'looking', 'hard', 'at', 'the', 'end'], ['of', 'his', 'cigar', 'the', 'sixth', 'the', 'journalist', 'fumbled', 'for', 'his', 'watch', 'the'], ['others', 'as', 'far', 'as', 'i', 'remember', 'were', 'motionless'], [], ['the', 'editor', 'stood', 'up', 'with', 'a', 'sigh', 'what', 'a', 'pity', 'it', 'is', 'you', 're', 'not'], ['a', 'writer', 'of', 'stories', 'he', 'said', 'putting', 'his', 'hand', 'on', 'the', 'time'], ['traveller', 's', 'shoulder'], [], ['you', 'don', 't', 'believe', 'it'], [], ['well'], [], ['i', 'thought', 'not'], [], ['the', 'time', 'traveller', 'turned', 'to', 'us', 'where', 'are', 'the', 'matches', 'he', 'said'], ['he', 'lit', 'one', 'and', 'spoke', 'over', 'his', 'pipe', 'puffing', 'to', 'tell', 'you', 'the', 'truth'], ['i', 'hardly', 'believe', 'it', 'myself', 'and', 'yet'], [], ['his', 'eye', 'fell', 'with', 'a', 'mute', 'inquiry', 'upon', 'the', 'withered', 'white', 'flowers'], ['upon', 'the', 'little', 'table', 'then', 'he', 'turned', 'over', 'the', 'hand', 'holding', 'his'], ['pipe', 'and', 'i', 'saw', 'he', 'was', 'looking', 'at', 'some', 'half', 'healed', 'scars', 'on', 'his'], ['knuckles'], [], ['the', 'medical', 'man', 'rose', 'came', 'to', 'the', 'lamp', 'and', 'examined', 'the', 'flowers'], ['the', 'gynaeceum', 's', 'odd', 'he', 'said', 'the', 'psychologist', 'leant', 'forward', 'to'], ['see', 'holding', 'out', 'his', 'hand', 'for', 'a', 'specimen'], [], ['i', 'm', 'hanged', 'if', 'it', 'isn', 't', 'a', 'quarter', 'to', 'one', 'said', 'the', 'journalist'], ['how', 'shall', 'we', 'get', 'home'], [], ['plenty', 'of', 'cabs', 'at', 'the', 'station', 'said', 'the', 'psychologist'], [], ['it', 's', 'a', 'curious', 'thing', 'said', 'the', 'medical', 'man', 'but', 'i', 'certainly', 'don', 't'], ['know', 'the', 'natural', 'order', 'of', 'these', 'flowers', 'may', 'i', 'have', 'them'], [], ['the', 'time', 'traveller', 'hesitated', 'then', 'suddenly', 'certainly', 'not'], [], ['where', 'did', 'you', 'really', 'get', 'them', 'said', 'the', 'medical', 'man'], [], ['the', 'time', 'traveller', 'put', 'his', 'hand', 'to', 'his', 'head', 'he', 'spoke', 'like', 'one', 'who'], ['was', 'trying', 'to', 'keep', 'hold', 'of', 'an', 'idea', 'that', 'eluded', 'him', 'they', 'were', 'put'], ['into', 'my', 'pocket', 'by', 'weena', 'when', 'i', 'travelled', 'into', 'time', 'he', 'stared'], ['round', 'the', 'room', 'i', 'm', 'damned', 'if', 'it', 'isn', 't', 'all', 'going', 'this', 'room', 'and', 'you'], ['and', 'the', 'atmosphere', 'of', 'every', 'day', 'is', 'too', 'much', 'for', 'my', 'memory', 'did', 'i'], ['ever', 'make', 'a', 'time', 'machine', 'or', 'a', 'model', 'of', 'a', 'time', 'machine', 'or', 'is', 'it', 'all'], ['only', 'a', 'dream', 'they', 'say', 'life', 'is', 'a', 'dream', 'a', 'precious', 'poor', 'dream', 'at'], ['times', 'but', 'i', 'can', 't', 'stand', 'another', 'that', 'won', 't', 'fit', 'it', 's', 'madness', 'and'], ['where', 'did', 'the', 'dream', 'come', 'from', 'i', 'must', 'look', 'at', 'that', 'machine', 'if'], ['there', 'is', 'one'], [], ['he', 'caught', 'up', 'the', 'lamp', 'swiftly', 'and', 'carried', 'it', 'flaring', 'red', 'through'], ['the', 'door', 'into', 'the', 'corridor', 'we', 'followed', 'him', 'there', 'in', 'the', 'flickering'], ['light', 'of', 'the', 'lamp', 'was', 'the', 'machine', 'sure', 'enough', 'squat', 'ugly', 'and'], ['askew', 'a', 'thing', 'of', 'brass', 'ebony', 'ivory', 'and', 'translucent', 'glimmering'], ['quartz', 'solid', 'to', 'the', 'touch', 'for', 'i', 'put', 'out', 'my', 'hand', 'and', 'felt', 'the', 'rail'], ['of', 'it', 'and', 'with', 'brown', 'spots', 'and', 'smears', 'upon', 'the', 'ivory', 'and', 'bits', 'of'], ['grass', 'and', 'moss', 'upon', 'the', 'lower', 'parts', 'and', 'one', 'rail', 'bent', 'awry'], [], ['the', 'time', 'traveller', 'put', 'the', 'lamp', 'down', 'on', 'the', 'bench', 'and', 'ran', 'his', 'hand'], ['along', 'the', 'damaged', 'rail', 'it', 's', 'all', 'right', 'now', 'he', 'said', 'the', 'story', 'i'], ['told', 'you', 'was', 'true', 'i', 'm', 'sorry', 'to', 'have', 'brought', 'you', 'out', 'here', 'in', 'the'], ['cold', 'he', 'took', 'up', 'the', 'lamp', 'and', 'in', 'an', 'absolute', 'silence', 'we'], ['returned', 'to', 'the', 'smoking', 'room'], [], ['he', 'came', 'into', 'the', 'hall', 'with', 'us', 'and', 'helped', 'the', 'editor', 'on', 'with', 'his'], ['coat', 'the', 'medical', 'man', 'looked', 'into', 'his', 'face', 'and', 'with', 'a', 'certain'], ['hesitation', 'told', 'him', 'he', 'was', 'suffering', 'from', 'overwork', 'at', 'which', 'he'], ['laughed', 'hugely', 'i', 'remember', 'him', 'standing', 'in', 'the', 'open', 'doorway', 'bawling'], ['good', 'night'], [], ['i', 'shared', 'a', 'cab', 'with', 'the', 'editor', 'he', 'thought', 'the', 'tale', 'a', 'gaudy', 'lie'], ['for', 'my', 'own', 'part', 'i', 'was', 'unable', 'to', 'come', 'to', 'a', 'conclusion', 'the', 'story', 'was'], ['so', 'fantastic', 'and', 'incredible', 'the', 'telling', 'so', 'credible', 'and', 'sober', 'i'], ['lay', 'awake', 'most', 'of', 'the', 'night', 'thinking', 'about', 'it', 'i', 'determined', 'to', 'go'], ['next', 'day', 'and', 'see', 'the', 'time', 'traveller', 'again', 'i', 'was', 'told', 'he', 'was', 'in', 'the'], ['laboratory', 'and', 'being', 'on', 'easy', 'terms', 'in', 'the', 'house', 'i', 'went', 'up', 'to', 'him'], ['the', 'laboratory', 'however', 'was', 'empty', 'i', 'stared', 'for', 'a', 'minute', 'at', 'the'], ['time', 'machine', 'and', 'put', 'out', 'my', 'hand', 'and', 'touched', 'the', 'lever', 'at', 'that', 'the'], ['squat', 'substantial', 'looking', 'mass', 'swayed', 'like', 'a', 'bough', 'shaken', 'by', 'the'], ['wind', 'its', 'instability', 'startled', 'me', 'extremely', 'and', 'i', 'had', 'a', 'queer'], ['reminiscence', 'of', 'the', 'childish', 'days', 'when', 'i', 'used', 'to', 'be', 'forbidden', 'to'], ['meddle', 'i', 'came', 'back', 'through', 'the', 'corridor', 'the', 'time', 'traveller', 'met', 'me'], ['in', 'the', 'smoking', 'room', 'he', 'was', 'coming', 'from', 'the', 'house', 'he', 'had', 'a', 'small'], ['camera', 'under', 'one', 'arm', 'and', 'a', 'knapsack', 'under', 'the', 'other', 'he', 'laughed', 'when'], ['he', 'saw', 'me', 'and', 'gave', 'me', 'an', 'elbow', 'to', 'shake', 'i', 'm', 'frightfully', 'busy'], ['said', 'he', 'with', 'that', 'thing', 'in', 'there'], [], ['but', 'is', 'it', 'not', 'some', 'hoax', 'i', 'said', 'do', 'you', 'really', 'travel', 'through'], ['time'], [], ['really', 'and', 'truly', 'i', 'do', 'and', 'he', 'looked', 'frankly', 'into', 'my', 'eyes', 'he'], ['hesitated', 'his', 'eye', 'wandered', 'about', 'the', 'room', 'i', 'only', 'want', 'half', 'an'], ['hour', 'he', 'said', 'i', 'know', 'why', 'you', 'came', 'and', 'it', 's', 'awfully', 'good', 'of', 'you'], ['there', 's', 'some', 'magazines', 'here', 'if', 'you', 'll', 'stop', 'to', 'lunch', 'i', 'll', 'prove', 'you'], ['this', 'time', 'travelling', 'up', 'to', 'the', 'hilt', 'specimen', 'and', 'all', 'if', 'you', 'll'], ['forgive', 'my', 'leaving', 'you', 'now'], [], ['i', 'consented', 'hardly', 'comprehending', 'then', 'the', 'full', 'import', 'of', 'his', 'words'], ['and', 'he', 'nodded', 'and', 'went', 'on', 'down', 'the', 'corridor', 'i', 'heard', 'the', 'door', 'of'], ['the', 'laboratory', 'slam', 'seated', 'myself', 'in', 'a', 'chair', 'and', 'took', 'up', 'a', 'daily'], ['paper', 'what', 'was', 'he', 'going', 'to', 'do', 'before', 'lunch', 'time', 'then', 'suddenly'], ['i', 'was', 'reminded', 'by', 'an', 'advertisement', 'that', 'i', 'had', 'promised', 'to', 'meet'], ['richardson', 'the', 'publisher', 'at', 'two', 'i', 'looked', 'at', 'my', 'watch', 'and', 'saw'], ['that', 'i', 'could', 'barely', 'save', 'that', 'engagement', 'i', 'got', 'up', 'and', 'went', 'down', 'the'], ['passage', 'to', 'tell', 'the', 'time', 'traveller'], [], ['as', 'i', 'took', 'hold', 'of', 'the', 'handle', 'of', 'the', 'door', 'i', 'heard', 'an', 'exclamation'], ['oddly', 'truncated', 'at', 'the', 'end', 'and', 'a', 'click', 'and', 'a', 'thud', 'a', 'gust', 'of', 'air'], ['whirled', 'round', 'me', 'as', 'i', 'opened', 'the', 'door', 'and', 'from', 'within', 'came', 'the'], ['sound', 'of', 'broken', 'glass', 'falling', 'on', 'the', 'floor', 'the', 'time', 'traveller', 'was'], ['not', 'there', 'i', 'seemed', 'to', 'see', 'a', 'ghostly', 'indistinct', 'figure', 'sitting', 'in'], ['a', 'whirling', 'mass', 'of', 'black', 'and', 'brass', 'for', 'a', 'moment', 'a', 'figure', 'so'], ['transparent', 'that', 'the', 'bench', 'behind', 'with', 'its', 'sheets', 'of', 'drawings', 'was'], ['absolutely', 'distinct', 'but', 'this', 'phantasm', 'vanished', 'as', 'i', 'rubbed', 'my', 'eyes'], ['the', 'time', 'machine', 'had', 'gone', 'save', 'for', 'a', 'subsiding', 'stir', 'of', 'dust', 'the'], ['further', 'end', 'of', 'the', 'laboratory', 'was', 'empty', 'a', 'pane', 'of', 'the', 'skylight', 'had'], ['apparently', 'just', 'been', 'blown', 'in'], [], ['i', 'felt', 'an', 'unreasonable', 'amazement', 'i', 'knew', 'that', 'something', 'strange', 'had'], ['happened', 'and', 'for', 'the', 'moment', 'could', 'not', 'distinguish', 'what', 'the', 'strange'], ['thing', 'might', 'be', 'as', 'i', 'stood', 'staring', 'the', 'door', 'into', 'the', 'garden', 'opened'], ['and', 'the', 'man', 'servant', 'appeared'], [], ['we', 'looked', 'at', 'each', 'other', 'then', 'ideas', 'began', 'to', 'come', 'has', 'mr'], ['gone', 'out', 'that', 'way', 'said', 'i'], [], ['no', 'sir', 'no', 'one', 'has', 'come', 'out', 'this', 'way', 'i', 'was', 'expecting', 'to', 'find', 'him'], ['here'], [], ['at', 'that', 'i', 'understood', 'at', 'the', 'risk', 'of', 'disappointing', 'richardson', 'i'], ['stayed', 'on', 'waiting', 'for', 'the', 'time', 'traveller', 'waiting', 'for', 'the', 'second'], ['perhaps', 'still', 'stranger', 'story', 'and', 'the', 'specimens', 'and', 'photographs', 'he'], ['would', 'bring', 'with', 'him', 'but', 'i', 'am', 'beginning', 'now', 'to', 'fear', 'that', 'i', 'must'], ['wait', 'a', 'lifetime', 'the', 'time', 'traveller', 'vanished', 'three', 'years', 'ago', 'and'], ['as', 'everybody', 'knows', 'now', 'he', 'has', 'never', 'returned'], [], [], [], [], ['epilogue'], [], [], ['one', 'cannot', 'choose', 'but', 'wonder', 'will', 'he', 'ever', 'return', 'it', 'may', 'be', 'that', 'he'], ['swept', 'back', 'into', 'the', 'past', 'and', 'fell', 'among', 'the', 'blood', 'drinking', 'hairy'], ['savages', 'of', 'the', 'age', 'of', 'unpolished', 'stone', 'into', 'the', 'abysses', 'of', 'the'], ['cretaceous', 'sea', 'or', 'among', 'the', 'grotesque', 'saurians', 'the', 'huge', 'reptilian'], ['brutes', 'of', 'the', 'jurassic', 'times', 'he', 'may', 'even', 'now', 'if', 'i', 'may', 'use', 'the'], ['phrase', 'be', 'wandering', 'on', 'some', 'plesiosaurus', 'haunted', 'oolitic', 'coral'], ['reef', 'or', 'beside', 'the', 'lonely', 'saline', 'lakes', 'of', 'the', 'triassic', 'age', 'or', 'did'], ['he', 'go', 'forward', 'into', 'one', 'of', 'the', 'nearer', 'ages', 'in', 'which', 'men', 'are', 'still'], ['men', 'but', 'with', 'the', 'riddles', 'of', 'our', 'own', 'time', 'answered', 'and', 'its', 'wearisome'], ['problems', 'solved', 'into', 'the', 'manhood', 'of', 'the', 'race', 'for', 'i', 'for', 'my', 'own'], ['part', 'cannot', 'think', 'that', 'these', 'latter', 'days', 'of', 'weak', 'experiment'], ['fragmentary', 'theory', 'and', 'mutual', 'discord', 'are', 'indeed', 'man', 's', 'culminating'], ['time', 'i', 'say', 'for', 'my', 'own', 'part', 'he', 'i', 'know', 'for', 'the', 'question', 'had', 'been'], ['discussed', 'among', 'us', 'long', 'before', 'the', 'time', 'machine', 'was', 'made', 'thought'], ['but', 'cheerlessly', 'of', 'the', 'advancement', 'of', 'mankind', 'and', 'saw', 'in', 'the'], ['growing', 'pile', 'of', 'civilization', 'only', 'a', 'foolish', 'heaping', 'that', 'must'], ['inevitably', 'fall', 'back', 'upon', 'and', 'destroy', 'its', 'makers', 'in', 'the', 'end', 'if', 'that'], ['is', 'so', 'it', 'remains', 'for', 'us', 'to', 'live', 'as', 'though', 'it', 'were', 'not', 'so', 'but', 'to', 'me'], ['the', 'future', 'is', 'still', 'black', 'and', 'blank', 'is', 'a', 'vast', 'ignorance', 'lit', 'at', 'a'], ['few', 'casual', 'places', 'by', 'the', 'memory', 'of', 'his', 'story', 'and', 'i', 'have', 'by', 'me', 'for'], ['my', 'comfort', 'two', 'strange', 'white', 'flowers', 'shrivelled', 'now', 'and', 'brown', 'and'], ['flat', 'and', 'brittle', 'to', 'witness', 'that', 'even', 'when', 'mind', 'and', 'strength', 'had'], ['gone', 'gratitude', 'and', 'a', 'mutual', 'tenderness', 'still', 'lived', 'on', 'in', 'the', 'heart'], ['of', 'man']]\n",
      "0: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "1: []\n",
      "2: []\n",
      "3: []\n",
      "4: []\n",
      "5: ['i']\n",
      "6: []\n",
      "7: []\n",
      "8: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "9: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "10: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "11: ['fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent']\n",
      "12: ['lights', 'in', 'the', 'lilies', 'of', 'silver', 'caught', 'the', 'bubbles', 'that', 'flashed', 'and']\n",
      "13: ['passed', 'in', 'our', 'glasses', 'our', 'chairs', 'being', 'his', 'patents', 'embraced', 'and']\n",
      "14: ['caressed', 'us', 'rather', 'than', 'submitted', 'to', 'be', 'sat', 'upon', 'and', 'there', 'was', 'that']\n",
      "15: ['luxurious', 'after', 'dinner', 'atmosphere', 'when', 'thought', 'roams', 'gracefully']\n",
      "16: ['free', 'of', 'the', 'trammels', 'of', 'precision', 'and', 'he', 'put', 'it', 'to', 'us', 'in', 'this']\n",
      "17: ['way', 'marking', 'the', 'points', 'with', 'a', 'lean', 'forefinger', 'as', 'we', 'sat', 'and', 'lazily']\n",
      "18: ['admired', 'his', 'earnestness', 'over', 'this', 'new', 'paradox', 'as', 'we', 'thought', 'it']\n",
      "19: ['and', 'his', 'fecundity']\n",
      "20: []\n",
      "21: ['you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two']\n",
      "22: ['ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for']\n",
      "23: ['instance', 'they', 'taught', 'you', 'at', 'school', 'is', 'founded', 'on', 'a', 'misconception']\n",
      "24: []\n",
      "25: ['is', 'not', 'that', 'rather', 'a', 'large', 'thing', 'to', 'expect', 'us', 'to', 'begin', 'upon']\n"
     ]
    }
   ],
   "source": [
    "# 按照word\n",
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "\n",
    "# [['The', 'machine', 'is', 'haha'], [], [], ...]\n",
    "tokens = tokenize(lines, token='word')\n",
    "# tokens = tokenize(lines, token='char')\n",
    "\n",
    "\n",
    "print('tokens:', tokens)\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f'{i}: {token}')\n",
    "    if i == 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.3. <a id='toc11_2_3_3_'></a>[词表（Vocabulary）](#toc0_)\n",
    "* 构建(token：索引)查询元组\n",
    "* 并将文本的token替换成索引\n",
    "\n",
    "    |token|indice|annotation|\n",
    "    |---|---|---|\n",
    "    |unk|0|unknown|\n",
    "    |PAD|1|padding|\n",
    "    |SOS|2|start of sentence|\n",
    "    |EOS|3|end of sentence| \n",
    "    |...|...|...|\n",
    "\n",
    "\n",
    "* id_to_token：索引到token的映射，列表格式\n",
    "* token_to_id：token到索引的映射，字典格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab type: <class '__main__.Vocab'>\n",
      "vocab size: 4580\n",
      "vocab[0:5]:\n",
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4)]\n",
      "====================================================================================================\n",
      "文本: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "====================================================================================================\n",
      "文本: ['fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent']\n",
      "索引: [148, 588, 825, 3, 1, 244, 2187, 4, 1, 2188]\n"
     ]
    }
   ],
   "source": [
    "class Vocab():  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    \n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)                                                      # 统计词频\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)       # 词频从高到低排序\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens                                     # 列表格式 ['<unk>', ...]\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}     # 字典格式 {token: idx}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:                                         # 如果词语的频率低于 min_freq，则停止添加。\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)                                 # 字典追加\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1           # 字典更新\n",
    "\n",
    "    def __len__(self):\n",
    "        '''魔法函数，返回词表的长度'''\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        '''魔法函数，返回词表的索引'''\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        '''\n",
    "        单个索引：如果传入的是单个词语（不是列表或元组），则返回对应的索引。如果词语未在词汇表中出现，则返回 <unk> 的索引（默认为 0）。\n",
    "        批量词语查询：如果传入的是词语列表或元组，则返回对应的索引列表。\n",
    "        '''\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        '''属性方法，返回未知词元的索引'''\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        '''属性方法，返回词频'''\n",
    "        return self._token_freqs\n",
    "\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "vocab = Vocab(tokens)\n",
    "\n",
    "\n",
    "print(f'vocab type: {type(vocab)}')\n",
    "print(f'vocab size: {len(vocab)}')\n",
    "print('vocab[0:5]:', list(vocab.token_to_idx.items())[:5], sep='\\n')\n",
    "\n",
    "for i in [0, 11]:\n",
    "    print(\"=\"*100)\n",
    "    print('文本:', tokens[i])\n",
    "    print('索引:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.4. <a id='toc11_2_3_4_'></a>[整合所有功能](#toc0_)\n",
    "* 读取数据\n",
    "* 分割成token\n",
    "* 并构建(token, indice)查询表\n",
    "* 替换token成indice，从而构成corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照char进行词元化 \n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    # 读取数据，返回 ['The machine is haha', '', '', ...]\n",
    "    lines = read_time_machine()\n",
    "\n",
    "    # 分词数据，返回 [['The', 'machine', 'is', 'haha'], [], [], ...]\n",
    "    tokens = tokenize(lines, 'char')      # char\n",
    "    # tokens = tokenize(lines, token='word')  # word\n",
    "\n",
    "    # 构建词表 vocab.__getitem__(token_to_idx.get(token)) 在字典中查找\n",
    "    # 列表：idx_to_token = ['<unk>', reserved_tokens, 'the', 'i', 'and', ...]\n",
    "    # 字典：token_to_idx = {'<unk>': indice, reserved_tokens: indice, 'the': indice, 'i': indice, 'and': indice, ...}\n",
    "    vocab = Vocab(tokens)\n",
    "\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中，构成语料库 (corpus) \n",
    "    # 展开为一维列表：corpus是[indice, indice, indice, ...]\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "\n",
    "    if max_tokens > 0:  # 如果max_tokens大于0，则截断corpus\n",
    "        corpus = corpus[:max_tokens]\n",
    "\n",
    "    return corpus, vocab\n",
    "\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "\n",
    "# 语料库长度，词表长度\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.5. <a id='toc11_2_3_5_'></a>[文本编码与向量化](#toc0_)\n",
    "文本编码的目的\n",
    "文本编码是将文本数据转换为数值形式，以便机器学习模型能够处理。常见的编码方法包括：\n",
    "  - 独热编码（One-Hot Encoding）\n",
    "  - 词袋模型（Bag-of-Words）\n",
    "  - TF-IDF（Term Frequency-Inverse Document Frequency）\n",
    "  - 词嵌入（Word Embeddings）： 词嵌入通过将词汇映射到高维连续向量空间，捕捉词汇之间的语义关系。常见的词嵌入方法包括Word2Vec、GloVe和FastText。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.2.3.5.1. <a id='toc11_2_3_5_1_'></a>[word2vec](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.4. <a id='toc11_2_4_'></a>[语言模型数据集](#toc0_)\n",
    "#### 11.2.4.1. <a id='toc11_2_4_1_'></a>[顺序采样 (Sequential Sampling)](#toc0_)\n",
    "\n",
    "- 顺序采样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch \n",
    "\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列 (生成器函数)\"\"\"\n",
    "    # 从随机偏移量开始划分序列，起始点。\n",
    "    # 生成一个随机偏移量 offset，范围在 0 到 num_steps 之间。这样做的目的是为了随机化数据的起始位置，避免模型过于依赖数据的开始部分，提高泛化能力。\n",
    "    offset = random.randint(0, num_steps)   \n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size    # 计算可以形成完整批次数(取整) * batch_size = 可以形成完整批次的所有token\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])                  # 提取 X 序列\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])          # 提取 Y 序列，只是X右移一位\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)         # 重塑形状\n",
    "    num_batches = Xs.shape[1] // num_steps                                  # 计算批次数\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]                                         # batch_size, num_steps\n",
    "        Y = Ys[:, i: i + num_steps]                                         # batch_size, num_steps\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " tensor([[ 5, 13,  2,  1, 13],\n",
      "        [ 3,  1,  3,  9,  5]])\n",
      "Y: \n",
      " tensor([[13,  2,  1, 13,  4],\n",
      "        [ 1,  3,  9,  5,  8]])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in seq_data_iter_sequential(corpus=corpus, batch_size=2, num_steps=5):\n",
    "    print('X: \\n', X)\n",
    "    print('Y: \\n', Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.2. <a id='toc11_2_4_2_'></a>[随机采样 (Random Sampling)](#toc0_)\n",
    "\n",
    "- 随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import torch \n",
    "\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列 (生成器函数)\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " tensor([[ 7,  1,  8, 13,  4],\n",
      "        [23,  2,  1,  3,  7]])\n",
      "Y: \n",
      " tensor([[ 1,  8, 13,  4, 12],\n",
      "        [ 2,  1,  3,  7,  1]])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in seq_data_iter_random(corpus=corpus, batch_size=2, num_steps=5):\n",
    "    print('X: \\n', X)\n",
    "    print('Y: \\n', Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.3. <a id='toc11_2_4_3_'></a>[PyTorch分装的顺序或随机采样](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils import data \n",
    "\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    def __init__(self, corpus:list, num_steps:int):\n",
    "        self.datas = self._get_data_list(corpus, num_steps)\n",
    "\n",
    "    def _get_data_list(self, corpus:list, num_steps:int):\n",
    "        datas = []\n",
    "        # 在corpus中随机选择一个起始位置，范围在0到num_steps之间\n",
    "        start_position = torch.randint(low=0, high=num_steps, size=(1,))\n",
    "        corpus = corpus[start_position:]\n",
    "        num_subseqs = len(corpus) // num_steps \n",
    "        for num in range(0, (num_subseqs-1) * num_steps, num_steps):\n",
    "            x = torch.tensor(corpus[num: num + num_steps])\n",
    "            y = torch.tensor(corpus[num + 1: num + 1 + num_steps])\n",
    "            datas.append((x, y))\n",
    "        return datas \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx]\n",
    "    \n",
    "\n",
    "# datasets\n",
    "datasets = TextDataset(corpus=corpus, num_steps=5)\n",
    "\n",
    "# 顺序采样, 不shuffle\n",
    "sequential_train_loader = data.DataLoader(datasets, batch_size=2, shuffle=False)\n",
    "\n",
    "# 随机采样, shuffle\n",
    "random_train_loader = data.DataLoader(datasets, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 9, 2, 1, 3]), tensor([9, 2, 1, 3, 5]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  9,  2,  1,  3],\n",
      "        [ 5, 13,  2,  1, 13]])\n",
      "tensor([[ 9,  2,  1,  3,  5],\n",
      "        [13,  2,  1, 13,  4]])\n"
     ]
    }
   ],
   "source": [
    "for X, y in sequential_train_loader:\n",
    "    print(X, y, sep='\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 10,  2,  1,  5],\n",
      "        [11,  1,  5,  6,  3]])\n",
      "tensor([[10,  2,  1,  5,  1],\n",
      "        [ 1,  5,  6,  3,  7]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in random_train_loader:\n",
    "    print(x, y, sep='\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.4. <a id='toc11_2_4_4_'></a>[总结](#toc0_)\n",
    "\n",
    "| 特性 | 顺序采样 | 随机采样 |\n",
    "| -------------- | --------------------------------- | --------------------------------- |\n",
    "| 采样方式 | 按照数据顺序选择 | 随机选择 |\n",
    "| 代表性 | 可能不具备良好代表性 | 通常具有较好代表性 |\n",
    "| 适用场景 | 时间序列数据、在线学习 | 大部分机器学习任务、模型评估 |\n",
    "| 优点 | 保留数据顺序、实现简单 | 减少偏差、适用于更多算法 |\n",
    "| 缺点 | 可能存在偏差、无法处理Non-IID数据 | 可能打破时间序列信息、实现复杂度较高 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.5. <a id='toc11_2_4_5_'></a>[包装](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"\n",
    "    返回时光机器数据集的迭代器和词表\n",
    "    \"\"\"\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens) \n",
    "       \n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3. <a id='toc11_3_'></a>[RNN](#toc0_)\n",
    "可以处理有顺序的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1. <a id='toc11_3_1_'></a>[RNN-循环神经网络原理](#toc0_)\n",
    "* 结构：\n",
    "    * 有一层（或多层）隐藏结构；\n",
    "    * 当前隐藏结构由上一侧隐藏结构和当前输入决定\n",
    "    * 依次类推\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "<img src=\"./Pytorch_Pictures/RNN//Simple-RNN.jpg\" width = \"800\" height = \"600\" alt=\"图片名称\" align=center />\n",
    "</div>\n",
    "\n",
    "更新隐藏状态：      \n",
    "$\\mathbf{h}_t=\\phi(\\mathbf{W}_{hh}\\mathbf{h}_{t-1}+\\mathbf{W}_{hx}\\mathbf{x}_{t}+\\mathbf{b}_h)$  \n",
    "输出：             \n",
    "$\\mathbf{o}_t=\\phi(\\mathbf{W}_\\textit{ho}\\mathbf{h}_t+\\mathbf{b}_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.1. <a id='toc11_3_1_1_'></a>[从头实现网络](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 初始化模型    \n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):                        \n",
    "    # 初始化第一个隐变量的值, (num_layers, batch_size, num_hiddens)，此时num_layers=1, num_layers=1\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小), (num_steps, batch_size, vocab_size)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(batch_size, vocab_size))\n",
    "    for X in inputs:    # 依次在num_steps中遍历，X的形状：(batch_size, vocab_size),依序列顺序展开\n",
    "        # X: (batch_size, vocab_size)\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)         # 隐藏变量: (batch_size, num_hiddens)\n",
    "        # H: (batch_size, num_hiddens)\n",
    "\n",
    "        Y = torch.mm(H, W_hq) + b_q                                         # 输出: (batch_size, num_outputs) 此时num_outputs=vocab_size\n",
    "        # Y: (batch_size, num_outputs/vocab_size)\n",
    "\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)  # 返回所有时间步的输出，以及最终的隐藏状态, (num_steps, batch_size, num_outputs)\n",
    "\n",
    "\n",
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32) # (num_steps, batch_size, vocab_size)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.Size([5, 2]), torch.Size([5, 2, 28]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "batch_size, num_steps = 2, 5\n",
    "\n",
    "X = torch.arange(10).reshape((batch_size, num_steps))\n",
    "\n",
    "X.shape, X.T.shape, F.one_hot(X.T, len(vocab)).shape    # 此时，vocab_size= len(vocab) = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "net = RNNModelScratch(\n",
    "    vocab_size= len(vocab), \n",
    "    num_hiddens= num_hiddens, \n",
    "    device= try_gpu(), \n",
    "    get_params= get_params,\n",
    "    init_state= init_rnn_state, \n",
    "    forward_fn= rnn\n",
    ")\n",
    "\n",
    "state = net.begin_state(X.shape[0], try_gpu())\n",
    "\n",
    "Y, new_state = net(X.to(try_gpu()), state)\n",
    "\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.2. <a id='toc11_3_1_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape: torch.Size([5, 2, 128])\n",
      "state_new.shape: torch.Size([1, 2, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(28, 128)\n",
       "  (linear): Linear(in_features=128, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), num_classes=self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态,hiddens=(h, c)\n",
    "            return (torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, num_steps, num_hiddens = 2, 5, 128\n",
    "\n",
    "# 用PyTorch直接实现\n",
    "rnn_layer = nn.RNN(\n",
    "    input_size=len(vocab),          # 输入特征的维度, vocab_size\n",
    "    hidden_size=num_hiddens,         # 隐藏层大小\n",
    "    num_layers=1,                    # 深层神经网络，默认是1层\n",
    "    bidirectional=False,            # 双向神经网络，默认是单向\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "# 我们(**使用张量来初始化隐状态**)，它的形状是（隐藏层数，批量大小，隐藏单元数），\n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "\n",
    "# [**通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。**]\n",
    "# 需要强调的是，`rnn_layer`的“输出”（`Y`）不涉及输出层的计算：\n",
    "# 它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。\n",
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))    # (num_steps, batch_size, vocab_size)\n",
    "\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "# Y: (num_steps, batch_size, num_hiddens)\n",
    "# state_new: (num_layers, batch_size, num_hiddens)\n",
    "print(f'Y.shape: {Y.shape}')\n",
    "print(f'state_new.shape: {state_new.shape}')\n",
    "\n",
    "device = try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.3. <a id='toc11_3_1_3_'></a>[训练和预测](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.4. <a id='toc11_3_1_4_'></a>[warm-up 预热期](#toc0_)\n",
    "* 预热期：在预测之前，先输入一些字符，让模型逐渐进入状态\n",
    "* 预热期长度：num_steps\n",
    "* 预测长度：num_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller llgggggggggggggggggggggggggggggggggggggggggggggggg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    # print(f'prefix: {prefix}')\n",
    "    # print(f'prefix[0]: {prefix[0]}')\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    # print(f'outputs: {outputs}')\n",
    "    # print(f'outputs[-1]: {outputs[-1]}')\n",
    "\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape(shape=(1, 1))  # (batch_size, num_steps)\n",
    "\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        # print(f'y: {y}')\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "        # print(f'outputs: {[vocab.idx_to_token[i] for i in outputs]}')\n",
    "\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "\n",
    "\n",
    "# 测试以下\n",
    "predict_ch8('time traveller ', 50, net, vocab, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度剪裁（Gradient Clipping）\n",
    "梯度剪裁（Gradient Clipping）是深度学习中用于控制梯度爆炸（Gradient Explosion）的一种技术。通过限制梯度的大小，梯度剪裁帮助稳定模型的训练过程，特别是在处理深层网络或复杂模型时。\n",
    "\n",
    "```python\n",
    "def train(model, optimizer, criterion, data, targets, clip_value=None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    if clip_value is not None:\n",
    "        # 基于梯度范数的剪裁\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        # 或者基于梯度值的剪裁\n",
    "        # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    " # 防止梯度爆炸\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度剪裁\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.0, 535295.6 词元/秒 cuda:0\n",
      "time travelleryou can show black is white by argument said filby\n",
      "traveller with a slight accession ofcheerfulness really thi\n",
      "<unk>ou must follow shad inspareve be caidfth y sollor dtism time lac\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练\n",
    "import math \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量,\n",
    "                # 对于nn.LSTM, state是个元组, state=(h, c)\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "            \n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
    "\n",
    "\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            print(predict('you must follow '))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))\n",
    "    print(predict('you must follow'))\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "batch_size, num_steps = 32, 35\n",
    "# train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "num_epochs, lr = 50000, 0.1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.5. <a id='toc11_3_1_5_'></a>[深层RNN](#toc0_)\n",
    "* 有多个隐藏层\n",
    "* 深层神经网络，默认是1层\n",
    "* 所以hiddens的形状是(num_layers, batch_size, num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 512]), torch.Size([2, 2, 512]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512\n",
    "\n",
    "\n",
    "rnn_layer = nn.RNN(\n",
    "    input_size=vocab_size,            # 输入特征的维度\n",
    "    hidden_size=num_hiddens,           # 隐藏层大小\n",
    "    bidirectional=False,     # 双向神经网络，默认是单向\n",
    "    num_layers=num_layers,             # 深层神经网络，默认是1层\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "\n",
    "# dir(rnn_layer)      # 查看属性\n",
    "# help(rnn_layer)   # 查看方法\n",
    "\n",
    "# 输入\n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.randn(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state \n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# new_state: (num_layers, batch_size, num_hiddens)\n",
    "y, new_state = rnn_layer(input, state)\n",
    "\n",
    "y.shape, new_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.6. <a id='toc11_3_1_6_'></a>[双向RNN](#toc0_)\n",
    "* 双向（其实就是将输入倒过来再输入）\n",
    "* 不能用双向循环神经网络来预测未来，因为从一开始就透露未来的信息。\n",
    "* 那实际引用场景是什么？\n",
    "    * 翻译\n",
    "    * 文本句子分类\n",
    "* 双向神经网络，所以是2倍\n",
    "* 所以hiddens的形状是(num_layers*2, batch_size, num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 1024]), torch.Size([4, 2, 512]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512\n",
    "\n",
    "rnn_layer = nn.RNN(\n",
    "    input_size = vocab_size,            # 输入特征维度\n",
    "    hidden_size = num_hiddens,           # 隐藏层大小\n",
    "    bidirectional = True,     # 双向神经网络，默认是单向\n",
    "    num_layers = num_layers,             # 深层神经网络，默认是1层 \n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "# dir(rnn_layer)      # 查看属性\n",
    "# help(rnn_layer)   # 查看方法\n",
    "\n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.zeros(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state\n",
    "# 双向神经网络，所以是2倍\n",
    "# (num_layers*2, batch_size, num_hiddens)\n",
    "state = torch.zeros(size=(num_layers*2, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens*2)\n",
    "# new_state: (num_layers*2, batch_size, num_hiddens)\n",
    "y, new_state = rnn_layer(input, state)\n",
    "\n",
    "y.shape, new_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2. <a id='toc11_3_2_'></a>[GRU](#toc0_)\n",
    "* GRU实际晚于LSTM，但是作用效果相当而更容易理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.1. <a id='toc11_3_2_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.2. <a id='toc11_3_2_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 512]), torch.Size([2, 2, 512]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512 \n",
    "\n",
    "\n",
    "gru_layer = nn.GRU(\n",
    "    input_size = vocab_size, \n",
    "    hidden_size = num_hiddens, \n",
    "    num_layers =  num_layers, \n",
    "    bidirectional = False,\n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "# input     \n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.zeros(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state \n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# new_state: (num_layers, batch_size, num_hiddens)\n",
    "y, new_state = gru_layer(input, state)\n",
    "\n",
    "y.shape, new_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3. <a id='toc11_3_3_'></a>[LSTM](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.3.1. <a id='toc11_3_3_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.3.2. <a id='toc11_3_3_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 512]), torch.Size([2, 2, 512]), torch.Size([2, 2, 512]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512\n",
    "\n",
    "\n",
    "lstm_layer = nn.LSTM(\n",
    "    input_size = vocab_size,  # 输入特征维度  \n",
    "    hidden_size = num_hiddens, \n",
    "    num_layers = num_layers, \n",
    "    bidirectional = False,\n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "\n",
    "# input\n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.zeros(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state \n",
    "# hidden_state 和 cell_state \n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "hidden_state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "cell_state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# new_state: (num_layers, batch_size, num_hiddens)\n",
    "y, new_state = lstm_layer(input, (hidden_state, cell_state))\n",
    "\n",
    "y.shape, new_state[0].shape, new_state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.4. <a id='toc11_3_4_'></a>[Encoder-Decoder框架](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Encoder-Decoder`: 架构是一种用于序列到序列（Seq2Seq）任务的常用结构，广泛应用于机器翻译、文本摘要、图像标注等任务。\n",
    "\n",
    "`Encoder`: 编码器将输入序列 𝑋=(𝑥1,𝑥2,...,𝑥𝑛) 转换为固定长度的上下文向量 𝐶 或一系列隐状态。\n",
    "\n",
    "`Decoder`: 解码器接收上下文向量 𝐶 和自身的历史输出，生成目标序列 𝑌=(𝑦1,𝑦2,...,𝑦𝑚)。\n",
    "\n",
    "一般来说，Encoder 和 Decoder 都基于 RNN、GRU、LSTM 或 Transformer。\n",
    "\n",
    "```shell\n",
    "输入-Encoder-中间状态-Decoder-输出\n",
    "                       输入\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.4.1. <a id='toc11_3_4_1_'></a>[Encoder部分](#toc0_)\n",
    "编码器（Encoder）：将输入序列转换为一个固定长度的上下文向量（或一系列上下文向量）。\n",
    "```shell\n",
    "可变长度的输入，固定长度的输出中间状态\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "#@save\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.4.2. <a id='toc11_3_4_2_'></a>[Decoder部分](#toc0_)\n",
    "解码器（Decoder）：根据上下文向量生成目标序列。\n",
    "```shell\n",
    "固定长度中间状态\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.4.3. <a id='toc11_3_4_3_'></a>[Encoder-Decoder（合并编码器和解码器）](#toc0_)\n",
    "```shell\n",
    "Encoder-Decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4. <a id='toc11_4_'></a>[seq2seq (Sequence to sequence learning)](#toc0_)\n",
    "Seq2Seq 模型最早由 Google 提出，用于机器翻译任务。其核心思想是使用两个递归神经网络（RNN）组成的架构：一个编码器（Encoder）将输入序列编码成上下文向量，另一个解码器（Decoder）根据该上下文向量生成输出序列。近年来，随着注意力机制（Attention）的引入，Seq2Seq 模型在各类序列转换任务中表现出了更强的性能。\n",
    "```shell\n",
    "基于RNN的编码器-解码器框架(Encoder-Decoder)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1. <a id='toc11_4_1_'></a>[机器翻译与数据集](#toc0_)\n",
    "机器翻译的数据集是由源语言和目标语言的文本序列对组成的，不是单个文本序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.1. <a id='toc11_4_1_1_'></a>[下载和预处理数据集](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Go.\tVa !\n",
      "1 : Hi.\tSalut !\n",
      "2 : Run!\tCours !\n",
      "3 : Run!\tCourez !\n",
      "4 : Who?\tQui ?\n",
      "5 : Wow!\tÇa alors !\n",
      "6 : Fire!\tAu feu !\n",
      "7 : Help!\tÀ l'aide !\n",
      "8 : Jump.\tSaute.\n",
      "9 : Stop!\tÇa suffit !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip', '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
    "\n",
    "#@save\n",
    "def read_data_nmt():    \n",
    "    \"\"\"载入“英语－法语”数据集\"\"\"\n",
    "    data_dir = d2l.download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "# raw_text[:100]\n",
    "for line_num, content in enumerate(raw_text.split('\\n')):\n",
    "    if line_num < 10:\n",
    "        print(f'{line_num} : {content}')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : go .\tva !\n",
      "1 : hi .\tsalut !\n",
      "2 : run !\tcours !\n",
      "3 : run !\tcourez !\n",
      "4 : who ?\tqui ?\n",
      "5 : wow !\tça alors !\n",
      "6 : fire !\tau feu !\n",
      "7 : help !\tà l'aide !\n",
      "8 : jump .\tsaute .\n",
      "9 : stop !\tça suffit !\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "下载数据集后，原始文本数据需要经过几个预处理步骤。\n",
    "例如，我们用空格代替不间断空格（non‐breaking space），\n",
    "使用小写字母替换大写字母，并在单词和标点符号之间插入空格。\n",
    "'''\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
    "    \n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # Replace non-breaking space with space, and convert uppercase letters to\n",
    "    # lowercase ones\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # Insert space between words and punctuation marks\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "\n",
    "for line_num, content in enumerate(text.split('\\n')):\n",
    "    if line_num < 10:\n",
    "        print(f'{line_num} : {content}')\n",
    "    else:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.2. <a id='toc11_4_1_2_'></a>[词元化](#toc0_)\n",
    "在机器翻译中，我们更喜欢单词级词元化（最先进的模型可能使用更高级的词元化技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ['go', '.'], ['va', '!']\n",
      "1 : ['hi', '.'], ['salut', '!']\n",
      "2 : ['run', '!'], ['cours', '!']\n",
      "3 : ['run', '!'], ['courez', '!']\n",
      "4 : ['who', '?'], ['qui', '?']\n",
      "5 : ['wow', '!'], ['ça', 'alors', '!']\n",
      "6 : ['fire', '!'], ['au', 'feu', '!']\n",
      "7 : ['help', '!'], ['à', \"l'aide\", '!']\n",
      "8 : ['jump', '.'], ['saute', '.']\n",
      "9 : ['stop', '!'], ['ça', 'suffit', '!']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "此函数返回两个词元列表,source和target：\n",
    "source[i]是源语言（这里是英语）第i个文本序列的词元列表，\n",
    "target[i]是目标语言（这里是法语）第i个文本序列的词元列表。\n",
    "'''\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"Tokenize the English-French dataset.\"\"\"\n",
    "    \n",
    "    # 源语言和目标语言的词元列表\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "\n",
    "# source[:6], target[:6]\n",
    "for line_num, (src, tgt) in enumerate(zip(source, target)):\n",
    "    if line_num < 10:\n",
    "        print(f'{line_num} : {src}, {tgt}')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.3. <a id='toc11_4_1_3_'></a>[词表](#toc0_)\n",
    "为`source`和`target`分别构建词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10012\n"
     ]
    }
   ],
   "source": [
    "src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "print(len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab['unk'], src_vocab['<pad>'], src_vocab['<bos>'], src_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<pad>': 1,\n",
       " '<bos>': 2,\n",
       " '<eos>': 3,\n",
       " '.': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'to': 7,\n",
       " 'the': 8,\n",
       " '?': 9,\n",
       " 'a': 10,\n",
       " 'is': 11,\n",
       " 'tom': 12,\n",
       " 'that': 13,\n",
       " 'he': 14,\n",
       " 'do': 15,\n",
       " 'of': 16,\n",
       " 'it': 17,\n",
       " 'this': 18,\n",
       " 'in': 19,\n",
       " 'me': 20,\n",
       " 'have': 21,\n",
       " \"don't\": 22,\n",
       " ',': 23,\n",
       " 'was': 24,\n",
       " 'my': 25,\n",
       " 'are': 26,\n",
       " 'for': 27,\n",
       " 'your': 28,\n",
       " 'what': 29,\n",
       " \"i'm\": 30,\n",
       " 'we': 31,\n",
       " 'be': 32,\n",
       " 'want': 33,\n",
       " 'she': 34,\n",
       " 'not': 35,\n",
       " 'know': 36,\n",
       " 'like': 37,\n",
       " 'on': 38,\n",
       " 'with': 39,\n",
       " 'can': 40,\n",
       " 'his': 41,\n",
       " 'all': 42,\n",
       " 'did': 43,\n",
       " 'at': 44,\n",
       " \"you're\": 45,\n",
       " 'how': 46,\n",
       " 'go': 47,\n",
       " 'they': 48,\n",
       " 'him': 49,\n",
       " 'think': 50,\n",
       " 'and': 51,\n",
       " \"it's\": 52,\n",
       " 'about': 53,\n",
       " 'time': 54,\n",
       " \"can't\": 55,\n",
       " 'here': 56,\n",
       " 'very': 57,\n",
       " \"didn't\": 58,\n",
       " 'get': 59,\n",
       " 'there': 60,\n",
       " 'her': 61,\n",
       " 'were': 62,\n",
       " 'as': 63,\n",
       " 'will': 64,\n",
       " 'had': 65,\n",
       " 'if': 66,\n",
       " 'why': 67,\n",
       " 'just': 68,\n",
       " 'up': 69,\n",
       " 'out': 70,\n",
       " 'no': 71,\n",
       " 'has': 72,\n",
       " 'one': 73,\n",
       " 'going': 74,\n",
       " 'would': 75,\n",
       " 'so': 76,\n",
       " 'good': 77,\n",
       " 'need': 78,\n",
       " 'tell': 79,\n",
       " 'an': 80,\n",
       " 'see': 81,\n",
       " \"i'll\": 82,\n",
       " 'come': 83,\n",
       " 'when': 84,\n",
       " 'from': 85,\n",
       " 'by': 86,\n",
       " 'really': 87,\n",
       " 'mary': 88,\n",
       " 'help': 89,\n",
       " 'who': 90,\n",
       " 'please': 91,\n",
       " 'us': 92,\n",
       " \"that's\": 93,\n",
       " 'should': 94,\n",
       " 'could': 95,\n",
       " 'been': 96,\n",
       " \"i've\": 97,\n",
       " 'never': 98,\n",
       " 'more': 99,\n",
       " 'now': 100,\n",
       " 'where': 101,\n",
       " 'take': 102,\n",
       " 'something': 103,\n",
       " 'got': 104,\n",
       " 'too': 105,\n",
       " 'than': 106,\n",
       " 'much': 107,\n",
       " 'make': 108,\n",
       " 'some': 109,\n",
       " \"i'd\": 110,\n",
       " \"we're\": 111,\n",
       " 'right': 112,\n",
       " 'but': 113,\n",
       " 'work': 114,\n",
       " 'am': 115,\n",
       " 'money': 116,\n",
       " 'any': 117,\n",
       " 'home': 118,\n",
       " 'last': 119,\n",
       " 'thought': 120,\n",
       " 'say': 121,\n",
       " 'sure': 122,\n",
       " 'anything': 123,\n",
       " 'look': 124,\n",
       " 'back': 125,\n",
       " '!': 126,\n",
       " 'day': 127,\n",
       " \"doesn't\": 128,\n",
       " 'give': 129,\n",
       " 'car': 130,\n",
       " 'told': 131,\n",
       " 'talk': 132,\n",
       " 'people': 133,\n",
       " 'made': 134,\n",
       " 'lot': 135,\n",
       " 'let': 136,\n",
       " 'way': 137,\n",
       " 'our': 138,\n",
       " 'must': 139,\n",
       " 'many': 140,\n",
       " 'said': 141,\n",
       " \"he's\": 142,\n",
       " 'love': 143,\n",
       " 'long': 144,\n",
       " 'went': 145,\n",
       " 'still': 146,\n",
       " 'feel': 147,\n",
       " 'only': 148,\n",
       " 'eat': 149,\n",
       " 'always': 150,\n",
       " 'better': 151,\n",
       " 'happy': 152,\n",
       " 'doing': 153,\n",
       " 'today': 154,\n",
       " 'french': 155,\n",
       " 'house': 156,\n",
       " \"isn't\": 157,\n",
       " \"let's\": 158,\n",
       " 'does': 159,\n",
       " 'new': 160,\n",
       " 'believe': 161,\n",
       " 'before': 162,\n",
       " 'leave': 163,\n",
       " \"what's\": 164,\n",
       " 'book': 165,\n",
       " 'again': 166,\n",
       " 'them': 167,\n",
       " 'room': 168,\n",
       " 'job': 169,\n",
       " 'off': 170,\n",
       " 'school': 171,\n",
       " 'night': 172,\n",
       " 'little': 173,\n",
       " 'well': 174,\n",
       " \"won't\": 175,\n",
       " 'may': 176,\n",
       " 'old': 177,\n",
       " 'down': 178,\n",
       " 'wanted': 179,\n",
       " 'everything': 180,\n",
       " 'yesterday': 181,\n",
       " 'alone': 182,\n",
       " 'happened': 183,\n",
       " 'tomorrow': 184,\n",
       " 'father': 185,\n",
       " 'stay': 186,\n",
       " 'two': 187,\n",
       " 'put': 188,\n",
       " 'left': 189,\n",
       " 'over': 190,\n",
       " 'enough': 191,\n",
       " 'every': 192,\n",
       " 'asked': 193,\n",
       " 'three': 194,\n",
       " 'speak': 195,\n",
       " 'find': 196,\n",
       " 'stop': 197,\n",
       " 'these': 198,\n",
       " 'saw': 199,\n",
       " 'man': 200,\n",
       " 'into': 201,\n",
       " 'done': 202,\n",
       " 'try': 203,\n",
       " 'understand': 204,\n",
       " 'ask': 205,\n",
       " 'or': 206,\n",
       " 'ever': 207,\n",
       " 'keep': 208,\n",
       " 'friends': 209,\n",
       " 'problem': 210,\n",
       " 'sorry': 211,\n",
       " 'next': 212,\n",
       " 'nothing': 213,\n",
       " \"there's\": 214,\n",
       " 'dog': 215,\n",
       " 'after': 216,\n",
       " 'call': 217,\n",
       " 'buy': 218,\n",
       " 'hard': 219,\n",
       " \"you've\": 220,\n",
       " 'hope': 221,\n",
       " 'busy': 222,\n",
       " 'read': 223,\n",
       " 'away': 224,\n",
       " 'live': 225,\n",
       " 'friend': 226,\n",
       " 'wrong': 227,\n",
       " 'late': 228,\n",
       " 'first': 229,\n",
       " 'things': 230,\n",
       " 'door': 231,\n",
       " 'hear': 232,\n",
       " \"tom's\": 233,\n",
       " 'life': 234,\n",
       " \"they're\": 235,\n",
       " 'thing': 236,\n",
       " 'other': 237,\n",
       " 'remember': 238,\n",
       " 'idea': 239,\n",
       " \"wasn't\": 240,\n",
       " 'boston': 241,\n",
       " 'anyone': 242,\n",
       " 'mother': 243,\n",
       " 'years': 244,\n",
       " 'took': 245,\n",
       " 'gave': 246,\n",
       " 'without': 247,\n",
       " 'being': 248,\n",
       " 'their': 249,\n",
       " 'everyone': 250,\n",
       " \"couldn't\": 251,\n",
       " 'mind': 252,\n",
       " 'came': 253,\n",
       " 'children': 254,\n",
       " 'yet': 255,\n",
       " 'knew': 256,\n",
       " 'already': 257,\n",
       " \"you'd\": 258,\n",
       " 'used': 259,\n",
       " 'name': 260,\n",
       " 'kind': 261,\n",
       " 'drink': 262,\n",
       " 'tired': 263,\n",
       " 'looking': 264,\n",
       " 'morning': 265,\n",
       " 'heard': 266,\n",
       " 'seen': 267,\n",
       " 'best': 268,\n",
       " 'bad': 269,\n",
       " \"you'll\": 270,\n",
       " 'lost': 271,\n",
       " 'teacher': 272,\n",
       " 'found': 273,\n",
       " 'even': 274,\n",
       " 'play': 275,\n",
       " 'water': 276,\n",
       " \"haven't\": 277,\n",
       " 'same': 278,\n",
       " 'care': 279,\n",
       " 'often': 280,\n",
       " 'week': 281,\n",
       " 'english': 282,\n",
       " \"aren't\": 283,\n",
       " 'use': 284,\n",
       " 'soon': 285,\n",
       " 'wait': 286,\n",
       " 'afraid': 287,\n",
       " 'ready': 288,\n",
       " 'wish': 289,\n",
       " 'answer': 290,\n",
       " 'big': 291,\n",
       " 'yourself': 292,\n",
       " 'bed': 293,\n",
       " 'party': 294,\n",
       " 'someone': 295,\n",
       " 'while': 296,\n",
       " 'few': 297,\n",
       " 'happen': 298,\n",
       " 'talking': 299,\n",
       " 'else': 300,\n",
       " 'parents': 301,\n",
       " 'wants': 302,\n",
       " 'cold': 303,\n",
       " 'train': 304,\n",
       " 'myself': 305,\n",
       " 'open': 306,\n",
       " 'around': 307,\n",
       " 'show': 308,\n",
       " 'might': 309,\n",
       " 'bought': 310,\n",
       " 'nice': 311,\n",
       " 'glad': 312,\n",
       " 'both': 313,\n",
       " 'getting': 314,\n",
       " 'married': 315,\n",
       " 'another': 316,\n",
       " \"we'll\": 317,\n",
       " 'place': 318,\n",
       " 'watch': 319,\n",
       " 'great': 320,\n",
       " 'turn': 321,\n",
       " 'year': 322,\n",
       " 'true': 323,\n",
       " 'looks': 324,\n",
       " 'early': 325,\n",
       " 'because': 326,\n",
       " 'such': 327,\n",
       " 'knows': 328,\n",
       " 'beautiful': 329,\n",
       " 'sleep': 330,\n",
       " 'write': 331,\n",
       " 'plan': 332,\n",
       " 'hurt': 333,\n",
       " \"wouldn't\": 334,\n",
       " 'almost': 335,\n",
       " 'able': 336,\n",
       " \"she's\": 337,\n",
       " 'those': 338,\n",
       " 'walk': 339,\n",
       " 'once': 340,\n",
       " 'which': 341,\n",
       " 'tried': 342,\n",
       " 'pay': 343,\n",
       " 'matter': 344,\n",
       " 'question': 345,\n",
       " 'food': 346,\n",
       " 'fun': 347,\n",
       " 'meet': 348,\n",
       " 'dinner': 349,\n",
       " 'days': 350,\n",
       " 'bus': 351,\n",
       " 'coffee': 352,\n",
       " 'brother': 353,\n",
       " 'letter': 354,\n",
       " 'truth': 355,\n",
       " 'met': 356,\n",
       " 'coming': 357,\n",
       " 'anymore': 358,\n",
       " 'everybody': 359,\n",
       " 'young': 360,\n",
       " 'meeting': 361,\n",
       " 'most': 362,\n",
       " 'person': 363,\n",
       " 'mine': 364,\n",
       " 'books': 365,\n",
       " \"shouldn't\": 366,\n",
       " 'careful': 367,\n",
       " 'sister': 368,\n",
       " 'each': 369,\n",
       " 'own': 370,\n",
       " 'family': 371,\n",
       " 'together': 372,\n",
       " 'hate': 373,\n",
       " 'felt': 374,\n",
       " 'seems': 375,\n",
       " 'tonight': 376,\n",
       " 'doctor': 377,\n",
       " 'change': 378,\n",
       " 'learn': 379,\n",
       " 'seem': 380,\n",
       " 'since': 381,\n",
       " 'likes': 382,\n",
       " 'study': 383,\n",
       " 'word': 384,\n",
       " 'waiting': 385,\n",
       " 'forget': 386,\n",
       " 'easy': 387,\n",
       " 'pretty': 388,\n",
       " 'died': 389,\n",
       " 'trying': 390,\n",
       " 'girl': 391,\n",
       " 'phone': 392,\n",
       " 'world': 393,\n",
       " 'far': 394,\n",
       " 'gone': 395,\n",
       " 'working': 396,\n",
       " \"we've\": 397,\n",
       " 'hand': 398,\n",
       " 'started': 399,\n",
       " 'child': 400,\n",
       " 'accident': 401,\n",
       " 'station': 402,\n",
       " 'mean': 403,\n",
       " 'nobody': 404,\n",
       " 'finished': 405,\n",
       " 'longer': 406,\n",
       " 'difficult': 407,\n",
       " 'sick': 408,\n",
       " 'boy': 409,\n",
       " 'surprised': 410,\n",
       " 'important': 411,\n",
       " 'start': 412,\n",
       " 'rain': 413,\n",
       " 'looked': 414,\n",
       " 'quite': 415,\n",
       " 'lunch': 416,\n",
       " 'cat': 417,\n",
       " 'drive': 418,\n",
       " 'wife': 419,\n",
       " 'questions': 420,\n",
       " 'weather': 421,\n",
       " 'anybody': 422,\n",
       " 'reading': 423,\n",
       " 'movie': 424,\n",
       " 'having': 425,\n",
       " 'yours': 426,\n",
       " 'makes': 427,\n",
       " 'mistake': 428,\n",
       " 'supposed': 429,\n",
       " 'thank': 430,\n",
       " 'office': 431,\n",
       " 'story': 432,\n",
       " 'until': 433,\n",
       " 'ten': 434,\n",
       " 'run': 435,\n",
       " 'swim': 436,\n",
       " 'small': 437,\n",
       " 'tv': 438,\n",
       " 'times': 439,\n",
       " 'close': 440,\n",
       " 'himself': 441,\n",
       " 'bit': 442,\n",
       " 'playing': 443,\n",
       " 'spend': 444,\n",
       " 'angry': 445,\n",
       " 'eyes': 446,\n",
       " 'trust': 447,\n",
       " 'stupid': 448,\n",
       " 'called': 449,\n",
       " 'ago': 450,\n",
       " 'guess': 451,\n",
       " 'advice': 452,\n",
       " 'japan': 453,\n",
       " 'hurry': 454,\n",
       " 'picture': 455,\n",
       " 'hours': 456,\n",
       " '.\"': 457,\n",
       " 'broke': 458,\n",
       " 'music': 459,\n",
       " 'exactly': 460,\n",
       " 'says': 461,\n",
       " 'caught': 462,\n",
       " 'students': 463,\n",
       " 'wonder': 464,\n",
       " 'son': 465,\n",
       " 'lives': 466,\n",
       " 'fire': 467,\n",
       " 'afternoon': 468,\n",
       " 'window': 469,\n",
       " 'eating': 470,\n",
       " 'turned': 471,\n",
       " 'police': 472,\n",
       " 'bicycle': 473,\n",
       " 'thinking': 474,\n",
       " 'fell': 475,\n",
       " 'ran': 476,\n",
       " 'decided': 477,\n",
       " 'table': 478,\n",
       " 'fast': 479,\n",
       " 'usually': 480,\n",
       " 'probably': 481,\n",
       " \"who's\": 482,\n",
       " 'minutes': 483,\n",
       " 'japanese': 484,\n",
       " 'interested': 485,\n",
       " 'interesting': 486,\n",
       " 'trouble': 487,\n",
       " 'free': 488,\n",
       " 'hair': 489,\n",
       " 'arrived': 490,\n",
       " 'hot': 491,\n",
       " 'bring': 492,\n",
       " \"weren't\": 493,\n",
       " 'light': 494,\n",
       " 'town': 495,\n",
       " 'homework': 496,\n",
       " 'advised': 497,\n",
       " 'worry': 498,\n",
       " 'number': 499,\n",
       " 'park': 500,\n",
       " 'game': 501,\n",
       " 'under': 502,\n",
       " 'safe': 503,\n",
       " 'listen': 504,\n",
       " 'forgot': 505,\n",
       " 'stand': 506,\n",
       " 'enjoy': 507,\n",
       " 'country': 508,\n",
       " 'living': 509,\n",
       " 'news': 510,\n",
       " 'through': 511,\n",
       " 'against': 512,\n",
       " 'appreciate': 513,\n",
       " 'agree': 514,\n",
       " 'age': 515,\n",
       " 'miss': 516,\n",
       " 'sit': 517,\n",
       " 'hands': 518,\n",
       " 'quit': 519,\n",
       " 'promise': 520,\n",
       " 'proud': 521,\n",
       " 'sing': 522,\n",
       " 'chance': 523,\n",
       " 'saying': 524,\n",
       " 'possible': 525,\n",
       " 'visit': 526,\n",
       " 'finish': 527,\n",
       " 'different': 528,\n",
       " 'maybe': 529,\n",
       " 'later': 530,\n",
       " 'high': 531,\n",
       " 'reason': 532,\n",
       " 'wine': 533,\n",
       " 'then': 534,\n",
       " 'outside': 535,\n",
       " 'summer': 536,\n",
       " 'kept': 537,\n",
       " 'taking': 538,\n",
       " 'catch': 539,\n",
       " 'hungry': 540,\n",
       " 'needs': 541,\n",
       " 'born': 542,\n",
       " 'lie': 543,\n",
       " 'making': 544,\n",
       " \"should've\": 545,\n",
       " 'cut': 546,\n",
       " 'business': 547,\n",
       " 'moment': 548,\n",
       " 'trip': 549,\n",
       " 'favorite': 550,\n",
       " 'dead': 551,\n",
       " 'end': 552,\n",
       " 'ok': 553,\n",
       " 'shoes': 554,\n",
       " 'older': 555,\n",
       " 'become': 556,\n",
       " 'win': 557,\n",
       " 'tree': 558,\n",
       " 'behind': 559,\n",
       " 'five': 560,\n",
       " 'box': 561,\n",
       " 'near': 562,\n",
       " 'works': 563,\n",
       " 'red': 564,\n",
       " 'girlfriend': 565,\n",
       " 'breakfast': 566,\n",
       " 'die': 567,\n",
       " 'class': 568,\n",
       " 'song': 569,\n",
       " 'tea': 570,\n",
       " 'eaten': 571,\n",
       " 'city': 572,\n",
       " 'dress': 573,\n",
       " 'student': 574,\n",
       " 'baby': 575,\n",
       " \"mary's\": 576,\n",
       " 'rich': 577,\n",
       " 'needed': 578,\n",
       " 'guy': 579,\n",
       " 'face': 580,\n",
       " 'funny': 581,\n",
       " 'secret': 582,\n",
       " 'team': 583,\n",
       " 'month': 584,\n",
       " 'company': 585,\n",
       " 'full': 586,\n",
       " 'quickly': 587,\n",
       " 'comes': 588,\n",
       " 'paid': 589,\n",
       " 'stayed': 590,\n",
       " \"where's\": 591,\n",
       " 'crazy': 592,\n",
       " 'fish': 593,\n",
       " 'rest': 594,\n",
       " 'ate': 595,\n",
       " 'lose': 596,\n",
       " 'woman': 597,\n",
       " 'point': 598,\n",
       " 'watching': 599,\n",
       " 'tennis': 600,\n",
       " 'beer': 601,\n",
       " 'explain': 602,\n",
       " 'part': 603,\n",
       " 'invited': 604,\n",
       " 'serious': 605,\n",
       " 'cannot': 606,\n",
       " 'break': 607,\n",
       " 'large': 608,\n",
       " 'clothes': 609,\n",
       " 'daughter': 610,\n",
       " 'smoking': 611,\n",
       " 'hotel': 612,\n",
       " 'kids': 613,\n",
       " 'key': 614,\n",
       " 'choice': 615,\n",
       " 'asleep': 616,\n",
       " 'hat': 617,\n",
       " 'feeling': 618,\n",
       " 'sound': 619,\n",
       " 'death': 620,\n",
       " 'cost': 621,\n",
       " 'somebody': 622,\n",
       " 'clean': 623,\n",
       " 'australia': 624,\n",
       " 'spent': 625,\n",
       " 'worried': 626,\n",
       " 'began': 627,\n",
       " 'words': 628,\n",
       " 'real': 629,\n",
       " 'lived': 630,\n",
       " 'wearing': 631,\n",
       " '?\"': 632,\n",
       " 'studying': 633,\n",
       " 'handle': 634,\n",
       " 'expensive': 635,\n",
       " 'sometimes': 636,\n",
       " 'goes': 637,\n",
       " 'hour': 638,\n",
       " 'became': 639,\n",
       " 'street': 640,\n",
       " 'touch': 641,\n",
       " 'hit': 642,\n",
       " 'milk': 643,\n",
       " 'river': 644,\n",
       " 'killed': 645,\n",
       " 'store': 646,\n",
       " 'hospital': 647,\n",
       " 'changed': 648,\n",
       " 'short': 649,\n",
       " 'rather': 650,\n",
       " 'decision': 651,\n",
       " 'computer': 652,\n",
       " 'others': 653,\n",
       " 'telling': 654,\n",
       " 'drunk': 655,\n",
       " 'deal': 656,\n",
       " 'between': 657,\n",
       " 'paper': 658,\n",
       " 'hold': 659,\n",
       " 'lucky': 660,\n",
       " 'cake': 661,\n",
       " 'scared': 662,\n",
       " 'takes': 663,\n",
       " 'birthday': 664,\n",
       " 'snow': 665,\n",
       " 'language': 666,\n",
       " 'closed': 667,\n",
       " 'present': 668,\n",
       " 'helped': 669,\n",
       " 'dark': 670,\n",
       " 'minute': 671,\n",
       " 'stopped': 672,\n",
       " 'problems': 673,\n",
       " 'restaurant': 674,\n",
       " 'sat': 675,\n",
       " 'monday': 676,\n",
       " 'speaking': 677,\n",
       " 'expect': 678,\n",
       " 'front': 679,\n",
       " 'whole': 680,\n",
       " 'quiet': 681,\n",
       " 'war': 682,\n",
       " 'mistakes': 683,\n",
       " 'figured': 684,\n",
       " 'worked': 685,\n",
       " 'finally': 686,\n",
       " 'gets': 687,\n",
       " 'along': 688,\n",
       " 'head': 689,\n",
       " 'report': 690,\n",
       " 'wrote': 691,\n",
       " 'happening': 692,\n",
       " 'dogs': 693,\n",
       " 'coat': 694,\n",
       " 'sense': 695,\n",
       " 'cup': 696,\n",
       " 'talked': 697,\n",
       " 'liked': 698,\n",
       " 'strong': 699,\n",
       " 'upset': 700,\n",
       " 'kill': 701,\n",
       " 'speaks': 702,\n",
       " 'thanks': 703,\n",
       " 'missed': 704,\n",
       " 'forward': 705,\n",
       " 'strange': 706,\n",
       " 'expected': 707,\n",
       " 'check': 708,\n",
       " 'boss': 709,\n",
       " 'dream': 710,\n",
       " 'dangerous': 711,\n",
       " 'beach': 712,\n",
       " 'known': 713,\n",
       " 'health': 714,\n",
       " 'situation': 715,\n",
       " 'actually': 716,\n",
       " 'running': 717,\n",
       " 'brought': 718,\n",
       " 'whether': 719,\n",
       " 'six': 720,\n",
       " 'its': 721,\n",
       " 'whatever': 722,\n",
       " 'dictionary': 723,\n",
       " 'move': 724,\n",
       " 'seeing': 725,\n",
       " 'shut': 726,\n",
       " 'order': 727,\n",
       " 'swimming': 728,\n",
       " 'weekend': 729,\n",
       " 'tall': 730,\n",
       " 'men': 731,\n",
       " 'air': 732,\n",
       " 'evening': 733,\n",
       " 'walked': 734,\n",
       " 'plane': 735,\n",
       " 'allowed': 736,\n",
       " 'thirty': 737,\n",
       " 'loves': 738,\n",
       " 'case': 739,\n",
       " 'least': 740,\n",
       " 'building': 741,\n",
       " 'broken': 742,\n",
       " 'list': 743,\n",
       " 'worth': 744,\n",
       " 'happens': 745,\n",
       " 'heart': 746,\n",
       " 'disappointed': 747,\n",
       " 'follow': 748,\n",
       " 'famous': 749,\n",
       " 'prefer': 750,\n",
       " 'written': 751,\n",
       " 'smoke': 752,\n",
       " 'christmas': 753,\n",
       " 'perfect': 754,\n",
       " 'drinking': 755,\n",
       " \"o'clock\": 756,\n",
       " 'luck': 757,\n",
       " 'choose': 758,\n",
       " 'completely': 759,\n",
       " 'dollars': 760,\n",
       " 'pain': 761,\n",
       " 'future': 762,\n",
       " 'either': 763,\n",
       " 'mad': 764,\n",
       " 'seat': 765,\n",
       " 'crying': 766,\n",
       " 'dance': 767,\n",
       " 'second': 768,\n",
       " 'piano': 769,\n",
       " 'offer': 770,\n",
       " 'necessary': 771,\n",
       " 'fine': 772,\n",
       " 'half': 773,\n",
       " 'sent': 774,\n",
       " 'sunday': 775,\n",
       " \"hasn't\": 776,\n",
       " 'garden': 777,\n",
       " 'apologize': 778,\n",
       " 'rules': 779,\n",
       " 'road': 780,\n",
       " 'library': 781,\n",
       " 'leaving': 782,\n",
       " 'noise': 783,\n",
       " 'weight': 784,\n",
       " 'flowers': 785,\n",
       " 'wear': 786,\n",
       " 'opinion': 787,\n",
       " 'cook': 788,\n",
       " 'writing': 789,\n",
       " 'camera': 790,\n",
       " 'set': 791,\n",
       " 'taken': 792,\n",
       " 'glass': 793,\n",
       " 'learned': 794,\n",
       " 'address': 795,\n",
       " 'somewhere': 796,\n",
       " 'poor': 797,\n",
       " 'several': 798,\n",
       " \"it'll\": 799,\n",
       " 'white': 800,\n",
       " 'danger': 801,\n",
       " 'attention': 802,\n",
       " \"he'll\": 803,\n",
       " 'tokyo': 804,\n",
       " 'floor': 805,\n",
       " 'save': 806,\n",
       " 'alive': 807,\n",
       " 'accept': 808,\n",
       " 'information': 809,\n",
       " 'clear': 810,\n",
       " 'suppose': 811,\n",
       " 'opened': 812,\n",
       " 'during': 813,\n",
       " 'kiss': 814,\n",
       " 'loved': 815,\n",
       " 'nervous': 816,\n",
       " 'grow': 817,\n",
       " 'girls': 818,\n",
       " 'waste': 819,\n",
       " 'showed': 820,\n",
       " 'join': 821,\n",
       " 'women': 822,\n",
       " 'bag': 823,\n",
       " 'sleeping': 824,\n",
       " 'listening': 825,\n",
       " 'less': 826,\n",
       " 'solve': 827,\n",
       " 'sad': 828,\n",
       " 'won': 829,\n",
       " 'date': 830,\n",
       " 'shopping': 831,\n",
       " 'blame': 832,\n",
       " 'desk': 833,\n",
       " 'tie': 834,\n",
       " 'medicine': 835,\n",
       " 'vacation': 836,\n",
       " 'pass': 837,\n",
       " 'college': 838,\n",
       " 'side': 839,\n",
       " 'success': 840,\n",
       " 'teach': 841,\n",
       " 'keys': 842,\n",
       " 'arrive': 843,\n",
       " 'umbrella': 844,\n",
       " 'fix': 845,\n",
       " 'sounds': 846,\n",
       " 'ticket': 847,\n",
       " 'whose': 848,\n",
       " 'ship': 849,\n",
       " 'radio': 850,\n",
       " 'lying': 851,\n",
       " 'spoke': 852,\n",
       " 'smart': 853,\n",
       " 'means': 854,\n",
       " 'lawyer': 855,\n",
       " 'thinks': 856,\n",
       " 'telephone': 857,\n",
       " 'abroad': 858,\n",
       " 'promised': 859,\n",
       " 'glasses': 860,\n",
       " 'willing': 861,\n",
       " 'wake': 862,\n",
       " 'shot': 863,\n",
       " 'black': 864,\n",
       " 'owe': 865,\n",
       " 'husband': 866,\n",
       " 'speech': 867,\n",
       " 'cats': 868,\n",
       " 'plans': 869,\n",
       " 'horse': 870,\n",
       " 'borrow': 871,\n",
       " 'driving': 872,\n",
       " 'staying': 873,\n",
       " 'apple': 874,\n",
       " 'message': 875,\n",
       " 'immediately': 876,\n",
       " 'guitar': 877,\n",
       " 'piece': 878,\n",
       " 'discuss': 879,\n",
       " 'across': 880,\n",
       " 'excuse': 881,\n",
       " 'dressed': 882,\n",
       " 'uncle': 883,\n",
       " 'pictures': 884,\n",
       " 'given': 885,\n",
       " 'fat': 886,\n",
       " 'guys': 887,\n",
       " 'waited': 888,\n",
       " 'prepared': 889,\n",
       " 'fight': 890,\n",
       " 'bank': 891,\n",
       " 'asking': 892,\n",
       " 'involved': 893,\n",
       " 'american': 894,\n",
       " 'lend': 895,\n",
       " 'carefully': 896,\n",
       " 'harder': 897,\n",
       " 'none': 898,\n",
       " 'boyfriend': 899,\n",
       " 'concert': 900,\n",
       " 'sign': 901,\n",
       " 'boys': 902,\n",
       " 'ride': 903,\n",
       " 'traffic': 904,\n",
       " 'cry': 905,\n",
       " 'certain': 906,\n",
       " 'gun': 907,\n",
       " 'shower': 908,\n",
       " 'afford': 909,\n",
       " 'cute': 910,\n",
       " 'joke': 911,\n",
       " 'price': 912,\n",
       " 'couple': 913,\n",
       " 'inside': 914,\n",
       " 'easily': 915,\n",
       " 'arm': 916,\n",
       " 'lake': 917,\n",
       " 'novel': 918,\n",
       " 'shirt': 919,\n",
       " 'storm': 920,\n",
       " 'painting': 921,\n",
       " 'agreed': 922,\n",
       " 'send': 923,\n",
       " 'satisfied': 924,\n",
       " 'worse': 925,\n",
       " 'regret': 926,\n",
       " 'america': 927,\n",
       " 'fired': 928,\n",
       " 'begin': 929,\n",
       " 'kid': 930,\n",
       " 'twice': 931,\n",
       " 'raining': 932,\n",
       " 'succeed': 933,\n",
       " 'travel': 934,\n",
       " 'fault': 935,\n",
       " \"how's\": 936,\n",
       " 'count': 937,\n",
       " 'correct': 938,\n",
       " 'act': 939,\n",
       " \"they'll\": 940,\n",
       " 'passed': 941,\n",
       " 'chair': 942,\n",
       " \"we'd\": 943,\n",
       " 'difference': 944,\n",
       " 'ice': 945,\n",
       " 'test': 946,\n",
       " 'empty': 947,\n",
       " 'honest': 948,\n",
       " 'laugh': 949,\n",
       " 'meat': 950,\n",
       " 'movies': 951,\n",
       " 'television': 952,\n",
       " 'four': 953,\n",
       " 'pen': 954,\n",
       " 'earlier': 955,\n",
       " 'hardly': 956,\n",
       " 'impossible': 957,\n",
       " 'held': 958,\n",
       " 'color': 959,\n",
       " 'figure': 960,\n",
       " 'ahead': 961,\n",
       " 'wonderful': 962,\n",
       " 'blue': 963,\n",
       " 'enjoyed': 964,\n",
       " 'winter': 965,\n",
       " 'likely': 966,\n",
       " 'machine': 967,\n",
       " 'carry': 968,\n",
       " 'warm': 969,\n",
       " 'refused': 970,\n",
       " 'fall': 971,\n",
       " 'line': 972,\n",
       " 'bill': 973,\n",
       " 'played': 974,\n",
       " 'decide': 975,\n",
       " 'kissed': 976,\n",
       " 'terrible': 977,\n",
       " 'animals': 978,\n",
       " 'younger': 979,\n",
       " 'herself': 980,\n",
       " 'mountain': 981,\n",
       " 'bother': 982,\n",
       " 'laughed': 983,\n",
       " 'slept': 984,\n",
       " 'walking': 985,\n",
       " 'ideas': 986,\n",
       " 'past': 987,\n",
       " 'heavy': 988,\n",
       " 'shop': 989,\n",
       " 'anywhere': 990,\n",
       " 'foreign': 991,\n",
       " 'sooner': 992,\n",
       " 'liar': 993,\n",
       " 'cooking': 994,\n",
       " 'injured': 995,\n",
       " 'leaves': 996,\n",
       " 'newspaper': 997,\n",
       " 'pick': 998,\n",
       " 'smell': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.token_to_idx  # dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<bos>',\n",
       " '<eos>',\n",
       " '.',\n",
       " 'i',\n",
       " 'you',\n",
       " 'to',\n",
       " 'the',\n",
       " '?',\n",
       " 'a',\n",
       " 'is',\n",
       " 'tom',\n",
       " 'that',\n",
       " 'he',\n",
       " 'do',\n",
       " 'of',\n",
       " 'it',\n",
       " 'this',\n",
       " 'in',\n",
       " 'me',\n",
       " 'have',\n",
       " \"don't\",\n",
       " ',',\n",
       " 'was',\n",
       " 'my',\n",
       " 'are',\n",
       " 'for',\n",
       " 'your',\n",
       " 'what',\n",
       " \"i'm\",\n",
       " 'we',\n",
       " 'be',\n",
       " 'want',\n",
       " 'she',\n",
       " 'not',\n",
       " 'know',\n",
       " 'like',\n",
       " 'on',\n",
       " 'with',\n",
       " 'can',\n",
       " 'his',\n",
       " 'all',\n",
       " 'did',\n",
       " 'at',\n",
       " \"you're\",\n",
       " 'how',\n",
       " 'go',\n",
       " 'they',\n",
       " 'him',\n",
       " 'think',\n",
       " 'and',\n",
       " \"it's\",\n",
       " 'about',\n",
       " 'time',\n",
       " \"can't\",\n",
       " 'here',\n",
       " 'very',\n",
       " \"didn't\",\n",
       " 'get',\n",
       " 'there',\n",
       " 'her',\n",
       " 'were',\n",
       " 'as',\n",
       " 'will',\n",
       " 'had',\n",
       " 'if',\n",
       " 'why',\n",
       " 'just',\n",
       " 'up',\n",
       " 'out',\n",
       " 'no',\n",
       " 'has',\n",
       " 'one',\n",
       " 'going',\n",
       " 'would',\n",
       " 'so',\n",
       " 'good',\n",
       " 'need',\n",
       " 'tell',\n",
       " 'an',\n",
       " 'see',\n",
       " \"i'll\",\n",
       " 'come',\n",
       " 'when',\n",
       " 'from',\n",
       " 'by',\n",
       " 'really',\n",
       " 'mary',\n",
       " 'help',\n",
       " 'who',\n",
       " 'please',\n",
       " 'us',\n",
       " \"that's\",\n",
       " 'should',\n",
       " 'could',\n",
       " 'been',\n",
       " \"i've\",\n",
       " 'never',\n",
       " 'more',\n",
       " 'now',\n",
       " 'where',\n",
       " 'take',\n",
       " 'something',\n",
       " 'got',\n",
       " 'too',\n",
       " 'than',\n",
       " 'much',\n",
       " 'make',\n",
       " 'some',\n",
       " \"i'd\",\n",
       " \"we're\",\n",
       " 'right',\n",
       " 'but',\n",
       " 'work',\n",
       " 'am',\n",
       " 'money',\n",
       " 'any',\n",
       " 'home',\n",
       " 'last',\n",
       " 'thought',\n",
       " 'say',\n",
       " 'sure',\n",
       " 'anything',\n",
       " 'look',\n",
       " 'back',\n",
       " '!',\n",
       " 'day',\n",
       " \"doesn't\",\n",
       " 'give',\n",
       " 'car',\n",
       " 'told',\n",
       " 'talk',\n",
       " 'people',\n",
       " 'made',\n",
       " 'lot',\n",
       " 'let',\n",
       " 'way',\n",
       " 'our',\n",
       " 'must',\n",
       " 'many',\n",
       " 'said',\n",
       " \"he's\",\n",
       " 'love',\n",
       " 'long',\n",
       " 'went',\n",
       " 'still',\n",
       " 'feel',\n",
       " 'only',\n",
       " 'eat',\n",
       " 'always',\n",
       " 'better',\n",
       " 'happy',\n",
       " 'doing',\n",
       " 'today',\n",
       " 'french',\n",
       " 'house',\n",
       " \"isn't\",\n",
       " \"let's\",\n",
       " 'does',\n",
       " 'new',\n",
       " 'believe',\n",
       " 'before',\n",
       " 'leave',\n",
       " \"what's\",\n",
       " 'book',\n",
       " 'again',\n",
       " 'them',\n",
       " 'room',\n",
       " 'job',\n",
       " 'off',\n",
       " 'school',\n",
       " 'night',\n",
       " 'little',\n",
       " 'well',\n",
       " \"won't\",\n",
       " 'may',\n",
       " 'old',\n",
       " 'down',\n",
       " 'wanted',\n",
       " 'everything',\n",
       " 'yesterday',\n",
       " 'alone',\n",
       " 'happened',\n",
       " 'tomorrow',\n",
       " 'father',\n",
       " 'stay',\n",
       " 'two',\n",
       " 'put',\n",
       " 'left',\n",
       " 'over',\n",
       " 'enough',\n",
       " 'every',\n",
       " 'asked',\n",
       " 'three',\n",
       " 'speak',\n",
       " 'find',\n",
       " 'stop',\n",
       " 'these',\n",
       " 'saw',\n",
       " 'man',\n",
       " 'into',\n",
       " 'done',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'ask',\n",
       " 'or',\n",
       " 'ever',\n",
       " 'keep',\n",
       " 'friends',\n",
       " 'problem',\n",
       " 'sorry',\n",
       " 'next',\n",
       " 'nothing',\n",
       " \"there's\",\n",
       " 'dog',\n",
       " 'after',\n",
       " 'call',\n",
       " 'buy',\n",
       " 'hard',\n",
       " \"you've\",\n",
       " 'hope',\n",
       " 'busy',\n",
       " 'read',\n",
       " 'away',\n",
       " 'live',\n",
       " 'friend',\n",
       " 'wrong',\n",
       " 'late',\n",
       " 'first',\n",
       " 'things',\n",
       " 'door',\n",
       " 'hear',\n",
       " \"tom's\",\n",
       " 'life',\n",
       " \"they're\",\n",
       " 'thing',\n",
       " 'other',\n",
       " 'remember',\n",
       " 'idea',\n",
       " \"wasn't\",\n",
       " 'boston',\n",
       " 'anyone',\n",
       " 'mother',\n",
       " 'years',\n",
       " 'took',\n",
       " 'gave',\n",
       " 'without',\n",
       " 'being',\n",
       " 'their',\n",
       " 'everyone',\n",
       " \"couldn't\",\n",
       " 'mind',\n",
       " 'came',\n",
       " 'children',\n",
       " 'yet',\n",
       " 'knew',\n",
       " 'already',\n",
       " \"you'd\",\n",
       " 'used',\n",
       " 'name',\n",
       " 'kind',\n",
       " 'drink',\n",
       " 'tired',\n",
       " 'looking',\n",
       " 'morning',\n",
       " 'heard',\n",
       " 'seen',\n",
       " 'best',\n",
       " 'bad',\n",
       " \"you'll\",\n",
       " 'lost',\n",
       " 'teacher',\n",
       " 'found',\n",
       " 'even',\n",
       " 'play',\n",
       " 'water',\n",
       " \"haven't\",\n",
       " 'same',\n",
       " 'care',\n",
       " 'often',\n",
       " 'week',\n",
       " 'english',\n",
       " \"aren't\",\n",
       " 'use',\n",
       " 'soon',\n",
       " 'wait',\n",
       " 'afraid',\n",
       " 'ready',\n",
       " 'wish',\n",
       " 'answer',\n",
       " 'big',\n",
       " 'yourself',\n",
       " 'bed',\n",
       " 'party',\n",
       " 'someone',\n",
       " 'while',\n",
       " 'few',\n",
       " 'happen',\n",
       " 'talking',\n",
       " 'else',\n",
       " 'parents',\n",
       " 'wants',\n",
       " 'cold',\n",
       " 'train',\n",
       " 'myself',\n",
       " 'open',\n",
       " 'around',\n",
       " 'show',\n",
       " 'might',\n",
       " 'bought',\n",
       " 'nice',\n",
       " 'glad',\n",
       " 'both',\n",
       " 'getting',\n",
       " 'married',\n",
       " 'another',\n",
       " \"we'll\",\n",
       " 'place',\n",
       " 'watch',\n",
       " 'great',\n",
       " 'turn',\n",
       " 'year',\n",
       " 'true',\n",
       " 'looks',\n",
       " 'early',\n",
       " 'because',\n",
       " 'such',\n",
       " 'knows',\n",
       " 'beautiful',\n",
       " 'sleep',\n",
       " 'write',\n",
       " 'plan',\n",
       " 'hurt',\n",
       " \"wouldn't\",\n",
       " 'almost',\n",
       " 'able',\n",
       " \"she's\",\n",
       " 'those',\n",
       " 'walk',\n",
       " 'once',\n",
       " 'which',\n",
       " 'tried',\n",
       " 'pay',\n",
       " 'matter',\n",
       " 'question',\n",
       " 'food',\n",
       " 'fun',\n",
       " 'meet',\n",
       " 'dinner',\n",
       " 'days',\n",
       " 'bus',\n",
       " 'coffee',\n",
       " 'brother',\n",
       " 'letter',\n",
       " 'truth',\n",
       " 'met',\n",
       " 'coming',\n",
       " 'anymore',\n",
       " 'everybody',\n",
       " 'young',\n",
       " 'meeting',\n",
       " 'most',\n",
       " 'person',\n",
       " 'mine',\n",
       " 'books',\n",
       " \"shouldn't\",\n",
       " 'careful',\n",
       " 'sister',\n",
       " 'each',\n",
       " 'own',\n",
       " 'family',\n",
       " 'together',\n",
       " 'hate',\n",
       " 'felt',\n",
       " 'seems',\n",
       " 'tonight',\n",
       " 'doctor',\n",
       " 'change',\n",
       " 'learn',\n",
       " 'seem',\n",
       " 'since',\n",
       " 'likes',\n",
       " 'study',\n",
       " 'word',\n",
       " 'waiting',\n",
       " 'forget',\n",
       " 'easy',\n",
       " 'pretty',\n",
       " 'died',\n",
       " 'trying',\n",
       " 'girl',\n",
       " 'phone',\n",
       " 'world',\n",
       " 'far',\n",
       " 'gone',\n",
       " 'working',\n",
       " \"we've\",\n",
       " 'hand',\n",
       " 'started',\n",
       " 'child',\n",
       " 'accident',\n",
       " 'station',\n",
       " 'mean',\n",
       " 'nobody',\n",
       " 'finished',\n",
       " 'longer',\n",
       " 'difficult',\n",
       " 'sick',\n",
       " 'boy',\n",
       " 'surprised',\n",
       " 'important',\n",
       " 'start',\n",
       " 'rain',\n",
       " 'looked',\n",
       " 'quite',\n",
       " 'lunch',\n",
       " 'cat',\n",
       " 'drive',\n",
       " 'wife',\n",
       " 'questions',\n",
       " 'weather',\n",
       " 'anybody',\n",
       " 'reading',\n",
       " 'movie',\n",
       " 'having',\n",
       " 'yours',\n",
       " 'makes',\n",
       " 'mistake',\n",
       " 'supposed',\n",
       " 'thank',\n",
       " 'office',\n",
       " 'story',\n",
       " 'until',\n",
       " 'ten',\n",
       " 'run',\n",
       " 'swim',\n",
       " 'small',\n",
       " 'tv',\n",
       " 'times',\n",
       " 'close',\n",
       " 'himself',\n",
       " 'bit',\n",
       " 'playing',\n",
       " 'spend',\n",
       " 'angry',\n",
       " 'eyes',\n",
       " 'trust',\n",
       " 'stupid',\n",
       " 'called',\n",
       " 'ago',\n",
       " 'guess',\n",
       " 'advice',\n",
       " 'japan',\n",
       " 'hurry',\n",
       " 'picture',\n",
       " 'hours',\n",
       " '.\"',\n",
       " 'broke',\n",
       " 'music',\n",
       " 'exactly',\n",
       " 'says',\n",
       " 'caught',\n",
       " 'students',\n",
       " 'wonder',\n",
       " 'son',\n",
       " 'lives',\n",
       " 'fire',\n",
       " 'afternoon',\n",
       " 'window',\n",
       " 'eating',\n",
       " 'turned',\n",
       " 'police',\n",
       " 'bicycle',\n",
       " 'thinking',\n",
       " 'fell',\n",
       " 'ran',\n",
       " 'decided',\n",
       " 'table',\n",
       " 'fast',\n",
       " 'usually',\n",
       " 'probably',\n",
       " \"who's\",\n",
       " 'minutes',\n",
       " 'japanese',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'trouble',\n",
       " 'free',\n",
       " 'hair',\n",
       " 'arrived',\n",
       " 'hot',\n",
       " 'bring',\n",
       " \"weren't\",\n",
       " 'light',\n",
       " 'town',\n",
       " 'homework',\n",
       " 'advised',\n",
       " 'worry',\n",
       " 'number',\n",
       " 'park',\n",
       " 'game',\n",
       " 'under',\n",
       " 'safe',\n",
       " 'listen',\n",
       " 'forgot',\n",
       " 'stand',\n",
       " 'enjoy',\n",
       " 'country',\n",
       " 'living',\n",
       " 'news',\n",
       " 'through',\n",
       " 'against',\n",
       " 'appreciate',\n",
       " 'agree',\n",
       " 'age',\n",
       " 'miss',\n",
       " 'sit',\n",
       " 'hands',\n",
       " 'quit',\n",
       " 'promise',\n",
       " 'proud',\n",
       " 'sing',\n",
       " 'chance',\n",
       " 'saying',\n",
       " 'possible',\n",
       " 'visit',\n",
       " 'finish',\n",
       " 'different',\n",
       " 'maybe',\n",
       " 'later',\n",
       " 'high',\n",
       " 'reason',\n",
       " 'wine',\n",
       " 'then',\n",
       " 'outside',\n",
       " 'summer',\n",
       " 'kept',\n",
       " 'taking',\n",
       " 'catch',\n",
       " 'hungry',\n",
       " 'needs',\n",
       " 'born',\n",
       " 'lie',\n",
       " 'making',\n",
       " \"should've\",\n",
       " 'cut',\n",
       " 'business',\n",
       " 'moment',\n",
       " 'trip',\n",
       " 'favorite',\n",
       " 'dead',\n",
       " 'end',\n",
       " 'ok',\n",
       " 'shoes',\n",
       " 'older',\n",
       " 'become',\n",
       " 'win',\n",
       " 'tree',\n",
       " 'behind',\n",
       " 'five',\n",
       " 'box',\n",
       " 'near',\n",
       " 'works',\n",
       " 'red',\n",
       " 'girlfriend',\n",
       " 'breakfast',\n",
       " 'die',\n",
       " 'class',\n",
       " 'song',\n",
       " 'tea',\n",
       " 'eaten',\n",
       " 'city',\n",
       " 'dress',\n",
       " 'student',\n",
       " 'baby',\n",
       " \"mary's\",\n",
       " 'rich',\n",
       " 'needed',\n",
       " 'guy',\n",
       " 'face',\n",
       " 'funny',\n",
       " 'secret',\n",
       " 'team',\n",
       " 'month',\n",
       " 'company',\n",
       " 'full',\n",
       " 'quickly',\n",
       " 'comes',\n",
       " 'paid',\n",
       " 'stayed',\n",
       " \"where's\",\n",
       " 'crazy',\n",
       " 'fish',\n",
       " 'rest',\n",
       " 'ate',\n",
       " 'lose',\n",
       " 'woman',\n",
       " 'point',\n",
       " 'watching',\n",
       " 'tennis',\n",
       " 'beer',\n",
       " 'explain',\n",
       " 'part',\n",
       " 'invited',\n",
       " 'serious',\n",
       " 'cannot',\n",
       " 'break',\n",
       " 'large',\n",
       " 'clothes',\n",
       " 'daughter',\n",
       " 'smoking',\n",
       " 'hotel',\n",
       " 'kids',\n",
       " 'key',\n",
       " 'choice',\n",
       " 'asleep',\n",
       " 'hat',\n",
       " 'feeling',\n",
       " 'sound',\n",
       " 'death',\n",
       " 'cost',\n",
       " 'somebody',\n",
       " 'clean',\n",
       " 'australia',\n",
       " 'spent',\n",
       " 'worried',\n",
       " 'began',\n",
       " 'words',\n",
       " 'real',\n",
       " 'lived',\n",
       " 'wearing',\n",
       " '?\"',\n",
       " 'studying',\n",
       " 'handle',\n",
       " 'expensive',\n",
       " 'sometimes',\n",
       " 'goes',\n",
       " 'hour',\n",
       " 'became',\n",
       " 'street',\n",
       " 'touch',\n",
       " 'hit',\n",
       " 'milk',\n",
       " 'river',\n",
       " 'killed',\n",
       " 'store',\n",
       " 'hospital',\n",
       " 'changed',\n",
       " 'short',\n",
       " 'rather',\n",
       " 'decision',\n",
       " 'computer',\n",
       " 'others',\n",
       " 'telling',\n",
       " 'drunk',\n",
       " 'deal',\n",
       " 'between',\n",
       " 'paper',\n",
       " 'hold',\n",
       " 'lucky',\n",
       " 'cake',\n",
       " 'scared',\n",
       " 'takes',\n",
       " 'birthday',\n",
       " 'snow',\n",
       " 'language',\n",
       " 'closed',\n",
       " 'present',\n",
       " 'helped',\n",
       " 'dark',\n",
       " 'minute',\n",
       " 'stopped',\n",
       " 'problems',\n",
       " 'restaurant',\n",
       " 'sat',\n",
       " 'monday',\n",
       " 'speaking',\n",
       " 'expect',\n",
       " 'front',\n",
       " 'whole',\n",
       " 'quiet',\n",
       " 'war',\n",
       " 'mistakes',\n",
       " 'figured',\n",
       " 'worked',\n",
       " 'finally',\n",
       " 'gets',\n",
       " 'along',\n",
       " 'head',\n",
       " 'report',\n",
       " 'wrote',\n",
       " 'happening',\n",
       " 'dogs',\n",
       " 'coat',\n",
       " 'sense',\n",
       " 'cup',\n",
       " 'talked',\n",
       " 'liked',\n",
       " 'strong',\n",
       " 'upset',\n",
       " 'kill',\n",
       " 'speaks',\n",
       " 'thanks',\n",
       " 'missed',\n",
       " 'forward',\n",
       " 'strange',\n",
       " 'expected',\n",
       " 'check',\n",
       " 'boss',\n",
       " 'dream',\n",
       " 'dangerous',\n",
       " 'beach',\n",
       " 'known',\n",
       " 'health',\n",
       " 'situation',\n",
       " 'actually',\n",
       " 'running',\n",
       " 'brought',\n",
       " 'whether',\n",
       " 'six',\n",
       " 'its',\n",
       " 'whatever',\n",
       " 'dictionary',\n",
       " 'move',\n",
       " 'seeing',\n",
       " 'shut',\n",
       " 'order',\n",
       " 'swimming',\n",
       " 'weekend',\n",
       " 'tall',\n",
       " 'men',\n",
       " 'air',\n",
       " 'evening',\n",
       " 'walked',\n",
       " 'plane',\n",
       " 'allowed',\n",
       " 'thirty',\n",
       " 'loves',\n",
       " 'case',\n",
       " 'least',\n",
       " 'building',\n",
       " 'broken',\n",
       " 'list',\n",
       " 'worth',\n",
       " 'happens',\n",
       " 'heart',\n",
       " 'disappointed',\n",
       " 'follow',\n",
       " 'famous',\n",
       " 'prefer',\n",
       " 'written',\n",
       " 'smoke',\n",
       " 'christmas',\n",
       " 'perfect',\n",
       " 'drinking',\n",
       " \"o'clock\",\n",
       " 'luck',\n",
       " 'choose',\n",
       " 'completely',\n",
       " 'dollars',\n",
       " 'pain',\n",
       " 'future',\n",
       " 'either',\n",
       " 'mad',\n",
       " 'seat',\n",
       " 'crying',\n",
       " 'dance',\n",
       " 'second',\n",
       " 'piano',\n",
       " 'offer',\n",
       " 'necessary',\n",
       " 'fine',\n",
       " 'half',\n",
       " 'sent',\n",
       " 'sunday',\n",
       " \"hasn't\",\n",
       " 'garden',\n",
       " 'apologize',\n",
       " 'rules',\n",
       " 'road',\n",
       " 'library',\n",
       " 'leaving',\n",
       " 'noise',\n",
       " 'weight',\n",
       " 'flowers',\n",
       " 'wear',\n",
       " 'opinion',\n",
       " 'cook',\n",
       " 'writing',\n",
       " 'camera',\n",
       " 'set',\n",
       " 'taken',\n",
       " 'glass',\n",
       " 'learned',\n",
       " 'address',\n",
       " 'somewhere',\n",
       " 'poor',\n",
       " 'several',\n",
       " \"it'll\",\n",
       " 'white',\n",
       " 'danger',\n",
       " 'attention',\n",
       " \"he'll\",\n",
       " 'tokyo',\n",
       " 'floor',\n",
       " 'save',\n",
       " 'alive',\n",
       " 'accept',\n",
       " 'information',\n",
       " 'clear',\n",
       " 'suppose',\n",
       " 'opened',\n",
       " 'during',\n",
       " 'kiss',\n",
       " 'loved',\n",
       " 'nervous',\n",
       " 'grow',\n",
       " 'girls',\n",
       " 'waste',\n",
       " 'showed',\n",
       " 'join',\n",
       " 'women',\n",
       " 'bag',\n",
       " 'sleeping',\n",
       " 'listening',\n",
       " 'less',\n",
       " 'solve',\n",
       " 'sad',\n",
       " 'won',\n",
       " 'date',\n",
       " 'shopping',\n",
       " 'blame',\n",
       " 'desk',\n",
       " 'tie',\n",
       " 'medicine',\n",
       " 'vacation',\n",
       " 'pass',\n",
       " 'college',\n",
       " 'side',\n",
       " 'success',\n",
       " 'teach',\n",
       " 'keys',\n",
       " 'arrive',\n",
       " 'umbrella',\n",
       " 'fix',\n",
       " 'sounds',\n",
       " 'ticket',\n",
       " 'whose',\n",
       " 'ship',\n",
       " 'radio',\n",
       " 'lying',\n",
       " 'spoke',\n",
       " 'smart',\n",
       " 'means',\n",
       " 'lawyer',\n",
       " 'thinks',\n",
       " 'telephone',\n",
       " 'abroad',\n",
       " 'promised',\n",
       " 'glasses',\n",
       " 'willing',\n",
       " 'wake',\n",
       " 'shot',\n",
       " 'black',\n",
       " 'owe',\n",
       " 'husband',\n",
       " 'speech',\n",
       " 'cats',\n",
       " 'plans',\n",
       " 'horse',\n",
       " 'borrow',\n",
       " 'driving',\n",
       " 'staying',\n",
       " 'apple',\n",
       " 'message',\n",
       " 'immediately',\n",
       " 'guitar',\n",
       " 'piece',\n",
       " 'discuss',\n",
       " 'across',\n",
       " 'excuse',\n",
       " 'dressed',\n",
       " 'uncle',\n",
       " 'pictures',\n",
       " 'given',\n",
       " 'fat',\n",
       " 'guys',\n",
       " 'waited',\n",
       " 'prepared',\n",
       " 'fight',\n",
       " 'bank',\n",
       " 'asking',\n",
       " 'involved',\n",
       " 'american',\n",
       " 'lend',\n",
       " 'carefully',\n",
       " 'harder',\n",
       " 'none',\n",
       " 'boyfriend',\n",
       " 'concert',\n",
       " 'sign',\n",
       " 'boys',\n",
       " 'ride',\n",
       " 'traffic',\n",
       " 'cry',\n",
       " 'certain',\n",
       " 'gun',\n",
       " 'shower',\n",
       " 'afford',\n",
       " 'cute',\n",
       " 'joke',\n",
       " 'price',\n",
       " 'couple',\n",
       " 'inside',\n",
       " 'easily',\n",
       " 'arm',\n",
       " 'lake',\n",
       " 'novel',\n",
       " 'shirt',\n",
       " 'storm',\n",
       " 'painting',\n",
       " 'agreed',\n",
       " 'send',\n",
       " 'satisfied',\n",
       " 'worse',\n",
       " 'regret',\n",
       " 'america',\n",
       " 'fired',\n",
       " 'begin',\n",
       " 'kid',\n",
       " 'twice',\n",
       " 'raining',\n",
       " 'succeed',\n",
       " 'travel',\n",
       " 'fault',\n",
       " \"how's\",\n",
       " 'count',\n",
       " 'correct',\n",
       " 'act',\n",
       " \"they'll\",\n",
       " 'passed',\n",
       " 'chair',\n",
       " \"we'd\",\n",
       " 'difference',\n",
       " 'ice',\n",
       " 'test',\n",
       " 'empty',\n",
       " 'honest',\n",
       " 'laugh',\n",
       " 'meat',\n",
       " 'movies',\n",
       " 'television',\n",
       " 'four',\n",
       " 'pen',\n",
       " 'earlier',\n",
       " 'hardly',\n",
       " 'impossible',\n",
       " 'held',\n",
       " 'color',\n",
       " 'figure',\n",
       " 'ahead',\n",
       " 'wonderful',\n",
       " 'blue',\n",
       " 'enjoyed',\n",
       " 'winter',\n",
       " 'likely',\n",
       " 'machine',\n",
       " 'carry',\n",
       " 'warm',\n",
       " 'refused',\n",
       " 'fall',\n",
       " 'line',\n",
       " 'bill',\n",
       " 'played',\n",
       " 'decide',\n",
       " 'kissed',\n",
       " 'terrible',\n",
       " 'animals',\n",
       " 'younger',\n",
       " 'herself',\n",
       " 'mountain',\n",
       " 'bother',\n",
       " 'laughed',\n",
       " 'slept',\n",
       " 'walking',\n",
       " 'ideas',\n",
       " 'past',\n",
       " 'heavy',\n",
       " 'shop',\n",
       " 'anywhere',\n",
       " 'foreign',\n",
       " 'sooner',\n",
       " 'liar',\n",
       " 'cooking',\n",
       " 'injured',\n",
       " 'leaves',\n",
       " 'newspaper',\n",
       " 'pick',\n",
       " 'smell',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.idx_to_token  # list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.4. <a id='toc11_4_1_4_'></a>[截断和填充](#toc0_)\n",
    "truncation和padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]                                     # Truncate\n",
    "    else:\n",
    "        return line + [padding_token] * (num_steps - len(line))     # Pad\n",
    "\n",
    "truncate_pad(line=src_vocab[source[0]], num_steps=10, padding_token=src_vocab['<pad>'])      # return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "现在我们定义一个函数，可以将文本序列转换成小批量数据集用于训练。\n",
    "我们将特定的“<eos>”词元添加到所有序列的末尾，用于表示序列的结束。\n",
    "当模型通过一个词元接一个词元地生成序列进行预测时，生成的“<eos>”词元说明完成了序列输出工作。\n",
    "此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息。\n",
    "'''\n",
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
    "    \n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    # 计算每个序列中非填充 <pad> 标记的数量，得到每个序列的有效长度。\n",
    "    # array != vocab['<pad>']：创建一个布尔张量，标识哪些位置不是填充标记。\n",
    "    # d2l.astype(..., d2l.int32)：将布尔值转换为整数类型（1 和 0）。\n",
    "    # d2l.reduce_sum(..., 1)：沿着序列长度的维度求和，得到每个序列的有效长度。\n",
    "    valid_len = d2l.reduce_sum(d2l.astype(array != vocab['<pad>'], torch.int32), 1)\n",
    "    \n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.5. <a id='toc11_4_1_5_'></a>[集合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[  0,   5,   3,   1,   1,   1,   1,   1],\n",
      "        [ 31, 168,   4,   3,   1,   1,   1,   1]], dtype=torch.int32)\n",
      "valid lengths for X: tensor([3, 4])\n",
      "Y: tensor([[114,   5,   3,   1,   1,   1,   1,   1],\n",
      "        [  0,   5,   3,   1,   1,   1,   1,   1]], dtype=torch.int32)\n",
      "valid lengths for Y: tensor([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
    "\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "\n",
    "    src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = d2l.Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    \n",
    "    # 数据集顺序：源语言序列、源语言序列有效长度、目标语言序列、目标语言序列有效长度\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)      \n",
    "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    \n",
    "    return data_iter, src_vocab, tgt_vocab\n",
    "\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)\n",
    "\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X:', d2l.astype(X, torch.int32))\n",
    "    print('valid lengths for X:', X_valid_len)\n",
    "    print('Y:', d2l.astype(Y, torch.int32))\n",
    "    print('valid lengths for Y:', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab['<pad>'], tgt_vocab['<pad>']  # 用1填充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2. <a id='toc11_4_2_'></a>[编码器-解码器架构](#toc0_)\n",
    "机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一个包含两个主要组件的架构：  \n",
    "  * 第一个组件是一个编码器（encoder）：它接受一个`长度可变的序列作为输入`，并将其转换为具有`固定形状的编码状态`。\n",
    "  * 第二个组件是解码器（decoder）：它将`固定形状的编码状态`映射到`长度可变的输出序列`。\n",
    "\n",
    "我们以英语到法语的机器翻译为例：给定一个英文的输入序列：“They”“are”“watching”“.”。首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”，然后对该状态进行解码，一个词元接着一个词元地生成翻译后的序列作为输出：“Ils”“regordent”“.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "编码器\n",
    "'''\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "解码器\n",
    "'''\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        '''我们新增一个init_state函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如：输入序列的有效长度'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "编码器-解码器架构\n",
    "'''\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder \n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.3. <a id='toc11_4_3_'></a>[序列到序列学习](#toc0_)\n",
    "我们将使用两个循环神经网络的编码器和解码器，并将其应用于序列到序列（sequencetosequence，seq2seq）类的学习任务.\n",
    "\n",
    "![序列到序列学习](./Pytorch_Pictures/seq2seq/seq2seq_learning.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7]), torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch\n",
    "\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    '''编码器'''\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(input_size=embed_size, hidden_size=num_hiddens, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X 的形状：(batch_size, num_steps, embed_size)\n",
    "        X = self.embedding(X)\n",
    "        # 在循环神经网络模型中，第一个时间步的输入需要使用形状为(num_layers, batch_size, num_hiddens)的初始化隐藏状态\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # 如果未提及初始化隐藏状态，则默认为0\n",
    "        output, state = self.rnn(X)\n",
    "        # output的形状:(num_steps, batch_size, num_hiddens)\n",
    "        # state的形状:(num_layers, batch_size, num_hiddens)，num_steps的最后一个时刻的hidden state\n",
    "        return output, state\n",
    "        \n",
    "\n",
    "# Test\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2, dropout=0)\n",
    "encoder.eval()\n",
    "\n",
    "batch_size = 4\n",
    "num_steps = 7\n",
    "\n",
    "# 输入：(batch_size, num_steps)\n",
    "X = torch.ones(size=(batch_size, num_steps), dtype=torch.long)\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# state: (num_layers, batch_size, num_hiddens)\n",
    "output, state = encoder(X)\n",
    "\n",
    "X.shape, output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out state: torch.Size([2, 4, 16])\n",
      "encoder_out state[-1]: torch.Size([4, 16])\n",
      "encoder_out state[-1].repeat(X.shape[0], 1, 1): torch.Size([7, 4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7]), torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "\n",
    "\n",
    "class Seq2SeqDecoder(nn.Module):\n",
    "    '''解码器'''\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将enc_outputs的hidden state的最后一层与dec_x拼接起来\n",
    "        self.rnn = nn.GRU(input_size=embed_size + num_hiddens, hidden_size=num_hiddens, num_layers=num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(in_features=num_hiddens, out_features=vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输入的X形状：(batch_size, num_steps)\n",
    "        # 经过embedding后，X的形状：(batch_size, num_steps, embed_size)\n",
    "        # 输出'X'的形状：(num_steps, batch_size, embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "\n",
    "        # state: (num_layers, batch_size, num_hiddens)  \n",
    "        # state[-1]: hidden state最后一层的hidden state，形状：(batch_size, num_hiddens)\n",
    "        # state[-1].repeat(X.shape[0], 1, 1)，形状：(num_steps, batch_size, num_hiddens)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        # 将X和context在最后一维上连接\n",
    "        # X: (num_steps, batch_size, embed_size) + context: (num_steps, batch_size, num_hiddens) = (num_steps, batch_size, embed_size + num_hiddens)\n",
    "        X_and_context = torch.cat((X, context), dim=2)\n",
    "        # 通过时间步展开\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size, num_steps, vocab_size)\n",
    "        # state的形状:(num_layers, batch_size, num_hiddens)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2, dropout=0)\n",
    "decoder.eval()\n",
    "\n",
    "batch_size = 4\n",
    "num_steps = 7\n",
    "\n",
    "# 输入：(batch_size, num_steps)\n",
    "X = torch.zeros(size=(batch_size, num_steps), dtype=torch.long)\n",
    "\n",
    "# state: (num_layers, batch_size, num_hiddens)\n",
    "state = decoder.init_state(encoder(X))\n",
    "print(f'encoder_out state: {state.shape}')\n",
    "print(f'encoder_out state[-1]: {state[-1].shape}')\n",
    "print(f'encoder_out state[-1].repeat(X.shape[0], 1, 1): {state[-1].repeat(num_steps, 1, 1).shape}')\n",
    "\n",
    "# output: (batch_size, num_steps, num_hiddens), 经过permute转置后的结果\n",
    "# state: (num_layers, batch_size, num_hiddens)\n",
    "output, state = decoder(X, state)\n",
    "\n",
    "X.shape, output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.4. <a id='toc11_4_4_'></a>[损失函数](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.4.1. <a id='toc11_4_4_1_'></a>[掩码](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "torch.Size([3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[1, 2, 3]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# [:, None] 和 [None, :] 是用于在张量的维度上添加一个新的维度，以便进行广播操作。\n",
    "# 在二维张量中，[:, None] 表示在列维度上添加一个维度，[None, :] 表示在行维度上添加一个维度。\n",
    "x1 = x[:, None]\n",
    "x2 = x[None, :]\n",
    "\n",
    "print(x, x.shape, sep='\\n')\n",
    "print(x1, x1.shape, sep='\\n')\n",
    "print(x2, x2.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3, -1, -1],\n",
       "        [ 4,  5,  6,  7,  8],\n",
       "        [ 9, 10, -1, -1, -1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "X = [['a', 'b', 'c', 'pad', 'pad'],\n",
    "     ['d', 'e', 'f', 'g', 'h'], \n",
    "     ['i', 'j', 'pad', 'pad', 'pad']]\n",
    "\n",
    "# corpus\n",
    "X = [[1, 2, 3, 0, 0], \n",
    "     [4, 5, 6, 7, 8], \n",
    "     [9, 10, 0, 0, 0]]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "\n",
    "# mask\n",
    "mask = [[True, True, True, False, False], \n",
    "        [True, True, True, True, True], \n",
    "        [True, True, False, False, False]]\n",
    "\n",
    "mask = torch.tensor(mask)\n",
    "\n",
    "# ~表示取反操作\n",
    "X[~mask] = -1\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 0],\n",
       "         [4, 5, 0]]),\n",
       " tensor([[1, 0, 0],\n",
       "         [4, 5, 0]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    '''\n",
    "    为序列生成掩码，将无效/填充位置的值替换为指定值\n",
    "    '''\n",
    "\n",
    "    maxlen = X.size(1)\n",
    "\n",
    "    # 作用：生成一个布尔掩码，用于标识每个序列中有效的时间步。\n",
    "    # [None, :] 和 [:, None]：通过添加新的维度，将一维张量扩展为二维，以便进行广播操作\n",
    "    # < valid_len[:, None]：比较操作，将生成一个布尔张量，当位置索引小于 valid_len 时为 True，否则为 False。valid_len 是一个包含每个序列实际长度的张量。\n",
    "    mask = torch.arange(maxlen, dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    \n",
    "    # 将无效位置的值替换为指定值\n",
    "    # ~mask：取反，将无效位置的值替换为指定值\n",
    "    X[~mask] = value \n",
    "\n",
    "    return X \n",
    "\n",
    "\n",
    "# tokens with padding\n",
    "X = torch.tensor([[1, -1, -1], \n",
    "                  [4, 5, -1]])\n",
    "\n",
    "# tokens 有效长度\n",
    "valid_len = torch.tensor([1, 2])\n",
    "\n",
    "# 输出：(batch_size, num_steps)\n",
    "X, sequence_mask(X=X, valid_len=valid_len, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]]),\n",
       " tensor([[[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens with padding\n",
    "X = torch.ones(size=(2, 3, 4))\n",
    "\n",
    "X, sequence_mask(X=X, valid_len=torch.tensor([1, 2]), value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.4.2. <a id='toc11_4_4_2_'></a>[带掩码的softmax交叉熵损失](#toc0_)\n",
    "我们可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        # 将损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测 (0乘以任何数为0，达到过滤的作用)   \n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss\n",
    "    \n",
    "\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "\n",
    "loss(\n",
    "    pred = torch.ones(size=(3, 4, 10)), \n",
    "    label = torch.ones(size=(3, 4), dtype=torch.long), \n",
    "    valid_len = torch.tensor([4, 2, 0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.5. <a id='toc11_4_5_'></a>[训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.018, 17795.2 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD/CAYAAADVGuzgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAALJ9JREFUeJzt3XlUVOf9P/D37Asw7DKAKChxwSgaF4JLbSuKms02TYzN9xtrbfJNG04WrGlMTzUmnmATa61Hm/y+bZOmv99pNDHRtDWhIgk2GtSI4r5hjLiwbwMMzPr8/hgYMwEBcYY7M75f58xxuPeZh+eD+p7Lc5+5VyaEECAiIr8nl3oARETUNwxsIqIAwcAmIgoQDGwiogDBwCYiChAMbCKiAMHAJiIKEEqpB+CPnE4nrl27hrCwMMhkMqmHQ0QBQgiB5uZmJCQkQC73/vEwA7sb165dQ1JSktTDIKIAdfnyZQwePNjr/TKwuxEWFgbA9UM3GAwSj6b/bDYbdu3ahTlz5kClUkk9HK8K5tqA4K4vmGurr69HSkqKO0O8jYHdjc5pEIPBEPCBrdfrYTAYgu4/RjDXBgR3fcFeGwCfTaX6xUnHzZs3Izk5GVqtFhkZGTh48OAN2/7pT3/CjBkzEBkZicjISGRlZXVpL4TAypUrER8fD51Oh6ysLJw/f97XZRAR+ZTkgb1161bk5uZi1apVOHz4MNLT05GdnY3q6upu2xcVFWHRokX47LPPUFxcjKSkJMyZMwdXr151t3nttdewceNGvPnmmzhw4ABCQkKQnZ2N9vb2gSqLiMj7hMSmTJkinnrqKffXDodDJCQkiLy8vD693m63i7CwMPHOO+8IIYRwOp3CaDSK119/3d2msbFRaDQa8e677/apz6amJgFANDU13UQl/sdqtYodO3YIq9Uq9VC8LphrEyK46wvm2mpra32aHZLOYVutVpSUlGDFihXubXK5HFlZWSguLu5TH2azGTabDVFRUQCAixcvorKyEllZWe424eHhyMjIQHFxMR555JEufVgsFlgsFvfXJpMJgGs+qnNOKhB1jj2Qa7iRYK4NCO76bofafEXSwK6trYXD4UBcXJzH9ri4OJw5c6ZPffzqV79CQkKCO6ArKyvdfXy7z85935aXl4fVq1d32b5r1y7o9fo+jcOfFRQUSD0Enwnm2oDgri8YazObzT7tP6BXiaxduxZbtmxBUVERtFptv/tZsWIFcnNz3V+bTCYkJSXhO9+bhUHRkd4YqiRsNhsKCgowe/bsoDwbH6y1AcFdXzDXVldX59P+JQ3smJgYKBQKVFVVeWyvqqqC0Wjs8bXr1q3D2rVrsXv3bowbN869vfN1VVVViI+P9+hz/Pjx3fal0Wig0Wi6bG+2CiQGwT8olUoVdP8xOgVzbUBw1xeMtfm6HklXiajVakycOBGFhYXubU6nE4WFhcjMzLzh61577TW88soryM/Px6RJkzz2paSkwGg0evRpMplw4MCBHvvsTn2rpfdGREQDRPIpkdzcXCxevBiTJk3ClClTsGHDBrS2tmLJkiUAgMceewyJiYnIy8sDAPz2t7/FypUr8fe//x3JycnueenQ0FCEhoZCJpPh2WefxZo1a3DHHXcgJSUFv/nNb5CQkIAFCxbc1NjqzcF3UoSIApfkgb1w4ULU1NRg5cqVqKysxPjx45Gfn+8+aVheXu5xEZU33ngDVqsVP/rRjzz6WbVqFV566SUAwPPPP4/W1lY88cQTaGxsxPTp05Gfn3/T89z1LdZbK46IyIskD2wAyMnJQU5OTrf7ioqKPL7++uuve+1PJpPh5Zdfxssvv3xL42owc0qEiPyH5J909GecEiEif8LA7gGnRIjInzCwe9BgZmATkf9gYPeAR9hE5E8Y2D2o5xE2EfkRBnYP6lsZ2ETkPxjYPWi3OWG22qUeBhERAAZ2r+o4j01EfoKB3QtOixCRv2Bg94KBTUT+goHdi9oWfjydiPwDA7sXPMImIn/BwO5FHQObiPwEA7sXXCVCRP6Cgd2LOt51hoj8BAO7F5zDJiJ/wcDuBadEiMhfMLB7wSkRIvIXDOxe8HoiROQvGNg9UCtdPx5OixCRP2Bg9yBKrwLAtdhE5B8Y2D2ICtEAAOr48XQi8gMM7B5EhvAIm4j8BwO7B1EhagCcwyYi/8DA7kFnYNdzaR8R+QEGdg8i9R1H2JwSISI/wMDuAadEiMifMLB7cH1KhIFNRNJjYPfg+hE257CJSHoM7B5EfWMOWwgh8WiI6HbHwO5BVKgrsC12J1qtDolHQ0S3OwZ2D/RqJbQq14+oniceiUhiDOxeRHd8PL2Wa7GJSGIM7F5Ed0yL8AibiKTGwO5FNJf2EZGfYGD3IopTIkTkJxjYveCUCBH5CwZ2LzqnRHg9ESKSmuSBvXnzZiQnJ0Or1SIjIwMHDx68YduTJ0/iwQcfRHJyMmQyGTZs2NClzUsvvQSZTObxGDVqVL/HF8XAJiI/IWlgb926Fbm5uVi1ahUOHz6M9PR0ZGdno7q6utv2ZrMZw4YNw9q1a2E0Gm/Y75gxY1BRUeF+7N27t99jjAnlXWeIyD9IGtjr16/H448/jiVLliAtLQ1vvvkm9Ho93nrrrW7bT548Ga+//joeeeQRaDSaG/arVCphNBrdj5iYmH6PkReAIiJ/oZTqG1utVpSUlGDFihXubXK5HFlZWSguLr6lvs+fP4+EhARotVpkZmYiLy8PQ4YMuWF7i8UCi+X6EbTJZAIA2Gw2hGtdtwmrbbHAarVCJpPd0tgGks1m8/gzmARzbUBw13c71OYrkgV2bW0tHA4H4uLiPLbHxcXhzJkz/e43IyMDf/3rXzFy5EhUVFRg9erVmDFjBk6cOIGwsLBuX5OXl4fVq1d32b5r1y4oNXoAStgcAtv/+Qm0kv3E+q+goEDqIfhMMNcGBHd9wVib2Wz2af8BGD89mzdvnvv5uHHjkJGRgaFDh+K9997D0qVLu33NihUrkJub6/7aZDIhKSkJc+bMgcFgwKrSQpitDkyc/l0MjdL7vAZvsdlsKCgowOzZs6FSqaQejlcFc21AcNcXzLXV1dX5tH/JAjsmJgYKhQJVVVUe26uqqno8oXizIiIiMGLECJSVld2wjUaj6XZOXKVSQaVSISpEDbO1DU3tzoD8B9ZZRzAK5tqA4K4vGGvzdT2SnXRUq9WYOHEiCgsL3ducTicKCwuRmZnpte/T0tKCCxcuID4+vt998OPpROQPJJ0Syc3NxeLFizFp0iRMmTIFGzZsQGtrK5YsWQIAeOyxx5CYmIi8vDwArhOVp06dcj+/evUqSktLERoaitTUVADAL3/5S9x3330YOnQorl27hlWrVkGhUGDRokX9Hmc0l/YRkR+QNLAXLlyImpoarFy5EpWVlRg/fjzy8/PdJyLLy8shl1//JeDatWuYMGGC++t169Zh3bp1mDlzJoqKigAAV65cwaJFi1BXV4fY2FhMnz4d+/fvR2xsbL/HyQ/PEJE/kPykY05ODnJycrrd1xnCnZKTk3u9VdeWLVu8NTS3zuuJ8O7pRCQlyT+aHgiuz2FzSoSIpMPA7oPOu85wSoSIpMTA7oMoTokQkR9gYPdBTMcRNpf1EZGUGNh94D7CbrX0etKTiMhXGNh90HnS0eYQaLbYJR4NEd2uGNh9oFUpEKJWAOA8NhFJh4HdR53TIlzaR0RSYWD3UefSvloeYRORRBjYfcQLQBGR1BjYfRQdysAmImkxsPsoyj0lwjlsIpIGA7uP4sO1AIArDW0Sj4SIblcM7D5KiQkBAHxV0yLxSIjodsXA7qNhsa7ALq83w+5wSjwaIrodMbD7KCFcB61KDptD4DKnRYhIAgzsPpLLZUiO5rQIEUmHgX0ThseGAgC+qmmVeCREdDtiYN+Eznnsr2p5hE1EA4+BfRM6A/sCj7CJSAIM7JswLIZTIkQkHQb2Teg8wq5tscDUbpN4NER0u2Fg34QwrQqxYa6PqPMom4gGGgP7Jg3jJx6JSCIM7Js0jEv7iEgi/Qrsd955Bzt37nR//fzzzyMiIgJTp07FpUuXvDY4fzS8Yx77Yi0Dm4gGVr8C+9VXX4VOpwMAFBcXY/PmzXjttdcQExOD5557zqsD9DfXl/ZxSoSIBpayPy+6fPkyUlNTAQA7duzAgw8+iCeeeALTpk3Dd7/7XW+Oz+90Lu37uq4VTqeAXC6TeEREdLvo1xF2aGgo6urqAAC7du3C7NmzAQBarRZtbcF9YaTBkTqoFDK025y41hTctRKRf+nXEfbs2bPxs5/9DBMmTMC5c+cwf/58AMDJkyeRnJzszfH5HaVCjqHRISirbsFXNa0YHKmXekhEdJvo1xH25s2bkZmZiZqaGnzwwQeIjo4GAJSUlGDRokVeHaA/4tI+IpJCv46wIyIisGnTpi7bV69efcsDCgSupX1V+IorRYhoAPXrCDs/Px979+51f71582aMHz8eP/7xj9HQ0OC1wfkr91X7uBabiAZQvwJ7+fLlMJlMAIDjx49j2bJlmD9/Pi5evIjc3FyvDtAfDY/llAgRDbx+TYlcvHgRaWlpAIAPPvgA9957L1599VUcPnzYfQIymHUu7bvW1A6z1Q69ul8/RiKim9KvI2y1Wg2z2QwA2L17N+bMmQMAiIqKch95B7PIEDUi9SoA/MQjEQ2cfgX29OnTkZubi1deeQUHDx7EPffcAwA4d+4cBg8e7NUB+iteU4SIBlq/AnvTpk1QKpXYtm0b3njjDSQmJgIAPvnkE8ydO9erA/RX15f2MbCJaGD0K7CHDBmCf/3rXzh69CiWLl3q3v773/8eGzduvKm+Nm/ejOTkZGi1WmRkZODgwYM3bHvy5Ek8+OCDSE5Ohkwmw4YNG265z/5yH2Hz/o5ENED6fXlVh8OBDz74AGvWrMGaNWuwfft2OByOm+pj69atyM3NxapVq3D48GGkp6cjOzsb1dXV3bY3m80YNmwY1q5dC6PR6JU++4tL+4hooPVreUNZWRnmz5+Pq1evYuTIkQCAvLw8JCUlYefOnRg+fHif+lm/fj0ef/xxLFmyBADw5ptvYufOnXjrrbfwwgsvdGk/efJkTJ48GQC63d+fPgHAYrHAYrG4v+48cWqz2WCzdX8rsCERnXeeaYHVaoVM5n8Xgeoc+41qCGTBXBsQ3PXdDrX5Sr8C++mnn8bw4cOxf/9+REVFAQDq6urwX//1X3j66ac9rpV9I1arFSUlJVixYoV7m1wuR1ZWFoqLi/szrH73mZeX1+2nNHft2gW9vvtrhdidgAwKtFod2PLRJwhX92vIA6KgoEDqIfhMMNcGBHd9wVhb5+o5X+lXYO/Zs8cjrAEgOjoaa9euxbRp0/rUR21tLRwOB+Li4jy2x8XF4cyZM/0ZVr/7XLFihccHfkwmE5KSkjBnzhwYDIYbvu4P5z9HeX0bUsbdjbuHRd2wnVRsNhsKCgowe/ZsqFQqqYfjVcFcGxDc9QVzbZ1XMfWVfgW2RqNBc3Nzl+0tLS1Qq/34UPMGNBoNNBpNl+0qlarHf1DDY0NRXt+GSw3tmOHH//B6qyOQBXNtQHDXF4y1+bqefp10vPfee/HEE0/gwIEDEEJACIH9+/fjySefxP3339+nPmJiYqBQKFBVVeWxvaqq6oYnFKXosydci01EA6lfgb1x40YMHz4cmZmZ0Gq10Gq1mDp1KlJTU2+41O7b1Go1Jk6ciMLCQvc2p9OJwsJCZGZm9mdYPumzJ+6VIlzaR0QDoN+XV/3oo49QVlaG06dPAwBGjx7tvm1YX+Xm5mLx4sWYNGkSpkyZgg0bNqC1tdW9wuOxxx5DYmIi8vLyALhOKp46dcr9/OrVqygtLUVoaKj7e/fWpzd1XlOER9hENBD6HNi9XYXvs88+cz9fv359n/pcuHAhampqsHLlSlRWVmL8+PHIz893nzQsLy+HXH79l4Br165hwoQJ7q/XrVuHdevWYebMmSgqKupTn97UedW+Kw1mWOwOaJQKr38PIqJOfQ7sI0eO9Kndza5HzsnJQU5OTrf7OkO4U3JyMoQQt9SnN8WGaWDQKmFqt+N0RTPGJ0X4/HsS0e2rz4H9zSNocpHJZMgYFo2CU1X4/FwNA5uIfKrfH00nl++MiAUAfH6+VuKREFGwY2Dfopl3uAL7cHkDmtuD76O2ROQ/GNi3aEi0HsnRetidAl9c8O2nnIjo9sbA9oLOaZH/nKuReCREFMwY2F7wnY5pkf+cr+nTKhYiov5gYHtB5vBoqBQyXK5vw9d1vr1aFxHdvhjYXhCiUWLSUNfV+jgtQkS+wsD2Es5jE5GvMbC95DsjYgAAxV/VwWp3SjwaIgpGDGwvGW00ICZUA7PVgUOX6qUeDhEFIQa2l8jlMnznDtdR9n/O8VOPROR9DGwv4jw2EfkSA9uLpnccYZ+qMKGm2dJLayKim8PA9qKYUA3uTHTdtPfz8zzKJiLvYmB7mftTj5wWISIvY2B72Tcvt+p08mPqROQ9DGwvu2tIJELUCtS1WnGqwiT1cIgoiDCwvUytlCNzuOvk4x5OixCRFzGwfWDmSNe0SP6JSolHQkTBhIHtA/eMjYdKIcPxq004dY3TIkTkHQxsH4gKUWN2WhwA4L1DlyUeDREFCwa2jzw8KQkAsP3IVbTbHBKPhoiCAQPbR2bcEYuEcC2a2mzYdapK6uEQURBgYPuIQi7DjyYOBgC8z2kRIvICBrYPPdQxLbK3rBZXGnjrMCK6NQxsH0qK0mNaajSEAN4/dEXq4RBRgGNg+1jnycdtJVfg4EfViegWMLB9LHuMEQatElcb27CvjDc2IKL+Y2D7mFalwIIJiQCArTz5SES3gIE9ADqnRQpOVqGh1SrxaIgoUDGwB8CdieEYk2CA1eHE9iNXpR4OEQUoBvYAWTjZdZT93qHLEIInH4no5jGwB8gD6YlQK+U4U9mMwtPVUg+HiAIQA3uAhOtVWJw5FADwwofHUdfCm/QS0c1hYA+gZXNG4o5BoahtseDF7cc5NUJEN4WBPYC0KgV+v3A8VAoZ/n2yCh8c5glIIuo7vwjszZs3Izk5GVqtFhkZGTh48GCP7d9//32MGjUKWq0WY8eOxccff+yx/yc/+QlkMpnHY+7cub4soc/uTAzHs1kjAAAv/eMkLtfzGiNE1DeSB/bWrVuRm5uLVatW4fDhw0hPT0d2djaqq7s/MffFF19g0aJFWLp0KY4cOYIFCxZgwYIFOHHihEe7uXPnoqKiwv149913B6KcPnly5nBMHBqJFosdy94/yo+sE1GfSB7Y69evx+OPP44lS5YgLS0Nb775JvR6Pd56661u2//hD3/A3LlzsXz5cowePRqvvPIK7rrrLmzatMmjnUajgdFodD8iIyMHopw+Uchl+P3D4xGiVuDgxXr8Ze9XUg+JiAKAUspvbrVaUVJSghUrVri3yeVyZGVlobi4uNvXFBcXIzc312NbdnY2duzY4bGtqKgIgwYNQmRkJL7//e9jzZo1iI6O7rZPi8UCi+X6qg2TyXUfRpvNBpvN1p/SehVvUOHFeSPx649O4fV/n0VmSiRGGcO8+j06x+6rGqQUzLUBwV3f7VCbr0ga2LW1tXA4HIiLi/PYHhcXhzNnznT7msrKym7bV1Zev0P53Llz8cMf/hApKSm4cOECXnzxRcybNw/FxcVQKBRd+szLy8Pq1au7bN+1axf0en1/SuuTEAHcGSnHiQY5nnjrC+SOdUDddXi3rKCgwPud+olgrg0I7vqCsTaz2bfnpCQNbF955JFH3M/Hjh2LcePGYfjw4SgqKsKsWbO6tF+xYoXHUbvJZEJSUhLmzJkDg8Hg07HePdOCezcXo6LFiqOyFKyaP9prfdtsNhQUFGD27NlQqVRe69cfBHNtQHDXF8y11dXV+bR/SQM7JiYGCoUCVVWe9zysqqqC0Wjs9jVGo/Gm2gPAsGHDEBMTg7Kysm4DW6PRQKPRdNmuUql8/g/KGKnC7x4ej8VvHcT/O3AZM0fGue+47i0DUYdUgrk2ILjrC8bafF2PpCcd1Wo1Jk6ciMLCQvc2p9OJwsJCZGZmdvuazMxMj/aA61erG7UHgCtXrqCurg7x8fHeGbiXzRwRi59NTwEAPL/tKCqb2iUeERH5I8lXieTm5uJPf/oT3nnnHZw+fRo///nP0draiiVLlgAAHnvsMY+Tks888wzy8/Pxu9/9DmfOnMFLL72EQ4cOIScnBwDQ0tKC5cuXY//+/fj6669RWFiIBx54AKmpqcjOzpakxr5YPnckxiQY0GC2Ife9Ui71I6IuJA/shQsXYt26dVi5ciXGjx+P0tJS5Ofnu08slpeXo6Kiwt1+6tSp+Pvf/47//d//RXp6OrZt24YdO3bgzjvvBAAoFAocO3YM999/P0aMGIGlS5di4sSJ+Pzzz7ud9vAXGqUCGxdNgE6lwBcX6vB//nNB6iERkZ/xi5OOOTk57iPkbysqKuqy7aGHHsJDDz3UbXudTod///vf3hzegBkeG4rV94/B8x8cw/pd55A5LBoThvjP+nEikpbkR9jk6aFJg3HPuHjYnQJPbznCq/oRkRsD28/IZDK8+oOxSIzQ4XJ9G37wxy9QVt0i9bCIyA8wsP1QuE6Fvy2dgiFRepTXm/HgG19g/1e+Xd9JRP6Pge2nhseGYvsvpmLCkAg0tdnw3385gO1Hrkg9LCKSEAPbj0WHavDu43dj/lgjbA6B57YexYbd53jjA6LbFAPbz2lVCmxadBf+Z+YwAMCG3efxzJZStFrsEo+MiAYaAzsAyOUyrJg3Gq/+YCwUchn+cfQa7tu0F2cqTVIPjYgGEAM7gPw4Ywi2PHE3jAYtvqppxQOb9mHLwXJOkRDdJhjYAWZychQ+fmYGZo6IhcXuxAsfHkfue0c5RUJ0G2BgB6CoEDXe/slkPD93JBRyGbYfuYr7Nu3F5+drpB4aEfkQAztAyeUy/OK7qXj38etTJP/9l4N49M/7cexKo9TDIyIfYGAHuCkpUfjkmRn46bQUqBVy7Curw/2b9uGpvx/G13WtUg+PiLyIgR0EIkPUWHlfGgqXzcQPJyRCJgN2HqvAvI1f4P+el2NvWR0v10oUBBjYQSQpSo/1C8fj46dn4HsjY2F3ChyqlWPJOyXIzCvEmn+dwomrTVxVQhSg/OLyquRdo+MNeHvJFHz5VQ02fLQfJ00aVDdb8Oe9F/HnvRcxLDYE4wdHYIQxDCPjwjDSGIb4cC1kMpnUQyeiHjCwg9j4pAg8PMyJrDkz8cXFRuw4chUFp6vwVU0rvqrxnN8O0ygxfkgEZqfFYdboOCRG6CQaNRHdCAP7NqBWyjE7zXVzX1O7DcUX6nCushlnq5pxrqoZX9W0otlix+fna/H5+Vqs/Ogk0uINyEqLQ9boQUiLN0Cp4OwZkdQY2LcZg1aF7DFGZI+5fpd5q92JsuoWfH6+BrtPV6HkUgNOVZhwqsKEjYXnoVXJMSYhHGMTwzFusOuREhMKhZxTKEQDiYFNUCvlSEswIC3BgP+ZORx1LRZ8drYGu09VYV9ZLZotdpRcakDJpQb3a1QKGYzhWiSE65AYoUNChA7GcC1CNUpoVXJoVApolQpoVXKE61RIiNBBq1JIWCVR4GNgUxfRoRr8aOJg/GjiYDidAhfrWnH8ShOOXWnC8auNOHHVhDabA5fr23C5vq3P/UbqVYgPd4V7fLgWBp0SerUSOpUCIRoFdGolInQqDI7UITFSB42SAU/0TQxs6pFcLsPw2FAMjw3FggmJAACHU6DK1I5rjW242tiGa42u5xVN7Wiz2dFuc6Ld5uh4ONFgtsJsdaDBbEOD2YZTFX27ymCcQYOkSD0GR+oQFaJBuE4Fg06JcJ0KepUM55tkKLnUAK1GDaVcBpVCDpVChgi9GpF6FVe9UNBhYNNNU8hlSOiYBpnUh/ZCCJja7LjW1IaKJlfAVza1o8ViR5vVgVbr9T/rW624XN+GNpsDVSYLqkwWHPrGVMy3RoJNp77sdo9KIUNsqAaxBi0GhbnC3uZwwmp3wmJ3/Wm1O+EUAnKZDJABchkggwwqpRxxYRoYw7WIM2hhNGhhDNfCYnd2vDG5arja2Ia6Fgs0SgV0agV0KgW0KgV0ajkGhWkxyhiG0fEGDI7UdfvmIYRAs8UOm92JEI0SGqW8SzshBJrbbWhotaHBbEWD2YqmNhua2mxoNLseze02GHQqDArTYJBBg7gwLQYZNIgO0SBEo4RaeXMnjC12B0xtdjS32xCqVSImRAM5z1f4BQY2+ZxMJkO4XoVwvQqj4w29thdCoL7ViisNbbjcYMaVhjY0mm0wtbuCytRmQ5PZiqr6Jmh0ejicgM3hhN0pYLM7XSHoELjW1I5rTe0DUGHPwjRKjDSGYVhsCJrb7ahutqC6uR01zRa025zudkq5DHq1wn0eoLZJgV8e3A2b49Y+6KRSyKBXKxGiVkCvUULZTfjanQKmNtfP+Jtj6hxXnEGL+HDXG1dMqAZKuQxyuQxymQxyGSCXyaBRyqFTKxCiUULf8QamUsrRarGjpd2O5nbXm0Cj2YoLF+U4+M/TUCjkkAHuNyqrx5uqA1a7Ezq1AgkdU2kJEa5zJoMMGpitDjSarWhss6HJ7OrXIYAwrRIGrQoGrRIGnQqhGiWUCtdYFXIZZB3jdToFWq0OtFrsaLN1/Gl1QCGXQaNSQK2QQ6OSQ9PxhtdqcaDFYkNzu939vLNutVIOjVIBS6tvr1HPwCa/I5PJEB2qQXSoBulJEd22sdls+PjjjzF//gyoVCqPfRa7A7UtVlSb2jvC0QJTmw0apbzjoYC64z+ZXAYIATgFICDgFIDF5kB1swWVTe2oaGpHlakdlaZ2qBVyJEboEB+hdYdHbKgaNodAW8cUUJvVAbPVgcsNZpyuaEZZdTOaLXYcutTQw28KLnangKndDlN756VyZQBcYa1VyRGlVyO8Y7onQq9CuE6NcJ0KYVolTG02VH2j3ipTO5o7+rE5hPuo/GaEapQwW+2wOwWudkx/eY8c+6oue7E//+C0mH3aPwObgo5GqUBix5GY1GwOJy7UtOBMRTO+rmtFhE6FQR3TNLFhGgwK00KtlKPVaofZ4kCLxY5Wix3NbRYcPbQf9835PmINeujUN38C1uZwwmx1wGx1HRF2/unsuDTBN69QIJe7lny6zhO4jkoVchlsDidqmi2oaGrveANrQ32rFU4BOIWA0yng6PjTYu/8fg602ewwW11HyCEaJQxaJcK0rn5D1HJ8/dUFpKamQi6XQ3xjLKqOo1q1Qu5+U2212HGt8fo01LXGNtS0WBCidp3PiOh4A4vQqSGXy9Dc7joKNrXZ3Ef1dqeAEHCNVbiey2VAiFoJvUYBvfr6bwVO4arF9XDAYnPCKQCDVolQrRKhmusPAbjbWOxONJua4Mu3IQY2kQ+pFHKMMhowytjzVJDrV/jrvynYbDbUnQbiw7VQ9XM5pEohR7jOtayyv1QKufu3CW9x/XZ0HvNnpXb57aivhBC3dFK583o63j4xXVdXh5hlXu3SAwObiALOrQZtoK4g4ueNiYgCBAObiChAMLCJiAIEA5uIKEAwsImIAgRXiXSjc8mPyeTbTy35ms1mg9lshslk6vfyKX8VzLUBwV1fMNfW3NwMAD67DR8DuxudP/SkpCSJR0JEgaiurg7h4eFe71cmeEfWLpxOJ65du4awsLCAXa8JuH5DSEpKwuXLl2Ew9H4Nj0ASzLUBwV1fMNfW1NSEIUOGoKGhAREREV7vn0fY3ZDL5Rg8eLDUw/Aag8EQdP8xOgVzbUBw1xfMtcnlvjk9yJOOREQBgoFNRBQgGNhBTKPRYNWqVdBoNFIPxeuCuTYguOtjbf3Hk45ERAGCR9hERAGCgU1EFCAY2EREAYKBTUQUIBjYAeY///kP7rvvPiQkJEAmk2HHjh0e+4UQWLlyJeLj46HT6ZCVlYXz5897tKmvr8ejjz4Kg8GAiIgILF26FC0tLQNYRffy8vIwefJkhIWFYdCgQViwYAHOnj3r0aa9vR1PPfUUoqOjERoaigcffBBVVVUebcrLy3HPPfdAr9dj0KBBWL58Oex2O6T2xhtvYNy4ce4PjGRmZuKTTz5x7w/k2r5p7dq1kMlkePbZZ93bArm2l156CTKZzOMxatQo9/4BrU1QQPn444/Fr3/9a/Hhhx8KAGL79u0e+9euXSvCw8PFjh07xNGjR8X9998vUlJSRFtbm7vN3LlzRXp6uti/f7/4/PPPRWpqqli0aNEAV9JVdna2ePvtt8WJEydEaWmpmD9/vhgyZIhoaWlxt3nyySdFUlKSKCwsFIcOHRJ33323mDp1qnu/3W4Xd955p8jKyhJHjhwRH3/8sYiJiRErVqyQoiQP//jHP8TOnTvFuXPnxNmzZ8WLL74oVCqVOHHihBAisGvrdPDgQZGcnCzGjRsnnnnmGff2QK5t1apVYsyYMaKiosL9qKmpce8fyNoY2AHs24HtdDqF0WgUr7/+untbY2Oj0Gg04t133xVCCHHq1CkBQHz55ZfuNp988omQyWTi6tWrAzb2vqiurhYAxJ49e4QQrlpUKpV4//333W1Onz4tAIji4mIhhOsNTS6Xi8rKSnebN954QxgMBmGxWAa2gD6IjIwUf/7zn4OitubmZnHHHXeIgoICMXPmTHdgB3ptq1atEunp6d3uG+jaOCUSRC5evIjKykpkZWW5t4WHhyMjIwPFxcUAgOLiYkRERGDSpEnuNllZWZDL5Thw4MCAj7knTU1NAICoqCgAQElJCWw2m0d9o0aNwpAhQzzqGzt2LOLi4txtsrOzYTKZcPLkyQEcfc8cDge2bNmC1tZWZGZmBkVtTz31FO655x6PGoDg+Hs7f/48EhISMGzYMDz66KMoLy8HMPC18eJPQaSyshIAPP5hdH7dua+yshKDBg3y2K9UKhEVFeVu4w+cTieeffZZTJs2DXfeeScA19jVanWXq6B9u77u6u/cJ7Xjx48jMzMT7e3tCA0Nxfbt25GWlobS0tKArm3Lli04fPgwvvzyyy77Av3vLSMjA3/9618xcuRIVFRUYPXq1ZgxYwZOnDgx4LUxsMkvPfXUUzhx4gT27t0r9VC8auTIkSgtLUVTUxO2bduGxYsXY8+ePVIP65ZcvnwZzzzzDAoKCqDVaqUejtfNmzfP/XzcuHHIyMjA0KFD8d5770Gn0w3oWDglEkSMRiMAdDlDXVVV5d5nNBpRXV3tsd9ut6O+vt7dRmo5OTn417/+hc8++8zjMrdGoxFWqxWNjY0e7b9dX3f1d+6TmlqtRmpqKiZOnIi8vDykp6fjD3/4Q0DXVlJSgurqatx1111QKpVQKpXYs2cPNm7cCKVSibi4uICtrTsREREYMWIEysrKBvzvjYEdRFJSUmA0GlFYWOjeZjKZcODAAWRmZgIAMjMz0djYiJKSEnebTz/9FE6nExkZGQM+5m8SQiAnJwfbt2/Hp59+ipSUFI/9EydOhEql8qjv7NmzKC8v96jv+PHjHm9KBQUFMBgMSEtLG5hCboLT6YTFYgno2mbNmoXjx4+jtLTU/Zg0aRIeffRR9/NAra07LS0tuHDhAuLj4wf+7+2mT5mSpJqbm8WRI0fEkSNHBACxfv16ceTIEXHp0iUhhGtZX0REhPjoo4/EsWPHxAMPPNDtsr4JEyaIAwcOiL1794o77rjDL5b1/fznPxfh4eGiqKjIYwmV2Wx2t3nyySfFkCFDxKeffioOHTokMjMzRWZmpnt/5xKqOXPmiNLSUpGfny9iY2P9YnnYCy+8IPbs2SMuXrwojh07Jl544QUhk8nErl27hBCBXdu3fXOViBCBXduyZctEUVGRuHjxoti3b5/IysoSMTExorq6WggxsLUxsAPMZ599JgB0eSxevFgI4Vra95vf/EbExcUJjUYjZs2aJc6ePevRR11dnVi0aJEIDQ0VBoNBLFmyRDQ3N0tQjafu6gIg3n77bXebtrY28Ytf/EJERkYKvV4vfvCDH4iKigqPfr7++msxb948odPpRExMjFi2bJmw2WwDXE1XP/3pT8XQoUOFWq0WsbGxYtasWe6wFiKwa/u2bwd2INe2cOFCER8fL9RqtUhMTBQLFy4UZWVl7v0DWRsvr0pEFCA4h01EFCAY2EREAYKBTUQUIBjYREQBgoFNRBQgGNhERAGCgU1EFCAY2EREAYKBTTQAioqKIJPJulwkiOhmMLCJiAIEA5uIKEAwsOm24HQ6kZeXh5SUFOh0OqSnp2Pbtm0Ark9X7Ny5E+PGjYNWq8Xdd9+NEydOePTxwQcfYMyYMdBoNEhOTsbvfvc7j/0WiwW/+tWvkJSUBI1Gg9TUVPzlL3/xaFNSUoJJkyZBr9dj6tSpXe4KT9SjW7yQFVFAWLNmjRg1apTIz88XFy5cEG+//bbQaDSiqKjIfQXE0aNHi127doljx46Je++9VyQnJwur1SqEEOLQoUNCLpeLl19+WZw9e1a8/fbbQqfTeVxJ8OGHHxZJSUniww8/FBcuXBC7d+8WW7ZsEUJcv8piRkaGKCoqEidPnhQzZszwuLs2UW8Y2BT02tvbhV6vF1988YXH9qVLl4pFixa5w7QzXIVwXYJWp9OJrVu3CiGE+PGPfyxmz57t8frly5eLtLQ0IYQQZ8+eFQBEQUFBt2Po/B67d+92b9u5c6cA4HGtcqKecEqEgl5ZWRnMZjNmz56N0NBQ9+Nvf/sbLly44G7XeYcQwHWn9pEjR+L06dMAgNOnT2PatGke/U6bNg3nz5+Hw+FAaWkpFAoFZs6c2eNYxo0b534eHx8PAF1u2UZ0I7wJLwW9lpYWAMDOnTuRmJjosU+j0XiEdn/19WasKpXK/VwmkwFwza8T9QWPsCnopaWlQaPRoLy8HKmpqR6PpKQkd7v9+/e7nzc0NODcuXMYPXo0AGD06NHYt2+fR7/79u3DiBEjoFAoMHbsWDidzoC/Azr5Nx5hU9ALCwvDL3/5Szz33HNwOp2YPn06mpqasG/fPhgMBgwdOhQA8PLLLyM6OhpxcXH49a9/jZiYGCxYsAAAsGzZMkyePBmvvPIKFi5ciOLiYmzatAl//OMfAQDJyclYvHgxfvrTn2Ljxo1IT0/HpUuXUF1djYcffliq0inYSD2JTjQQnE6n2LBhgxg5cqRQqVQiNjZWZGdniz179rhPCP7zn/8UY8aMEWq1WkyZMkUcPXrUo49t27aJtLQ0oVKpxJAhQ8Trr7/usb+trU0899xz7vv/paamirfeeksIcf2kY0NDg7t9542UL1686OvyKUjwno502ysqKsL3vvc9NDQ0ICIiQurhEN0Q57CJiAIEA5uIKEBwSoSIKEDwCJuIKEAwsImIAgQDm4goQDCwiYgCBAObiChAMLCJiAIEA5uIKEAwsImIAsT/B4/VDzeZU9f3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)                       # 对于每个线性层 nn.Linear，应用 xavier_uniform_ 初始化。\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])   # 模型中的线性层和GRU层应用Xavier均匀初始化，以确保权重在训练开始时处于合适的范围，促进梯度流动。\n",
    "    \n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()                                    # 使用带掩码的Softmax交叉熵损失 MaskedSoftmaxCELoss，适用于序列到序列任务，可以处理不同长度的序列。\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            # 为每个目标序列添加开始符 <bos>，并去除原序列的最后一个标记，以实现教师强制（Teacher Forcing），即使用真实的目标标记作为下一个时间步的输入。\n",
    "            # Y: (batch_size, num_steps)\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "            dec_input = d2l.concat([bos, Y[:, :-1]], dim=1)  # Teacher forcing\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)   # X_valid_len没有被用上？\n",
    "            # Y_hat, _ = net(X, dec_input)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "          f'tokens/sec on {str(device)}')\n",
    "    \n",
    "\n",
    "#@tab all\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 500, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.6. <a id='toc11_4_6_'></a>[预测](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=False):\n",
    "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
    "    \n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # Add the batch axis\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # Add the batch axis\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights (to be covered later)\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # Once the end-of-sequence token is predicted, the generation of the\n",
    "        # output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):  \n",
    "    \"\"\"Compute the BLEU.\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[''.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[''.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[''.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va un !, bleu 0.000\n",
      "i lost . => j'ai perdu ., bleu 1.000\n",
      "he's calm . => <unk> gagné qui est gagné moi ?, bleu 0.000\n",
      "i'm home . => je suis chez moi !, bleu 0.832\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5. <a id='toc11_5_'></a>[Attention](#toc0_)\n",
    "\n",
    "- 不是一个新的概念，很早之前就已经出现，只是在Google发表论文[Attention is all you need](https://arxiv.org/abs/1706.03762)后，越来越知名； \n",
    "- 如果非要找一个依据，从心理学上讲： \n",
    " \n",
    "    1. 之前学习的神经网络（CNN、RNN等）都是提取特征->全连接网络，属于“非随意识注意力”-即非主观，如一排黑色咖啡杯中有一个红色的就会很吸引人；  \n",
    "    2. Attention提出的是“随意识注意力”即主观的去注意那个物体，如喝完咖啡后想去找一本关于Attention方面的书去看。\n",
    "\n",
    "        Query:人主动去查询（注意）  \n",
    "        Key:  物体的属性  \n",
    "        Value:  物体的属性  \n",
    "\n",
    "- `说白了，注意力就是加权平均数，首先计算query与key的相似度，越相似就给越高的权重，最后用权重乘以value再求和，即得注意力值。`\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention//Attention_principle.jpg\" width = \"700\" height = \"300\" alt=\"图片名称\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.1. <a id='toc11_5_1_'></a>[实例数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1b2bd08d40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import torch \n",
    "\n",
    "\n",
    "def f(x):\n",
    "    '''真实函数'''\n",
    "    return 2 * torch.sin(x)\n",
    "\n",
    "\n",
    "def f_noise(x):\n",
    "    '''添加噪声'''\n",
    "    return f(x) + torch.normal(mean=0, std=1, size=x.shape)\n",
    "\n",
    "\n",
    "# 生成数据\n",
    "x = torch.arange(start=0, end=10, step=1)\n",
    "y = f_noise(x)\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(fontsize='x-small', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.2. <a id='toc11_5_2_'></a>[无注意力的方式-如平均汇聚](#toc0_)\n",
    "如直接就平均y值，得到的结果就是一条平滑的曲线，及所有数据的加权信息都一样。\n",
    "\n",
    "其实就是注意力汇聚的平均值，即所有数据都一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1a552c7f80>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import torch \n",
    "\n",
    "\n",
    "def average_pooling(y):\n",
    "    '''平均汇聚后再复制len(y)次'''\n",
    "    # return torch.mean(y) * torch.ones_like(y)\n",
    "    return torch.repeat_interleave(torch.mean(y), len(y)) # repeat_interleave 重复元素\n",
    "\n",
    "\n",
    "# 平均汇聚\n",
    "y_avg = average_pooling(y)\n",
    "\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "plt.plot(x, y_avg, color='green', linestyle='--', label='Average')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(fontsize='x-small', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.3. <a id='toc11_5_3_'></a>[非参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以前，统计学家计算机用的不是很溜。用`统计模型`进行预测，而不是利用计算机的计算资源进行迭代优化逼近真实分布。所得的结果就是只是利用`统计模型进行预测的曲线会比较平滑但是准确性不高`，可能随着数据量的增高可以提高准确性，但是，现实中能有那么多够用的数据吗？而利用计算迭代优化逼近的方法可以很准确的拟合现有的数据，虽然不是很平滑，优点是数据虽少但可以被充分利用。\n",
    "\n",
    "如Nadraya-Watson核回归，利用核函数计算`x和x'的相似度`，即权重，然后利用权重对y进行加权求和，得到最终的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: 0.032021284103393555 s, Matrix: 0.0011219978332519531 s, Broadcast: 0.0008094310760498047 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import torch \n",
    "import time\n",
    "\n",
    "\n",
    "def Nadraya_Watson_kernel(x, y):\n",
    "    '''用Python的for循环实现Nadraya-Watson核回归'''\n",
    "    y_hat = []\n",
    "    for x_i, y_i in zip(x, y):        \n",
    "        div = 0 \n",
    "        for i in x:\n",
    "            div_term = 0\n",
    "            for j in x:\n",
    "                div_term += torch.exp(-0.5 * (x_i - j)** 2)\n",
    "            div_up = torch.exp(-0.5 * (x_i - i) ** 2)\n",
    "            div += (div_up / div_term) * y_i\n",
    "        y_hat.append(div)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def Nadraya_Watson_kernel_matrix(x, y):\n",
    "    '''用Pytorch的矩阵（广播之后的）实现Nadraya-Watson核回归'''\n",
    "    sub = x.unsqueeze(dim=1).repeat(1, len(x)) - x.repeat(len(x), 1)\n",
    "    attention_scores = torch.exp(-0.5 * sub** 2)\n",
    "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "    attention_output = (attention_weights * y.unsqueeze(dim=1).repeat(1, len(x))).sum(dim=-1)\n",
    "    return attention_output\n",
    "\n",
    "\n",
    "def Nadraya_Watson_kernel_broadcast(x, y):\n",
    "    '''用Pytorch的广播实现Nadraya-Watson核回归'''\n",
    "    sub = x.unsqueeze(dim=1) - x.unsqueeze(dim=0)\n",
    "    attention_scores = torch.exp(-0.5 * sub**2)\n",
    "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "    attention_output = (attention_weights * y.unsqueeze(dim=1)).sum(dim=-1)\n",
    "    return attention_output\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "t1 = time.time()\n",
    "# 很慢\n",
    "plt.plot(x, Nadraya_Watson_kernel(x, y), color='green', linestyle='--', label='Loop')\n",
    "t2 = time.time()\n",
    "# 展开后，速度快\n",
    "plt.plot(x, Nadraya_Watson_kernel_matrix(x, y), color='purple', linestyle='-.', label='Matrix')\n",
    "t3 = time.time()\n",
    "# 广播后，速度更快\n",
    "plt.plot(x, Nadraya_Watson_kernel_broadcast(x, y), color='cyan', linestyle=':', label='Broadcast')\n",
    "t4 = time.time()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(fontsize= \"x-small\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "print(f'Loop: {t2 - t1} s, Matrix: {t3 - t2} s, Broadcast: {t4 - t3} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.4. <a id='toc11_5_4_'></a>[参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc0_)\n",
    "加入可学习的参数`w`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1a5415f7a0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "class Nadraya_Watson_kernel_w(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.randn(size=(1,), requires_grad=True))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        sub = x.unsqueeze(dim=1) - x.unsqueeze(dim=0)\n",
    "        attention_scores = torch.exp(-0.5 * (sub * self.w)** 2)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_output = (attention_weights * y.unsqueeze(dim=1)).sum(dim=-1)\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "net = Nadraya_Watson_kernel_w()\n",
    "loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001) \n",
    "\n",
    "\n",
    "epochs = 5\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad()\n",
    "    y_hat = net(x, y)\n",
    "    loss = loss_fn(y_hat, y).sum()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    # print(f'epoch {epoch + 1}, loss {loss:.3f}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(loss_list, color='blue', linestyle='-', label='loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(fontsize='x-small', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "    plt.plot(x, net(x, y), color='green', linestyle='-.', label='Broadcast_w')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend(fontsize='x-small', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.4523], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学习的参数w\n",
    "net.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.5. <a id='toc11_5_5_'></a>[注意力分数函数-计算q和k相似度](#toc0_)\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention/Attention_score.jpg\" width = \"500\" height = \"300\" alt=\"图片名称\" align=center />\n",
    "\n",
    "原理：\n",
    "- 本质上`Attention机制是Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数`；\n",
    "- 首先利用`注意力分数函数`计算`Query`和`Key`的`相似度 (注意力分数)`；\n",
    "- 利用`softmax`计算相似度 (注意力分数)后，得到`加权数值` (`注意力权重`，query和key越相似该权重越大，即获取的注意力越大)；\n",
    "- 利用注意力权重对value进行`加权求和`，即最终的`注意力值`。\n",
    "\n",
    "解释：\n",
    "  |注释|公式|\n",
    "  |:-|:-|\n",
    "  |注意力评分函数|$a(q, k)$|\n",
    "  |注意力权重|$softmax( a(q, k) )$|\n",
    "  |注意力|$softmax( a(q, k) ) * v$|\n",
    "\n",
    "注意力评分函数：\n",
    "  - `加性注意力 (Additive Attention)`\n",
    "  - `缩放点积注意力 (Scaled Dot-Product Attention)`\n",
    "  - 乘性注意力 (Multiplicative Attention)\n",
    "  - 位置注意力 (Location-based Attention)\n",
    "  - 线性注意力 (Linear Attention)\n",
    "  - 自适应注意力 (Adaptive Attention)\n",
    "  - 稀疏注意力 (Sparse Attention)\n",
    "\n",
    "总结: \n",
    " - 不同的注意力机制在计算注意力分数时采用了不同的方法，以适应不同的任务需求和计算资源。选择合适的注意力机制可以提高模型的性能和效率。随着研究的不断深入，新的注意力机制也在不断涌现，为各种应用场景提供了更多的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.5.5.1. <a id='toc11_5_5_1_'></a>[加性注意力 (Additive Attention)-计算q、k相似度](#toc0_)\n",
    "加性注意力最早由 Bahdanau 等人在 2014 年的论文中提出，通常用于序列到序列模型中。其核心思想是通过一个小型的前馈神经网络来计算注意力权重。\n",
    "计算过程:\n",
    "  - 对于给定的查询（query）和键（key），首先通过线性变换将它们`映射到相同的维度`。\n",
    "  - 将映射后的查询和键`相加`，并通过一个激活函数（如 tanh）进行`非线性变换`。\n",
    "  - 使用一个`可学习的参数向量`对变换后的结果进行`线性变换`，得到注意力得分。\n",
    "  - 对所有注意力得分进行 softmax 操作，得到注意力权重。\n",
    "\n",
    "公式：$\\mathrm{score}(q,k)=v^T\\cdot\\mathrm{tanh}(W_qq+W_kk)$，其中，$W_q$和$W_k$是可学习的线性变换矩阵，$v$是可学习的参数向量。  \n",
    "​\n",
    "优点：\n",
    "  - 能够处理不同维度的查询和键。\n",
    "  - 适用于较小的序列长度。\n",
    "\n",
    "缺点：\n",
    "  - 计算复杂度较高，尤其在序列长度较大时。\n",
    "\n",
    "---\n",
    "\n",
    "动手学深度学习：\n",
    "- 当`查询`和`键`是`不同长度`的`矢量`时，可以使用`加性注意力作为评分函数`。\n",
    "- 注意力评分函数：$a(\\mathbf{q},\\mathbf{k})=\\mathbf{w}_v^\\top\\tanh(\\mathbf{W}_q\\mathbf{q}+\\mathbf{W}_k\\mathbf{k})\\in\\mathbb{R}$\n",
    "- $\\mathbf{q}\\in\\mathbb{R}^q\\text{和 键}\\mathbf{k}\\in\\mathbb{R}^k,$\n",
    "- $\\mathbf{W}_q\\in\\mathbb{R}^{h\\times q}\\mathrm{、}\\mathbf{W}_k\\in\\mathbb{R}^{h\\times k}\\text{和 }\\mathbf{w}_v\\in\\mathbb{R}^h$, `投影`到`相同维度h`上\n",
    "- `有可学习的参数`，效果会好一些。\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention/Additive_attention.jpg\" width = \"800\" height = \"600\"/>\n",
    "\n",
    "\n",
    "- 使用:\n",
    "```python\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num, key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "            # query_size, key_size, value_size 可以不一样\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参考案例-李沐\n",
    "  - 带有掩码 (masked) 和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"\n",
    "    为序列生成掩码，将无效/填充位置的值替换为指定值\n",
    "    \n",
    "    参数:\n",
    "        X: 输入张量，形状为 (batch_size, seq_len, ...)\n",
    "        valid_len: 每个序列的有效长度，形状为 (batch_size,)\n",
    "        value: 用于替换无效位置的值，默认为0\n",
    "        \n",
    "    返回:\n",
    "        掩码后的张量，形状与输入X相同\n",
    "        \n",
    "    实现步骤:\n",
    "        1. 获取序列最大长度maxlen\n",
    "        2. 生成掩码矩阵:\n",
    "           - torch.arange生成[0,1,...,maxlen-1]\n",
    "           - [None,:]增加batch维度变为(1,maxlen) \n",
    "           - valid_len[:,None]将(batch_size,)变为(batch_size,1)\n",
    "           - 比较生成(batch_size,maxlen)的布尔掩码\n",
    "        3. 将~mask位置(无效位置)的值替换为value\n",
    "    \"\"\"\n",
    "    # 获取序列最大长度\n",
    "    maxlen = X.size(1)\n",
    "    \n",
    "    # 生成掩码矩阵: (batch_size, maxlen)\n",
    "    # 其中True表示有效位置,False表示无效位置\n",
    "\n",
    "    ## 实现方式一：\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    ## 实现方式二：\n",
    "    # mask = torch.arange(maxlen, dtype=torch.float32, device=X.device).reshape(1, -1) < valid_len.reshape(-1, 1)\n",
    "    \n",
    "    # 将无效位置(~mask)替换为value，取反\n",
    "    X[~mask] = value\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"\n",
    "    通过在最后一个轴上掩蔽元素来执行softmax操作     \n",
    "    参数:\n",
    "        X: 3D张量, shape为(batch_size, seq_len, feature_dim)\n",
    "        valid_lens: 1D或2D张量,指定每个序列的有效长度\n",
    "            - 1D时shape为(batch_size,),表示每个batch中所有序列的有效长度\n",
    "            - 2D时shape为(batch_size, seq_len),可以为每个序列的每个位置指定不同的有效长度\n",
    "    返回:\n",
    "        经过masked softmax后的张量,shape与输入X相同\n",
    "    \"\"\"\n",
    "    # 如果没有指定valid_lens,直接在最后一维上做softmax\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        # 如果valid_lens是1D\n",
    "        if valid_lens.dim() == 1:\n",
    "            ## 以dim=0方向按元素个数重复shape[1] (seq_len) 次, \n",
    "            ## e.g., [1, 2] -> [1, 1, 1, 2, 2, 2]\n",
    "            valid_lens = torch.repeat_interleave(input=valid_lens, repeats=shape[1])\n",
    "        else:\n",
    "            # 如果是2D,按顺序将其展平为1D\n",
    "            ## e.g., [[2, 3, 4], [3, 2, 1]] -> [2, 3, 4, 3, 2, 1]\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "            \n",
    "        # 使用sequence_mask生将超出有效长度的位置用一个很小的负值(-1e6)或者-inf替换,使其softmax后接近0\n",
    "        ## X重塑为:(batch_size * seq_len, feature_dim)  \n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "\n",
    "        # 重塑回原始形状并做softmax\n",
    "        ## X: (batch_size, seq_len, feature_dim)\n",
    "        output = nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "        return output \n",
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)         # (key_size, num_hiddens)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)       # (query_size, num_hiddens)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)                # (num_hiddens, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # queries:              (batch_size, num_query, num_hiddens)\n",
    "        # keys:                 (batch_size, k_v_pair_num, num_hiddens)\n",
    "\n",
    "        # 在维度扩展后，使用广播方式进行求和\n",
    "        # queries的形状：       (batch_size，num_query，        1，        num_hiddens)\n",
    "        # key的形状：           (batch_size，    1，    k_v_pair_num，  num_hiddens)\n",
    "        # (batch_size, num_query, 1, num_hiddens) + (batch_size, 1, k_v_pair_num, num_hiddens) \n",
    "        # = (batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # features的形状：(batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        \n",
    "        # self.w_v: (num_hiddens, 1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num, 1)\n",
    "        # 移除最后一个维度squeeze(-1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "\n",
    "        # 注意力权重\n",
    "        ## 使用masked_softmax计算注意力权重, 有效长度为valid_lens\n",
    "        ## attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        ## 使用dropout\n",
    "        self.attention_weights = self.dropout(self.attention_weights)\n",
    "\n",
    "        # 注意力输出值\n",
    "        ## values的形状：(batch_size，k_v_pair_num，value_size)\n",
    "        attention_output = torch.bmm(self.attention_weights, values)\n",
    "\n",
    "        return attention_output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 2\n",
    "num_query, query_size = 1, 20\n",
    "k_v_pair_num, key_size, value_size = 10, 2, 4\n",
    "\n",
    "\n",
    "# 查询的小批量 (batch_size, num_query, query_size)\n",
    "# queries = torch.normal(mean=0, std=1, size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "# 键的小批量 (batch_size, k_v_pair_num, key_size)\n",
    "keys = torch.ones(size=(batch_size, k_v_pair_num, key_size))\n",
    "# 值的小批量 (batch_size, k_v_pair_num, value_size)\n",
    "values = torch.randn(size=(batch_size, k_v_pair_num, value_size))\n",
    "\n",
    "# 每个batch中序列的有效长度 (batch_size,)\n",
    "valid_lens = torch.tensor([2, 6])   # 每个batch中序列的有效长度\n",
    "\n",
    "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=False)\n",
    "attention.eval()\n",
    "\n",
    "attention(queries=queries, keys=keys, values=values, valid_lens=valid_lens).shape\n",
    "\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''valid_lens：[2, 6], 切query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for batch in range(attention.attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention.attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (详细) 从头手写-逐步分析“加性注意力机制代码”\n",
    "  - 无掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries size:  torch.Size([2, 1, 20])\n",
      "keys size:  torch.Size([2, 10, 2])\n",
      "values size:  torch.Size([2, 10, 4])\n",
      "Q size:  torch.Size([2, 1, 1, 4])\n",
      "K size:  torch.Size([2, 1, 10, 4])\n",
      "features size:  torch.Size([2, 1, 10, 4])\n",
      "features size (tanh):  torch.Size([2, 1, 10, 4])\n",
      "scores size:  torch.Size([2, 1, 10, 1])\n",
      "scores size squeeze:  torch.Size([2, 1, 10])\n",
      "attention_weights:  torch.Size([2, 1, 10])\n",
      "attention:  torch.Size([2, 1, 4])\n",
      "attention_wights_droputed size:  torch.Size([2, 1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 测试数据\n",
    "# ------------------------------------------------------\n",
    "batch_size = 2\n",
    "num_query = 1\n",
    "query_size = 20             # 一个query的向量长度\n",
    "\n",
    "num_key = 10                # “键－值”对的个数\n",
    "key_size = 2                # 一个key的向量长度\n",
    "\n",
    "num_value = num_key         # “键－值”对的个数\n",
    "value_size = 4              # 一个value的向量长度\n",
    "\n",
    "# queries = torch.randn(size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "print('queries size: ', queries.size())\n",
    "# batch_size, num_query, query_size\n",
    "# 2, 1, 20\n",
    "\n",
    "keys = torch.ones(size=(batch_size, num_key, key_size))\n",
    "print('keys size: ', keys.size())\n",
    "# batch_size, kv_pair_num, key_size\n",
    "# 2, 10, 2\n",
    "\n",
    "values = torch.randn(size=(batch_size, num_value, value_size))\n",
    "print('values size: ', values.size())\n",
    "# batch_size, kv_pair_num, value_size\n",
    "# 2, 10, 4\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 投影 (可学习的参数W)\n",
    "# ------------------------------------------------------\n",
    "## 全部投影到value_size一致的维度，便于计算\n",
    "num_hiddens = value_size\n",
    "bias = False\n",
    "W_q = nn.Linear(query_size, num_hiddens, bias=bias)     # 20 , 4\n",
    "W_k = nn.Linear(key_size, num_hiddens, bias=bias)       # 2, 4\n",
    "w_v = nn.Linear(num_hiddens, 1, bias=bias)              # 4, 1\n",
    "\n",
    "Q = W_q(queries)                    # (batch_size，查询的个数，num_hidden) 3维\n",
    "# 2, 1, 20 * 20, 4 = 2, 1, 4\n",
    "Q = Q.unsqueeze(2)                  # (batch_size，查询的个数，1，num_hidden) 插入一个维度  (重要) 4维\n",
    "# 2, 1, (1), 4                        # 为什么要插入一个维度？便于后续做广播\n",
    "print('Q size: ', Q.size())\n",
    "\n",
    "K = W_k(keys)                       # (batch_size，“键－值”对的个数，num_hiddens)   3维\n",
    "# 2, 10, 2 * 2, 4 = 2, 10, 4\n",
    "K = K.unsqueeze(1)                  # (batch_size，1，“键－值”对的个数，num_hiddens) 插入一个维度 (重要) 4维度\n",
    "# 2, (1), 10, 4                       # 为什么要插入一个维度？便于后续做广播\n",
    "print('K size: ', K.size())\n",
    "\n",
    "\n",
    "features = Q + K                    # 自动做广播后做加法    (重要)                                  (2,1,(1),4) + (2,(1),10,4) = (2,1,10,4)\n",
    "# 2, 1, 10, 4                       # (batch_size，查询个数，“键－值”对的个数，num_hiddens) 广播后   (2,1,(10),4)+ (2,(1),10,4) = (2,1,10,4)\n",
    "print('features size: ', features.size())\n",
    "\n",
    "features = torch.tanh(features)\n",
    "# 2, 1, 10, 4\n",
    "print('features size (tanh): ', features.size())\n",
    "\n",
    "scores = w_v(features)              # 自动做广播后做乘法    (2,1,10,4) @ (    4,1) = (2,1,10,1)\n",
    "                                    #                      (2,1,10,4) @ (2,1,4,1) = (2,1,10,1)\n",
    "# 2, 1, 10, 1\n",
    "print('scores size: ', scores.size())\n",
    "\n",
    "# w_v仅有一个输出，因此从形状中移除最后那个维度\n",
    "# scores的形状：(batch_size，查询的个数，“键-值”对的个数)\n",
    "scores = scores.squeeze(-1) \n",
    "# 2, 1, 10\n",
    "print('scores size squeeze: ', scores.size())\n",
    "\n",
    "attention_weights = torch.softmax(scores, dim=-1)\n",
    "# 2, 1, 10\n",
    "print('attention_weights: ', attention_weights.size())\n",
    "# attention_weights\n",
    "\n",
    "attention = torch.bmm(attention_weights, values)    # (2,1,10) @ (  10,4) = (2,1,4)\n",
    "                                                    # (2,1,10) @ (2,10,4) = (2,1,4) 广播后\n",
    "print('attention: ', attention.size())\n",
    "# batch_size, num_query, value_size\n",
    "# 2, 1, 4\n",
    "\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "\n",
    "\n",
    "# 添加dropout\n",
    "dropout = nn.Dropout(p=0.1)     # 0.1的概率失活\n",
    "attention_weights_droputed = dropout(attention_weights)\n",
    "# attention_weights_droputed.shape = 2, 1, 10    dropout后不改变attention_weights的形状\n",
    "print('attention_wights_droputed size: ', attention_weights_droputed.size())\n",
    "\n",
    "attention_droputed = torch.bmm(attention_weights_droputed, values)\n",
    "\n",
    "attention_droputed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 注意力权重可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''没有使用valid_lens, 且query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.5.5.2. <a id='toc11_5_5_2_'></a>[缩放点积注意力 (Scaled Dot-Product Attention)-计算q、k相似度](#toc0_)\n",
    "缩放点积注意力是由 Vaswani 等人在 2017 年的 Transformer 论文中提出的。它通过点积计算注意力得分，并对得分进行缩放以提高数值稳定性。\n",
    "计算过程：\n",
    "  - 对于给定的查询和键，计算它们的`点积`。\n",
    "  - 将点积结果`除以键的维度的平方根`，以避免数值过大。\n",
    "  - 对缩放后的得分进行 `softmax` 操作，得到注意力权重。\n",
    "\n",
    "公式：$\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax} (\\frac{QK^T}{\\sqrt{d_k}}) V$, 其中，$d_k$是键的维度。\n",
    "\n",
    "优点：\n",
    "  - 计算效率高，适合并行化。\n",
    "  - 在大多数现代深度学习模型中广泛使用。\n",
    "\n",
    "缺点：\n",
    "  - 对于不同维度的查询和键，需要进行额外的线性变换。\n",
    "\n",
    "---\n",
    "\n",
    "- q和k的长度`一致`，为`d`\n",
    "- 注意力评分函数：$a(\\mathbf{q},\\mathbf{k})=\\mathbf{q}^\\top\\mathbf{k}/\\sqrt{d}$\n",
    "- 向量版本注意力权重：$\\mathrm{softmax}\\left(\\frac{\\mathrm{QK}^\\top}{\\sqrt{d}}\\right)\\mathbf{V}\\in\\mathbb{R}^{n\\times v}$\n",
    "- $\\text{查询}\\mathbf{Q}\\in\\mathbb{R}^{n\\times d}\\text{、键}\\mathbf{K}\\in\\mathbb{R}^{m\\times d}\\text{和 值}\\mathbf{V}\\in\\mathbb{R}^{m\\times v}$\n",
    "- `无可学习`参数\n",
    "\n",
    "\n",
    "<div style=\"display:flex;justify-content:center\">\n",
    "<img src=\"./Pytorch_Pictures/Attention/scale-dot-product.png\" width = \"300\" height = \"300\" alt=\"图片名称\">\n",
    "</div>\n",
    "\n",
    "- 使用：\n",
    "```python\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "            # query_size, key_size必须一样，最好是等于value_size\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch简洁实现  \n",
    "query_size = key_size = value_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "batch_size, num_query, query_size = 2, 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4\n",
    "\n",
    "# queries = torch.randn(size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "keys = torch.ones(size=(batch_size, k_v_pair_num, key_size))\n",
    "values = torch.randn(size=(batch_size, k_v_pair_num, value_size))\n",
    "\n",
    "# num_heads = 1\n",
    "att = nn.MultiheadAttention(embed_dim=value_size, num_heads=1, batch_first=True)\n",
    "out, weights = att(queries, keys, values)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASYAAADQCAYAAACqeMxqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL+FJREFUeJzt3XlYU1f+P/B3AoZN1gKyiFAEqSKggjDY4cFRBix9sFqXqaJSZOxMla1ov2qnBes4LqO1WHHrjGitWmndir+2PLUWR6ugfBGwKmXU4oIsimI0smU5vz/45tZAIBACScjn9Tz3eci5596cBPLh3JPPPYfHGGMghBAdwtd2AwghpD0KTIQQnUOBiRCicygwEUJ0DgUmQojOocBECNE5FJgIITqHAhMhROdQYCKE6BwKTIQQnUOBiRADt23bNnh4eMDU1BQhISG4ePFip3WvXr2KGTNmwMPDAzweD5mZmR3qnDlzBjExMXBxcQGPx8Px48d73CYKTIQYsJycHKSlpSEjIwOXLl1CQEAAoqKicP/+faX1Gxsb4enpifXr18PJyUlpnWfPniEgIADbtm1Tu108uomXEMMVEhKC8ePHIysrCwAgk8ng5uaGpKQkrFixostjPTw8kJqaitTU1E7r8Hg8HDt2DNOmTetRu4x7VJsQonOam5vR2toKAGCMgcfjKew3MTGBiYlJh+NaW1tRXFyMlStXcmV8Ph8REREoKCjo20arQJdyhOix5uZmeLgPhrW1NaytrTF06FDuZ/m2bt06pcfW19dDKpViyJAhCuVDhgxBbW1tfzS/UxSYDMSqVavA4/FQX1+v7aYAAE6fPg0ej4fDhw9ruyl6rbW1FXX3pSgtGoLSoiEQiUS4e/cuhEIhtz3fI9IXFJiI2rZv3469e/dq7flbWlqwfPlyuLi4wMzMDCEhITh58qTW2qNNfAsGvkXbcLGVlZXCpuwyDgDs7e1hZGSEuro6hfK6urpOB7b7CwUmojZtB6Y333wTmzdvRmxsLLZs2QIjIyNER0fjp59+0lqbtKWJ8dHEevZxFggECAwMxKlTp7gymUyGU6dOITQ0VNNN7BEa/CZ66eLFizh06BA2btyIZcuWAQAWLFiA0aNH43/+539w/vx5Lbewfz1jg9Q6Li0tDXFxcQgKCkJwcDAyMzPx7NkzxMfHA2h7T11dXblxqtbWVly7do37+d69eygtLcXgwYPh5eUFABCJRLhx4wb3HJWVlSgtLYWdnR2GDRvWvYYxYhAyMjIYAFZeXs5mzZrFLC0tmZ2dHUtOTmZNTU0KdbOzs9kf/vAH5uDgwAQCARs5ciTbvn27Qh13d3cGQGELDw/n9jc0NLDU1FTm7u7OBAIBc3V1ZfPnz2cPHjxgjDGWn5/PALCcnBy2Zs0a5urqykxMTNikSZPY9evXVb6ed999lxkZGTGhUKhQvnbtWgaA3blzR813Sr8IhUIGgH1z2ZN9c9mTAejwnqiydetWNmzYMCYQCFhwcDArLCzk9oWHh7O4uDjucWVlZYffe/vfvfx32357/jyqUI/JwMyePRseHh5Yt24dCgsL8cknn6ChoQH79u3j6uzYsQO+vr6YOnUqjI2NceLECSxevBgymQxLliwBAGRmZiIpKQmDBw/G3/72NwDgvt0RiUQICwtDeXk5Fi5ciHHjxqG+vh65ubmoqqqCvb0991zr168Hn8/HsmXLIBQK8c9//hOxsbG4cOFCl6+jpKQEI0aMgJWVlUJ5cHAwAKC0tBRubm69f8P0RCMTqH1sYmIiEhMTle47ffq0wmMPDw8wFamPEydOVFlHpR6FVqK35D2mqVOnKpQvXryYAWBlZWVcWWNjY4fjo6KimKenp0KZr6+vwn9KufT0dAaAHT16tMM+mUzGGPvtv+rIkSNZS0sLt3/Lli0MAPv555+7fD2+vr5s0qRJHcqvXr3KALCdO3d2efxAIe8x7S0JYHtLAtTqMekiGvw2MPIej1xSUhIA4Ntvv+XKzMzMuJ+FQiHq6+sRHh6OX3/9FUKhUOVzHDlyBAEBAZg+fXqHfe2T/+Lj4yEQ/PbfPiwsDADw66+/dvkcTU1NSr9tMjU15fYbkiYmQFMvek26hgKTgfH29lZ4PHz4cPD5fNy6dYsrO3fuHCIiImBhYQEbGxs4ODjgvffeA4BuBaabN29i9OjR3WpP+8FQW1tbAEBDQ0OXx5mZmaGlpaVDeXNzM7ffkDyVmuKp1FTbzdAYGmMycO17MDdv3sTkyZPx0ksvYfPmzXBzc4NAIMC3336Ljz/+GDKZTKPPb2RkpLScqRijcHZ2xr179zqU19TUAABcXFx63zg90iRTnqukrygwGZjr16/jxRdf5B7fuHEDMpkMHh4eAIATJ06gpaUFubm5Cr2Z/Pz8DudqH9Tkhg8fjitXrmi24e2MGTMG+fn5ePLkicIAuHzQfMyYMX36/LqmSaZeuoCuoks5A9N+KoqtW7cCAF555RUAv/Vgnu+xCIVC7Nmzp8O5LCws8Pjx4w7lM2bMQFlZGY4dO9Zhn6qeUHfNnDkTUqkUn376KVfW0tKCPXv2ICQkxKC+kQOARqkAjdKBM8ZEPSYDU1lZialTp2LKlCkoKCjA/v37MXfuXAQEBAAAIiMjIRAIEBMTg7/85S8QiUT417/+BUdHR+4ySS4wMBA7duzAmjVr4OXlBUdHR0yaNAnvvvsuDh8+jFmzZmHhwoUIDAzEo0ePkJubi507d3LP1RshISGYNWsWVq5cifv378PLywufffYZbt26hd27d/f6/PqmeQAFJQDqpQukp6ezW7duafTrQdK35OkC165dYzNnzmSWlpbM1taWJSYmdkiwzM3NZf7+/szU1JR5eHiwDRs2sOzsbAaAVVZWcvVqa2vZq6++yiwtLTsk2T18+JAlJiYyV1dXJhAI2NChQ1lcXByrr69njP2WLvDVV18pPLc8gW/Pnj0qX1NTUxNbtmwZc3JyYiYmJmz8+PEsLy9P7fdIH8nTBebnz2Hz8+cMmHQBtSaKGzNmDK5cuYLw8HAkJCRgxowZnd4oSAjpO0+ePIG1tTVmnVoAAPhq8j4IhcIOiaf6Rq0xptLSUhQVFcHX1xcpKSlwcnLC22+/jaKiIk23jxDSDc3SQWiWDpwBcLUHv8eOHYtPPvkE1dXV2L17N6qqqvDyyy/D398fW7Zs6Va+CyFEM5okxmiSDJwh415/K8cYg1gsRmtrKxhjsLW1RVZWFtzc3JCTk6OJNhJCVGiWGKOZAhNQXFyMxMREODs745133sHYsWNRXl6O//znP7h+/Tr+8Y9/IDk5WWMNffToEWJjY2FlZQUbGxskJCRAJBJ1eczEiRPB4/EUtr/+9a8aaxMhuqJVaoxW6cAJTGq9Ej8/P/zyyy+IjIzE7t27ERMT0yGDd86cOUhJSdFIIwEgNjYWNTU1OHnyJMRiMeLj4/HWW2/h4MGDXR63aNEirF69mntsbm6usTYRoivEUuUZ9PpKrcA0e/ZsLFy4EK6urp3Wsbe319jtC+Xl5cjLy0NRURGCgoIAtCUGRkdHY9OmTV3efmBubq71aUIJ6Wuthh6YxGIx9u7di5kzZ3YZmDSpoKAANjY2XFACgIiICPD5fFy4cEHpXexyBw4cwP79++Hk5ISYmBh88MEHXfaaWlpaFG4OlclkePToEV544YVOb8EgRNMYY3j69ClcXFzA56secRFLDDwwDRo0iLuDu7/U1tbC0dFRoczY2Bh2dnZdLjMzd+5cuLu7w8XFBZcvX8by5ctRUVGBo0ePdnrMunXr8OGHH2qs7YT0xt27dzF06FCV9STSgXV3mVqXckuWLMGGDRvw73//G8bG6g+4rVixAhs2bOiyTnl5udrnf+utt7if/fz84OzsjMmTJ+PmzZsYPny40mNWrlyJtLQ07rFQKMSwYcMwNON98E0HzrQSRLfJmptR9eEaWFpadq++hAITioqKcOrUKXz//ffw8/ODhYWFwv6ueiTPW7p0Kd58880u63h6esLJyanDWuoSiQSPHj3q0fhRSEgIgLY76jsLTJ2tWso3NaXARPpdd4cPKDABsLGxwYwZM3r95A4ODnBwcFBZLzQ0FI8fP0ZxcTECAwMBAD/++CNkMhkXbLqjtLQUQNtcPoQMJIwu5aB0Coy+NHLkSEyZMgWLFi3C5MmTsX//ftTV1cHOzg5VVVVwcXHBvXv3MHnyZOzbtw/BwcG4efMmDh48CIFAgE8//RR3794FAIwePRr+/v792n5C+hqTqP/FzLZt27Bx40bU1tYiICAAW7du5RZ1aO/q1atIT09HcXExbt++jY8//hipqam9OqcyaodZiUSCH374Abt27cLTp08BANXV1SqTHtV14MABmJmZYdOmTXj8+DGmTZuGmJgYREVF4f79+xCLxaioqEBjYyOAtsX8jhw5ghUrVuD27dtwdnZGUFAQKioq+nwSM0L6G0/CB0+Ny7mcnBykpaUhIyMDly5dQkBAAPeZUqaxsRGenp5Yv359p8MoPT2nMmoFptu3b8PPzw+vvfYalixZggcPHgAANmzYwC0+qGl2dnaQSCRYsmQJmpqacPToUezevRvm5ubIzs7mlpWZOHEiAMDNzQ0+Pj549dVXIZFIcPv2bZw/fx7jxo1DVlZWn7SREK2R8Nq2Htq8eTMWLVqE+Ph4jBo1Cjt37uQ+U8qMHz8eGzduxBtvvNHpjCI9PacyagWmlJQUBAUFoaGhQWHS9+nTpyssN6xJra2tKC4uRkREBFfG5/MRERGBgoICpccUFBQo1AeAqKioTusDbXlMT548UdgI0XV8WdsGoMPfr7JFGwD1PlOqaOqcagWms2fP4v3331dYdgdoWwxP2QTxmlBfXw+pVMotqig3ZMiQTnOZamtre1QfaMtjsra25jZDm6KV6CeehAfe//WY3NzcFP6G5ct7t6fOZ0oVTZ1TrcFvmUwGqVTaobyqqqrbeRe6qn0e05MnTyg4EZ3He+4y7u7duwoTxenjJI5q9ZgiIyORmZnJPebxeBCJRMjIyEB0dLSm2qbA3t4eRkZGqKurUyivq6vrdBDOycmpR/WBtl+ilZWVwkaIruNJ2jYAHf5+OwtM6nymVNHUOdUKTB999BHOnTuHUaNGobm5GXPnzuUu41RlcqtLIBAgMDBQYQxLJpPh1KlTCA0NVXpMaGhohzGvkydPdlqfEH31fGDqLnU+U/11TrUu5YYOHYqysjIcOnQIly9fhkgkQkJCAmJjY/t0BdS0tDTMmzcPX3zxBZ4+fQpra2s0NTUhPj4eALBgwQK4urpy19ReXl5Yu3Zth+zZ55f8IWQg4HccWemWtLQ0xMXFISgoCMHBwcjMzMSzZ886/Uy1trbi2rVr3M/37t1DaWkpBg8eDC8vr26dszvUvtHN2NgY8+bNU/fwXmGMceuTySeAA4A7d+4o3Int7e0NMzMzODs74+7du3jxxReRnp7e7eWrCdEXPe0tyf3pT3/CgwcPkJ6ejtraWowZMwZ5eXnc4HX7z1R1dTXGjh3LPd60aRM2bdqE8PBwnD59ulvn7NbrUWeVlH379nW5f8GCBT09ZbeEhIRg/PjxXB6STCaDm5sbkpKSsGLFig719+7di9TUVKWLMnaXfBWKYevW0L1ypN/ImptxZ+X7Klc8kf99vpS8FgDwyyfvDYhVUtTqMbWfmVIsFqOxsRECgQDm5uZ9Epjk+RErV67kyrqTHyESieDu7g6ZTIZx48Zh7dq18PX17bR++/mY5IsqyPp5qhdi2OR/b93tN6jbY9JVagWmhoaGDmXXr1/H22+/jXfffbfXjVKmq/yIX375RekxPj4+yM7Ohr+/P4RCITZt2oQJEybg6tWrnc5x09l8TFUfrun9iyCkh+RjqaqoO8akqzQ2e7m3tzfWr1+PefPmdRoo+ltoaKjCNwETJkzAyJEjsWvXLvz9739Xekz7PCZVM1jK85za544Q1ei969zzM1h2BwWmrk5mbIzq6mpNnpKjifyIQYMGYezYsbhx40andZTNx2RjY6Py3JTzpD5675TrTk9Jji7lAOTm5io8ZoyhpqYGWVlZePnllzXSsPaez4+YNm0agN/yIxITE7t1DqlUip9//rnPkkAJ0RYjcY+/w9JpagUmeWCQ4/F4cHBwwKRJk/DRRx9pol1K9TTnYvXq1fjd734HLy8vPH78GBs3bsTt27fx5z//uc/aSIg28KnHBI0ty9RTPc25aGhowKJFi1BbWwtbW1sEBgbi/PnzGDVqlMbaZGJigoyMDL28H0nb6L3THJ50YPWY1Mpjen5wWJXNmzf39PSEkG6S5zEFv9b2rfHFr1XnPukDtXpMJSUluHTpEiQSCXx8fAAA//3vf2FkZIRx48Zx9WgdNkL6B18ysHpMagWmmJgYWFpa4rPPPoOtrS2Atsum+Ph4hIWFYenSpRptJCGkawMtMKl1Kefq6orvv/++Qwb1lStXEBkZ2WcpA4QQRfJLuZcnrwIAnDu1ynAv5Z48ecLN8/28Bw8ecAsTEEL6D3+ApQuoNR/T9OnTER8fj6NHj6KqqgpVVVU4cuQIEhIS8Prrr2u6jTpr27Zt8PDwgKmpKUJCQnDx4kVtN0kvrFq1ipsVQr699NJL2m6WXuOLpeCLB076t1o9pp07d2LZsmWYO3cuxGJx24mMjZGQkICNGzdqtIG6Sr5Ezc6dOxESEoLMzExERUWhoqICjo6O2m6ezvP19cUPP/zAPe7NUvME4Iu1k8LTV9TqMZmbm2P79u14+PAhSkpKUFJSgkePHmH79u0dlgsfqDSxRI0hMzY2hpOTE7fZ29tru0l6jSeRgScZOMGpV+sKW1hYwN/fH/7+/gYTkIC+WfbG0Fy/fh0uLi7w9PREbGws7ty5o+0m6TWeWAreALqUG1gLnveTvlj2xpCEhIRg7969yMvLw44dO1BZWYmwsDD64qQXBlpgogt70u9eeeUV7md/f3+EhITA3d0dX375JRISErTYMj0mGVg3y1FgUkNfLHtjyGxsbDBixIgup6MhKvzfl1ADBV3KqaEvlr0xZCKRCDdv3oSzs7O2m6K/xNK2bYCgHpOaNLFEjaFatmwZYmJi4O7ujurqamRkZMDIyAhz5szRdtP0FntunvqBgAKTmjSxRI2hqqqqwpw5c/Dw4UM4ODjg97//PQoLC+Hg4KDtpumvAXYpp9a9coQQ3SC/V26SYBYA4MfWrwz3XjlCiG5pbX2m7SZoFAUmQvSYQCCAk5MTfqr9FgDg5OQEgUCg5Vb1Hl3KEaLnmpub0draCqAtUJkOgBWjKTARQnQO5TERQnQOBSZCiM6hwEQI0TkUmAghOocCkwGbOHEiUlNTtd2MbuHxeDh+/Li2m0H6CeUxEb1QU1PDLRVGBj4KTEQvqJpORiwWY9CgQf3UGtLX6FKOcL755htYW1vjwIEDSvdPmDABy5cvVyh78OABBg0ahDNnzig9ZtWqVRgzZgx27doFNzc3mJubY/bs2RAKhVydoqIi/PGPf4S9vT2sra0RHh6OS5cuKZzn+Uu5W7dugcfjIScnB+Hh4TA1Ne20zUQ/UWAiAICDBw9izpw5OHDgAGJjY5XWiY2NxaFDh/B8Tm5OTg5cXFwQFhbW6blv3LiBL7/8EidOnEBeXh5KSkqwePFibv/Tp08RFxeHn376CYWFhfD29kZ0dLTKqXZXrFiBlJQUlJeXIyoqqoevmOg0RgxWeHg4S0lJYVlZWcza2pqdPn26y/r3799nxsbG7MyZM1xZaGgoW758eafHZGRkMCMjI1ZVVcWVfffdd4zP57Oamhqlx0ilUmZpaclOnDjBlQFgx44dY4wxVllZyQCwzMzM7rxMooeox2TgDh8+jHfeeQcnT55EeHg4V3727FkMHjyY2w4cOAAHBwdERkZyl02VlZUoKCjotIclN2zYMLi6unKPQ0NDIZPJUFFRAaBtSuJFixbB29sb1tbWsLKygkgkUrlySlBQkLovm+g4CkwGbuzYsXBwcEB2drbCJVpQUBBKS0u5berUqQDaLucOHz4MsViMgwcPws/PD35+fr1qQ1xcHEpLS7FlyxacP38epaWleOGFF7gbUztjSEuGGRoKTAZu+PDhyM/Px9dff42kpCSu3MzMDF5eXtxmaWkJAHjttdfQ3NyMvLw8HDx4UGVvCQDu3LmD6upq7nFhYSH4fD58fHwAAOfOnUNycjKio6Ph6+sLExMT1NfXa/iVEn1CgYlgxIgRyM/Px5EjR1QmXFpYWGDatGn44IMPUF5e3q15uk1NTREXF4eysjKcPXsWycnJmD17NpcC4O3tjc8//xzl5eW4cOECYmNjYWZmpomXRvQUBSYCAPDx8cGPP/6IL774AkuXLu2ybmxsLMrKyhAWFoZhw4apPLeXlxdef/11REdHIzIyEv7+/ti+fTu3f/fu3WhoaMC4ceMwf/58JCcnw9HRsdeviegvmo+J9KlVq1bh+PHjKC0t1XZTiB6hHhMhROdQYCKE6By6lCOE6BzqMRFCdA4FJkKIzqHARAjRORSYCCE6hwITIUTnUGAihOgcCkyEEJ1DgYkQonMoMBFCdA4FJkKIzqHARAjRORSYCCE6hwITIUTnUGAixMBt27YNHh4eMDU1RUhICC5evNhp3atXr2LGjBnw8PAAj8dDZmZmhzpnzpxBTEwMXFxcFBYq7QkKTIQYsJycHKSlpSEjIwOXLl1CQEAAoqKicP/+faX1Gxsb4enpifXr13e6bPuzZ88QEBCAbdu2qd0umo+JEAMWEhKC8ePHIysrCwAgk8ng5uaGpKQkrFixostjPTw8kJqa2uUCFjweD8eOHcO0adN61C7jHtUmhOic5uZmbg0+xhh4PJ7CfhMTE5iYmHQ4rrW1FcXFxVi5ciVXxufzERERgYKCgr5ttAp0KUeIHmtuboaH+2BYW1vD2toaQ4cO5X6Wb+vWrVN6bH19PaRSKYYMGaJQPmTIENTW1vZH8ztFgclArFq1CjweT2cWkjx9+jR4PB4OHz6s7abotdbWVtTdl+JykRMuFzlBJBLh7t27EAqF3PZ8j0hfUGAiatu+fTv27t2rlecWiUTIyMjAlClTYGdnBx6Pp7W26AKjwTIYDZYBAKysrBQ2ZZdxAGBvbw8jIyPU1dUplNfV1XU6sN1fKDARtWkzMNXX12P16tUoLy9HQECAVtqgS5pkPDTJeKorPkcgECAwMBCnTp3iymQyGU6dOoXQ0FBNN7FHaPCb6CVnZ2fU1NTAyckJ//u//4vx48dru0la9Yyp91FOS0tDXFwcgoKCEBwcjMzMTDx79gzx8fEAgAULFsDV1ZUbp2ptbcW1a9e4n+/du4fS0lIMHjwYXl5eANp6szdu3OCeo7KyEqWlpbCzs+vWys0AAEYMQkZGBgPAysvL2axZs5ilpSWzs7NjycnJrKmpSaFudnY2+8Mf/sAcHByYQCBgI0eOZNu3b1eo4+7uzgAobOHh4dz+hoYGlpqaytzd3ZlAIGCurq5s/vz57MGDB4wxxvLz8xkAlpOTw9asWcNcXV2ZiYkJmzRpErt+/XqPXltRUREDwPbs2aPWe6PPhEIhA8C+u/wi++7yiwwAEwqFPTrH1q1b2bBhw5hAIGDBwcGssLCQ2xceHs7i4uK4x5WVlR1+7+1/9/Lfbfvt+fOoQj0mAzN79mx4eHhg3bp1KCwsxCeffIKGhgbs27ePq7Njxw74+vpi6tSpMDY2xokTJ7B48WLIZDIsWbIEAJCZmYmkpCQMHjwYf/vb3wCA+3ZHJBIhLCwM5eXlWLhwIcaNG4f6+nrk5uaiqqoK9vb23HOtX78efD4fy5Ytg1AoxD//+U/ExsbiwoUL/fiu6L9GNkjtYxMTE5GYmKh03+nTpxUee3h4gKlIfZw4caLKOir1KLQSvSXvMU2dOlWhfPHixQwAKysr48oaGxs7HB8VFcU8PT0Vynx9fRX+U8qlp6czAOzo0aMd9slkMsbYb/9VR44cyVpaWrj9W7ZsYQDYzz//3O3XRj0msM9L/NjnJX5q9Zh0EQ1+Gxh5j0cuKSkJAPDtt99yZWZmZtzPQqEQ9fX1CA8Px6+//gqhUKjyOY4cOYKAgABMnz69w772yX/x8fEQCATc47CwMADAr7/+2o1XQ+REzBQiZqrtZmgMBSYD4+3trfB4+PDh4PP5uHXrFld27tw5REREwMLCAjY2NnBwcMB7770HAN0KTDdv3sTo0aO71Z72g6G2trYAgIaGhm4dT9o8k5ngmUx5WoA+ojEmA9e+B3Pz5k1MnjwZL730EjZv3gw3NzcIBAJ8++23+PjjjyGTyTT6/EZGRkrLGd3C2SNNMoHqSnqEApOBuX79Ol588UXu8Y0bNyCTyeDh4QEAOHHiBFpaWpCbm6vQm8nPz+9wrvZBTW748OG4cuWKZhtOutQoHViBiS7lDEz7qSi2bt0KAHjllVcA/NaDeb7HIhQKsWfPng7nsrCwwOPHjzuUz5gxA2VlZTh27FiHfdQT6htNMsGA6jVRj8nAVFZWYurUqZgyZQoKCgqwf/9+zJ07l8uejoyMhEAgQExMDP7yl79AJBLhX//6FxwdHVFTU6NwrsDAQOzYsQNr1qyBl5cXHB0dMWnSJLz77rs4fPgwZs2ahYULFyIwMBCPHj1Cbm4udu7cqbFM7aysLDx+/BjV1dUA2np7VVVVANoG9a2trTXyPPqgSap+uoBOUuervPT0dHbr1i2Nfj1I+pY8XeDatWts5syZzNLSktna2rLExMQOCZa5ubnM39+fmZqaMg8PD7ZhwwaWnZ3NALDKykquXm1tLXv11VeZpaVlhyS7hw8fssTERObq6soEAgEbOnQoi4uLY/X19Yyx39IFvvrqK4Xnlifwdeerf2VJnvLt+XYOZPJ0gbj8N1hc/hsDJl1ArYnixowZgytXriA8PBwJCQmYMWNGpzcKEkL6zpMnT2BtbY3Zp+YDAL6c/DmEQiGsrKy03LLeUWuMqbS0FEVFRfD19UVKSgqcnJzw9ttvo6ioSNPtI4R0Q5NUgKYBNACu9uD32LFj8cknn6C6uhq7d+9GVVUVXn75Zfj7+2PLli3dynchhGhGs8QYzZKBM2Tc62/lGGMQi8VobW0FYwy2trbIysqCm5sbcnJyNNFGQogKLVIjtEiV54TpI7UDU3FxMRITE+Hs7Ix33nkHY8eORXl5Of7zn//g+vXr+Mc//oHk5GSNNfTRo0eIjY2FlZUVbGxskJCQAJFI1OUxEydOBI/HU9j++te/aqxNhOiKFqkxWqQDp8ek1ivx8/PDL7/8gsjISOzevRsxMTEdMnjnzJmDlJQUjTQSAGJjY1FTU4OTJ09CLBYjPj4eb731Fg4ePNjlcYsWLcLq1au5x+bm5hprEyG6QjyALuMANQPT7NmzsXDhQri6unZax97eXmO3L5SXlyMvLw9FRUUICgoC0JYYGB0djU2bNsHFxaXTY83NzbU+TSghfa1VNnAu4wA1ApNYLMbevXsxc+bMLgOTJhUUFMDGxoYLSgAQEREBPp+PCxcuKL2LXe7AgQPYv38/nJycEBMTgw8++KDLXlNLSwtaWlq4xzKZDI8ePcILL7zQ6S0YhGgaYwxPnz6Fi4sL+HzVIy5iiYEHpkGDBqG5ubkv2tKp2tpaODo6KpQZGxvDzs6uy2Vm5s6dC3d3d7i4uODy5ctYvnw5KioqcPTo0U6PWbduHT788EONtZ2Q3rh79y6GDh2qsp5EOrD+aap1KbdkyRJs2LAB//73v2FsrP617YoVK7Bhw4Yu65SXl6t9/rfeeov72c/PD87Ozpg8eTJu3ryJ4cOHKz1m5cqVSEtL4x4LhUIMGzYMQzPeB9904Mx3Q3SbrLkZVR+ugaWlZbfqS8UG3mMCgKKiIpw6dQrff/89/Pz8YGFhobC/qx7J85YuXYo333yzyzqenp5wcnLqsJa6RCLBo0ePejR+FBISAqDtjvrOAlNnq5byTU0pMJF+193hA5lkYN2Pr1ZgsrGxwYwZM3r95A4ODnBwcFBZLzQ0FI8fP0ZxcTECAwMBAD/++CNkMhkXbLqjtLQUQNsKG4QMJExGgUnpFBh9aeTIkZgyZQoWLVqEyZMnY//+/airq4OdnR2qqqrg4uKCe/fuYfLkydi3bx+Cg4Nx8+ZNHDx4EAKBAJ9++inu3r0LABg9ejT8/f37tf2E9DUmUX+Madu2bdi4cSNqa2sREBCArVu3Ijg4WGndq1evIj09HcXFxbh9+zY+/vhjpKam9uqcyqgdZiUSCX744Qfs2rULT58+BQBUV1erTHpU14EDB2BmZoZNmzbh8ePHmDZtGmJiYhAVFYX79+9DLBajoqICjY2NANoW8zty5AhWrFiB27dvw9nZGUFBQaioqKBJzMiAw5PwwFMjOOXk5CAtLQ0ZGRm4dOkSAgICuM+UMo2NjfD09MT69es7HUbp6TmVUSsw3b59G35+fnjttdewZMkSPHjwAACwYcMGLFu2TJ1TqmRnZweJRIIlS5agqakJR48exe7du2Fubo7s7GxuWZmJEycCANzc3ODj44NXX30VEokEt2/fxvnz5zFu3DhkZWX1SRsJ0RoJr23roc2bN2PRokWIj4/HqFGjsHPnTu4zpcz48eOxceNGvPHGG53OKNLTcyqjVmBKSUlBUFAQGhoaFFbUmD59usJyw5rU2tqK4uJiREREcGV8Ph8REREoKChQekxBQYFCfQCIiorqtD7Qlsf05MkThY0QXceT8sD7v5SB9n+/z+flPU+dz5QqmjqnWoHp7NmzeP/99xWW3QHaFsO7d++eOqdUqb6+HlKplFtUUW7IkCGd5jLV1tb2qD7QlsdkbW3NbW5ubr1vPCF97PnA5ObmpvA3LF/euz11PlOqaOqcag1+y2QySKXSDuVVVVXdzrvQVe3zmJ48eULBieg8vvi3y7i7d+8qTBSnj5M4qtVjioyMRGZmJveYx+NBJBIhIyMD0dHRmmqbAnt7exgZGaGurk6hvK6urtNBOCcnpx7VB9p+iVZWVgobIbqOJ2nbAHT4++0sMKnzmVJFU+dUKzB99NFHOHfuHEaNGoXm5mbMnTuXu4xTlcmtLoFAgMDAQIUxLJlMhlOnTiE0NFTpMaGhoR3GvE6ePNlpfUL0FU/atvWEOp+p/jqnWpdyQ4cORVlZGQ4dOoTLly9DJBIhISEBsbGxCoPhmpaWloZ58+bhiy++wNOnT2FtbY2mpibEx8cDABYsWABXV1fumtrLywtr167tkD376aef9lkbCdGGngYlubS0NMTFxSEoKAjBwcHIzMzEs2fPOv1Mtba24tq1a9zP9+7dQ2lpKQYPHgwvL69unbM71L7RzdjYGPPmzVP38F5hjHHrk8kngAOAO3fuKNyJ7e3tDTMzMzg7O+Pu3bt48cUXkZ6e3u3lqwnRF3yJesf96U9/woMHD5Ceno7a2lqMGTMGeXl53OB1+89UdXU1xo4dyz3etGkTNm3ahPDwcJw+fbpb5+wOtVZJ2bdvX5f7FyxY0NNTdktISAjGjx/P5SHJZDK4ubkhKSkJK1as6FB/7969SE1NVbooY3fJV6EYtm4N3StH+o2suRl3Vr6vcsUT+d+nT8paAEDFlvcGxCopavWY2s9MKRaL0djYCIFAAHNz8z4JTPL8iJUrV3Jl3cmPEIlEcHd3h0wmw7hx47B27Vr4+vp2Wr/9fEzyRRVk/TzVCzFs8r+37vYb1O0x6Sq1AlNDQ0OHsuvXr+Ptt9/Gu+++2+tGKdNVfsQvv/yi9BgfHx9kZ2fD398fQqEQmzZtwoQJE3D16tVO57jpbD6mqg/X9P5FENJD8rFUVfhqjjHpKo1NFOzt7Y3169dj3rx5nQaK/hYaGqrwTcCECRMwcuRI7Nq1C3//+9+VHtM+j0nVDJbyPKf2uSNENXrvOvf8DJbdoe7gt67S6AzmxsbG3DrymqaJ/IhBgwZh7NixuHHjRqd1lM3HZGNjo/LclPOkPnrvlOtOT0mOL+7DhmiBWoEpNzdX4TFjDDU1NcjKysLLL7+skYa193x+xLRp0wD8lh+RmJjYrXNIpVL8/PPPfZYESoi28CU9/g5Lp6kVmOSBQY7H48HBwQGTJk3CRx99pIl2KdXTnIvVq1fjd7/7Hby8vPD48WNs3LgRt2/fxp///Oc+ayMh2kCD34DGlmXqqZ7mXDQ0NGDRokWora2Fra0tAgMDcf78eYwaNUpjbTIxMUFGRoZe3o+kbfTeaQ5POrB6TGrlMT0/OKzK5s2be3p6Qkg3yfOYxk9r+9a46Ljq3Cd9oFaPqaSkBJcuXYJEIoGPjw8A4L///S+MjIwwbtw4rh6tw0ZI/6AxJgAxMTGwtLTEZ599BltbWwBtl03x8fEICwvD0qVLNdpIQkjXBlpgUutSztXVFd9//32HDOorV64gMjKyz1IGCCGK5JdyL0e0JQWf+yHDcC/lnjx5ws3z/bwHDx5wCxMQQvoPv1U7X0j1FbXmY5o+fTri4+Nx9OhRVFVVoaqqCkeOHEFCQgJef/11TbdRZ23btg0eHh4wNTVFSEgILl68qO0m6YVVq1Zxs0LIt5deeknbzdJrfIkMfMnACU5q9Zh27tyJZcuWYe7cuRCL21JOjY2NkZCQgI0bN2q0gbpKvkTNzp07ERISgszMTERFRaGiogKOjo7abp7O8/X1xQ8//MA97s1S8wTgiQdOUALU7DGZm5tj+/btePjwIUpKSlBSUoJHjx5h+/btHZYLH6g0sUSNITM2NoaTkxO32dvba7tJeo0vkYIvGTg3zPVqXWELCwv4+/vD39/fYAIS0DfL3hia69evw8XFBZ6enoiNjcWdO3e03SS9xhPLBlSvaWAteN5P+mLZG0MSEhKCvXv3Ii8vDzt27EBlZSXCwsLoi5PeEEvbtgGCLuxJv3vllVe4n/39/RESEgJ3d3d8+eWXSEhI0GLL9BdPPLBulqPApIa+WPbGkNnY2GDEiBFdTkdDVJAMrHlP6FJODX2x7I0hE4lEuHnzJpydnbXdFP3VKmnbBgjqMalJE0vUGKply5YhJiYG7u7uqK6uRkZGBoyMjDBnzhxtN01/tQ6sHhMFJjVpYokaQ1VVVYU5c+bg4cOHcHBwwO9//3sUFhbCwcFB203TW0zcqu0maJRa98oRQnSD/F65SSazAQA/tnxpuPfKEUJ0S2tro7aboFEUmAjRYwKBAE5OTvip9v8BAJycnCAQCLTcqt6jSzlC9FxzczNaW9vGmAQCAUwHwIrRFJgIITqH8pgIITqHAhMhROdQYCKE6BwKTIQQnUOByYBNnDgRqamp2m5Gt/B4PBw/flzbzSD9hPKYiF6oqanhlgojAx8FJqIXVE0nIxaLMWjQoH5qDelrdClHON988w2sra1x4MABpfsnTJiA5cuXK5Q9ePAAgwYNwpkzZ5Qes2rVKowZMwa7du2Cm5sbzM3NMXv2bAiFQq5OUVER/vjHP8Le3h7W1tYIDw/HpUuXFM7z/KXcrVu3wOPxkJOTg/DwcJiamnbaZqKfKDARAMDBgwcxZ84cHDhwALGxsUrrxMbG4tChQ3g+JzcnJwcuLi4ICwvr9Nw3btzAl19+iRMnTiAvLw8lJSVYvHgxt//p06eIi4vDTz/9hMLCQnh7eyM6OlrlVLsrVqxASkoKysvLERUV1cNXTHQaIwYrPDycpaSksKysLGZtbc1Onz7dZf379+8zY2NjdubMGa4sNDSULV++vNNjMjIymJGREauqquLKvvvuO8bn81lNTY3SY6RSKbO0tGQnTpzgygCwY8eOMcYYq6ysZABYZmZmd14m0UPUYzJwhw8fxjvvvIOTJ08iPDycKz979iwGDx7MbQcOHICDgwMiIyO5y6bKykoUFBR02sOSGzZsGFxdXbnHoaGhkMlkqKioANA2JfGiRYvg7e0Na2trWFlZQSQSqVw5JSgoSN2XTXQcBSYDN3bsWDg4OCA7O1vhEi0oKAilpaXcNnXqVABtl3OHDx+GWCzGwYMH4efnBz8/v161IS4uDqWlpdiyZQvOnz+P0tJSvPDCC9yNqZ0xpCXDDA0FJgM3fPhw5Ofn4+uvv0ZSUhJXbmZmBi8vL26ztLQEALz22mtobm5GXl4eDh48qLK3BAB37txBdXU197iwsBB8Ph8+Pj4AgHPnziE5ORnR0dHw9fWFiYkJ6uvrNfxKiT6hwEQwYsQI5Ofn48iRIyoTLi0sLDBt2jR88MEHKC8v79Y83aampoiLi0NZWRnOnj2L5ORkzJ49m0sB8Pb2xueff47y8nJcuHABsbGxMDMz08RLI3qKAhMBAPj4+ODHH3/EF198gaVLl3ZZNzY2FmVlZQgLC8OwYcNUntvLywuvv/46oqOjERkZCX9/f2zfvp3bv3v3bjQ0NGDcuHGYP38+kpOT4ejo2OvXRPQXzcdE+tSqVatw/PhxlJaWarspRI9Qj4kQonMoMBFCdA5dyhFCdA71mAghOocCEyFE51BgIoToHApMhBCdQ4GJEKJzKDARQnQOBSZCiM6hwEQI0Tn/H550D0mndeHqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''没有使用valid_lens, 且query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for batch in range(weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 案例-李沐\n",
    "  - 掩码 (masked) 和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import math\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    def __init__(self, query_size, key_size, value_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # queries的形状：(batch_size，查询的个数，d)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens) \n",
    "        # keys的形状：(batch_size，“键－值”对的个数，d)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens)\n",
    "        # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "        self.w_v = nn.Linear(value_size, num_hiddens)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        queries, keys, values = self.W_q(queries), self.W_k(keys), self.w_v(values)\n",
    "        # queries: (batch_size, num_query, num_hiddens)\n",
    "        # keys: (batch_size, k_v_pair_num, num_hiddens)\n",
    "        # values: (batch_size, k_v_pair_num, value_size)\n",
    "        d = queries.shape[-1]\n",
    "\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        # (batch_size, num_query, num_hiddens) @ (batch_size, num_hiddens, k_v_pair_num) = (batch_size, num_query, k_v_pair_num)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "\n",
    "        # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n",
    "        # 使用masked_softmax计算注意力权重\n",
    "        # attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\n",
    "        # (batch_size, num_query, k_v_pair_num) @ (batch_size, k_v_pair_num, value_size) = (batch_size, num_query, value_size)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "    \n",
    "\n",
    "# Test   \n",
    "batch_size, num_query, query_size = 2, 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4\n",
    "num_hiddens = 8\n",
    "dropout = 0.1\n",
    "\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "keys = torch.ones(size=(batch_size, k_v_pair_num, key_size))\n",
    "values = torch.randn(size=(batch_size, k_v_pair_num, value_size))\n",
    "\n",
    "attention = DotProductAttention(query_size=query_size, key_size=key_size, value_size=value_size, num_hiddens=num_hiddens, dropout=dropout)\n",
    "attention.eval()\n",
    "\n",
    "# batch_size, 1\n",
    "## 依次的每个批次中所有num_steps有效个数一致\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention(queries, keys, values, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''valid_lens：[2, 6], 切query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for batch in range(attention.attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention.attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (详细) 从头手写\n",
    "  - 无掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries size:  torch.Size([2, 1, 4])\n",
      "keys size:  torch.Size([2, 10, 4])\n",
      "values size:  torch.Size([2, 10, 4])\n",
      "features size: torch.Size([2, 1, 10])\n",
      "scores size: torch.Size([2, 1, 10])\n",
      "attention_weights size: torch.Size([2, 1, 10])\n",
      "attention size: torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "# 示例数据  \n",
    "batch_size = 2\n",
    "num_query = 1\n",
    "query_size = 4             # 一个query的向量长度，d\n",
    "\n",
    "num_key = 10                    # “键－值”对的个数，m\n",
    "key_size = 4                # 一个key的向量长度，d\n",
    "\n",
    "num_value = num_key             # “键－值”对的个数，m\n",
    "value_size = 4              # 一个value的向量长度，v\n",
    "\n",
    "# queries = torch.randn(size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "print('queries size: ', queries.size())\n",
    "# (batch_size, num_query, query_size)\n",
    "# (2, 1, 20)\n",
    "\n",
    "keys = torch.ones(size=(batch_size, num_key, key_size))\n",
    "print('keys size: ', keys.size())\n",
    "# (batch_size, num_key, key_size)\n",
    "# (2, 10, 20)\n",
    "\n",
    "values = torch.randn(size=(batch_size, num_value, value_size))\n",
    "print('values size: ', values.size())\n",
    "# (batch_size, num_value, value_size)\n",
    "# (2, 10, 4)\n",
    "\n",
    "features = (queries @ keys.transpose(1, 2)) \n",
    "# features = torch.bmm(queries, keys.transpose(1, 2))               # 同上，都可以\n",
    "# (2, 1, 20) @ (2, 20, 10) = (2, 1, 10)\n",
    "print(f'features size: {features.shape}')\n",
    "\n",
    "scores = features / torch.sqrt(torch.tensor(queries.shape[2]))\n",
    "# (2, 1, 10)\n",
    "print(f'scores size: {scores.shape}')\n",
    "\n",
    "attention_weights = torch.softmax(scores, dim=-1)\n",
    "# (2, 1, 10) / 标量 = (2, 1, 10)\n",
    "print(f'attention_weights size: {attention_weights.shape}')\n",
    "\n",
    "attention = torch.bmm(attention_weights, values)\n",
    "# (2, 1, 4)\n",
    "print(f'attention size: {attention.shape}')\n",
    "\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.6. <a id='toc11_5_6_'></a>[自注意力机制-q、k和v相同](#toc0_)\n",
    "\n",
    "自注意力机制：就是用同一个`X`分别于`W_q`、`W_k`和`W_v`矩阵相乘得到`Q`、`K`和`V` `向量/矩阵`。因为用的是同一个X同时作为q、k和v，所以得名为 `自注意力` 。\n",
    "\n",
    "- 使用：\n",
    "```python\n",
    "# self-attention:                       queries = keys = values\n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import time \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "dbs = './data/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(dataset=train_dataset, batch_size=128,  shuffle=True)\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构1:CNN\n",
    "class MLPMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "    \n",
    "# 构建模型2:Additive attention\n",
    "class SelfAttentionMNISTModel(nn.Module):\n",
    "    def __init__(self, attention_type):\n",
    "        super().__init__()\n",
    "        # 输入层\n",
    "        self.input = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU())\n",
    "\n",
    "        # 注意力层\n",
    "        if attention_type == 'add':\n",
    "            self.attention = AdditiveAttention(query_size=128, key_size=128, num_hiddens=128, dropout=False)\n",
    "        elif attention_type == 'dot':\n",
    "            self.attention = DotProductAttention(query_size=128, key_size=128, value_size=128, num_hiddens=128, dropout=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid attention type: {attention_type}\")\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 28*28)\n",
    "        x = self.input(x)\n",
    "        ## x: (batch_size, 128) -> (batch_size, 1, 128)\n",
    "        x = x.unsqueeze(1)  # dim=1增加一个维度，即表示一个query维度是(1, 128)\n",
    "\n",
    "        # 自注意力即q、k、v相同\n",
    "        ## queries: (batch_size, 1, 128)\n",
    "        ## keys: (batch_size, 1, 128)\n",
    "        ## values: (batch_size, 1, 128)\n",
    "        ## 输出维度(batch_size, num_query, value_size) -> (batch_size, 1, 128) -> (batch_size, 128)\n",
    "        x = self.attention(queries=x, keys=x, values=x).squeeze(1) # dim=1的维度取消，只剩128\n",
    "\n",
    "        # 输出层\n",
    "        ## x: (batch_size, 128) -> (batch_size, 10)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "    print('='*100, '\\n', f\"Runing on {device}\", '\\n','='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "    net = nn.DataParallel(module=net).to(device)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上      \n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)# 计算loss\n",
    "            loss.backward()         # 计算梯度\n",
    "            opt.step()              # 更新网络参数\n",
    "        net.eval() \n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=1.6299532651901245, train_acc=84.07333374023438, test_acc=84.68000030517578\n",
      "epoch 2/10: train_loss=1.5488343238830566, train_acc=92.30000305175781, test_acc=92.41999816894531\n",
      "epoch 3/10: train_loss=1.5348297357559204, train_acc=93.49166870117188, test_acc=93.33999633789062\n",
      "epoch 4/10: train_loss=1.5229099988937378, train_acc=94.55833435058594, test_acc=94.40999603271484\n",
      "epoch 5/10: train_loss=1.516686201095581, train_acc=95.17333221435547, test_acc=94.77999877929688\n",
      "epoch 6/10: train_loss=1.5113195180892944, train_acc=95.60833740234375, test_acc=95.30999755859375\n",
      "epoch 7/10: train_loss=1.5061757564544678, train_acc=96.09833526611328, test_acc=95.47999572753906\n",
      "epoch 8/10: train_loss=1.5018916130065918, train_acc=96.5, test_acc=95.88999938964844\n",
      "epoch 9/10: train_loss=1.4986926317214966, train_acc=96.80667114257812, test_acc=96.13999938964844\n",
      "epoch 10/10: train_loss=1.4951783418655396, train_acc=97.11499786376953, test_acc=96.56999969482422\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 7.142648935317993 s\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "net = MLPMNISTModel()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)  \n",
    "\n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=0.1544727087020874, train_acc=95.5133285522461, test_acc=95.36000061035156\n",
      "epoch 2/10: train_loss=0.09661002457141876, train_acc=97.17333221435547, test_acc=96.9000015258789\n",
      "epoch 3/10: train_loss=0.0762743130326271, train_acc=97.77999877929688, test_acc=97.19000244140625\n",
      "epoch 4/10: train_loss=0.06407824158668518, train_acc=98.1433334350586, test_acc=97.38999938964844\n",
      "epoch 5/10: train_loss=0.050450365990400314, train_acc=98.53333282470703, test_acc=97.56999969482422\n",
      "epoch 6/10: train_loss=0.04177556186914444, train_acc=98.8499984741211, test_acc=97.73999786376953\n",
      "epoch 7/10: train_loss=0.035748060792684555, train_acc=98.9816665649414, test_acc=97.78999328613281\n",
      "epoch 8/10: train_loss=0.029548468068242073, train_acc=99.22500610351562, test_acc=97.90999603271484\n",
      "epoch 9/10: train_loss=0.03091316670179367, train_acc=99.08833312988281, test_acc=97.81999206542969\n",
      "epoch 10/10: train_loss=0.021095028147101402, train_acc=99.52166748046875, test_acc=98.09999084472656\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 11.929174184799194 s\n"
     ]
    }
   ],
   "source": [
    "# AdditiveAttention\n",
    "net = SelfAttentionMNISTModel(attention_type='add')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)  \n",
    "\n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=0.14006046950817108, train_acc=95.78666687011719, test_acc=95.30999755859375\n",
      "epoch 2/10: train_loss=0.09201224893331528, train_acc=97.14500427246094, test_acc=96.58000183105469\n",
      "epoch 3/10: train_loss=0.06664934754371643, train_acc=97.98999786376953, test_acc=96.9000015258789\n",
      "epoch 4/10: train_loss=0.05251000449061394, train_acc=98.38166809082031, test_acc=97.33999633789062\n",
      "epoch 5/10: train_loss=0.03569905459880829, train_acc=98.9433364868164, test_acc=97.63999938964844\n",
      "epoch 6/10: train_loss=0.04038134589791298, train_acc=98.75167083740234, test_acc=97.36000061035156\n",
      "epoch 7/10: train_loss=0.036739956587553024, train_acc=98.79500579833984, test_acc=97.08000183105469\n",
      "epoch 8/10: train_loss=0.04800890013575554, train_acc=98.45832824707031, test_acc=96.93000030517578\n",
      "epoch 9/10: train_loss=0.017892982810735703, train_acc=99.45500183105469, test_acc=97.7699966430664\n",
      "epoch 10/10: train_loss=0.03470558673143387, train_acc=98.79500579833984, test_acc=97.11000061035156\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 42.17068266868591 s\n"
     ]
    }
   ],
   "source": [
    "# DotProductAttention\n",
    "net = SelfAttentionMNISTModel(attention_type='dot')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)  \n",
    "\n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.7. <a id='toc11_5_7_'></a>[多头注意力机制-h个q、k和v对](#toc0_)\n",
    "上述只求一次注意力的过程可以叫做单头注意力。多头注意力就是对同样的Q, K, V求多次注意力，并行计算h个得到h个不同的attention，再把这些不同的h个attention连接起来得到最终的attentions，每一个attention都是一个head（头），总共有h个head（头）。  \n",
    "\n",
    "<div style=\"display:flex;justify-content:center\">\n",
    "<img src=\"./Pytorch_Pictures/Attention/multi_head.jpg\" width = \"300\" height = \"400\" alt=\"多头注意力机制\">\n",
    "</div>\n",
    "\n",
    "在实现过程中通常选择`缩放点积注意力`作为每一个注意力头，除以根号d可以使计算数值减小。\n",
    "\n",
    "```python\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "\n",
    "# 先 transpose_input()\n",
    "# 后 transpose_output()\n",
    "# 终 self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=exitBias)   # 最后concat所有head的结果 (其实就是投影)\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 简洁实现  \n",
    "query_size = key_size = value_size = d  \n",
    "nn.MultiheadAttention(embed_dim=value_size, num_heads=num_heads, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 1, 4])\n",
      "注意力权重形状: torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 定义参数\n",
    "batch_size = 2\n",
    "query_num, query_size = 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4\n",
    "\n",
    "num_heads = 2\n",
    "# 创建多头注意力模块\n",
    "multihead_attention = nn.MultiheadAttention(embed_dim=value_size, num_heads=num_heads, batch_first=True)\n",
    "# 使用缩放点积注意力机制，所以 query_size = key_size = value_size = d\n",
    "\n",
    "# 创建输入张量\n",
    "# 输入形状为 (批次大小, 序列长度, 嵌入维度)\n",
    "query = torch.ones(batch_size, query_num, query_size)\n",
    "key = torch.ones(batch_size, k_v_pair_num, key_size)\n",
    "value = torch.rand(batch_size, k_v_pair_num, value_size)\n",
    "\n",
    "# 可选的注意力掩码\n",
    "# mask = torch.zeros(batch_size, seq_length, seq_length).type(torch.bool)\n",
    "\n",
    "# 计算多头注意力\n",
    "# 如果需要掩码，可以传入 mask 参数\n",
    "output, attention_weights = multihead_attention(query, key, value)\n",
    "\n",
    "print(\"输出形状:\", output.shape)  # 输出形状: (batch_size, query_num, value_size)\n",
    "print(\"注意力权重形状:\", attention_weights.shape)  # 注意力权重形状: (batch_size, num_query, k_v_pair_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 案例-李沐 (修改)\n",
    "  - 去除掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (W_q): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (W_k): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (W_v): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (attention): AdditiveAttentionForMultiHeadAttention(\n",
      "    (dropout): Dropout(p=False, inplace=False)\n",
      "    (w_v): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "raw queries size:  torch.Size([2, 1, 4])\n",
      "raw keys size:  torch.Size([2, 10, 4])\n",
      "raw values size:  torch.Size([2, 10, 4])\n",
      "attention_values size:  torch.Size([2, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn  \n",
    "import math \n",
    "\n",
    "\n",
    "# 加性注意力\n",
    "class AdditiveAttentionForMultiHeadAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # queries:              (batch_size, num_query, num_hiddens)\n",
    "        # keys:                 (batch_size, k_v_pair_num, num_hiddens)\n",
    "        # 在维度扩展后，\n",
    "        # queries的形状：       (batch_size，num_query，        1，        num_hiddens)\n",
    "        # key的形状：           (batch_size，    1，    k_v_pair_num，  num_hiddens)\n",
    "        # 使用广播方式进行求和  (batch_size, num_query, 1, num_hiddens) + (batch_size, 1, k_v_pair_num, num_hiddens) = (batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # features的形状：(batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        \n",
    "        # self.w_v: (num_hiddens, 1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num, 1)\n",
    "        # 移除最后一个维度squeeze(-1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "\n",
    "        # 注意力权重\n",
    "        # 使用masked_softmax计算注意力权重, 有效长度为valid_lens\n",
    "        # attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\n",
    "        # values的形状：(batch_size，k_v_pair_num，value_size)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "# 点积注意力\n",
    "class DotProductAttentionForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dropout=False):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries, keys, values = self.W_q(queries), self.W_k(keys), self.w_v(values)\n",
    "        # queries: (batch_size, num_query, num_hiddens)\n",
    "        # keys: (batch_size, k_v_pair_num, num_hiddens)\n",
    "        # values: (batch_size, k_v_pair_num, value_size)\n",
    "        d = queries.shape[-1]\n",
    "\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        # (batch_size, num_query, num_hiddens) @ (batch_size, num_hiddens, k_v_pair_num) = (batch_size, num_query, k_v_pair_num)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "\n",
    "        # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n",
    "        # 使用masked_softmax计算注意力权重\n",
    "        # attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\n",
    "        # (batch_size, num_query, k_v_pair_num) @ (batch_size, k_v_pair_num, value_size) = (batch_size, num_query, value_size)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "# 多头注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, query_size, key_size, num_hiddens, value_size, attention_type, dropout=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens)\n",
    "        if num_hiddens % num_heads != 0:\n",
    "            # raise ValueError(f'num_hiddens must be divisible by num_heads, but got num_hiddens={num_hiddens} and num_heads={num_heads}')\n",
    "            raise ValueError(f'num_hiddens必须能整除num_heads，但是接受的num_hiddens={num_hiddens}，num_heads={num_heads}')\n",
    "        \n",
    "        # 选择注意力机制\n",
    "        if attention_type == 'add':\n",
    "            # self.attention = AdditiveAttention(query_size=query_size, key_size=key_size, num_hiddens=num_hiddens, dropout=dropout)\n",
    "            self.attention = AdditiveAttentionForMultiHeadAttention(num_hiddens=int(num_hiddens/num_heads), dropout=dropout)\n",
    "        elif attention_type == 'dot':\n",
    "            # self.attention = DotProductAttention(query_size=query_size, key_size=key_size, value_size=value_size, num_hiddens=num_hiddens, dropout=dropout)\n",
    "            self.attention = DotProductAttentionForMultiHeadAttention(dropout=dropout)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid attention type: {attention_type}')        \n",
    "\n",
    "        # 最后concat所有head的结果 (其实就是投影)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens)   \n",
    "\n",
    "    def transpose_input(self, X, num_heads):\n",
    "        \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "        # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        # 输出X的形状:(batch_size，查询或者“键－值”对的个数，`num_heads`，num_hiddens/num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "        # 输出X的形状:(batch_size，`num_heads`，查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)                                                                   # 调整顺序以便做广播 (向量化并行计算multi heads)\n",
    "\n",
    "        # 最终输出的形状:(batch_size*`num_heads`,查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def transpose_output(self, X, num_heads):\n",
    "        \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "        # 输入X的形状:(batch_size*`num_heads`,查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        # 输出X的形状:(batch_size,`num_heads``,查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "\n",
    "        # 输出X的形状:(batch_size,查询或者“键－值”对的个数，`num_heads`,num_hiddens/num_heads)                                        # 不改变顺序\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "        # 最终输出X的形状:(batch_size,查询或者“键－值”对的个数，num_hiddens)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries: (batch_size, num_query, query_size)\n",
    "        queries = self.W_q(queries)\n",
    "        # queries: (batch_size, num_query, num_hiddens)\n",
    "        queries = self.transpose_input(queries, self.num_heads)\n",
    "        # queries: (batch_size*num_heads, num_query, num_hiddens/num_heads)\n",
    "\n",
    "        # keys: (batch_size, k_v_pair_num, key_size)    \n",
    "        keys = self.W_k(keys)\n",
    "        # keys: (batch_size, k_v_pair_num, num_hiddens)\n",
    "        keys = self.transpose_input(keys, self.num_heads)\n",
    "        # keys: (batch_size*num_heads, k_v_pair_num, num_hiddens/num_heads)\n",
    "\n",
    "        # values: (batch_size, k_v_pair_num, value_size)\n",
    "        values = self.W_v(values)    \n",
    "        # values: (batch_size, k_v_pair_num, num_hiddens)     \n",
    "        values = self.transpose_input(values, self.num_heads)\n",
    "        # values: (batch_size*num_heads, k_v_pair_num, num_hiddens/num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在0轴，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，num_query，num_hiddens/num_heads)\n",
    "        output = self.attention(queries=queries, keys=keys, values=values, valid_lens=valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，num_query，num_hiddens) \n",
    "        output_concat = self.transpose_output(output, self.num_heads)\n",
    "\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "\n",
    "# Test\n",
    "batch_size = 2\n",
    "num_query, query_size = 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4 \n",
    "num_hiddens = 128\n",
    "num_heads = 2\n",
    "\n",
    "# 实例化\n",
    "multiHeadAttention = MultiHeadAttention(\n",
    "    num_heads = num_heads, \n",
    "    query_size = query_size, \n",
    "    key_size = key_size, \n",
    "    num_hiddens = num_hiddens, \n",
    "    value_size = value_size,\n",
    "    attention_type='add'\n",
    ").eval()\n",
    "print(multiHeadAttention)\n",
    "\n",
    "# 传参\n",
    "query = torch.ones((batch_size, num_query, query_size)); print('raw queries size: ', query.shape)\n",
    "key = torch.ones((batch_size, k_v_pair_num, key_size)); print('raw keys size: ', key.shape)\n",
    "value = torch.randn((batch_size, k_v_pair_num, value_size)); print('raw values size: ', value.shape)\n",
    "\n",
    "attention_values = multiHeadAttention(queries=query, keys=key, values=value); print('attention_values size: ', attention_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiHeadAttention.attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从头手写\n",
    "  - 无掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=0.21716535091400146, train_acc=93.36000061035156, test_acc=93.3699951171875\n",
      "epoch 2/10: train_loss=0.11867091804742813, train_acc=96.30000305175781, test_acc=95.58000183105469\n",
      "epoch 3/10: train_loss=0.0838998481631279, train_acc=97.4316635131836, test_acc=96.55999755859375\n",
      "epoch 4/10: train_loss=0.06451273709535599, train_acc=97.9366683959961, test_acc=96.7300033569336\n",
      "epoch 5/10: train_loss=0.05004078894853592, train_acc=98.44499969482422, test_acc=97.25999450683594\n",
      "epoch 6/10: train_loss=0.05150056630373001, train_acc=98.33833312988281, test_acc=96.93000030517578\n",
      "epoch 7/10: train_loss=0.13189047574996948, train_acc=96.13666534423828, test_acc=94.75\n",
      "epoch 8/10: train_loss=0.04993750527501106, train_acc=98.40999603271484, test_acc=96.97999572753906\n",
      "epoch 9/10: train_loss=0.04037073999643326, train_acc=98.58333587646484, test_acc=96.95999908447266\n",
      "epoch 10/10: train_loss=0.032772768288850784, train_acc=98.90666961669922, test_acc=97.23999786376953\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 48.20983099937439 s\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn  \n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "dbs = './data/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(dataset=train_dataset, batch_size=128,  shuffle=True)\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练\n",
    "\n",
    "  \n",
    "# 4\n",
    "class MultiHeadAttentionMNISTModel(nn.Module):\n",
    "    def __init__(self, num_heads, attention_type):\n",
    "        super().__init__()\n",
    "        self.input = nn.Sequential(\n",
    "                nn.Flatten(), \n",
    "                nn.Linear(28 * 28, 128),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        num_hiddens = 128\n",
    "        # 多头注意力机制 from PyTorch\n",
    "        # self.attention = nn.MultiheadAttention(embed_dim=num_hiddens, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # 选择注意力机制 from 自己\n",
    "        if attention_type == 'add': \n",
    "            self.attention = MultiHeadAttention(\n",
    "                num_heads = num_heads, \n",
    "                query_size = num_hiddens, \n",
    "                key_size = num_hiddens, \n",
    "                num_hiddens = num_hiddens, \n",
    "                value_size = num_hiddens,\n",
    "                attention_type='add'\n",
    "            )\n",
    "        elif attention_type == 'dot':\n",
    "            self.attention = MultiHeadAttention(\n",
    "                num_heads = num_heads, \n",
    "                query_size = num_hiddens, \n",
    "                key_size = num_hiddens, \n",
    "                num_hiddens = num_hiddens, \n",
    "                value_size = num_hiddens,\n",
    "                attention_type='dot'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f'Invalid attention type: {attention_type}')\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.input(X)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.attention(queries=x, keys=x, values=x)\n",
    "        # x, weights = self.attention(query=x, key=x, value=x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "    print('='*100, '\\n', f\"Runing on {device}\", '\\n','='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "    net = nn.DataParallel(module=net).to(device)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)# 计算loss\n",
    "            loss.backward()         # 计算梯度\n",
    "            opt.step()              # 更新网络参数\n",
    "\n",
    "        net.eval()  # 切换至评估模式\n",
    "                    # 模型默认是net.train()\n",
    "                    # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                    # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None\n",
    "\n",
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "net = MultiHeadAttentionMNISTModel(num_heads=2, attention_type='add')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=1.8220118284225464, train_acc=61.48666763305664, test_acc=62.11000061035156\n",
      "epoch 2/10: train_loss=0.7363181114196777, train_acc=78.43500518798828, test_acc=78.79999542236328\n",
      "epoch 3/10: train_loss=0.5229008793830872, train_acc=84.62333679199219, test_acc=85.0199966430664\n",
      "epoch 4/10: train_loss=0.4330369532108307, train_acc=87.66333770751953, test_acc=87.90999603271484\n",
      "epoch 5/10: train_loss=0.38493847846984863, train_acc=89.1483383178711, test_acc=89.30999755859375\n",
      "epoch 6/10: train_loss=0.3535473048686981, train_acc=89.9816665649414, test_acc=90.22000122070312\n",
      "epoch 7/10: train_loss=0.33060142397880554, train_acc=90.60833740234375, test_acc=90.83000183105469\n",
      "epoch 8/10: train_loss=0.3097841143608093, train_acc=91.2366714477539, test_acc=91.43999481201172\n",
      "epoch 9/10: train_loss=0.2935551404953003, train_acc=91.54833221435547, test_acc=91.89999389648438\n",
      "epoch 10/10: train_loss=0.2762805223464966, train_acc=92.0999984741211, test_acc=92.28999328613281\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 44.652228116989136 s\n"
     ]
    }
   ],
   "source": [
    "net = MultiHeadAttentionMNISTModel(num_heads=2, attention_type='dot')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.01)   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DotProductAttentionForMultiHeadAttention' object has no attribute 'attention_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_weights\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deeplearning/lib/python3.12/site-packages/torch/nn/modules/module.py:1729\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1728\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1729\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DotProductAttentionForMultiHeadAttention' object has no attribute 'attention_weights'"
     ]
    }
   ],
   "source": [
    "net.attention.attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (0)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"449pt\" height=\"1318pt\"\n",
       " viewBox=\"0.00 0.00 449.21 1318.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(.7902 .7902) rotate(0) translate(4 1664)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1664 564.5,-1664 564.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"#ffffe0\" stroke=\"transparent\" points=\"347.5,-1660 211.5,-1660 211.5,-1628 347.5,-1628 347.5,-1660\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"211.5,-1628 211.5,-1660 281.5,-1660 281.5,-1628 211.5,-1628\"/>\n",
       "<text text-anchor=\"start\" x=\"216.5\" y=\"-1647\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"228\" y=\"-1636\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"281.5,-1628 281.5,-1660 347.5,-1660 347.5,-1628 281.5,-1628\"/>\n",
       "<text text-anchor=\"start\" x=\"286.5\" y=\"-1641.5\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 28, 28)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"361.5,-1592 197.5,-1592 197.5,-1550 361.5,-1550 361.5,-1592\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"197.5,-1550 197.5,-1592 244.5,-1592 244.5,-1550 197.5,-1550\"/>\n",
       "<text text-anchor=\"start\" x=\"204\" y=\"-1574\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Flatten</text>\n",
       "<text text-anchor=\"start\" x=\"202.5\" y=\"-1563\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"244.5,-1571 244.5,-1592 292.5,-1592 292.5,-1571 244.5,-1571\"/>\n",
       "<text text-anchor=\"start\" x=\"254.5\" y=\"-1579\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"292.5,-1571 292.5,-1592 361.5,-1592 361.5,-1571 292.5,-1571\"/>\n",
       "<text text-anchor=\"start\" x=\"297.5\" y=\"-1579\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"244.5,-1550 244.5,-1571 292.5,-1571 292.5,-1550 244.5,-1550\"/>\n",
       "<text text-anchor=\"start\" x=\"249.5\" y=\"-1558\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"292.5,-1550 292.5,-1571 361.5,-1571 361.5,-1550 292.5,-1550\"/>\n",
       "<text text-anchor=\"start\" x=\"303.5\" y=\"-1558\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 784) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1627.7989C279.5,-1620.2512 279.5,-1611.0055 279.5,-1602.1832\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-1602.1483 279.5,-1592.1483 276.0001,-1602.1484 283.0001,-1602.1483\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"355.5,-1514 203.5,-1514 203.5,-1472 355.5,-1472 355.5,-1514\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"203.5,-1472 203.5,-1514 250.5,-1514 250.5,-1472 203.5,-1472\"/>\n",
       "<text text-anchor=\"start\" x=\"211.5\" y=\"-1496\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"208.5\" y=\"-1485\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"250.5,-1493 250.5,-1514 298.5,-1514 298.5,-1493 250.5,-1493\"/>\n",
       "<text text-anchor=\"start\" x=\"260.5\" y=\"-1501\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"298.5,-1493 298.5,-1514 355.5,-1514 355.5,-1493 298.5,-1493\"/>\n",
       "<text text-anchor=\"start\" x=\"303.5\" y=\"-1501\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 784) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"250.5,-1472 250.5,-1493 298.5,-1493 298.5,-1472 250.5,-1472\"/>\n",
       "<text text-anchor=\"start\" x=\"255.5\" y=\"-1480\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"298.5,-1472 298.5,-1493 355.5,-1493 355.5,-1472 298.5,-1472\"/>\n",
       "<text text-anchor=\"start\" x=\"303.5\" y=\"-1480\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 128) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1549.6862C279.5,-1541.7975 279.5,-1532.684 279.5,-1524.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-1524.018 279.5,-1514.018 276.0001,-1524.018 283.0001,-1524.018\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"355.5,-1436 203.5,-1436 203.5,-1394 355.5,-1394 355.5,-1436\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"203.5,-1394 203.5,-1436 250.5,-1436 250.5,-1394 203.5,-1394\"/>\n",
       "<text text-anchor=\"start\" x=\"214.5\" y=\"-1418\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"208.5\" y=\"-1407\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"250.5,-1415 250.5,-1436 298.5,-1436 298.5,-1415 250.5,-1415\"/>\n",
       "<text text-anchor=\"start\" x=\"260.5\" y=\"-1423\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"298.5,-1415 298.5,-1436 355.5,-1436 355.5,-1415 298.5,-1415\"/>\n",
       "<text text-anchor=\"start\" x=\"303.5\" y=\"-1423\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"250.5,-1394 250.5,-1415 298.5,-1415 298.5,-1394 250.5,-1394\"/>\n",
       "<text text-anchor=\"start\" x=\"255.5\" y=\"-1402\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"298.5,-1394 298.5,-1415 355.5,-1415 355.5,-1394 298.5,-1394\"/>\n",
       "<text text-anchor=\"start\" x=\"303.5\" y=\"-1402\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 128) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1471.6862C279.5,-1463.7975 279.5,-1454.684 279.5,-1446.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-1446.018 279.5,-1436.018 276.0001,-1446.018 283.0001,-1446.018\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"369.5,-1358 189.5,-1358 189.5,-1316 369.5,-1316 369.5,-1358\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"189.5,-1316 189.5,-1358 252.5,-1358 252.5,-1316 189.5,-1316\"/>\n",
       "<text text-anchor=\"start\" x=\"194.5\" y=\"-1340\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">unsqueeze</text>\n",
       "<text text-anchor=\"start\" x=\"202.5\" y=\"-1329\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"252.5,-1337 252.5,-1358 300.5,-1358 300.5,-1337 252.5,-1337\"/>\n",
       "<text text-anchor=\"start\" x=\"262.5\" y=\"-1345\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"300.5,-1337 300.5,-1358 369.5,-1358 369.5,-1337 300.5,-1337\"/>\n",
       "<text text-anchor=\"start\" x=\"311.5\" y=\"-1345\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"252.5,-1316 252.5,-1337 300.5,-1337 300.5,-1316 252.5,-1316\"/>\n",
       "<text text-anchor=\"start\" x=\"257.5\" y=\"-1324\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"300.5,-1316 300.5,-1337 369.5,-1337 369.5,-1316 300.5,-1316\"/>\n",
       "<text text-anchor=\"start\" x=\"305.5\" y=\"-1324\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1393.6862C279.5,-1385.7975 279.5,-1376.684 279.5,-1368.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-1368.018 279.5,-1358.018 276.0001,-1368.018 283.0001,-1368.018\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"361.5,-1280 197.5,-1280 197.5,-1238 361.5,-1238 361.5,-1280\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"197.5,-1238 197.5,-1280 244.5,-1280 244.5,-1238 197.5,-1238\"/>\n",
       "<text text-anchor=\"start\" x=\"205.5\" y=\"-1262\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"202.5\" y=\"-1251\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"244.5,-1259 244.5,-1280 292.5,-1280 292.5,-1259 244.5,-1259\"/>\n",
       "<text text-anchor=\"start\" x=\"254.5\" y=\"-1267\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"292.5,-1259 292.5,-1280 361.5,-1280 361.5,-1259 292.5,-1259\"/>\n",
       "<text text-anchor=\"start\" x=\"297.5\" y=\"-1267\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"244.5,-1238 244.5,-1259 292.5,-1259 292.5,-1238 244.5,-1238\"/>\n",
       "<text text-anchor=\"start\" x=\"249.5\" y=\"-1246\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"292.5,-1238 292.5,-1259 361.5,-1259 361.5,-1238 292.5,-1238\"/>\n",
       "<text text-anchor=\"start\" x=\"297.5\" y=\"-1246\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1315.6862C279.5,-1307.7975 279.5,-1298.684 279.5,-1290.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-1290.018 279.5,-1280.018 276.0001,-1290.018 283.0001,-1290.018\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"547.5,-1280 383.5,-1280 383.5,-1238 547.5,-1238 547.5,-1280\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"383.5,-1238 383.5,-1280 430.5,-1280 430.5,-1238 383.5,-1238\"/>\n",
       "<text text-anchor=\"start\" x=\"391.5\" y=\"-1262\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"388.5\" y=\"-1251\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"430.5,-1259 430.5,-1280 478.5,-1280 478.5,-1259 430.5,-1259\"/>\n",
       "<text text-anchor=\"start\" x=\"440.5\" y=\"-1267\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"478.5,-1259 478.5,-1280 547.5,-1280 547.5,-1259 478.5,-1259\"/>\n",
       "<text text-anchor=\"start\" x=\"483.5\" y=\"-1267\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"430.5,-1238 430.5,-1259 478.5,-1259 478.5,-1238 430.5,-1238\"/>\n",
       "<text text-anchor=\"start\" x=\"435.5\" y=\"-1246\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"478.5,-1238 478.5,-1259 547.5,-1259 547.5,-1238 478.5,-1238\"/>\n",
       "<text text-anchor=\"start\" x=\"483.5\" y=\"-1246\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M329.8343,-1315.8921C353.1992,-1306.0939 381.1513,-1294.372 405.5955,-1284.1213\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"407.2571,-1287.2198 415.1255,-1280.1248 404.55,-1280.7644 407.2571,-1287.2198\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"172.5,-1202 8.5,-1202 8.5,-1160 172.5,-1160 172.5,-1202\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8.5,-1160 8.5,-1202 55.5,-1202 55.5,-1160 8.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"16.5\" y=\"-1184\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"13.5\" y=\"-1173\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"55.5,-1181 55.5,-1202 103.5,-1202 103.5,-1181 55.5,-1181\"/>\n",
       "<text text-anchor=\"start\" x=\"65.5\" y=\"-1189\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"103.5,-1181 103.5,-1202 172.5,-1202 172.5,-1181 103.5,-1181\"/>\n",
       "<text text-anchor=\"start\" x=\"108.5\" y=\"-1189\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"55.5,-1160 55.5,-1181 103.5,-1181 103.5,-1160 55.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"60.5\" y=\"-1168\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"103.5,-1160 103.5,-1181 172.5,-1181 172.5,-1160 103.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"108.5\" y=\"-1168\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;13 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M241.4329,-1315.836C224.6031,-1305.8128 204.9455,-1293.1642 188.5,-1280 161.7134,-1258.558 134.5176,-1230.4757 115.5933,-1209.6989\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.0589,-1207.2067 108.7635,-1202.1227 112.8597,-1211.8937 118.0589,-1207.2067\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"366,-1202 193,-1202 193,-1160 366,-1160 366,-1202\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"193.5,-1160 193.5,-1202 243.5,-1202 243.5,-1160 193.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"198.5\" y=\"-1184\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"200\" y=\"-1173\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"243.5,-1181 243.5,-1202 291.5,-1202 291.5,-1181 243.5,-1181\"/>\n",
       "<text text-anchor=\"start\" x=\"253.5\" y=\"-1189\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"291.5,-1181 291.5,-1202 366.5,-1202 366.5,-1181 291.5,-1181\"/>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-1189\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"243.5,-1160 243.5,-1181 291.5,-1181 291.5,-1160 243.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"248.5\" y=\"-1168\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"291.5,-1160 291.5,-1181 366.5,-1181 366.5,-1160 291.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"296.5\" y=\"-1168\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1237.6862C279.5,-1229.7975 279.5,-1220.684 279.5,-1212.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-1212.018 279.5,-1202.018 276.0001,-1212.018 283.0001,-1212.018\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"367,-1124 192,-1124 192,-1082 367,-1082 367,-1124\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"192.5,-1082 192.5,-1124 244.5,-1124 244.5,-1082 192.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"197.5\" y=\"-1106\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"200\" y=\"-1095\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"244.5,-1103 244.5,-1124 292.5,-1124 292.5,-1103 244.5,-1103\"/>\n",
       "<text text-anchor=\"start\" x=\"254.5\" y=\"-1111\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"292.5,-1103 292.5,-1124 367.5,-1124 367.5,-1103 292.5,-1103\"/>\n",
       "<text text-anchor=\"start\" x=\"297.5\" y=\"-1111\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"244.5,-1082 244.5,-1103 292.5,-1103 292.5,-1082 244.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"249.5\" y=\"-1090\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"292.5,-1082 292.5,-1103 367.5,-1103 367.5,-1082 292.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"297.5\" y=\"-1090\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1159.6862C279.5,-1151.7975 279.5,-1142.684 279.5,-1134.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-1134.018 279.5,-1124.018 276.0001,-1134.018 283.0001,-1134.018\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"366,-968 193,-968 193,-926 366,-926 366,-968\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"193.5,-926 193.5,-968 243.5,-968 243.5,-926 193.5,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"198.5\" y=\"-950\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"200\" y=\"-939\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"243.5,-947 243.5,-968 291.5,-968 291.5,-947 243.5,-947\"/>\n",
       "<text text-anchor=\"start\" x=\"253.5\" y=\"-955\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"291.5,-947 291.5,-968 366.5,-968 366.5,-947 291.5,-947\"/>\n",
       "<text text-anchor=\"start\" x=\"296.5\" y=\"-955\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"243.5,-926 243.5,-947 291.5,-947 291.5,-926 243.5,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"248.5\" y=\"-934\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"291.5,-926 291.5,-947 366.5,-947 366.5,-926 291.5,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-934\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.5,-1081.9611C279.5,-1055.3277 279.5,-1009.2122 279.5,-978.4051\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.0001,-978.2037 279.5,-968.2037 276.0001,-978.2038 283.0001,-978.2037\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"418.5,-890 192.5,-890 192.5,-848 418.5,-848 418.5,-890\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"192.5,-848 192.5,-890 239.5,-890 239.5,-848 192.5,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"203\" y=\"-872\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">bmm</text>\n",
       "<text text-anchor=\"start\" x=\"197.5\" y=\"-861\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"239.5,-869 239.5,-890 287.5,-890 287.5,-869 239.5,-869\"/>\n",
       "<text text-anchor=\"start\" x=\"249.5\" y=\"-877\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"287.5,-869 287.5,-890 418.5,-890 418.5,-869 287.5,-869\"/>\n",
       "<text text-anchor=\"start\" x=\"292.5\" y=\"-877\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 64), (128, 64, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"239.5,-848 239.5,-869 287.5,-869 287.5,-848 239.5,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"244.5\" y=\"-856\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"287.5,-848 287.5,-869 418.5,-869 418.5,-848 287.5,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"326.5\" y=\"-856\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;18 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>8&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M286.6046,-925.6862C289.292,-917.6241 292.4057,-908.2828 295.3207,-899.5379\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"298.652,-900.6117 298.494,-890.018 292.0113,-898.398 298.652,-900.6117\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"558,-1202 385,-1202 385,-1160 558,-1160 558,-1202\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"385.5,-1160 385.5,-1202 435.5,-1202 435.5,-1160 385.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"390.5\" y=\"-1184\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"392\" y=\"-1173\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"435.5,-1181 435.5,-1202 483.5,-1202 483.5,-1181 435.5,-1181\"/>\n",
       "<text text-anchor=\"start\" x=\"445.5\" y=\"-1189\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"483.5,-1181 483.5,-1202 558.5,-1202 558.5,-1181 483.5,-1181\"/>\n",
       "<text text-anchor=\"start\" x=\"491.5\" y=\"-1189\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"435.5,-1160 435.5,-1181 483.5,-1181 483.5,-1160 435.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"440.5\" y=\"-1168\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"483.5,-1160 483.5,-1181 558.5,-1181 558.5,-1160 483.5,-1160\"/>\n",
       "<text text-anchor=\"start\" x=\"488.5\" y=\"-1168\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M467.1395,-1237.6862C467.7463,-1229.7975 468.4474,-1220.684 469.1075,-1212.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"472.6059,-1212.257 469.8832,-1202.018 465.6265,-1211.7201 472.6059,-1212.257\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"560,-1124 385,-1124 385,-1082 560,-1082 560,-1124\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"385.5,-1082 385.5,-1124 437.5,-1124 437.5,-1082 385.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"390.5\" y=\"-1106\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"393\" y=\"-1095\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"437.5,-1103 437.5,-1124 485.5,-1124 485.5,-1103 437.5,-1103\"/>\n",
       "<text text-anchor=\"start\" x=\"447.5\" y=\"-1111\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"485.5,-1103 485.5,-1124 560.5,-1124 560.5,-1103 485.5,-1103\"/>\n",
       "<text text-anchor=\"start\" x=\"490.5\" y=\"-1111\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"437.5,-1082 437.5,-1103 485.5,-1103 485.5,-1082 437.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"442.5\" y=\"-1090\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"485.5,-1082 485.5,-1103 560.5,-1103 560.5,-1082 485.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"490.5\" y=\"-1090\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M471.7733,-1159.6862C471.8744,-1151.7975 471.9912,-1142.684 472.1012,-1134.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.602,-1134.0621 472.2305,-1124.018 468.6025,-1133.9723 475.602,-1134.0621\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"559,-1046 386,-1046 386,-1004 559,-1004 559,-1046\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"386.5,-1004 386.5,-1046 436.5,-1046 436.5,-1004 386.5,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"391.5\" y=\"-1028\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"393\" y=\"-1017\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"436.5,-1025 436.5,-1046 484.5,-1046 484.5,-1025 436.5,-1025\"/>\n",
       "<text text-anchor=\"start\" x=\"446.5\" y=\"-1033\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"484.5,-1025 484.5,-1046 559.5,-1046 559.5,-1025 484.5,-1025\"/>\n",
       "<text text-anchor=\"start\" x=\"489.5\" y=\"-1033\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"436.5,-1004 436.5,-1025 484.5,-1025 484.5,-1004 436.5,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"441.5\" y=\"-1012\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"484.5,-1004 484.5,-1025 559.5,-1025 559.5,-1004 484.5,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"492.5\" y=\"-1012\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M472.5,-1081.6862C472.5,-1073.7975 472.5,-1064.684 472.5,-1056.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"476.0001,-1056.018 472.5,-1046.018 469.0001,-1056.018 476.0001,-1056.018\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"560.5,-968 384.5,-968 384.5,-926 560.5,-926 560.5,-968\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"384.5,-926 384.5,-968 443.5,-968 443.5,-926 384.5,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"389.5\" y=\"-950\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">transpose</text>\n",
       "<text text-anchor=\"start\" x=\"395.5\" y=\"-939\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"443.5,-947 443.5,-968 491.5,-968 491.5,-947 443.5,-947\"/>\n",
       "<text text-anchor=\"start\" x=\"453.5\" y=\"-955\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"491.5,-947 491.5,-968 560.5,-968 560.5,-947 491.5,-947\"/>\n",
       "<text text-anchor=\"start\" x=\"496.5\" y=\"-955\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"443.5,-926 443.5,-947 491.5,-947 491.5,-926 443.5,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"448.5\" y=\"-934\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"491.5,-926 491.5,-947 560.5,-947 560.5,-926 491.5,-926\"/>\n",
       "<text text-anchor=\"start\" x=\"496.5\" y=\"-934\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 64, 1) </text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;17 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>12&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M472.5,-1003.6862C472.5,-995.7975 472.5,-986.684 472.5,-978.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"476.0001,-978.018 472.5,-968.018 469.0001,-978.018 476.0001,-978.018\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"174,-1124 1,-1124 1,-1082 174,-1082 174,-1124\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1.5,-1082 1.5,-1124 51.5,-1124 51.5,-1082 1.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"6.5\" y=\"-1106\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1095\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"51.5,-1103 51.5,-1124 99.5,-1124 99.5,-1103 51.5,-1103\"/>\n",
       "<text text-anchor=\"start\" x=\"61.5\" y=\"-1111\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"99.5,-1103 99.5,-1124 174.5,-1124 174.5,-1103 99.5,-1103\"/>\n",
       "<text text-anchor=\"start\" x=\"107.5\" y=\"-1111\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"51.5,-1082 51.5,-1103 99.5,-1103 99.5,-1082 51.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"56.5\" y=\"-1090\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"99.5,-1082 99.5,-1103 174.5,-1103 174.5,-1082 99.5,-1082\"/>\n",
       "<text text-anchor=\"start\" x=\"104.5\" y=\"-1090\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M89.6802,-1159.6862C89.3768,-1151.7975 89.0263,-1142.684 88.6963,-1134.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"92.1902,-1133.8761 88.3084,-1124.018 85.1954,-1134.1452 92.1902,-1133.8761\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"175,-1046 0,-1046 0,-1004 175,-1004 175,-1046\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\".5,-1004 .5,-1046 52.5,-1046 52.5,-1004 .5,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"5.5\" y=\"-1028\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1017\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"52.5,-1025 52.5,-1046 100.5,-1046 100.5,-1025 52.5,-1025\"/>\n",
       "<text text-anchor=\"start\" x=\"62.5\" y=\"-1033\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"100.5,-1025 100.5,-1046 175.5,-1046 175.5,-1025 100.5,-1025\"/>\n",
       "<text text-anchor=\"start\" x=\"105.5\" y=\"-1033\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"52.5,-1004 52.5,-1025 100.5,-1025 100.5,-1004 52.5,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"57.5\" y=\"-1012\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"100.5,-1004 100.5,-1025 175.5,-1025 175.5,-1004 100.5,-1004\"/>\n",
       "<text text-anchor=\"start\" x=\"105.5\" y=\"-1012\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M87.5,-1081.6862C87.5,-1073.7975 87.5,-1064.684 87.5,-1056.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"91.0001,-1056.018 87.5,-1046.018 84.0001,-1056.018 91.0001,-1056.018\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"174,-890 1,-890 1,-848 174,-848 174,-890\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1.5,-848 1.5,-890 51.5,-890 51.5,-848 1.5,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"6.5\" y=\"-872\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-861\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"51.5,-869 51.5,-890 99.5,-890 99.5,-869 51.5,-869\"/>\n",
       "<text text-anchor=\"start\" x=\"61.5\" y=\"-877\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"99.5,-869 99.5,-890 174.5,-890 174.5,-869 99.5,-869\"/>\n",
       "<text text-anchor=\"start\" x=\"104.5\" y=\"-877\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"51.5,-848 51.5,-869 99.5,-869 99.5,-848 51.5,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"56.5\" y=\"-856\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"99.5,-848 99.5,-869 174.5,-869 174.5,-848 99.5,-848\"/>\n",
       "<text text-anchor=\"start\" x=\"107.5\" y=\"-856\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M87.5,-1003.9611C87.5,-977.3277 87.5,-931.2122 87.5,-900.4051\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"91.0001,-900.2037 87.5,-890.2037 84.0001,-900.2038 91.0001,-900.2037\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"305.5,-578 85.5,-578 85.5,-536 305.5,-536 305.5,-578\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"85.5,-536 85.5,-578 132.5,-578 132.5,-536 85.5,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-560\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">bmm</text>\n",
       "<text text-anchor=\"start\" x=\"90.5\" y=\"-549\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"132.5,-557 132.5,-578 180.5,-578 180.5,-557 132.5,-557\"/>\n",
       "<text text-anchor=\"start\" x=\"142.5\" y=\"-565\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"180.5,-557 180.5,-578 305.5,-578 305.5,-557 180.5,-557\"/>\n",
       "<text text-anchor=\"start\" x=\"185.5\" y=\"-565\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1), (128, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"132.5,-536 132.5,-557 180.5,-557 180.5,-536 132.5,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-544\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"180.5,-536 180.5,-557 305.5,-557 305.5,-536 180.5,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"213.5\" y=\"-544\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>16&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M94.9233,-847.555C113.6579,-793.4328 162.9157,-651.1325 184.7806,-587.9672\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"188.2039,-588.7773 188.1676,-578.1825 181.589,-586.4875 188.2039,-588.7773\"/>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M427.3074,-925.8921C406.5149,-916.1806 381.6763,-904.5793 359.8687,-894.3937\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"361.2703,-891.1855 350.7287,-890.1248 358.308,-897.5278 361.2703,-891.1855\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"377.5,-812 219.5,-812 219.5,-770 377.5,-770 377.5,-812\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"219.5,-770 219.5,-812 266.5,-812 266.5,-770 219.5,-770\"/>\n",
       "<text text-anchor=\"start\" x=\"235\" y=\"-794\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">div</text>\n",
       "<text text-anchor=\"start\" x=\"224.5\" y=\"-783\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"266.5,-791 266.5,-812 314.5,-812 314.5,-791 266.5,-791\"/>\n",
       "<text text-anchor=\"start\" x=\"276.5\" y=\"-799\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"314.5,-791 314.5,-812 377.5,-812 377.5,-791 314.5,-791\"/>\n",
       "<text text-anchor=\"start\" x=\"319.5\" y=\"-799\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"266.5,-770 266.5,-791 314.5,-791 314.5,-770 266.5,-770\"/>\n",
       "<text text-anchor=\"start\" x=\"271.5\" y=\"-778\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"314.5,-770 314.5,-791 377.5,-791 377.5,-770 314.5,-770\"/>\n",
       "<text text-anchor=\"start\" x=\"319.5\" y=\"-778\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M303.5872,-847.6862C302.8793,-839.7975 302.0614,-830.684 301.2913,-822.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"304.7662,-821.6651 300.3862,-812.018 297.7942,-822.2909 304.7662,-821.6651\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"365.5,-734 203.5,-734 203.5,-692 365.5,-692 365.5,-734\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"203.5,-692 203.5,-734 254.5,-734 254.5,-692 203.5,-692\"/>\n",
       "<text text-anchor=\"start\" x=\"208.5\" y=\"-716\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">softmax</text>\n",
       "<text text-anchor=\"start\" x=\"210.5\" y=\"-705\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"254.5,-713 254.5,-734 302.5,-734 302.5,-713 254.5,-713\"/>\n",
       "<text text-anchor=\"start\" x=\"264.5\" y=\"-721\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"302.5,-713 302.5,-734 365.5,-734 365.5,-713 302.5,-713\"/>\n",
       "<text text-anchor=\"start\" x=\"307.5\" y=\"-721\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"254.5,-692 254.5,-713 302.5,-713 302.5,-692 254.5,-692\"/>\n",
       "<text text-anchor=\"start\" x=\"259.5\" y=\"-700\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"302.5,-692 302.5,-713 365.5,-713 365.5,-692 302.5,-692\"/>\n",
       "<text text-anchor=\"start\" x=\"307.5\" y=\"-700\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M294.6744,-769.6862C293.2585,-761.7975 291.6228,-752.684 290.0826,-744.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"293.4841,-743.2424 288.2725,-734.018 286.5942,-744.4791 293.4841,-743.2424\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"358,-656 197,-656 197,-614 358,-614 358,-656\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"197.5,-614 197.5,-656 247.5,-656 247.5,-614 197.5,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"202.5\" y=\"-638\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"204\" y=\"-627\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"247.5,-635 247.5,-656 295.5,-656 295.5,-635 247.5,-635\"/>\n",
       "<text text-anchor=\"start\" x=\"257.5\" y=\"-643\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"295.5,-635 295.5,-656 358.5,-656 358.5,-635 295.5,-635\"/>\n",
       "<text text-anchor=\"start\" x=\"300.5\" y=\"-643\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"247.5,-614 247.5,-635 295.5,-635 295.5,-614 247.5,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"252.5\" y=\"-622\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"295.5,-614 295.5,-635 358.5,-635 358.5,-614 295.5,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"300.5\" y=\"-622\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;21 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>20&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M282.5872,-691.6862C281.8793,-683.7975 281.0614,-674.684 280.2913,-666.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.7662,-665.6651 279.3862,-656.018 276.7942,-666.2909 283.7662,-665.6651\"/>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;22 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>21&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M255.0931,-613.6862C245.7975,-604.8439 234.884,-594.4629 224.9543,-585.0175\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"227.2537,-582.3742 217.5959,-578.018 222.4292,-587.4461 227.2537,-582.3742\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"282,-500 109,-500 109,-458 282,-458 282,-500\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"109.5,-458 109.5,-500 159.5,-500 159.5,-458 109.5,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"114.5\" y=\"-482\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-471\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"159.5,-479 159.5,-500 207.5,-500 207.5,-479 159.5,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"169.5\" y=\"-487\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"207.5,-479 207.5,-500 282.5,-500 282.5,-479 207.5,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"215.5\" y=\"-487\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(128, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"159.5,-458 159.5,-479 207.5,-479 207.5,-458 159.5,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"164.5\" y=\"-466\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"207.5,-458 207.5,-479 282.5,-479 282.5,-458 207.5,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"212.5\" y=\"-466\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;23 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>22&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5,-535.6862C195.5,-527.7975 195.5,-518.684 195.5,-510.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.0001,-510.018 195.5,-500.018 192.0001,-510.018 199.0001,-510.018\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"283,-422 108,-422 108,-380 283,-380 283,-422\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"108.5,-380 108.5,-422 160.5,-422 160.5,-380 108.5,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-404\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-393\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"160.5,-401 160.5,-422 208.5,-422 208.5,-401 160.5,-401\"/>\n",
       "<text text-anchor=\"start\" x=\"170.5\" y=\"-409\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"208.5,-401 208.5,-422 283.5,-422 283.5,-401 208.5,-401\"/>\n",
       "<text text-anchor=\"start\" x=\"213.5\" y=\"-409\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"160.5,-380 160.5,-401 208.5,-401 208.5,-380 160.5,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"165.5\" y=\"-388\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"208.5,-380 208.5,-401 283.5,-401 283.5,-380 208.5,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"213.5\" y=\"-388\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;24 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>23&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5,-457.6862C195.5,-449.7975 195.5,-440.684 195.5,-432.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.0001,-432.018 195.5,-422.018 192.0001,-432.018 199.0001,-432.018\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"282,-344 109,-344 109,-302 282,-302 282,-344\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"109.5,-302 109.5,-344 159.5,-344 159.5,-302 109.5,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"114.5\" y=\"-326\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-315\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"159.5,-323 159.5,-344 207.5,-344 207.5,-323 159.5,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"169.5\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"207.5,-323 207.5,-344 282.5,-344 282.5,-323 207.5,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"212.5\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"159.5,-302 159.5,-323 207.5,-323 207.5,-302 159.5,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"164.5\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"207.5,-302 207.5,-323 282.5,-323 282.5,-302 207.5,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"215.5\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5,-379.6862C195.5,-371.7975 195.5,-362.684 195.5,-354.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.0001,-354.018 195.5,-344.018 192.0001,-354.018 199.0001,-354.018\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"277.5,-266 113.5,-266 113.5,-224 277.5,-224 277.5,-266\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"113.5,-224 113.5,-266 160.5,-266 160.5,-224 113.5,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"121.5\" y=\"-248\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"118.5\" y=\"-237\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"160.5,-245 160.5,-266 208.5,-266 208.5,-245 160.5,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"170.5\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"208.5,-245 208.5,-266 277.5,-266 277.5,-245 208.5,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"213.5\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"160.5,-224 160.5,-245 208.5,-245 208.5,-224 160.5,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"165.5\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"208.5,-224 208.5,-245 277.5,-245 277.5,-224 208.5,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"213.5\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;26 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>25&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5,-301.6862C195.5,-293.7975 195.5,-284.684 195.5,-276.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.0001,-276.018 195.5,-266.018 192.0001,-276.018 199.0001,-276.018\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<polygon fill=\"#f0f8ff\" stroke=\"transparent\" points=\"279.5,-188 111.5,-188 111.5,-146 279.5,-146 279.5,-188\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"111.5,-146 111.5,-188 162.5,-188 162.5,-146 111.5,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"116.5\" y=\"-170\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">squeeze</text>\n",
       "<text text-anchor=\"start\" x=\"118.5\" y=\"-159\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"162.5,-167 162.5,-188 210.5,-188 210.5,-167 162.5,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"172.5\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"210.5,-167 210.5,-188 279.5,-188 279.5,-167 210.5,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"215.5\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"162.5,-146 162.5,-167 210.5,-167 210.5,-146 162.5,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"167.5\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"210.5,-146 210.5,-167 279.5,-167 279.5,-146 210.5,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"221.5\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 128) </text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;27 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>26&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5,-223.6862C195.5,-215.7975 195.5,-206.684 195.5,-198.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.0001,-198.018 195.5,-188.018 192.0001,-198.018 199.0001,-198.018\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"271.5,-110 119.5,-110 119.5,-68 271.5,-68 271.5,-110\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"119.5,-68 119.5,-110 166.5,-110 166.5,-68 119.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-92\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"124.5\" y=\"-81\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"166.5,-89 166.5,-110 214.5,-110 214.5,-89 166.5,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"176.5\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"214.5,-89 214.5,-110 271.5,-110 271.5,-89 214.5,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"219.5\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"166.5,-68 166.5,-89 214.5,-89 214.5,-68 166.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"171.5\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"214.5,-68 214.5,-89 271.5,-89 271.5,-68 214.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"222.5\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 10) </text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;28 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>27&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5,-145.6862C195.5,-137.7975 195.5,-128.684 195.5,-120.1029\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.0001,-120.018 195.5,-110.018 192.0001,-120.018 199.0001,-120.018\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<polygon fill=\"#ffffe0\" stroke=\"transparent\" points=\"258,-32 133,-32 133,0 258,0 258,-32\"/>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"133.5,0 133.5,-32 210.5,-32 210.5,0 133.5,0\"/>\n",
       "<text text-anchor=\"start\" x=\"138.5\" y=\"-19\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"153.5\" y=\"-8\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"210.5,0 210.5,-32 258.5,-32 258.5,0 210.5,0\"/>\n",
       "<text text-anchor=\"start\" x=\"215.5\" y=\"-13.5\" font-family=\"Linux libertine\" font-size=\"10.00\" fill=\"#000000\">(64, 10)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;29 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>28&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5,-67.8854C195.5,-59.9505 195.5,-50.8282 195.5,-42.4674\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.0001,-42.4309 195.5,-32.431 192.0001,-42.431 199.0001,-42.4309\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f1b36d3f2c0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "\n",
    "model_graph = draw_graph(\n",
    "    model=MultiHeadAttentionMNISTModel(num_heads=2, attention_type='dot'), \n",
    "    input_size=(64, 28, 28),\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.8. <a id='toc11_5_8_'></a>[attention-seq2seq](#toc0_)\n",
    "\n",
    "- 加入attention机制的Seq2Seq；\n",
    "\n",
    "- 基于Attention的Seq2Seq。\n",
    "\n",
    "![attention_seq2seq](./Pytorch_Pictures/Attention/seq2seq_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(d2l.Decoder):\n",
    "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = d2l.AdditiveAttention(num_hiddens, num_hiddens, num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        # Shape of `outputs`: (`num_steps`, `batch_size`, `num_hiddens`).\n",
    "        # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n",
    "        # `num_hiddens`)\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Shape of `enc_outputs`: (`batch_size`, `num_steps`, `num_hiddens`).\n",
    "        # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n",
    "        # `num_hiddens`)\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        # Shape of the output `X`: (`num_steps`, `batch_size`, `embed_size`)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            # 解码器输入，query是编码器输出最后时刻、最后层的隐状态，key和value是编码器输出所有时刻、所有层的隐状态\n",
    "            # hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "            # hidden_state[-1]: (batch_size, num_hiddens)\n",
    "            # hidden_state[-1].unsqueeze(1): (batch_size, 1, num_hiddens) \n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            # Shape of `context`: (`batch_size`, 1, `num_hiddens`)\n",
    "            context = self.attention(queries=query, keys=enc_outputs, values=enc_outputs, valid_lens=enc_valid_lens)\n",
    "\n",
    "            # Concatenate on the feature dimension\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
    "            # Reshape `x` as (1, `batch_size`, `embed_size` + `num_hiddens`)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        # After fully-connected layer transformation, shape of `outputs`:\n",
    "        # (`num_steps`, `batch_size`, `vocab_size`)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens]\n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n",
    "    \n",
    "\n",
    "# test\n",
    "#@tab pytorch\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval()\n",
    "\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "decoder.eval()\n",
    "\n",
    "X = d2l.zeros((4, 7), dtype=torch.long)  # (`batch_size`, `num_steps`)\n",
    "\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "\n",
    "output, state = decoder(X, state)\n",
    "\n",
    "output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.020, 5116.0 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD/CAYAAADVGuzgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL2NJREFUeJzt3Xl0FGW+N/Bv753OvnYWshIg7CBLDIvMDCEBdAR1joDeC6LiKyOvSxQVZwZEfCcMMgzjMSP3OqLMvS6MG46CGUIkKBBAAhFBliQEAmQP2Tvp7nQ/7x8hLW1CyNJJpZvv55w+SVdVP/x+JH4pq56qkgkhBIiIaMCTS10AERF1DQObiMhJMLCJiJwEA5uIyEkwsImInAQDm4jISTCwiYichFLqAgYiq9WK4uJieHp6QiaTSV0OETkJIQTq6+sRGhoKudzx+8MM7A4UFxcjPDxc6jKIyEldunQJgwYNcvi4DOwOeHp6Amj9S/fy8pK4mq4xm83YvXs3kpKSoFKppC6nT7h6j67eH+D6PV69ehXR0dG2DHE0BnYH2g6DeHl5OVVg63Q6eHl5ueR/CIDr9+jq/QGu36PZbAaAPjuUypOOREROgoFNROQkGNhERE6CgU1E5CQY2EREToKB3Qlji0XqEoiIbAZEYKelpSEqKgparRbx8fE4cuTIDbd96623MH36dPj6+sLX1xeJiYntthdCYPXq1QgJCYGbmxsSExORl5fX7bqKqgzd/gwRUV+RPLC3b9+OlJQUrFmzBseOHcPYsWORnJyM8vLyDrfPysrCokWLsHfvXmRnZyM8PBxJSUm4cuWKbZsNGzbg9ddfx5YtW3D48GG4u7sjOTkZzc3N3aqtoKKhV70RETmS5BfObNq0CcuWLcPSpUsBAFu2bMHOnTuxdetWvPjii+22f++99+ze//3vf8cnn3yCzMxMLF68GEIIbN68Gb///e8xb948AMA//vEP6PV67NixAwsXLmw3ptFohNFotL2vq6sDAJwrqbNNhB/o2up0lnp7wtV7dPX+ANfvsa/7kjSwTSYTcnJysGrVKtsyuVyOxMREZGdnd2kMg8EAs9kMPz8/AEBhYSFKS0uRmJho28bb2xvx8fHIzs7uMLBTU1Oxdu3adsuzT53HLmVxd9uSVEZGhtQl9DlX79HV+wNct0eDoW8Po0oa2JWVlbBYLNDr9XbL9Xo9zpw506UxXnjhBYSGhtoCurS01DbGz8dsW/dzq1atQkpKiu19XV0dwsPD0aDwwNy5SV3uR0pmsxkZGRmYNWuWS17yC7h+j67eH+D6PVZVVfXp+JIfEumN9evX48MPP0RWVha0Wm2Px9FoNNBoNO2WX7raBCFTQK2U/FB/l6lUKpf8D+F6rt6jq/cHuG6Pfd2TpEkUEBAAhUKBsrIyu+VlZWUIDg7u9LMbN27E+vXrsXv3bowZM8a2vO1zPRnz51qsAheqGrv1GSKiviJpYKvVakyYMAGZmZm2ZVarFZmZmUhISLjh5zZs2IB169YhPT0dEydOtFsXHR2N4OBguzHr6upw+PDhTse8kbwyzhQhooFB8kMiKSkpWLJkCSZOnIjJkydj8+bNaGxstM0aWbx4McLCwpCamgoA+NOf/oTVq1fj/fffR1RUlO24tIeHBzw8PCCTyfD000/j1VdfxZAhQxAdHY0//OEPCA0Nxfz587tdX155PYAQR7VLRNRjkgf2ggULUFFRgdWrV6O0tBTjxo1Denq67aRhUVGR3aN23nzzTZhMJvzmN7+xG2fNmjV4+eWXAQDPP/88Ghsb8dhjj6GmpgbTpk1Denp6j45zcw+biAYKyQMbAFasWIEVK1Z0uC4rK8vu/YULF246nkwmwyuvvIJXXnml17W17mETEUnPeaY/SKSwshFmi1XqMoiIGNid0anlMFsELnKmCBENAAzsTsQEeAAAzvE4NhENAAzsTgwOag1snngkooGAgd2JwYHuAIBzPPFIRAMAA7sTbXvY+dzDJqIBgIHdicHXjmGfr2xAC2eKEJHEGNidCPVxg5tKAbNF4AKfPkNEEmNgd0Iul2GI/tphER7HJiKJMbBvIjaIU/uIaGBgYN/EUL0nACCvnIFNRNJiYN/EENtcbB4SISJpMbBvYkhQ6x72+YpGzhQhIkkxsG9ikK8btCo5TBYriq5ypggRSYeBfRNyuYwnHoloQGBgd8HQa4dFOLWPiKTEwO6CWD33sIlIegzsLmjbw+bUPiKSEgO7C9qudiyoaIDFKiSuhohuVQzsLhjkq4NGKYephTNFiEg6DOwuUFw3U4QX0BCRVBjYXWS74pHHsYlIIgzsLhrSdk8R7mETkUQY2F00hBfPEJHEGNhd1HbXPs4UISKpMLC7KNxPB7VSDmOLFZc4U4SIJMDA7iKFXIbBgTzxSETSYWB3w1DbJeo88UhE/Y+B3Q1tJx7zuYdNRBJgYHeDbWof79pHRBJgYHfD9XvYVs4UIaJ+xsDuhohrM0WazVZcrm6SuhwiusUwsLtBqZAjJsAdAE88ElH/Y2B301A9741NRNJgYHfTEN61j4gkwsDupraHGXAPm4j6m+SBnZaWhqioKGi1WsTHx+PIkSM33PbUqVO47777EBUVBZlMhs2bN7fb5uWXX4ZMJrN7xcXFOazetql9nClCRP1N0sDevn07UlJSsGbNGhw7dgxjx45FcnIyysvLO9zeYDAgJiYG69evR3Bw8A3HHTlyJEpKSmyv/fv3O6zmSD8d1Ao5mswWXKnhTBEi6j+SBvamTZuwbNkyLF26FCNGjMCWLVug0+mwdevWDrefNGkSXnvtNSxcuBAajeaG4yqVSgQHB9teAQEBDqtZqZAjJrB1pggvoCGi/qSU6g82mUzIycnBqlWrbMvkcjkSExORnZ3dq7Hz8vIQGhoKrVaLhIQEpKamIiIi4obbG41GGI1G2/u6ujoAgNlshtlsbrf94AB3nCmtx+niWkwf7NerWh2lrc6O6nUVrt6jq/cHuH6Pfd2XZIFdWVkJi8UCvV5vt1yv1+PMmTM9Hjc+Ph7vvvsuhg0bhpKSEqxduxbTp0/HyZMn4enp2eFnUlNTsXbt2nbLd+/eDZ1O1265qJUBUGDf8bMYVH+6x7X2hYyMDKlL6HOu3qOr9we4bo8GQ9/eelmywO4rc+bMsX0/ZswYxMfHIzIyEv/85z/xyCOPdPiZVatWISUlxfa+rq4O4eHhSEpKgpeXV7vt5afKsOvD79Gk9sHcubc7vokeMJvNyMjIwKxZs6BSqaQup0+4eo+u3h/g+j1WVVX16fiSBXZAQAAUCgXKysrslpeVlXV6QrG7fHx8MHToUOTn599wG41G0+ExcZVK1eEv1fBQbwBAQUUjFAol5HKZw+rtrRvV7EpcvUdX7w9w3R77uifJTjqq1WpMmDABmZmZtmVWqxWZmZlISEhw2J/T0NCAgoIChISEOGzMSH93qBQyGEwWFNdypggR9Q9JZ4mkpKTgrbfewrZt23D69GksX74cjY2NWLp0KQBg8eLFdiclTSYTcnNzkZubC5PJhCtXriA3N9du7/m5557Dvn37cOHCBRw8eBD33HMPFAoFFi1a5LC6VQo5oq/dUySPD+Ulon4i6THsBQsWoKKiAqtXr0ZpaSnGjRuH9PR024nIoqIiyOU//ZtSXFyM8ePH295v3LgRGzduxIwZM5CVlQUAuHz5MhYtWoSqqioEBgZi2rRpOHToEAIDAx1a+xC9J86VNSCvvB6/jAty6NhERB2R/KTjihUrsGLFig7XtYVwm6ioKAjR+dWFH374oaNK61TbPUXOcQ+biPqJ5JemOyvetY+I+hsDu4dsT58pq7/pXj8RkSMwsHso0t8dSrkMjSYLimubpS6HiG4BDOweUit/minCp88QUX9gYPdC272x83nikYj6AQO7F4YEtZ145B42EfU9BnYvtO1hc2ofEfUHBnYvDL3u6TOcKUJEfY2B3QtR/u5QyGVoMLaghDNFiKiPMbB7Qa2UI8q/9X7ZnClCRH2Ngd1LYwb5AAC+zauUthAicnkM7F66c3TrbVu/+L4YFj5FnYj6EAO7l+4YGghvNxXK6404dL5vnzZBRLc2BnYvqZVyzL22l/157hWJqyEiV8bAdoB540IBAF+dLEWz2SJxNUTkqhjYDjA5yg/BXlrUN7cg62yF1OUQkYtiYDuAXC7D3df2sv/1PQ+LEFHfYGA7yN1jWwN7z+ly1DebJa6GiFwRA9tBRoZ6ITbIA6YWK/59qkzqcojIBTGwHUQmk2Hetb1szhYhor7AwHagtuPYB/IrUVFvlLgaInI1PQrsbdu2YefOnbb3zz//PHx8fDBlyhRcvHjRYcU5m0h/d4wL94FVADtPFEtdDhG5mB4F9h//+Ee4ubkBALKzs5GWloYNGzYgICAAzzzzjEMLdDZtc7J35DKwicixehTYly5dQmxsLABgx44duO+++/DYY48hNTUV3377rUMLdDZ3jgmBXAbkXqrBxapGqcshIhfSo8D28PBAVVXrfTN2796NWbNmAQC0Wi2ampocV50TCvLUYmpsAADgX9zLJiIH6lFgz5o1C48++igeffRRnDt3DnPnzgUAnDp1ClFRUY6szym1zcnekXuFT6IhIofpUWCnpaUhISEBFRUV+OSTT+Dv7w8AyMnJwaJFixxaoDNKHhUMtVKOgopG/FhSJ3U5ROQilD35kI+PD9544412y9euXdvrglyBl1aFmXFB+OpkKf6VW4yRod5Sl0RELqBHe9jp6enYv3+/7X1aWhrGjRuHBx54ANXV1Q4rzpnNs91bpBhWPtiAiBygR4G9cuVK1NW1/q/+Dz/8gGeffRZz585FYWEhUlJSHFqgs/rFsCB4apUoqW3GdxeuSl0OEbmAHgV2YWEhRowYAQD45JNPcNddd+GPf/wj0tLS8NVXXzm0QGelVSkwZ1QwAODz7zlbhIh6r0eBrVarYTAYAAB79uxBUlISAMDPz8+2503AvHFhAIBdP5TA1GKVuBoicnY9Ouk4bdo0pKSkYOrUqThy5Ai2b98OADh37hwGDRrk0AKd2e0x/gj01KCi3ohv8yowc7he6pKIyIn1aA/7jTfegFKpxMcff4w333wTYWGte5JfffUVZs+e7dACnZlCLsOvx/BSdSJyjB7tYUdERODLL79st/wvf/lLrwtyNfPGhWLrgUJk/FiKRmML3DU9+isnIupZYAOAxWLBjh07cPr0aQDAyJEjcffdd0OhUDisOFcwZpA3ovx1uFBlQMaPZZg/PkzqkojISfXokEh+fj6GDx+OxYsX49NPP8Wnn36K//iP/8DIkSNRUFDg6Bqdmkwmw93XTj7ywQZE1Bs9Cuwnn3wSgwcPxqVLl3Ds2DEcO3YMRUVFiI6OxpNPPtmtsdLS0hAVFQWtVov4+HgcOXLkhtueOnUK9913H6KioiCTybB58+Zej9kf2u4t8k1eJaoa+GADIuqZHgX2vn37sGHDBvj5+dmW+fv7Y/369di3b1+Xx9m+fTtSUlKwZs0aHDt2DGPHjkVycjLKy8s73N5gMCAmJgbr169HcHCwQ8bsD7FBHhgV5gWLVWDXyVLJ6iAi59ajwNZoNKivr2+3vKGhAWq1usvjbNq0CcuWLcPSpUsxYsQIbNmyBTqdDlu3bu1w+0mTJuG1117DwoULodFoHDJmf5k3tvWwyL94WISIeqhHJx3vuusuPPbYY3j77bcxefJkAMDhw4fx+OOP4+677+7SGCaTCTk5OVi1apVtmVwuR2JiIrKzs3tSVo/HNBqNMBp/OlTRdvGP2WyG2WzuUS0/N3tkIP741Wl8d6EaFyrqEObj5pBx27TV6ah6ByJX79HV+wNcv8e+7qtHgf36669jyZIlSEhIgEqlAtBa6Lx58254XPnnKisrYbFYoNfbX0yi1+tx5syZnpTV4zFTU1M7vNPg7t27odPpelRLR2I95cirk2PjR1mYFdY3N4TKyMjok3EHElfv0dX7A1y3x7YrwPtKj2+v+vnnnyM/P982rW/48OG2x4Y5m1WrVtndtKqurg7h4eFISkqCl5eXw/6cRv0VvLTjFA5ddcO6xdPg4cA52WazGRkZGZg1a5btH1FX4+o9unp/gOv32PYkrr7S5cS42V349u7da/t+06ZNNx0vICAACoUCZWVldsvLyspueEKxr8bUaDQdHhNXqVQO/aX6zcQI/Pe3hbhQZcA72ZeQMmuow8Zu4+iaByJX79HV+wNct8e+7qnLgX38+PEubSeTybq0nVqtxoQJE5CZmYn58+cDAKxWKzIzM7FixYqultXnYzqSWinH87Pj8Nv3juGtb87jwfgI6L20UpdFRE6iy4F9/R60o6SkpGDJkiWYOHEiJk+ejM2bN6OxsRFLly4FACxevBhhYWFITU0F0HpS8ccff7R9f+XKFeTm5sLDw8N2OOZmY0ptzqhg3Bbhg2NFNdi0+xz+9JsxUpdERE5C0htbLFiwABUVFVi9ejVKS0sxbtw4pKen204aFhUVQS7/aeZhcXExxo8fb3u/ceNGbNy4ETNmzEBWVlaXxpSaTCbD7+4cjvvezMZHOZewdFoU4oIdd5yciFyX5HciWrFixQ0PV7SFcJuoqKguPYW8szEHggmRfpg7Ohi7fihF6q4z2PbwZKlLIiIn0KMLZ6j3nk+Og0ohw75zFdifVyl1OUTkBBjYEokKcMeD8ZEAgP+36zQsfFAvEd0EA1tCT84cAk+tEqdL6vDZcV6yTkSdY2BLyM9djSd+2Tq75c+7z6LZbJG4IiIayBjYEntoShTCfNxQUtuMt/cXSl0OEQ1gDGyJaVUKrEweBgB4M6sAlbxfNhHdAAN7ALh7bChGhXmhwdiC1zPzpC6HiAYoBvYAIJfL8NLc4QCA9w4XoaCiQeKKiGggYmAPEFMGB2BmXBAsVoE/fdWz28sSkWtjYA8gL86Jg1wG7P6xDEcKr0pdDhENMAzsAWSI3hMLJ0cAaL2YpiuX4RPRrYOBPcA8nTgEOrUC31+qwZcnSqQuh4gGEAb2ABPkqcXjMwYDAP6UfgbGFl5MQ0StGNgD0KPToxHkqcHl6ib8T/ZFqcshogGCgT0A6dRKPJvU+viwzXvycLa0XuKKiGggYGAPUL+ZEI74aD80GFuw9J0jKK9rlrokIpIYA3uAUshl+K//nICYAHcU1zbjkW1HYTC1SF0WEUmIgT2A+ejUeGfpJPi5q/HDlVo8+cFx3jeb6BbGwB7gIv3d8dbiiVAr5dhzuhzrvvxR6pKISCIMbCcwIdIXf7l/HADg3YMXsJW3YSW6JTGwncSdY0Lw4pw4AMC6nT9i96lSiSsiov7GwHYi/+eOGCyaHAEhgKc+zMWJyzVSl0RE/YiB7URkMhnWzRuJGUMD0WS24OF3j+JytUHqsoionzCwnYxSIccbD4xHXLAnKhuMePjd71DXbJa6LCLqBwxsJ+SpVeGdpZOg99LgXFkDlv9vDkwtVqnLIqI+xsB2UiHebnh7ySTo1AocyK/C6i9+BO/GSuTaGNhObFSYN9IeuA1yGfDJsWJkXJFJXRIR9SEGtpP7ZVwQ1t49EgCw85ICb+wt4IMPiFwUA9sF/GdCFJbPiAYA/PXrAjy9PRfNZt5Hm8jVMLBdREriECyIsUApl+Hz3GI88NYhVNQbpS6LiByIge1CpugFti65DV5aJY4V1WB+2gGcKa2TuiwichAGtotJiPHHjiemIjrAHVdqmnDf3w7i6zNlUpdFRA7AwHZBMYEe+Oy3U5AQ449GkwWPbjuKv397nicjiZwcA9tF+ejU2PbwZCycFA6rAF7deRovffYDzBZeYEPkrBjYLkytlCP13tH4/Z3DIZMBHxy5hCVbj6DGYJK6NCLqAQa2i5PJZHh0egze+s+JcFcrcLCgCvf87SDOVzRIXRoRdRMD+xaROEKPj5dPQZiPGworG3HP3w7iyxPFPK5N5EQGRGCnpaUhKioKWq0W8fHxOHLkSKfbf/TRR4iLi4NWq8Xo0aOxa9cuu/UPPfQQZDKZ3Wv27Nl92YJTGB7ihR1PTMX4CB/UNpmx4v3jWPBfh3DySq3UpRFRF0ge2Nu3b0dKSgrWrFmDY8eOYezYsUhOTkZ5eXmH2x88eBCLFi3CI488guPHj2P+/PmYP38+Tp48abfd7NmzUVJSYnt98MEH/dHOgBfoqcEHy27HUzOHQKuS48iFq/j1G/vx/Mffo7y+WeryiKgTSqkL2LRpE5YtW4alS5cCALZs2YKdO3di69atePHFF9tt/9e//hWzZ8/GypUrAQDr1q1DRkYG3njjDWzZssW2nUajQXBwcJdqMBqNMBp/uiqwrq71YhOz2Qyz2TnuNd1WZ1fqVQBY8Yto3Dc+BBv+fQ5f/lCKfx69jJ0/lGD5HTF4aEokNErJ/y1vpzs9OiNX7w9w/R77ui9JA9tkMiEnJwerVq2yLZPL5UhMTER2dnaHn8nOzkZKSordsuTkZOzYscNuWVZWFoKCguDr64tf/epXePXVV+Hv79/hmKmpqVi7dm275bt374ZOp+tmV9LKyMjo1vazPIDYUcCnhQoUNVqwMSMP73xzDvMirRjjJyAbgDcA7G6PzsbV+wNct0eDoW+fACVpYFdWVsJisUCv19st1+v1OHPmTIefKS0t7XD70tKfHko7e/Zs3HvvvYiOjkZBQQFeeuklzJkzB9nZ2VAoFO3GXLVqld0/AnV1dQgPD0dSUhK8vLx602K/MZvNyMjIwKxZs6BSqbr9+eVWgc+/L8HGjDyU1xux9ZwCt0f74ndz4xAX7NkHFXdfb3sc6Fy9P8D1e6yqqurT8SU/JNIXFi5caPt+9OjRGDNmDAYPHoysrCzMnDmz3fYajQYajabdcpVK5XS/VL2p+f7JkbhzbBjezCrAf397HocKqzHvb9lYMCkCKbOGItCz/d+RFJzx59Idrt4f4Lo99nVPkh6oDAgIgEKhQFmZ/b0uysrKbnj8OTg4uFvbA0BMTAwCAgKQn5/f+6JdnLtGieeShyEzZQbuHB0CqwA+OFKEKesz8eQHx3H4fBWnAhJJRNLAVqvVmDBhAjIzM23LrFYrMjMzkZCQ0OFnEhIS7LYHWo+H3Wh7ALh8+TKqqqoQEhLimMJvAeF+OqQ9eBu2P3Y7xoX7wGwR+Nf3xVjw34cw6y/f4J0Dhag1uOaJI6KBSvKpACkpKXjrrbewbds2nD59GsuXL0djY6Nt1sjixYvtTko+9dRTSE9Px5///GecOXMGL7/8Mo4ePYoVK1YAABoaGrBy5UocOnQIFy5cQGZmJubNm4fY2FgkJydL0qMzi792978vVkzDosnh0KkVyC9vwNovfkR86h6s/Oh75F6q4V43UT+Q/Bj2ggULUFFRgdWrV6O0tBTjxo1Denq67cRiUVER5PKf/l2ZMmUK3n//ffz+97/HSy+9hCFDhmDHjh0YNWoUAEChUODEiRPYtm0bampqEBoaiqSkJKxbt67D49TUNaMHeSN10Bi8NHc4duQW471DF3GmtB4f5VzGRzmXMTLUCw/GR2LeuFC4ayT/tSJySTLBXaN26urq4O3tjdraWqeaJbJr1y7MnTu3X07mCCFwrKgG7x2+iC9PlMDU0noXQA+NEveMD8PD06IRHeDu0D+zv3vsb67eH+D6PVZVVSEgIKDPsoO7QtQjMpkMEyJ9MSHSF3+4cwQ+OXYZ7x8uwvnKRvzPoYv438MXkTwiGI/NiMFtEb5Sl0vkEhjY1Gu+7mo8Oj0Gj0yLRnZBFd7eX4jMM+VIP1WK9FOlmBTli8fuGIyZcUGQywfglThEToKBTQ4jk8kwJTYAU2IDkFdWj7e+PY/Pjl/Bdxeq8d2Foxgc6I5l02Mwf3wYtKr2FzARUecknyVCrmmI3hMbfjMW+1/4FR6fMRieWiUKKhrx4qc/YNqf9iJtbz6nBRJ1EwOb+pTeS4sX58Qhe9VM/P7O4Qj11qKywYjX/n0WCesz8fK/TiG7oArNZovUpRINeDwkQv3CQ6PEo9NjsGRKFL48UYz/2nceZ0rr8e7BC3j34AVolHJMiPRFQow/Egb7Y8wgH6gH4B0DiaTEwKZ+pVLIcc/4QZg/Lgzf5lXi45zLOFhQhcoGIw4WVOFgQRWQAbipFJgY5YuEwf5IiPHH6DBvqUsnkhwDmyQhk8lwx9BA3DE0EEIIFFQ0ILugCtnnq5BdUIVqgxnf5lXi27xKAK176BMifaBqkKP6yCVE+nsg1McNoT5aeGpdbz4vUUcY2CQ5mUyG2CBPxAZ54j8TomC1Cpwtq7cF+OHzVahrbsG+c5UA5NhTfNru855aJcJ83GwBHurjhjAfN8QEeCAuxBMqBQ+tkGtgYNOAI5fLMDzEC8NDvPDwtGhYrAI/Ftchu6AC3x4/DbWPHiW1RhTXNqHGYEZ9cwvOlNbjTGl9u7G0KjnGhPngtkhf3BbR+jXAg7coIOfEwKYBTyGXYfQgb8TpddDXnMLcueNtlzU3GltQUtuEKzXNKK5pQnFNE67UNOFKdRPOlNajtsmMIxeu4siFq7bxIv11uC2iNcDHR/giLtgTSu6FkxNgYJNTc9cobYdTfs5qFThf2YhjRdU4XlSNnIvVyCtvwMUqAy5WGfDZ8SsAAJ1agVFh3hjk64ZQbzeE+GhtX0O83eClVUI2EJ+VRrccBja5LLlchtggD8QGeeD+ieEAgLpmM3KLapBzsRrHiqqRW1SDemMLjhRexZHCjsdxVysQ7N16bDzEuzXEB/m6IcJPh3A/HfReWih4yT31AwY23VK8tCrb7BSgdS88v6IBp4prUVLbjJKaZpTUNqH42tdqgxmNJgsKKhpRUNHY4ZgqhQyDfHUY5OuGcD9da5D76hDu54ZwXx18dCruoZNDMLDpliaXyzBU74mh+o4fNNxksqCktqk1zGubUVLThOLaJlyubkLRVQOuVDfBbBEorGxEYWXHgQ4ASrkMiutere/lUMgBpVwOhVwGuQxoaVYgve57xOo9ER3gjphAD0QHuMPbjVMXiYFN1Ck3tQIxgR6ICfTocL3FKlBa14yiKgMuVRtw6eq117VAr6g3AgBarAIt1q7cel6Gy6fKgFP2zy0N8FAjJsDjWoi7Izqg9eXrroanVgmNkjfTuhUwsIl6QSGXIezavO8E+Ldb32SyoMHYAqtoDWyLRaDFarW9b7EIWKwCFiFgNJnx9beHEBA9HBeuNqOwsgHnKxpRXm9EZYMJlQ32s12up1bK4aVVwlOrgodGCU9t26v1vZdWCY1KAZkMkMta9+blMhlk130vl+Haexl0aoXtEE+gh4a3xR0gGNhEfchNrYCbumt7v2azGeWnBOZOjbJ7Gkt9sxkXKg04X9mAgorWQy/nKxpQVGVAvbEFAGBqsV4LdZPDe9Ao5bbwvv7YfNt7b11rrVargLHFimazBc0tFjSbr31vvvZ9iwWNTSacvCqD7/kqeLppoFMroVMrrr2U0KrkPN7fCQY20QDnqVVh9CBvjB7U/n4qFqtAg7EF9c2tFxDVN7egwdj6fV1z6/KGa8vNltY9e6sArEJAXPv603sBq7X1+7pmMy5dbUJJbROMLdZOT7rq1Aq0WIXtMXE3p8BbZ3M6XCOTtd5HRnftHzpPjQp+7mr4uqvhp1O1fm176dS29z46FdQKOeqaW1BRb0RFvRGVDa1fK9q+1v/0vrrRBL2XFkP1Hhga7Ilh185jxAZ5DOh7tTOwiZyYQi6Dt5uqz05Kmi1WlNQ0247PF107Pn/pqgGXqw2obDDBYGp/a1ylXAatSgGtSg6NsjV8tSo51Ao5KquqodZ5oMlsRZPZAoOpBc3m1rAXAjCYLNeN2dTlWpVyWRfPE7S6cu0iq71nK2zL5DIg0t8dQ/UeGKb3xBC9J4YFe8LPXQ1jixWmay9ji+XaV6vtq7HFgqqrHR+ychQGNhHdkEohR4S/DhH+ug7XNxpbUF5vhErRFtAKaJXyG145+tNDeKfaHfaxWsW18LagyWSBwdwCg8mCuiYzqg0mXG00o7rRhKsGU+vXRtNPyw0mWK47qeupVSLQU4MADw0CPTUIbPvq+dN7H50KxTXNOFdWj3Nl9Thb2vq12mC2zfj5989O/HaF1Wjo9me6g4FNRD3mrlEiWtP7GJHLZXDXKOHeg7GsVoH65hY0mlrg567u8iGNQb46TI72s70XQqCiwYi8sgZbgJ8tq0deWQMajC1QK+XQKOTQXPs/BbWy9aVRKlq/V8ghTFps73YHXcfAJiKnJpfL4K1T2U5+9pRMJkOQpxZBnlpMjQ2wLRdC2NbfTFVVFbb/316V0SkGNhFRJwbSrBXeooyIyEkwsImInAQDm4jISTCwiYicBAObiMhJcJZIB9qm8dTV1UlcSdeZzWYYDAbU1dXZXZDgSly9R1fvD3D9HuvrW58r2pYhjsbA7kDbX3p4eLjElRCRM6qqqoK3d/t7v/SWTPTVPwVOzGq1ori4GJ6engNqDmZn6urqEB4ejkuXLsHLy0vqcvqEq/fo6v0Brt9jbW0tIiIiUF1dDR8fH4ePzz3sDsjlcgwaNEjqMnrEy8vLJf9DuJ6r9+jq/QGu36Nc3jenB3nSkYjISTCwiYicBAPbRWg0GqxZswYajUbqUvqMq/fo6v0Brt9jX/fHk45ERE6Ce9hERE6CgU1E5CQY2EREToKBTUTkJBjYTubll1+GTCaze8XFxdnWNzc344knnoC/vz88PDxw3333oays+w8T7S/ffPMNfv3rXyM0NBQymQw7duywWy+EwOrVqxESEgI3NzckJiYiLy/PbpurV6/iwQcfhJeXF3x8fPDII4+goaGhH7vo3M16fOihh9r9TGfPnm23zUDuMTU1FZMmTYKnpyeCgoIwf/58nD171m6brvxeFhUV4c4774ROp0NQUBBWrlyJlpaW/mylQ13p7xe/+EW7n+Hjjz9ut40j+mNgO6GRI0eipKTE9tq/f79t3TPPPIMvvvgCH330Efbt24fi4mLce++9ElbbucbGRowdOxZpaWkdrt+wYQNef/11bNmyBYcPH4a7uzuSk5PR3Nxs2+bBBx/EqVOnkJGRgS+//BLffPMNHnvssf5q4aZu1iMAzJ492+5n+sEHH9itH8g97tu3D0888QQOHTqEjIwMmM1mJCUlobGx0bbNzX4vLRYL7rzzTphMJhw8eBDbtm3Du+++i9WrV0vRkp2u9AcAy5Yts/sZbtiwwbbOYf0Jcipr1qwRY8eO7XBdTU2NUKlU4qOPPrItO336tAAgsrOz+6nCngMgPvvsM9t7q9UqgoODxWuvvWZbVlNTIzQajfjggw+EEEL8+OOPAoD47rvvbNt89dVXQiaTiStXrvRb7V318x6FEGLJkiVi3rx5N/yMs/VYXl4uAIh9+/YJIbr2e7lr1y4hl8tFaWmpbZs333xTeHl5CaPR2L8N3MTP+xNCiBkzZoinnnrqhp9xVH/cw3ZCeXl5CA0NRUxMDB588EEUFRUBAHJycmA2m5GYmGjbNi4uDhEREcjOzpaq3B4rLCxEaWmpXT/e3t6Ij4+39ZOdnQ0fHx9MnDjRtk1iYiLkcjkOHz7c7zX3VFZWFoKCgjBs2DAsX74cVVVVtnXO1mNtbS0AwM/PD0DXfi+zs7MxevRo6PV62zbJycmoq6vDqVOn+rH6m/t5f23ee+89BAQEYNSoUVi1ahUMBoNtnaP6482fnEx8fDzeffddDBs2DCUlJVi7di2mT5+OkydPorS0FGq1ut1dwvR6PUpLS6UpuBfaar7+l7ztfdu60tJSBAUF2a1XKpXw8/Nzmp5nz56Ne++9F9HR0SgoKMBLL72EOXPmIDs7GwqFwql6tFqtePrppzF16lSMGjUKALr0e1laWtrhz7lt3UDRUX8A8MADDyAyMhKhoaE4ceIEXnjhBZw9exaffvopAMf1x8B2MnPmzLF9P2bMGMTHxyMyMhL//Oc/4ebmJmFl1FMLFy60fT969GiMGTMGgwcPRlZWFmbOnClhZd33xBNP4OTJk3bnVVzJjfq7/nzC6NGjERISgpkzZ6KgoACDBw922J/PQyJOzsfHB0OHDkV+fj6Cg4NhMplQU1Njt01ZWRmCg4OlKbAX2mr++WyC6/sJDg5GeXm53fqWlhZcvXrVKXsGgJiYGAQEBCA/Px+A8/S4YsUKfPnll9i7d6/d7Ym78nsZHBzc4c+5bd1AcKP+OhIfHw8Adj9DR/THwHZyDQ0NKCgoQEhICCZMmACVSoXMzEzb+rNnz6KoqAgJCQkSVtkz0dHRCA4Otuunrq4Ohw8ftvWTkJCAmpoa5OTk2Lb5+uuvYbVabf/ROJvLly+jqqoKISEhAAZ+j0IIrFixAp999hm+/vprREdH263vyu9lQkICfvjhB7t/mDIyMuDl5YURI0b0TyM3cLP+OpKbmwsAdj9Dh/TXg5OkJKFnn31WZGVlicLCQnHgwAGRmJgoAgICRHl5uRBCiMcff1xERESIr7/+Whw9elQkJCSIhIQEiau+sfr6enH8+HFx/PhxAUBs2rRJHD9+XFy8eFEIIcT69euFj4+P+Pzzz8WJEyfEvHnzRHR0tGhqarKNMXv2bDF+/Hhx+PBhsX//fjFkyBCxaNEiqVpqp7Me6+vrxXPPPSeys7NFYWGh2LNnj7jtttvEkCFDRHNzs22Mgdzj8uXLhbe3t8jKyhIlJSW2l8FgsG1zs9/LlpYWMWrUKJGUlCRyc3NFenq6CAwMFKtWrZKiJTs36y8/P1+88sor4ujRo6KwsFB8/vnnIiYmRtxxxx22MRzVHwPbySxYsECEhIQItVotwsLCxIIFC0R+fr5tfVNTk/jtb38rfH19hU6nE/fcc48oKSmRsOLO7d27VwBo91qyZIkQonVq3x/+8Aeh1+uFRqMRM2fOFGfPnrUbo6qqSixatEh4eHgILy8vsXTpUlFfXy9BNx3rrEeDwSCSkpJEYGCgUKlUIjIyUixbtsxu+pcQA7vHjnoDIN555x3bNl35vbxw4YKYM2eOcHNzEwEBAeLZZ58VZrO5n7tp72b9FRUViTvuuEP4+fkJjUYjYmNjxcqVK0Vtba3dOI7oj7dXJSJyEjyGTUTkJBjYREROgoFNROQkGNhERE6CgU1E5CQY2EREToKBTUTkJBjYREROgoFN1A+ysrIgk8na3QCJqDsY2EREToKBTUTkJBjYdEuwWq1ITU1FdHQ03NzcMHbsWHz88ccAfjpcsXPnTowZMwZarRa33347Tp48aTfGJ598gpEjR0Kj0SAqKgp//vOf7dYbjUa88MILCA8Ph0ajQWxsLN5++227bXJycjBx4kTodDpMmTKl3dO3iTrlmPtZEQ1sr776qoiLixPp6emioKBAvPPOO0Kj0YisrCzb3fSGDx8udu/eLU6cOCHuuusuERUVJUwmkxBCiKNHjwq5XC5eeeUVcfbsWfHOO+8INzc3uzvS3X///SI8PFx8+umnoqCgQOzZs0d8+OGHQoif7tgXHx8vsrKyxKlTp8T06dPFlClTpPjrICfFwCaX19zcLHQ6nTh48KDd8kceeUQsWrTIFqZt4SpE6+1M3dzcxPbt24UQQjzwwANi1qxZdp9fuXKlGDFihBBCiLNnzwoAIiMjo8Ma2v6MPXv22Jbt3LlTALC7tzdRZ3hIhFxefn4+DAYDZs2aBQ8PD9vrH//4BwoKCmzbXf9UHj8/PwwbNgynT58GAJw+fRpTp061G3fq1KnIy8uDxWJBbm4uFAoFZsyY0WktY8aMsX3f9jSSnz/+i+hG+BBecnkNDQ0AgJ07dyIsLMxunUajsQvtnurqA5BVKpXte5lMBqD1+DpRV3APm1zeiBEjoNFoUFRUhNjYWLtXeHi4bbtDhw7Zvq+ursa5c+cwfPhwAMDw4cNx4MABu3EPHDiAoUOHQqFQYPTo0bBardi3b1//NEW3JO5hk8vz9PTEc889h2eeeQZWqxXTpk1DbW0tDhw4AC8vL0RGRgIAXnnlFfj7+0Ov1+N3v/sdAgICMH/+fADAs88+i0mTJmHdunVYsGABsrOz8cYbb+Bvf/sbACAqKgpLlizBww8/jNdffx1jx47FxYsXUV5ejvvvv1+q1snVSH0Qnag/WK1WsXnzZjFs2DChUqlEYGCgSE5OFvv27bOdEPziiy/EyJEjhVqtFpMnTxbff/+93Rgff/yxGDFihFCpVCIiIkK89tprduubmprEM888Y3vmZmxsrNi6dasQ4qeTjtXV1bbt2x7KW1hY2Nftk4vgMx3plpeVlYVf/vKXqK6uho+Pj9TlEN0Qj2ETETkJBjYRkZPgIREiIifBPWwiIifBwCYichIMbCIiJ8HAJiJyEgxsIiInwcAmInISDGwiIifBwCYichL/HwkEmIkpzn04AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 250, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !,  bleu 1.000\n",
      "i lost . => j'ai perdu .,  bleu 1.000\n",
      "he's calm . => il est <unk> .,  bleu 0.658\n",
      "i'm home . => je suis chez moi .,  bleu 1.000\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = d2l.predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}, ', f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = d2l.reshape(d2l.concat([step[0][0][0] for step in dec_attention_weight_seq], 0), (1, 1, -1, num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOkAAAD/CAYAAAAUuPFlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJC9JREFUeJzt3XtYVNX6B/DvnoGB4Y5xS0ERQREUMRG8ZFmiqHnN1GOeI1HZk4KXeLrx9BPU7CA+qeQNywyOmqmVaHmSUgw5KihqmJVxyFBBwNuRqwk4s35/2OwYZoDZw8DeA+/nedbzuPfs2etFfVl7r73WXhxjjIEQIlkysQMghLSMkpQQiaMkJUTiKEkJkThKUkIkjpKUEImjJCVE4izEDqAt1Go1SktLYW9vD47jxA6HdDDGGKqrq9G9e3fIZJ23vTHrJC0tLYWXl5fYYRCRFRcXw9PTU+ww2o1ZJ6m9vT0AoCgqHA4KS1FiqP6hSJR6Na5crRa1fgAQa8xarVqNSbfL+P8HnZVZJ6nmEtdBYQkHK3GSlLOQi1Kvhp0ELvPEHlja2W91xP8XJoS0iJKUEImjJCVE4ihJCZE4SlJCJI6SlBCJoyQlROIoSQmROEpSQiSOkpQQiZNEkm7evBne3t6wtrZGWFgYzpw5I3ZIhEiG6Em6d+9exMbGIiEhAefPn8egQYMQERGBmzdvih0aIZIgepKuW7cO8+fPR1RUFAICArB161bY2Njgk08+0Tm2rq4OVVVVWoWQzk7UJK2vr8e5c+cQHh7O75PJZAgPD0dOTo7O8YmJiXB0dOQLzSUlXYGoSXr79m2oVCq4u7tr7Xd3d0d5ebnO8XFxcaisrORLcXFxR4VKiGjMaj6plZUVrKysxA6DkA4lakvq4uICuVyOGzduaO2/ceMGPDw8RIqKEGkRNUkVCgWGDBmCzMxMfp9arUZmZiaGDx8uYmSESIfol7uxsbGIjIxESEgIQkNDkZycjNraWkRFRYkdGiGSIHqSzp49G7du3UJ8fDzKy8sRHByMjIwMnc4kQroq0ZMUAGJiYhATEyN2GIRIkuiDGQghLaMkJUTiKEkJkThKUkIkjpKUEImjJCVE4ihJCZE4SlJCJE4Sgxna6n9nLqNBLs7qZpaW4v6ee9TDRtT6AeCBSpxl1ZQqFdAFXuBBLSkhEkdJSojEUZISInGUpIRIHCUpIRLXKXp3CWnN/fv3UV9fr7NfoVDA2tpahIgMR0lKOr379+/jEaUN7kH3UZGHhweKiooknagmSdKKigo4OTmZ4lSEmFx9fT3ugSEKdlCA+2s/GFLLy1FfXy/pJBV8T5qUlIS9e/fy27NmzcIjjzyCHj164MKFCyYNjhBTspXJYCf/q9jKzKNLRnCUW7du5d8cf+TIERw5cgSHDx/GhAkT8MYbb5g8QEJMxYLTLeZA8OVueXk5n6SHDh3CrFmzMG7cOHh7eyMsLMzkARJiKnKOg5z7KzPFGUgqnOCW1NnZmV/eISMjg1/HhTEGlUpl2ugIMSFzbUkFJ+mzzz6L559/HmPHjsWdO3cwYcIEAMAPP/wAX19fQefKzs7G5MmT0b17d3AchwMHDggNhxCDWXIcFI2KJWceWSo4SdevX4+YmBgEBATgyJEjsLOzAwCUlZVh4cKFgs5VW1uLQYMGYfPmzULDIEQwc21JBd+TWlpa4vXXX9fZ/9prrwmufMKECXxLbIi6ujrU1dXx27Q+KRHCguNg0aj1NJdBAkbFWVhYiO+//x43b96EWq3W+iw+Pt4kgemTmJiIFStWtNv5SefWtPXstEm6bds2LFiwAC4uLvDw8ADX6DcTx3HtmqRxcXGIjY3lt6uqqmghYWIwOQetllSuZwSSFAlO0lWrVuG9997DW2+91R7xtIjWJyVtoekw0lDBPG5KBSfp3bt3MXPmzPaIhZB2JeceFn5bvFAEEdy7O3PmTHz33XftEQsh7UrTcdS4mAPBLamvry+WLVuG3NxcDBw4EJaWllqfL1682OBz1dTU4LfffuO3i4qKkJ+fj27duqFnz55CQyOkRfImHUfm0pJyjDFBd8+9e/du/mQch99//93gc2VlZeGpp57S2R8ZGYm0tLRWv19VVQVHR0cUDu4L+y76tsD79x+IWj8g3tsCq1UqDPj5MiorK+Hg4NDscZr/J584usCG++vf6x5T48XK261+X2yCW9KioiKTVT569GgI/B1BiNHk0G49zaUlbdOjIk2CcWZybU+6NhnHQdbo/6rMTHp3jbpW27FjBwYOHAilUgmlUomgoCDs3LnT1LERYlKa3t3GxRwIbknXrVuHZcuWISYmBiNHjgQAnDhxAq+++ipu375t1PBAQjqCDJxW69lpW9KNGzciJSUFSUlJmDJlCqZMmYI1a9Zgy5Yt2LBhQ3vESIhJWKDJIxgjk3Tz5s3w9vaGtbU1wsLCcObMmWaP3b9/P0JCQuDk5ARbW1sEBwcLvuoUnKRlZWUYMWKEzv4RI0agrKxM6OkI6TAcB8gaFWO6Uvbu3YvY2FgkJCTg/PnzGDRoECIiInDzpv5Fabp164Z33nkHOTk5+PHHHxEVFYWoqCh8++23BtcpOEl9fX2xb98+vcH7+fkJPR0hHUYOTqcADx/RNC6NZ1o1tW7dOsyfPx9RUVEICAjA1q1bYWNjg08++UTv8aNHj8b06dPRv39/9OnTB0uWLEFQUBBOnDhhcNyC70lXrFiB2bNnIzs7m78nPXnyJDIzM/UmLyFSoWlBG28D0JmkkZCQgOXLl+t8v76+HufOnUNcXNxf55DJEB4ejpycnFbrZ4zh2LFjKCgoQFJSksFxC07SGTNm4PTp01i/fj3/JoX+/fvjzJkzGDx4sNDTEdJhmus4Ki4u1hrM0Nwkjtu3b0OlUsHd3V1rv7u7O3799ddm662srESPHj1QV1cHuVyOLVu2YOzYsQbHbdRz0iFDhmDXrl3GfLVduO4/AAd7e1Hq/t+kCFHq1djz6y1R6weA//6h+2b4jlAvcKqZznzSP//s4ODQriOO7O3tkZ+fj5qaGmRmZiI2NhY+Pj4YPXq0Qd83KEmrqqr4H6K1tyFIeXgV6dp0BjMI7DlycXGBXC7HjRs3tPbfuHEDHh4ezdcrk/Hv/woODsalS5eQmJhocJIa1HHk7OzM9145OTnB2dlZp2j2EyJVcj1FCIVCgSFDhiAzM5Pfp1arkZmZieHDhxt8HrVa3WLnVFMGtaTHjh1Dt27dAADff/+9wScnREq4Ji2pMcNZY2NjERkZiZCQEISGhiI5ORm1tbWIiooCAMybNw89evRAYmIigIev/AkJCUGfPn1QV1eHb775Bjt37kRKSorBdRqUpE8++ST/5969e8PLy0vnB2SM8e/jJUSKLGQcLBp17xozmGH27Nm4desW4uPjUV5ejuDgYGRkZPCdSdeuXYOs0fIVtbW1WLhwIUpKSqBUKuHv749du3Zh9uzZBtcpeKqaXC5HWVkZ3NzctPbfuXMHbm5uHfqCbM0UpIqiX7psx9G/LpSLWj8gbsdRKmoNnqp2oocX7BolUI1ajcevF3e+qWqMMb2XCTU1NZJemYoQmYyDTGZ+Y3cNTlLNW/o4jsOyZctgY2PDf6ZSqXD69GkEBwebPEBCTKXTJ+kPP/wA4GFLevHiRSgUCv4zhUKBQYMG6X1pNiFSIZdxkDdKUnlnS1JNr25UVBQ++OADSV/DE6KPXM5B3mgSqVxlHkkqeIB9amqqVoJWVVXhwIEDLQ6LIkQKuD8vdzWFk3XSJJ01axY2bdoEAPjjjz8QEhKCWbNmYeDAgfjyyy9NHiAhpiKTQStJzWShb+FJmp2djVGjRgEA0tPTwRhDRUUFNmzYgFWrVpk8QEJMRXNP2riYA8FJWllZyY8+ysjIwIwZM2BjY4NnnnkGhYWFgs6VmJiIoUOHwt7eHm5ubpg2bRoKCgqEhkSIQWRNLndlnTVJvby8kJOTg9raWmRkZGDcuHEAHi4/IfQ56fHjxxEdHY3c3FwcOXIEDQ0NGDduHGpra4WGRUir5BacTjEHggczLF26FHPnzoWdnR169erFj+TPzs7GwIEDBZ0rIyNDazstLQ1ubm44d+4cnnjiCZ3jaX1S0hY6j2BYJ03ShQsXIjQ0FMXFxRg7diw/TtHHx6fN96SVlZUAwF9ON0Xrk5K2kHFNBjOozSNJBY/dbS9qtRpTpkxBRUVFs+9/0deSenl50dhdkZnL2N2iof5wsPhrglrVAxV65/3aOcbuxsbG4t1334Wtra3WIr76rFu3zqhAoqOj8dNPP7X4giZan5S0hdxCBrnFX90wnWoR4R9++AENDQ38n5tj7HITMTExOHToELKzs+Hp6WnUOQhplVz2sGhI4yKyVQYlaeOJ3qac9M0Yw6JFi5Ceno6srKwWV2wjpK04uQxcoyTlOlOStqSqqgrHjh2Dv78//P39BX03Ojoau3fvxsGDB2Fvb4/y8of3V46OjlAqlW0NjRAt5pqkog4LTElJQWVlJUaPHo1HH32UL3v37hUaFiGt4mQyPlE5uQycmYwLFHVYIGNMb3nhhReEhkVIqzgLGTgLeaPSSZPUlMMCCelQmo6jxsUMiDoskJCOxMk5nWIORB0WSEhH0uk4UptHSyqpYYGEtCdzTVKjogwJCcH06dNha2sLzajCZ555hl9ljRAp4uRy7Y4judB32D8kZBHhbdu2YdSoUfxKD+Hh4S0er49RSbpjxw4MHDgQSqUSSqUSQUFBglcvJqSjaT1+adKqGkroIsJZWVmYM2cOvv/+e+Tk5MDLywvjxo3D9evXDa5TcJTr1q3DggULMHHiROzbtw/79u3D+PHj8eqrr2L9+vVCT0dIx2mmd7c9FxH+9NNPsXDhQgQHB8Pf3x8ff/wxv36MoQTfk27cuBEpKSmYN28ev2/KlCkIDAzE8uXL8dprrwk9JSEdQuee9M8/d9QiwgBw7949NDQ0NDsdUx/BSVpWVoYRI0bo7B8xYgTKysqEns4kWHEBmJ2tKHVfvVYjSr0aATaK1g9qZ90Vxt3btdU9pkZqleFv8eAsZeAs/4qVY2oA7b+IcGNvvfUWunfvjvDwcIPjFny56+vri3379uns37t3L/z8/ISejpCO8/B1gdoFfy0irCntNR1y9erV2LNnD9LT0wWNKRDckq5YsQKzZ89GdnY235t78uRJZGZm6k1eQiRDJgca9+jK1IK+buwiwgDw/vvvY/Xq1Th69CiCgoIE1Su4JZ0xYwZOnz4NFxcXHDhwAAcOHICLiwvOnDmD6dOnCz0dIR2nmZbUUMYuIrxmzRq8++67yMjIQEhIiOCwjZqqNmTIEOzatcuYrxIiHnmTllQurCUFhC8inJSUhPj4eOzevRve3t78dEw7OzvY2dkZVKdRSapSqZCeno5Lly4BAAICAjB16lRYWLR5eioh7ccESSp0EeGUlBTU19fjueee0zpPcz3I+gjOqp9//hlTpkxBeXk5+vXrB+DhbwtXV1d8/fXXGDBggNBTEtIxTJCkwMPX/cTExOj9LCsrS2v7ypUrRtXRmOB70pdffhmBgYEoKSnB+fPncf78eRQXFyMoKAivvPJKmwMipN3IuCb3pJ10Fkx+fj7Onj0LZ2dnfp+zszPee+89DB061KTBEWJKnEx7vC4nE+f5rlCCW9K+ffvqdEEDwM2bN+Hr62uSoAhpFxYWusUMCE7SxMRELF68GF988QVKSkpQUlKCL774AkuXLkVSUpLWGEhCJKWNj2DEIvhXyaRJkwA8fCGZ5j27mulqkydP5rc5joNKpTJVnIS0nU7HkXlc7gpOUlO+d5eQDtW09eysLemTTz5psspTUlKQkpLCd1MHBgYiPj4eEyZMMFkdhPDksiYtqXkkqahRenp6YvXq1Th37hzOnj2Lp59+GlOnTsXPP/8sZliks5I36TSSm0fHkahRau5hNd577z2kpKQgNzcXgYGBIkVFOq2ucrnbXlQqFT7//HPU1tY2O1iZFhEmbWKmHUei/yq5ePEi7OzsYGVlhVdffRXp6ekICAjQe2xiYiIcHR350nRGPSEtksl1ixkQnKQJCQm4evWqyQLo168f8vPzcfr0aSxYsACRkZH45Zdf9B4bFxeHyspKvhQXF5ssDtIFaFrSxsUMCE7SgwcPok+fPhgzZgx2797d4kubDKFQKODr64shQ4YgMTERgwYNwgcffKD3WCsrK51Z9IQYzMICsLBsVCRzt9ciwUman5+PvLw8BAYGYsmSJfDw8MCCBQuQl5dnkoDUanWbE58QvbrK5S4ADB48GBs2bEBpaSm2b9+OkpISjBw5EkFBQfjggw9QWVlp0Hni4uKQnZ2NK1eu4OLFi4iLi0NWVhbmzp1rTFiEtEzW5FK3MyepBmMMDQ0NqK+vB2MMzs7O2LRpE7y8vAxaY/TmzZuYN28e+vXrhzFjxiAvLw/ffvstxo4d25awCNHPTFtSoy7Kz507h9TUVHz22WewsrLCvHnzsHnzZn4WzMaNG7F48WLMnj27xfNs377dmOoJMU7TmS+d9Z504MCBGDZsGIqKirB9+3YUFxdj9erVWtPU5syZg1u3bpk0UELaTDMskC+iP4E0iOBfJbNmzcKLL76IHj16NHuMi4sL1GrjXk1BSLtpeolrJpe7gn6VNDQ0IC0tjUb6EPNkps9JBbWklpaWuH//fnvFQkj76irDAqOjo5GUlIQHDx60RzyEtB/ZnzNfNEXWSTuO8vLysH//fvTs2RMRERF49tlntQohkmWiy10hiwj//PPPmDFjBry9vcFxHJKTkwXXJ/hXiZOTE2bMmCG4IkJEJ5M16TgyfhHhrVu3IiwsDMnJyYiIiEBBQQHc3Nx0jr937x58fHwwc+ZMo5cFFZykqampRlVEiOiauSdt2hFqZWXV7MpqjRcRBoCtW7fi3//+Nz755BO8/fbbOscPHTqUf9Wtvs8NYdSDogcPHuDo0aP48MMPUV1dDQAoLS1FTY24a3US0qJmRhx5eXlpTYHUrOPSlGYR4cZriwpdRNgYglvSq1evYvz48bh27Rrq6uowduxY2NvbIykpCXV1ddi6dWt7xNkidqUAzMbw9R5NSa1motSr4SCBHkpLTpxBAbUCn8VzcktwckutbaBjFxE2huC/3SVLliAkJAR3796FUqnk90+fPl1rSThCJKeZAfYdtYiwsQS3pP/5z39w6tQpKBTay8B7e3vj+vXrJguMEJNr44ijtiwi3BaCW1K1Wq33pdclJSWwt7c3SVCEtAtN7y5fOmYR4bYSnKTjxo3TetbDcRxqamqQkJCAiRMnmjI2QkxL660MfxaBYmNjsW3bNvzrX//CpUuXsGDBAp1FhOPi4vjj6+vrkZ+fj/z8fNTX1+P69evIz8/Hb7/9ZnjYQoNcu3YtIiIiEBAQgPv37+P5559HYWEhXFxc8Nlnnwk9HSEdxwQD7IUuIlxaWorBgwfz2++//z7ef/99PPnkkzprmTZHcJJ6enriwoUL2LNnD3788UfU1NTgpZdewty5c7U6kgiRHBPNghGyiLC3tze/VpKxjBq8aGFhgb///e9tqpiQjsbJm6xPKoHHV4YQnKQ7duxo8fN58+YZHQwh7coEwwLFIDhJlyxZorXd0NCAe/fuQaFQwMbGhpKUSJe8SWeRXHjHkRgE/yq5e/euVqmpqUFBQQEef/xx6jgi0mamLyIzSXvv5+eH1atX67SyhEgKJ9MtZsBkUVpYWKC0tNTo769evRocx2Hp0qWmCokQbZpV1RoXMyD4nvSrr77S2maMoaysDJs2bcLIkSONCiIvLw8ffvghgoKCjPo+IQbpKksfTps2TWub4zi4urri6aefxtq1awUHUFNTg7lz52Lbtm1YtWqV4O8TYrCml7hmcrkrOElN/arO6OhoPPPMMwgPD281SWl9UtImHNckSTnxYhHA6Dcx3b59GwqFok0rm+3Zswfnz583eLGnxMRErFixwuj6SBdnpi2poCgrKioQHR0NFxcXuLu7w9nZGR4eHoiLi8O9e/cEVVxcXIwlS5bg008/hbW1YRO2aX1S0iadvePof//7H4YPH47r169j7ty56N+/PwDgl19+wcaNG3HkyBGcOHECP/74I3Jzc7F48eIWz3fu3DncvHkTjz32GL9PpVIhOzsbmzZtQl1dHeRNhm219O4ZQlrDyeTgGj0b5czkOanBSbpy5UooFApcvnxZ5/URK1euxLhx4/CPf/wD3333HTZs2NDq+caMGYOLFy9q7YuKioK/vz/eeustnQQlpO2aPhvtZC3pgQMH8OGHH+okKAB4eHhgzZo1mDhxIhISEhAZGdnq+ezt7TFgwACtfba2tnjkkUd09hNiEmZ6T2pwkpaVlSEwMLDZzwcMGACZTIaEhASTBEaIycm4h6XxthkwOEldXFxw5coVeHp66v28qKhI78uBhTB0EiwhRjHTltTgKCMiIvDOO++gvr5e57O6ujosW7YM48ePN2lwhJhUG99xJBZBHUchISHw8/NDdHQ0/P39wRjDpUuXsGXLFtTV1bU615QQMVXV1Gq1nlU1tSJGYziDk9TT0xM5OTlYuHAh4uLi+FdCcByHsWPHYtOmTejZs2e7BUqIsRQKBTw8PODVT7dD0sPDQ+f1tFIjaMRR7969cfjwYdy9exeFhYUAAF9fX3Tr1q1dgiPEFKytrVFUVKT3Vk2hUBg8mEYsRg0LdHZ2RmhoqKljIaTdWFtbSz4Zm2Med86EdGGUpIRIHCUpIRJHSUqIxBk9n1RK5GOfh7wN81rbovLBGlHq1bhap9tj2dHuNOgu4NUR7rfxzfDmglpSQiSOkpQQiaMkJUTiKEkJkThKUkIkjpKUEImjJCVE4ihJCZE4SlJCJI6SlBCJoyQlROJETdLly5eD4zit4u/vL2ZIhEiO6APsAwMDcfToUX7bwkL0kAiRFNEzwsLCAh4eHmKHQYhkiX5PWlhYiO7du8PHxwdz587FtWvXmj22rq4OVVVVWoWQzk7UJA0LC0NaWhoyMjKQkpKCoqIijBo1CtXV1XqPT0xMhKOjI1+8vLw6OGJCOh7HmHRmzlZUVKBXr15Yt24dXnrpJZ3P9a307eXlhcqya21azLgtMnsFiFKvRlef9B1fV4HKykrR/v07guj3pI05OTmhb9+++O233/R+TuuTkq5I9HvSxmpqanD58mU8+uijYodCiGSImqSvv/46jh8/jitXruDUqVOYPn065HI55syZI2ZYhEiKqJe7JSUlmDNnDu7cuQNXV1c8/vjjyM3Nhaurq5hhESIpoibpnj17xKyeELMgqXtSQoguSlJCJI6SlBCJoyQlROIoSQmROEpSQiSOkpQQiaMkJUTiJDXAXijNBJ6qZqa2dYRatVq0ugHgDyZu/YB4SxBq6pXQRK52YdZJqpl36tU3UORIiJiqq6vh6OgodhjtRlLzSYVSq9UoLS2Fvb09OI4T/H3NfNTi4uJOPR+xOeb+8zPGUF1dje7du0Mm67x3bmbdkspkMnh6erb5PA4ODmb5n9RUzPnn78wtqEbn/fVDSCdBSUqIxHXpJLWyskJCQkKXfSVLV//5zYVZdxwR0hV06ZaUEHNASUqIxFGSEiJxlKSESFyXTtLNmzfD29sb1tbWCAsLw5kzZ8QOqUMkJiZi6NChsLe3h5ubG6ZNm4aCggKxwyLN6LJJunfvXsTGxiIhIQHnz5/HoEGDEBERgZs3b4odWrs7fvw4oqOjkZubiyNHjqChoQHjxo1DbW2t2KERPbrsI5iwsDAMHToUmzZtAvBwHLCXlxcWLVqEt99+W+ToOtatW7fg5uaG48eP44knnhA7HNJEl2xJ6+vrce7cOYSHh/P7ZDIZwsPDkZOTI2Jk4qisrAQAdOvWTeRIiD5dMklv374NlUoFd3d3rf3u7u4oLy8XKSpxqNVqLF26FCNHjsSAAQPEDofoYdazYEjbRUdH46effsKJEyfEDoU0o0smqYuLC+RyOW7cuKG1/8aNG/Dw8BApqo4XExODQ4cOITs72yRT/kj76JKXuwqFAkOGDEFmZia/T61WIzMzE8OHDxcxso7BGENMTAzS09Nx7Ngx9O7dW+yQSAu6ZEsKALGxsYiMjERISAhCQ0ORnJyM2tpaREVFiR1au4uOjsbu3btx8OBB2Nvb8/fhjo6OUCqVIkdHdLAubOPGjaxnz55MoVCw0NBQlpubK3ZIHQKA3pKamip2aESPLvuclBBz0SXvSQkxJ5SkhEgcJSkhEkdJSojEUZISInGUpIRIHCUpIRJHSUqIxFGSdlLe3t5ITk42+vtpaWlwcnIyWTzEeF0+SV944QVMmzZNa98XX3wBa2trrF27VpygTCAvLw+vvPKKQcfqS+jZs2fjv//9bztERoTqsgPsm/Pxxx8jOjoaW7duNevB9q6urm36vlKppMH2EtHlW9LG1qxZg0WLFmHPnj1aCXrw4EE89thjsLa2ho+PD1asWIEHDx4AAF588UVMmjRJ6zwNDQ1wc3PD9u3b9dajuZQ8cOAA/Pz8YG1tjYiICBQXF2sdl5KSgj59+kChUKBfv37YuXMn/xljDMuXL0fPnj1hZWWF7t27Y/HixfznjVvHlo4dPXo0rl69itdeew0cx/HrvOq73G0pHgDgOA4ff/wxpk+fDhsbG/j5+eGrr77iP7979y7mzp0LV1dXKJVK+Pn5ITU1tdl/D/Inccf3iy8yMpJNnTqVvfnmm8zOzo4dPXpU6/Ps7Gzm4ODA0tLS2OXLl9l3333HvL292fLlyxljjJ08eZLJ5XJWWlrKf2f//v3M1taWVVdX660zNTWVWVpaspCQEHbq1Cl29uxZFhoaykaMGKF1DktLS7Z582ZWUFDA1q5dy+RyOTt27BhjjLHPP/+cOTg4sG+++YZdvXqVnT59mn300Uf893v16sXWr1/f6rF37txhnp6ebOXKlaysrIyVlZXxMTo6OhocD2MPZ9d4enqy3bt3s8LCQrZ48WJmZ2fH7ty5wxhjLDo6mgUHB7O8vDxWVFTEjhw5wr766itB/15dESVpZCRTKBQMAMvMzNT5fMyYMeyf//yn1r6dO3eyRx99lN8OCAhgSUlJ/PbkyZPZCy+80GydqampDIDW1LhLly4xAOz06dOMMcZGjBjB5s+fr/W9mTNnsokTJzLGGFu7di3r27cvq6+v11tH4yQVcmzjGBsnaWvxMPYwSf/v//6P366pqWEA2OHDhxljD/9eoqKi9MZAmkeXuwCCgoLg7e2NhIQE1NTUaH124cIFrFy5EnZ2dnyZP38+ysrKcO/ePQDAyy+/zF+23bhxA4cPH8aLL77YYp0WFhYYOnQov+3v7w8nJydcunQJAHDp0iWMHDlS6zsjR47kP585cyb++OMP+Pj4YP78+UhPT+cvwZsScmxzWotHIygoiP+zra0tHBwc+HcZL1iwAHv27EFwcDDefPNNnDp1SlAMXRUlKYAePXogKysL169fx/jx41FdXc1/VlNTgxUrViA/P58vFy9eRGFhIaytrQEA8+bNw++//46cnBzs2rULvXv3xqhRo9o1Zi8vLxQUFGDLli1QKpVYuHAhnnjiCTQ0NLTp2LaytLTU2uY4Dmq1GgAwYcIE/v63tLQUY8aMweuvv27yGDobStI/9erVC8ePH0d5eblWoj722GMoKCiAr6+vTpHJHv71PfLII5g2bRpSU1ORlpZmUK/wgwcPcPbsWX67oKAAFRUV6N+/PwCgf//+OHnypNZ3Tp48iYCAAH5bqVRi8uTJ2LBhA7KyspCTk4OLFy/qra+lYxUKBVQqVYvxGhKPIVxdXREZGYldu3YhOTkZH330kaDvd0X0CKYRLy8vZGVl4amnnkJERAQyMjIQHx+PSZMmoWfPnnjuuecgk8lw4cIF/PTTT1i1ahX/3ZdffhmTJk2CSqVCZGRkq3VZWlpi0aJF2LBhAywsLBATE4Nhw4YhNDQUAPDGG29g1qxZGDx4MMLDw/H1119j//79OHr0KICHva8qlQphYWGwsbHBrl27oFQq0atXL526WjvW29sb2dnZ+Nvf/gYrKyu4uLjonKO1eAwRHx+PIUOGIDAwEHV1dTh06BD/S4m0QOybYrFpencbKykpYX5+fmzYsGGssrKSZWRksBEjRjClUskcHBxYaGioVk8qY4yp1WrWq1cvrY6U5mg6Zb788kvm4+PDrKysWHh4OLt69arWcVu2bGE+Pj7M0tKS9e3bl+3YsYP/LD09nYWFhTEHBwdma2vLhg0bptUz3bgzqLVjc3JyWFBQELOysmKa/xJNO45ai4exhx1H6enpWvscHR35dye9++67rH///kypVLJu3bqxqVOnst9//73Vv6+ujt5xZCI1NTXo0aMHUlNT8eyzz7Z4bFpaGpYuXYqKioqOCY6YNbrcbSO1Wo3bt29j7dq1cHJywpQpU8QOiXQylKRtdO3aNfTu3Ruenp5IS0uDhQX9lRLTostdQiSOHsEQInGUpIRIHCUpIRJHSUqIxFGSEiJxlKSESBwlKSESR0lKiMT9P0JlkN7ZyGyHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plus one to include the end-of-sequence token\n",
    "d2l.show_heatmaps(attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(), xlabel='Key posistions', ylabel='Query posistions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6. <a id='toc11_6_'></a>[Transformer](#toc0_)\n",
    "Transformer是一种神经网络架构，最初由Vaswani等人在2017年的论文《Attention is All You Need》中提出。它主要用于处理序列到序列的任务，如机器翻译。Transformer架构的核心是自注意力机制（Self-Attention），它能够捕捉序列中不同位置之间的依赖关系。\n",
    "```python\n",
    "完全基于注意力机制的Encoder-Decoder架构。\n",
    "1.多头自注意力机制；\n",
    "2.掩码；\n",
    "3.Encoder-Decoder框架。\n",
    "```\n",
    "\n",
    "![Transformer](./Pytorch_Pictures/Transformer/Transformer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.1. <a id='toc11_6_1_'></a>[简洁实现](#toc0_)\n",
    "PyTorch中提供了nn.Transformer类，可以方便地实现Transformer模型。同时，nn.TransformerEncoder和nn.TransformerDecoder类分别用于实现编码器和解码器。以及nn.TransformerEncoderLayer和nn.TransformerDecoderLayer类分别用于实现编码器和解码器的每一层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "trans = nn.Transformer(\n",
    "    d_model = 512, \n",
    "    nhead = 8, \n",
    "    num_encoder_layers = 6, \n",
    "    num_decoder_layers = 6, \n",
    "    dim_feedforward = 2048, \n",
    "    dropout = 0.1, \n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "\n",
    "# 创建输入张量\n",
    "src = torch.rand(2, 32, 512)  # (batch_size, seq_len, embed_size)\n",
    "tgt = torch.rand(2, 32, 512)  # (batch_size, seq_len, embed_size)\n",
    "\n",
    "# 前向传播\n",
    "output = trans(src, tgt)\n",
    "\n",
    "print(\"输出形状:\", output.shape)  # 输出形状: (batch_size, seq_len, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "# ----------------\n",
    "# parameters\n",
    "# ----------------\n",
    "d_model = 512 \n",
    "nhead = 8\n",
    "dim_feedforward = 2048 \n",
    "dropout = 0.1\n",
    "batch_first = True \n",
    "num_layers = 6\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# model\n",
    "# ----------------\n",
    "encoder = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(\n",
    "        d_model = d_model, \n",
    "        nhead = nhead, \n",
    "        dim_feedforward = dim_feedforward, \n",
    "        dropout = dropout, \n",
    "        batch_first = batch_first\n",
    "    ), \n",
    "    num_layers = num_layers\n",
    ")\n",
    "\n",
    "decoder = nn.TransformerDecoder(\n",
    "    nn.TransformerDecoderLayer(\n",
    "        d_model = d_model, \n",
    "        nhead = nhead, \n",
    "        dim_feedforward = dim_feedforward, \n",
    "        dropout = dropout, \n",
    "        batch_first = batch_first\n",
    "    ), \n",
    "    num_layers = num_layers\n",
    ")\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_outputs = self.encoder(src)\n",
    "        dec_state = self.init_state(enc_outputs)\n",
    "        dec_outputs, dec_state = self.decoder(tgt, dec_state)\n",
    "        return dec_outputs, dec_state \n",
    "    \n",
    "    def init_state(self, enc_outputs):\n",
    "        '''直接在Seq2SeqModel中实现对Decoder的初始化'''\n",
    "        return enc_outputs, [[None] * self.decoder.num_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.2. <a id='toc11_6_2_'></a>[位置编码](#toc0_)\n",
    "为什么需要位置编码？  \n",
    "  - 在Transformer中，所有输入数据是并行处理的，因此模型并不直接知道每个单词或元素在序列中的顺序。例如，输入序列 [A, B, C] 和 [C, B, A] 对于Transformer的自注意力机制来说，如果没有位置编码，将会完全一样。 \n",
    "  \n",
    "位置编码的原理：\n",
    "  - 位置编码将位置信息通过某种方式编码为向量，添加到每个输入元素的表示中，使得每个元素不仅包含其自身的特征，还包括它在序列中的位置。 \n",
    "\n",
    "\n",
    "常见的两种位置编码方式\n",
    "  - 绝对位置编码（Absolute Positional Encoding）：每个位置都有唯一的编码，通常采用正弦和余弦函数生成。Google一帮人发明了利用sin和cos函数编码位置信息并添加到输入X中。\n",
    "  - 相对位置编码（Relative Positional Encoding）：编码的是元素间的相对位置关系，而非具体的绝对位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.6.2.1. <a id='toc11_6_2_1_'></a>[绝对位置编码](#toc0_)\n",
    "\n",
    "$\\begin{aligned}&\\text{假设输入序列的长度为 }L,\\text{ 每个输入的词问量维度为 }d,\\text{ 那么对于位置 }pos\\text{ 和维度 }i\\text{ 的位置编码}\\\\&PE(pos,i)\\text{,定义如下:}\\\\&PE(pos,2i)=\\sin\\left(\\frac{pos}{10000\\frac{2i}{d}}\\right)\\\\&PE(pos,2i+1)=\\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\\\&\\bullet\\quad pos:\\text{ 表示序列中每个元素的位置。}\\\\&\\bullet\\quad i:\\text{ 向量的维度索引。}\\\\&\\bullet\\quad d:\\quad\\text{嵌入向量的维度，偶数维度使用正弦，奇数维度使用余弦。}\\end{aligned}$\n",
    "\n",
    "假设我们有一个序列长度 𝐿=4，embedding维度𝑑=4。用简单的正弦和余弦函数计算得出位置编码的矩阵：\n",
    "\n",
    "|Position|\tPE(pos, 0) (sin)|\tPE(pos, 1) (cos)|\tPE(pos, 2) (sin)|\tPE(pos, 3) (cos)|\n",
    "|-|-|-|-|-|\n",
    "|0\t|0\t|1\t|0\t|1|\n",
    "|1\t|0.8415\t|0.5403\t|0.00999\t|0.99995|\n",
    "|2\t|0.9093\t|-0.4161\t|0.01998\t|0.9998|\n",
    "|3\t|0.1411\t|-0.98999\t|0.02997\t|0.99955|\n",
    "\n",
    "最终，Transformer会将这些位置编码与输入embedding相加，得到位置敏感的嵌入表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3]]),\n",
       " tensor([  1., 100.]),\n",
       " tensor([[0.0000, 0.0000],\n",
       "         [1.0000, 0.0100],\n",
       "         [2.0000, 0.0200],\n",
       "         [3.0000, 0.0300]]),\n",
       " tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "         [ 0.1411, -0.9900,  0.0300,  0.9996]]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_len = 4\n",
    "num_hiddens = 4\n",
    "\n",
    "# 创建一个位置编码矩阵框架\n",
    "# shape: (pos_len, num_hiddens)\n",
    "pos_matrix = torch.zeros(size=(pos_len, num_hiddens))\n",
    "\n",
    "# 创建位置索引\n",
    "pos = torch.arange(pos_len).reshape(-1, 1) \n",
    "# 创建位置编码的除数\n",
    "div_term = torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "# 计算位置除以除数，自动横向做了广播\n",
    "pos_div_term = pos / div_term\n",
    "\n",
    "# 计算位置编码\n",
    "## 偶数列使用正弦函数\n",
    "pos_matrix[:, 0::2] = torch.sin(pos_div_term)\n",
    "## 奇数列使用余弦函数\n",
    "pos_matrix[:, 1::2] = torch.cos(pos_div_term)\n",
    "\n",
    "pos, div_term, pos_div_term, pos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1b37be8500>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "#@save\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 创建一个足够长的P\n",
    "        # shape: (1, max_len, num_hiddens)\n",
    "        self.P = torch.zeros(size=(1, max_len, num_hiddens))\n",
    "\n",
    "        # pos / 10000^(2i/d)\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, (torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens))\n",
    "        \n",
    "        # 计算位置编码\n",
    "        ## 偶数列使用正弦函数\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        ## 奇数列使用余弦函数\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, seq_len, embedding_dim) = (batch_size, pos_len, num_hiddens)\n",
    "        # P[:, :X.shape[1], :] : (1, pos_len, num_hiddens)\n",
    "        # 自动广播了batch维度：(batch_size, pos_len, num_hiddens) + (1, pos_len, num_hiddens) = (batch_size, pos_len, num_hiddens)\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)  # On the same device\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, num_steps, encoding_dim = 32, 60, 32\n",
    "pos_encoding = PositionalEncoding(num_hiddens=encoding_dim, max_len=10000, dropout=0.1)\n",
    "pos_encoding.eval()\n",
    "\n",
    "# 创建一个随机输入张量\n",
    "X = torch.rand(size=(batch_size, num_steps, encoding_dim))\n",
    "\n",
    "# 对输入张量进行位置编码\n",
    "X = pos_encoding(X=X)\n",
    "\n",
    "# 获取位置编码矩阵\n",
    "batch_size, num_steps, encoding_dim = X.shape\n",
    "P = pos_encoding.P[:, :num_steps, :encoding_dim]\n",
    "\n",
    "# Draw a plot picture\n",
    "plt.figure(figsize=(6, 2.5))\n",
    "# num_steps: torch.arange(num_steps)\n",
    "# P[0, :, 6:10].T[0]: 取P的第0个batch，num_hiddens的第6到10列，并转置\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[0], label='col 6')\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[1], label='col 7')\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[2], label='col 8')\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[3], label='col 9')\n",
    "plt.xlabel('Row (position)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.6.2.2. <a id='toc11_6_2_2_'></a>[相对位置编码](#toc0_)\n",
    "相对位置编码（Relative Positional Encoding）可以通过学习矩阵或嵌入层来表示位置差值。相对位置编码不直接关心位置 𝑝𝑜𝑠 的绝对位置，而是关心两个位置之间的距离 𝑝𝑜𝑠𝑖−𝑝𝑜𝑠𝑗。这在长序列中非常有用，因为它不局限于特定位置，而是考虑相对距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class RelativePositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 创建一个足够长的相对位置编码矩阵\n",
    "        self.relative_positions = torch.zeros(size=(max_len, max_len, num_hiddens))\n",
    "        for i in range(max_len):\n",
    "            for j in range(max_len):\n",
    "                relative_position = i - j\n",
    "                self.relative_positions[i, j, :] = self._get_relative_position_encoding(relative_position)\n",
    "\n",
    "    def _get_relative_position_encoding(self, relative_position):\n",
    "        # 使用正弦和余弦函数计算相对位置编码\n",
    "        encoding = torch.zeros(self.num_hiddens)\n",
    "        for i in range(0, self.num_hiddens, 2):\n",
    "            encoding[i] = torch.sin(torch.tensor(relative_position / (10000 ** (i / self.num_hiddens))))\n",
    "            if i + 1 < self.num_hiddens:\n",
    "                encoding[i + 1] = torch.cos(torch.tensor(relative_position / (10000 ** ((i + 1) / self.num_hiddens))))\n",
    "        return encoding\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "        relative_positions = self.relative_positions[:seq_len, :seq_len, :].to(X.device)\n",
    "        return X + relative_positions\n",
    "\n",
    "\n",
    "# 测试相对位置编码\n",
    "batch_size, encoding_dim, num_steps = 32, 60, 32\n",
    "relative_pos_encoding = RelativePositionalEncoding(num_hiddens=encoding_dim, max_len=num_steps)\n",
    "relative_pos_encoding.eval()\n",
    "\n",
    "# 创建一个随机输入张量\n",
    "X = torch.zeros(size=(batch_size, num_steps, encoding_dim))\n",
    "X_encoded = relative_pos_encoding(X=X)\n",
    "\n",
    "print(X_encoded.shape)  # 应输出: torch.Size([32, 60, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.6.2.3. <a id='toc11_6_2_3_'></a>[可学习的位置编码](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        # 可学习的参数\n",
    "        self.position_encoding = nn.Parameter(torch.zeros(size=(max_len, num_hiddens)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, seq_len, embedding_dim)\n",
    "        seq_len = X.size(1)\n",
    "        X = X + self.position_encoding[:seq_len, :].unsqueeze(0)\n",
    "        return self.dropout(X)\n",
    "    \n",
    "\n",
    "# Test\n",
    "batch_size, num_steps, encoding_dim = 32, 60, 32\n",
    "\n",
    "learned_pos_encoding = LearnedPositionalEncoding(num_hiddens=encoding_dim, dropout=0, max_len=num_steps)\n",
    "learned_pos_encoding.eval()\n",
    "\n",
    "X = torch.zeros(size=(batch_size, num_steps, encoding_dim))\n",
    "X = learned_pos_encoding(X=X)\n",
    "\n",
    "print(X.shape)  # 应输出: torch.Size([32, 60, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.3. <a id='toc11_6_3_'></a>[基于位置的前馈网络](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "#@save\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, seq_len, embed_size = 2, 3, 4\n",
    "\n",
    "# 实例化对象\n",
    "ffn = PositionWiseFFN(ffn_num_input=embed_size, ffn_num_hiddens=4, ffn_num_outputs=embed_size)\n",
    "ffn.eval()\n",
    "\n",
    "# (batch_size, seq_len, embed_size)\n",
    "x = torch.ones(size=(batch_size, seq_len, embed_size))\n",
    "\n",
    "# (batch_size, seq_len, ffn_num_outputs)\n",
    "y = ffn(x)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.4. <a id='toc11_6_4_'></a>[残差连接和层规范化](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tensor([[-1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>) \n",
      "batch norm: tensor([[-1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)\n",
    "\n",
    "\n",
    "# 测试\n",
    "X = torch.tensor([[1, 2], \n",
    "                  [2, 3]], dtype=torch.float32)\n",
    "\n",
    "# 在训练模式下计算X的均值和方差\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, seq_len, embed_size = 2, 3, 4\n",
    "\n",
    "# 实例化对象\n",
    "add_norm = AddNorm(normalized_shape=embed_size, dropout=0.5)\n",
    "add_norm.eval()\n",
    "\n",
    "# 测试\n",
    "X = torch.ones(size=(batch_size, seq_len, embed_size))\n",
    "\n",
    "add_norm(X=X, Y=X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.5. <a id='toc11_6_5_'></a>[编码器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        # From d2l\n",
    "        # self.attention = d2l.MultiHeadAttention(\n",
    "        #     key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n",
    "        #     use_bias)\n",
    "\n",
    "        # From 自己\n",
    "        self.attention = MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            query_size = query_size, \n",
    "            key_size = key_size, \n",
    "            num_hiddens = num_hiddens, \n",
    "            value_size = value_size,\n",
    "            attention_type='dot',\n",
    "            dropout = dropout\n",
    "        )\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        Y = self.addnorm1(X=X, Y=self.attention(queries=X, keys=X, values=X, valid_lens=valid_lens))\n",
    "        return self.addnorm2(X=Y, Y=self.ffn(Y))\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, seq_len, embed_size = 2, 100, 24\n",
    "key_size, query_size, value_size, num_heads, num_hiddens, dropout = 24, 24, 24, 8, 24, 0.5 \n",
    "norm_shape = [100, 24]\n",
    "ffn_num_input, ffn_num_hiddens = 24, 48\n",
    " \n",
    "encoder_blk = EncoderBlock(key_size=key_size, query_size=query_size, value_size=value_size, num_hiddens=num_hiddens, norm_shape=norm_shape, ffn_num_input=ffn_num_input, ffn_num_hiddens=ffn_num_hiddens, num_heads=num_heads, dropout=dropout)\n",
    "encoder_blk.eval()\n",
    "\n",
    "X = torch.ones(size=(batch_size, seq_len, embed_size))\n",
    "\n",
    "encoder_blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer编码器\"\"\"\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 因为位置编码值在-1和1之间，\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放，\n",
    "        # 然后再与位置编码相加。\n",
    "        # X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        X = self.pos_encoding(self.embedding(X) * torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X)\n",
    "            # self.attention_weights[i] = blk.attention.attention.attention_weights_numpy\n",
    "        return X\n",
    "\n",
    "\n",
    "# 测试\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=200, \n",
    "    key_size=24, \n",
    "    query_size=24, \n",
    "    value_size=24, \n",
    "    num_hiddens=24, \n",
    "    norm_shape=[100, 24], \n",
    "    ffn_num_input=24, \n",
    "    ffn_num_hiddens=48, \n",
    "    num_heads=8, \n",
    "    num_layers=2, \n",
    "    dropout=0.5\n",
    ")\n",
    "encoder.eval()\n",
    "\n",
    "encoder(torch.ones((2, 100), dtype=torch.long)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.6. <a id='toc11_6_6_'></a>[解码器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    # The `i`-th block in the decoder\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        # self.attention1 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.attention1 = MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            query_size = query_size, \n",
    "            key_size = key_size, \n",
    "            num_hiddens = num_hiddens, \n",
    "            value_size = value_size,\n",
    "            attention_type='dot',\n",
    "            dropout = dropout\n",
    "        )   \n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        # self.attention2 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.attention2 = MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            query_size = query_size, \n",
    "            key_size = key_size, \n",
    "            num_hiddens = num_hiddens, \n",
    "            value_size = value_size,\n",
    "            attention_type='dot'\n",
    "        )\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # During training, all the tokens of any output sequence are processed\n",
    "        # at the same time, so `state[2][self.i]` is `None` as initialized.\n",
    "        # When decoding any output sequence token by token during prediction,\n",
    "        # `state[2][self.i]` contains representations of the decoded output at\n",
    "        # the `i`-th block up to the current time step\n",
    "        # X: (batch_size, seq_len, embed_size)\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        \n",
    "        if self.training:  \n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            # Shape of `dec_valid_lens`: (`batch_size`, `num_steps`), where\n",
    "            # every row is [1, 2, ..., `num_steps`]\n",
    "            dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "\n",
    "        # Self-attention\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        # Encoder-decoder attention. Shape of `enc_outputs`:\n",
    "        # (`batch_size`, `num_steps`, `num_hiddens`)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state\n",
    "    \n",
    "\n",
    "# 测试\n",
    "decoder_blk = DecoderBlock(\n",
    "    key_size=key_size, \n",
    "    query_size=query_size, \n",
    "    value_size=value_size, \n",
    "    num_hiddens=num_hiddens, \n",
    "    norm_shape=norm_shape, \n",
    "    ffn_num_input=ffn_num_input, \n",
    "    ffn_num_hiddens=ffn_num_hiddens, \n",
    "    num_heads=num_heads, \n",
    "    dropout=dropout, \n",
    "    i=0\n",
    ")\n",
    "decoder_blk.eval()\n",
    "\n",
    "X = d2l.ones((2, 100, 24))\n",
    "\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): Embedding(200, 24)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (blks): Sequential(\n",
       "    (block0): DecoderBlock(\n",
       "      (attention1): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attention2): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=False, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionWiseFFN(\n",
       "        (dense1): Linear(in_features=24, out_features=48, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=48, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm3): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block1): DecoderBlock(\n",
       "      (attention1): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attention2): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=False, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionWiseFFN(\n",
       "        (dense1): Linear(in_features=24, out_features=48, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=48, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm3): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=24, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            # Decoder self-attention weights\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # Encoder-decoder attention weights\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n",
    "    \n",
    "\n",
    "# 测试\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size=200, \n",
    "    key_size=24, \n",
    "    query_size=24, \n",
    "    value_size=24, \n",
    "    num_hiddens=24, \n",
    "    norm_shape=[100, 24], \n",
    "    ffn_num_input=24, \n",
    "    ffn_num_hiddens=48, \n",
    "    num_heads=8, \n",
    "    num_layers=2, \n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.7. <a id='toc11_6_7_'></a>[基于Transformer的Seq2Seq网络](#toc0_)\n",
    "```shell\n",
    "基于Transformer的Seq2Seq神经网络框架。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.028, 6658.7 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD/CAYAAADVGuzgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKgZJREFUeJzt3Xl8U2W+P/BPkmbrEtK9pXSDQktZihSoBdRxgFJwgRGvgN6fyKBeF8bRIs7gvAZEvcIVZPh57ZU74zp3URRHVBB+lGpRsICUvUChtVDoStd0TdLk+f0RGqxdKG3S06Sf9+uVV5Nznp5887yaD4fnPOccmRBCgIiIBjy51AUQEVHPMLCJiFwEA5uIyEUwsImIXAQDm4jIRTCwiYhcBAObiMhFeEhdwEBktVpRUlICHx8fyGQyqcshIhchhEB9fT2GDh0Kudzx+8MM7E6UlJQgPDxc6jKIyEVdvnwZw4YNc/h2Gdid8PHxAWDrdJ1OJ3E1zmE2m7Fnzx6kpKRAqVRKXc6Aw/7pGvuma9XV1YiOjrZniKMxsDvRNgyi0+ncOrA9PT2h0+n4pesE+6dr7Juumc1mAHDaUCoPOhIRuQgGNhGRi2BgExG5CAY2EZGLYGATEbkIBnY3TK1WqUsgIrJjYHfjXJlB6hKIiOwY2N04frlW6hKIiOwY2N04UVQrdQlERHYM7G4cu1wD3qOYiAYKBnY3KupNKKlrkboMIiIADOwbyrlUI3UJREQAGNg3dJSBTUQDBAP7Bo4WMbCJaGBgYN9AbokBTaZWqcsgImJgdydYp4bFKnDySp3UpRARMbC7M2GYHgAPPBLRwMDA7kZChB4ADzwS0cDAwO7GhHA9ACCniCfQEJH0GNjdiAvRQe0hR22TGT9VNkpdDhENcgzsbqg85EjgODYRDRAM7BuYGOkLADjG+dhEJDEG9g0kXgts7mETkdQY2Dcw8dpMkfPlDahrNktbDBENagzsG/D3ViPK3xMAh0WISFoM7B5oG8fmfGwikhIDuwfs49jcwyYiCTGwe6AtsI8X1aLVwjupE5E0GNg9MDLIBz5qDzSaLMgrr5e6HCIapBjYPaCQyzCh7boivDEvEUmEgd1DiTzwSEQSY2D3EE+gISKpMbB7aEK4HjIZUFTdhIp63kmdiPofA7uHfDRKxAb7AACOXqqVthgiGpQY2DfBfgIN52MTkQQY2DchMYLj2EQkHQb2TWg78HjqSh2MrRaJqyGiwUbywE5PT0dUVBQ0Gg2SkpJw+PDhLtvm5uZiwYIFiIqKgkwmw+bNmzu0eemllyCTydo94uLiHFJrpL8n/L1UMFmsOF1scMg2iYh6StLA3rp1K9LS0rBmzRocPXoUCQkJmD17NioqKjpt39TUhOHDh2P9+vUICQnpcrtjxoxBaWmp/bF//36H1CuTyXhDAyKSjKSBvWnTJjz22GNYunQp4uPjsWXLFnh6euK9997rtP3kyZOxYcMGLFq0CGq1usvtenh4ICQkxP4ICAhwWM2cj01EUvGQ6o1NJhNycnKwatUq+zK5XI6ZM2ciOzu7T9u+cOEChg4dCo1Gg+TkZKxbtw4RERFdtjcajTAajfbXBoNtuMNsNsNsbn/TgoQw29S+IxerYTKZIJPJ+lSrVNo+1y8/H9mwf7rGvumas/tEssCurKyExWJBcHBwu+XBwcE4d+5cr7eblJSEDz74ALGxsSgtLcXatWtx22234fTp0/Dx8en0d9atW4e1a9d2WL5nzx54enq2W2ayAHKZAlcbTPjvz3fBX9PrUgeEjIwMqUsY0Ng/XWPfdNTU1OTU7UsW2M4yZ84c+/Px48cjKSkJkZGR+OSTT7Bs2bJOf2fVqlVIS0uzvzYYDAgPD0dKSgp0Ol2H9v9VchAnrxgwZMQtmJsQ6vgP0Q/MZjMyMjIwa9YsKJVKqcsZcNg/XWPfdK2qqsqp25cssAMCAqBQKFBeXt5ueXl5ebcHFG+WXq/HqFGjkJ+f32UbtVrd6Zi4Uqns9A9yUqQ/Tl4x4ESxAQsmdT3U4gq6+oxkw/7pGvumI2f3h2QHHVUqFRITE5GZmWlfZrVakZmZieTkZIe9T0NDAwoKChAa6rg9YR54JCIpSDokkpaWhiVLlmDSpEmYMmUKNm/ejMbGRixduhQA8PDDDyMsLAzr1q0DYDtQeebMGfvz4uJiHD9+HN7e3oiJiQEAPP/887jnnnsQGRmJkpISrFmzBgqFAosXL3ZY3RMj9QCAs6UGNBpb4aV2u5ElIhqAJE2ahQsX4urVq1i9ejXKysowYcIE7N69234gsqioCHL59f8ElJSU4JZbbrG/3rhxIzZu3Ig77rgDWVlZAIArV65g8eLFqKqqQmBgIKZPn46DBw8iMDDQYXWHDtEiTK9FcW0zTlypxdQRjps2SETUFcl3DZcvX47ly5d3uq4thNtERUVBCNHt9j7++GNHldatiZG+KK5txtFLNQxsIuoXkp+a7qoSr90yjOPYRNRfGNi9dP1Sq7WwWrvf6ycicgQGdi+NDtVBo5SjrtmMnyobpC6HiAYBBnYvKRVyJAzTA+CwCBH1DwZ2H3A+NhH1JwZ2HzCwiag/MbD74JZrtwwruNqI2iaTxNUQkbtjYPeBn5cKwwO9AADHimqlLYaI3B4Du494Y14i6i8M7D6ayHFsIuonDOw+ajvwePxyLVotVomrISJ3xsDuo5hAb/hoPNBstuBcWb3U5RCRG2Ng95FcLsNEjmMTUT9gYDsA52MTUX9gYDsAA5uI+gMD2wESwvWQy4Di2maUG1qkLoeI3BQD2wG81R6IC7HdXf0o97KJyEkY2A7CYREicjYGtoO03Zg3p4iBTUTOwcB2kMQIPwDA6eI6tJgtEldDRO6Ige0g4X5aBHirYbYInCquk7ocInJDDGwHkclkuHW4bS979+kyiashInfEwHag+yaGAQC2HyuGmdcVISIHY2A70O0jAxHgrUZVowlZeVelLoeI3AwD24E8FHL85pahAIBtOZclroaI3A0D28EWJA4DAGSerUBVg1HiaojInTCwHSwuRIdxYUPQahX48kSJ1OUQkRthYDvB/df2srflXJG4EiJyJwxsJ7g3YSiUChlySww4U2KQuhwichMMbCfw9VJh5uhgAMBnR7mXTUSOwcB2krZhEc7JJiJHYWA7ye2jOCebiByLge0kSs7JJiIHY2A7Uduc7G/OVaC60SRxNUTk6hjYTtQ2J9tsEfjyeLHU5RCRi2NgO5l9TjZnixBRHzGwnaxtTvbpYgPOlnJONhH1Xq8C+8MPP8TOnTvtr1944QXo9XpMnToVly5dclhx7qDdnGye+UhEfdCrwH7ttdeg1WoBANnZ2UhPT8frr7+OgIAAPPfccw4t0B3Y52Qf55xsIuo9j9780uXLlxETEwMA2L59OxYsWIDHH38c06ZNw69+9StH1ucWbHOyVahsMGFf3lXMjA+WuiQickG92sP29vZGVVUVAGDPnj2YNWsWAECj0aC5udlx1bkJpUKO+RNsd6PhBaGIqLd6FdizZs3Co48+ikcffRTnz5/H3LlzAQC5ubmIiopyZH1uw36d7HPlnJNNRL3Sq8BOT09HcnIyrl69is8++wz+/v4AgJycHCxevNihBbqL0aE6jA3TcU42EfVar8aw9Xo93nrrrQ7L165d2+eC3Nn9E4fhdPEZbDt6BY9Mi5a6HCJyMb3aw969ezf2799vf52eno4JEybgwQcfRE1NjcOKczf3TgjjnGwi6rVeBfbKlSthMNgC59SpU1ixYgXmzp2LwsJCpKWlObRAd+LnpcKMOM7JJqLe6VVgFxYWIj4+HgDw2Wef4e6778Zrr72G9PR07Nq166a2lZ6ejqioKGg0GiQlJeHw4cNdts3NzcWCBQsQFRUFmUyGzZs393mb/Y1zsomot3oV2CqVCk1NTQCAvXv3IiUlBQDg5+dn3/Puia1btyItLQ1r1qzB0aNHkZCQgNmzZ6OioqLT9k1NTRg+fDjWr1+PkJAQh2yzv90R235ONhFRT/UqsKdPn460tDS88sorOHz4MO666y4AwPnz5zFs2LAeb2fTpk147LHHsHTpUsTHx2PLli3w9PTEe++912n7yZMnY8OGDVi0aBHUarVDttnffj4nm7cPI6Kb0atZIm+99RaeeuopbNu2DW+//TbCwmwBtGvXLqSmpvZoGyaTCTk5OVi1apV9mVwux8yZM5Gdnd2bsnq9TaPRCKPRaH/d9r8Es9kMs9ncq1q6My8hBO/sL8Tes+WoqGuEr6fK4e9xI22fyxmfzx2wf7rGvumas/ukV4EdERGBHTt2dFj+l7/8pcfbqKyshMViQXBw+9O0g4ODce7cud6U1ettrlu3rtMpiXv27IGnp2evarmRYV4KXGkE/u2jTNweKpzyHj2RkZEh2Xu7AvZP19g3HbUNFTtLrwIbACwWC7Zv346zZ88CAMaMGYN7770XCoXCYcX1l1WrVrWb3WIwGBAeHo6UlBTodDqnvGelXxFe2XkOeSZfrJ97q1PeoztmsxkZGRmYNWsWlEplv7//QMf+6Rr7pmttl+xwll4Fdn5+PubOnYvi4mLExsYCsO2lhoeHY+fOnRgxYsQNtxEQEACFQoHy8vJ2y8vLy7s8oOisbarV6k7HxJVKpdP+IH8zMRzrd+fhdIkBBVXNiAtxzj8MN+LMz+gO2D9dY9905Oz+6NVBx2eeeQYjRozA5cuXcfToURw9ehRFRUWIjo7GM88806NtqFQqJCYmIjMz077MarUiMzMTycnJvSnLKdt0Fs7JJqKb1as97H379uHgwYPw8/OzL/P398f69esxbdq0Hm8nLS0NS5YswaRJkzBlyhRs3rwZjY2NWLp0KQDg4YcfRlhYGNatWwfAdlDxzJkz9ufFxcU4fvw4vL297Zd7vdE2B5L7E4dhd24ZPj9WghdS46BU8AZARNS1XgW2Wq1GfX19h+UNDQ1QqXo+42HhwoW4evUqVq9ejbKyMkyYMAG7d++2HzQsKiqCXH49xEpKSnDLLbfYX2/cuBEbN27EHXfcgaysrB5tcyC5IzYQ/l4qVDYY8d35q5gxeuDVSEQDR68C++6778bjjz+Od999F1OmTAEAHDp0CE888QTuvffem9rW8uXLsXz58k7XtYVwm6ioKAhx4xkV3W1zIFEq5Jh/Sxje3V+IbTlXGNhE1K1e/R/8zTffxIgRI5CcnAyNRgONRoOpU6ciJiamy9PFqXNtp6rvPVuOGl4nm4i60evLq37xxRfIz8+3T+sbPXq0fRyZem50qA5jhuqQW2LAF8eLedlVIupSjwP7Rlfh+/bbb+3PN23a1PuKBqEHJoVjzZe5+Nv3hVg0JQIapevNZSci5+txYB87dqxH7WQyWa+LGawWTg7H21kFKK5txv8cKsKy6dzLJqKOehzYP9+DJsfSKBV4duZI/PEfp5D+bT4emDQMPhqekEBE7XHi7wBxf+IwDA/wQnWjCe98Xyh1OUQ0ADGwBwgPhRzPz7ad5v/O9z+hssF4g98gosGGgT2AzBkbgnFhQ9BosiD923ypyyGiAYaBPYDIZDL8ITUOAPA/B4twudq5l2okItfCwB5gpo8MwLQYf5gsVmzee0HqcohoAGFgD0AvzLbtZf/j2BXklXW8ZgsRDU4M7AEoIVyPOWNDIASwcU+e1OUQ0QDBwB6gVqTEQi4DMs6UI+dSjdTlENEAwMAeoGKCvPFPieEAgH/bfa5HVykkIvfGwB7Afj9zJFQechwurMa+81elLoeIJMbAHsCG6rVYkhwJAHh9dx6sVu5lEw1mDOwB7slfxcBb7YEzpQbsOFUqdTlEJCEG9gDn56XC47cPBwC8sScPZotV4oqISCoMbBewbHo0/L1UuFTVhE+OXJa6HCKSCAPbBXipPfC7X9vu5vN/915As8kicUVEJAUGtotYnBSBYb5aVNQb8cEPF6Uuh4gkwMB2EWoPBdJmjQIAvJ2Vj7oms8QVEVF/Y2C7kHkTwhAb7ANDSyu2fFcgdTlE1M8Y2C5EIZdh5bWbHLx/oBDlhhaJKyKi/sTAdjEzRgchMdIXLWYr3szk5VeJBhMGtov5+U0OPv7xMgorGyWuiIj6CwPbBU2J9sOdsYGwWAU2ZZyXuhwi6icMbBe18tpNDr46UYLTxXUSV0NE/YGB7aLih+owb8JQAMCaL3NR38JpfkTujoHtwtJmjYJWqUDOpRrc/3Y2b9pL5OYY2C4s0t8LW//lVgT5qJFXXo/56QeQc6la6rKIyEkY2C5u/DA9vlg+DfGhOlQ1mrD4r4ew/Vix1GURkRMwsN1A6BAtPn0iGSnxwTBZrHh263Fs2sMbHhC5Gwa2m/BSe2DLPyfiiTtGAADe/CYfv/voGK/sR+RGGNhuRC6X4Y9z4rDh/vFQKmTYeaoUi/6ajQqewk7kFhjYbuifJoXjv5clQe+pxIkrdZiXfgC5JZyrTeTqGNhuKmm4P754ehpGBHqhtK4F/7QlG3tyy6Qui4j6gIHtxiL9vfCPp6ZhekwAmkwW/Mt/5+A/9xVACB6MJHJFDGw3N0SrxPtLJ+OhpAgIAazbdQ5/+OwkTK28mS+Rq/GQugByPqVCjlfnj0VMkDde2XEGnxy5gouVjZgfIHVlRHQzuIc9SMhkMiydFo13H5kMb7UHDl+swaZTCpwuNkhdGhH1EAN7kLkzNgifPTkVYXoNKo0yPPC3Q3jn+594kg2RC2BgD0KxIT74/MlbMc7XCrNF4NWdZ/HbD39EZYNR6tKIqBsM7EHK11OFZbFWvHTPaKg85MjKu4rUzd/j+wtXpS6NiLrAwB7EZDLgoSnh+HL5NIwM8kZlgxH/593DWLfrLMwWziIhGmgY2IS4EB2+XD4dDyVFAAD+c99PuH9LNoqqeH1tooGEgU0AAK1KgX/9zThs+eeJ0Gk8cOJyLea++T2+OM5LtRINFAMisNPT0xEVFQWNRoOkpCQcPny42/affvop4uLioNFoMG7cOHz99dft1j/yyCOQyWTtHqmpqc78CG4jdWwodj17OyZH+aLB2Irff3wcz396Ao3GVqlLIxr0JA/srVu3Ii0tDWvWrMHRo0eRkJCA2bNno6KiotP2P/zwAxYvXoxly5bh2LFjmD9/PubPn4/Tp0+3a5eamorS0lL746OPPuqPj+MWwvRafPTYrfj9jJGQy4BtOVdw97/v581+iSQmExJfWCIpKQmTJ0/GW2+9BQCwWq0IDw/H7373O/zxj3/s0H7hwoVobGzEjh077MtuvfVWTJgwAVu2bAFg28Oura3F9u3be1SD0WiE0Xh9SpvBYEB4eDgqKyuh0+n68OkGLrPZjIyMDMyaNQtKpbLLdocvVmPFp6dQZjBCqZBhZcooPJIcAZlM1o/V9r+e9s9gxL7pWlVVFUJDQ1FXV+eU7JD01HSTyYScnBysWrXKvkwul2PmzJnIzs7u9Heys7ORlpbWbtns2bM7hHNWVhaCgoLg6+uLX//613j11Vfh7+/f6TbXrVuHtWvXdli+Z88eeHp63uSnci0ZGRk3bPPMKODjn+Q4WS3Ha7vysD37LB4YboWvuh8KlFhP+mewYt901NTk3AP1kgZ2ZWUlLBYLgoOD2y0PDg7GuXPnOv2dsrKyTtuXlV2/dGhqairuu+8+REdHo6CgAC+++CLmzJmD7OxsKBSKDttctWpVu38E2vawU1JSBv0edpv7hcBHP17Ba7vycKYWWH/KA8umReGx6VHwUrvfJWm4F9k19k3XqqqqnLp99/umAVi0aJH9+bhx4zB+/HiMGDECWVlZmDFjRof2arUaanXH3UWlUun2f5A38xmXTBuOW0cE4s/bT+PwxWqkZ/2ET3OK8fzsWCyYOAwKufsNkwyGv4HeYt905Oz+kPSgY0BAABQKBcrLy9stLy8vR0hISKe/ExISclPtAWD48OEICAhAfn5+34se5GJDfLD1X27Fln+eiAg/T1TUG/HCtpO459/344eCSqnLI3Jrkga2SqVCYmIiMjMz7cusVisyMzORnJzc6e8kJye3aw/YxtK6ag8AV65csR8MoL6TyWRIHRuKjLTb8ae5o+Gj8cCZUgMe/NshPPb3I/jpaoPUJRK5Jcmn9aWlpeFvf/sbPvzwQ5w9exZPPvkkGhsbsXTpUgDAww8/3O6g5O9//3vs3r0bb7zxBs6dO4eXXnoJR44cwfLlywEADQ0NWLlyJQ4ePIiLFy8iMzMT8+bNQ0xMDGbPni3JZ3RXag8FHrt9OPatvBNLkiOhkMuQcaYcKX/5Di9/dQa1TSapSyRyK5IH9sKFC7Fx40asXr0aEyZMwPHjx7F79277gcWioiKUlpba20+dOhX/+7//i7/+9a9ISEjAtm3bsH37dowdOxYAoFAocPLkSdx7770YNWoUli1bhsTERHz//fedjlNT3/l5qbB23lj8v2dvw6/jgtBqFXjvQCHu2JCF9w8U8rokRA4i+TzsgchgMGDIkCFOm0s5EJjNZnz99deYO3euww+UfH/hKv5151mcK6sHAAwP8MKLc0djxuggl5m/7cz+cXXsm65VVVUhICDAadkh+R42uZ/bRgZi5zO3Yd194xDgrcJPlY149O9H8NA7h7D7dCmu1vO620S94ZbT+kh6CrkMi6dE4O7xoXg7qwDv7C/EDwVV+KHANk81yt8TiZF+mBTli0mRvhgR6A25G04LJHIkBjY5lY9GiRdS4/BgUgTe+b4QB3+qQl55PS5WNeFiVRM+O3oFAKD3VCIxwheJUb6YHOWHcWFDoFF2PMmJaDBjYFO/GObriZfuHQMAqGs242hRDXIu1uDHi9U4caUWtU1mZJ6rQOY520W/VAo5xobpMDnKD4mRvhiq16LFbEGz2YIWs9X+3HjttW359edGswUtrRZ4yOWIDfFB/FAdxoTqEKTTSNkNRH3CwKZ+N0SrxJ2xQbgzNggAYGq14kypAUcuVuPIxRocuVSDygYjjhbV4mhRbd/f8MT1pwHeasQP1SE+VGf/GR3g5ZZnaZL7YWCT5FQeckwI12NCuB6P3gYIIXCpqglHLtUg51I1ci7VwNDcCo1SDo1Sce1he679xWuNUgGNhwJale11k8mCs6UG5JYY8NPVBlQ2GPHd+av47vz1e1dqlQrEhfq0C/ER/loJe4SocwxsGnBkMhmiArwQFeCF+xOHOWy7zSYL8srrcabEgDOldcgtMeBcaT2azRYcK6rFsZ/tzctlQKBGgYyGkxg7TI/4UB1Gh+oQ6MO5/CQdBjYNGlqVwr4n38ZiFbhY1YgzJba98DOlBpwpqUNlgwnlzTLsOFWGHaeuXwkyyMc2pDI69PqwSpQ/h1SofzCwaVBTyGUYEeiNEYHeuCdhqH15cXUD/uvLb+AdHodz5Q04U2pAYWUjKuqNqMi7iqy8rodUhuq10GmUGKL1gE6rhE6j5IwXcggGNlEngnzUGO0rMPf2aPvZfI3GVpwrq7+2F27bGz9Xauh0SOWXVB5yDNEqodNcD/EhWiV0Wg/7c39vNYb5ajHMV4sQnQYeCp7XRu0xsIl6yEvtgcRIXyRG+tqXtVqsuFjVaB9OOVtaj6oGIwwtZtQ1mVFvbIUQtpkwV+uNPT7L00MuQ6heg2F6T4T7aTHM1xPDfLUI97P9DPLRcBhmEGJgE/WBh0KOmCAfxAT5YN6EsA7rrVaBBlMrDM1mGJpbUddshqHFbHvdcu31tcfVBiOu1DSjuKYZJosVl6ubcbm6Gdk/dXxfpUKGML0tyEOHaOCl9oBWpYCnUgGt6tpDqYCnyjZzxlPlAe3P1rW141CNa2FgEzmRXC6DTmMbAoHvjdsDtpCvqDfick0TrtQ04Up187Xntp8ltS0wW4T9bNG+8FIpEOijRoC3GoE+1x7Xnv98WYC3GioPDtFIjYFNNMDI5TKEDNEgZIgGk6P8OqxvtVhRXm/E5WpbiJcbWtBkakWzyYpmcyuaTRY0mWxngjb/4mfbclOr7ZK3jSYLGnsY/HpPJQK81QjwUqK5To6s5lPw1qjgqVbAU+kBL7VtT95TZduzb9vr97q2zEvtAW+1B4O/DxjYRC7GQyFHmF6LMH3vT+5ptVjRZLagusGEqw1G+/j61XojKtteN1x/bbYI1DaZUdtkhu1Ge3KcqC69wbt0LsBbZfsHSadF6LV/mK7/tB1w1aocM1RjsQrIZXCZy/reCAObaBDyUMihU8ih0ygRFeDVbVshBOqazfZAL61tQnbOcQwfFYeWVqDJ2IomswVNxlY0mmx7842mVjQZbT/bXreYbXv1lQ0mVDaYcLrY0OV76j2VCNG1BbkWgd4qmK0Czaa2a8Zcv65M27Vjmn9+rRmT7VoyZouARimHv5caAd4q+Hur4e9l+2l7rYK/lxr+3ioEeqvh66WCcgDPzmFgE1G3ZDIZ9J4q6D1VGBnsA7PZDI/iY5g7PfqmbmBgsQrUNplQZmhBWV0LSut+9tPQjNK6FpTWtqDZbLHvzbfdBKMvWsxWFNc2o7i2uUft9Z5K+HvZPq+nSgFvtYd9OMdLfX1ox0vVfrm32gOmppY+19sdBjYR9QuFXGbbw/VWY8zQIZ22EULA0NJ6Lcib7YFe1WiESnHtGjEe12e4tF1Ppm25RqWwr9cqFVB7yNFgbEVlgxFVDSZUNRqv7eFff111bY+/utEIq4D9Hwug8aY/o9XYt4PAN8LAJqIBQyaTYYjWdiJRbIiPQ7bp66VCuJ/nDdtZrQK1zWZUNdjG7w3NrWg0tqLR1IoG47XnRov9eYdlplbUtzp3rJyBTUQE2+wcPy8V/LxsQz+9UVVVhYA3HFzYzwzc0XUiImqHgU1E5CIY2ERELoKBTUTkIhjYREQugoFNROQiOK2vE0IIAIDB0PWps67ObDajqakJBoPhps5WGyzYP11j33Stvt52ZmZbhjgaA7sTbZ0eHh4ucSVE5IqqqqowZEjnZ3P2hUw4658CF2a1WlFSUgIfHx+3ucrXLxkMBoSHh+Py5cvQ6XRSlzPgsH+6xr7pWl1dHSIiIlBTUwO9Xu/w7XMPuxNyuRzDhg2Tuox+odPp+KXrBvuna+ybrsnlzjk8yIOOREQugoFNROQiGNiDlFqtxpo1a6BWq6UuZUBi/3SNfdM1Z/cNDzoSEbkI7mETEbkIBjYRkYtgYBMRuQgGNhGRi2Bgu7GXXnoJMpms3SMuLs6+vqWlBU8//TT8/f3h7e2NBQsWoLy8XMKKneu7777DPffcg6FDh0Imk2H79u3t1gshsHr1aoSGhkKr1WLmzJm4cOFCuzbV1dV46KGHoNPpoNfrsWzZMjQ0NPTjp3COG/XNI4880uFvKTU1tV0bd+2bdevWYfLkyfDx8UFQUBDmz5+PvLy8dm168l0qKirCXXfdBU9PTwQFBWHlypVobW29qVoY2G5uzJgxKC0ttT/2799vX/fcc8/hq6++wqeffop9+/ahpKQE9913n4TVOldjYyMSEhKQnp7e6frXX38db775JrZs2YJDhw7By8sLs2fPRktLi73NQw89hNzcXGRkZGDHjh347rvv8Pjjj/fXR3CaG/UNAKSmprb7W/roo4/arXfXvtm3bx+efvppHDx4EBkZGTCbzUhJSUFj4/W7qt/ou2SxWHDXXXfBZDLhhx9+wIcffogPPvgAq1evvrliBLmtNWvWiISEhE7X1dbWCqVSKT799FP7srNnzwoAIjs7u58qlA4A8fnnn9tfW61WERISIjZs2GBfVltbK9Rqtfjoo4+EEEKcOXNGABA//vijvc2uXbuETCYTxcXF/Va7s/2yb4QQYsmSJWLevHld/s5g6RshhKioqBAAxL59+4QQPfsuff3110Iul4uysjJ7m7ffflvodDphNBp7/N7cw3ZzFy5cwNChQzF8+HA89NBDKCoqAgDk5OTAbDZj5syZ9rZxcXGIiIhAdna2VOVKprCwEGVlZe36Y8iQIUhKSrL3R3Z2NvR6PSZNmmRvM3PmTMjlchw6dKjfa+5vWVlZCAoKQmxsLJ588klUVVXZ1w2mvqmrqwMA+Pn5AejZdyk7Oxvjxo1DcHCwvc3s2bNhMBiQm5vb4/fmxZ/cWFJSEj744APExsaitLQUa9euxW233YbTp0+jrKwMKpWqwxXFgoODUVZWJk3BEmr7zD//QrW9bltXVlaGoKCgdus9PDzg5+fn9n2WmpqK++67D9HR0SgoKMCLL76IOXPmIDs7GwqFYtD0jdVqxbPPPotp06Zh7NixANCj71JZWVmnf1tt63qKge3G5syZY38+fvx4JCUlITIyEp988gm0Wq2ElZGrWbRokf35uHHjMH78eIwYMQJZWVmYMWOGhJX1r6effhqnT59udyyoP3FIZBDR6/UYNWoU8vPzERISApPJhNra2nZtysvLERISIk2BEmr7zL88sv/z/ggJCUFFRUW79a2traiurh50fTZ8+HAEBAQgPz8fwODom+XLl2PHjh349ttv211+uSffpZCQkE7/ttrW9RQDexBpaGhAQUEBQkNDkZiYCKVSiczMTPv6vLw8FBUVITk5WcIqpREdHY2QkJB2/WEwGHDo0CF7fyQnJ6O2thY5OTn2Nt988w2sViuSkpL6vWYpXblyBVVVVQgNDQXg3n0jhMDy5cvx+eef45tvvkF0dHS79T35LiUnJ+PUqVPt/lHLyMiATqdDfHz8TRVDbmrFihUiKytLFBYWigMHDoiZM2eKgIAAUVFRIYQQ4oknnhARERHim2++EUeOHBHJyckiOTlZ4qqdp76+Xhw7dkwcO3ZMABCbNm0Sx44dE5cuXRJCCLF+/Xqh1+vFF198IU6ePCnmzZsnoqOjRXNzs30bqamp4pZbbhGHDh0S+/fvFyNHjhSLFy+W6iM5THd9U19fL55//nmRnZ0tCgsLxd69e8XEiRPFyJEjRUtLi30b7to3Tz75pBgyZIjIysoSpaWl9kdTU5O9zY2+S62trWLs2LEiJSVFHD9+XOzevVsEBgaKVatW3VQtDGw3tnDhQhEaGipUKpUICwsTCxcuFPn5+fb1zc3N4qmnnhK+vr7C09NT/OY3vxGlpaUSVuxc3377rQDQ4bFkyRIhhG1q35///GcRHBws1Gq1mDFjhsjLy2u3jaqqKrF48WLh7e0tdDqdWLp0qaivr5fg0zhWd33T1NQkUlJSRGBgoFAqlSIyMlI89thj7aaoCeG+fdNZvwAQ77//vr1NT75LFy9eFHPmzBFarVYEBASIFStWCLPZfFO18PKqREQugmPYREQugoFNROQiGNhERC6CgU1E5CIY2ERELoKBTUTkIhjYREQugoFNROQiGNhE/SArKwsymazDBYKIbgYDm4jIRTCwiYhcBAObBgWr1Yp169YhOjoaWq0WCQkJ2LZtG4DrwxU7d+7E+PHjodFocOutt+L06dPttvHZZ59hzJgxUKvViIqKwhtvvNFuvdFoxB/+8AeEh4dDrVYjJiYG7777brs2OTk5mDRpEjw9PTF16tQOd98m6lbfr2VFNPC9+uqrIi4uTuzevVsUFBSI999/X6jVapGVlWW/Ut3o0aPFnj17xMmTJ8Xdd98toqKihMlkEkIIceTIESGXy8XLL78s8vLyxPvvvy+0Wm27K7Y98MADIjw8XPzjH/8QBQUFYu/eveLjjz8WQly/Gl5SUpLIysoSubm54rbbbhNTp06VojvIRTGwye21tLQIT09P8cMPP7RbvmzZMrF48WJ7mLaFqxC2S4VqtVqxdetWIYQQDz74oJg1a1a731+5cqWIj48XQgiRl5cnAIiMjIxOa2h7j71799qX7dy5UwBod71tou5wSITcXn5+PpqamjBr1ix4e3vbH3//+99RUFBgb/fzO+34+fkhNjYWZ8+eBQCcPXsW06ZNa7fdadOm4cKFC7BYLDh+/DgUCgXuuOOObmsZP368/Xnb3Vp+eWstoq7wJrzk9hoaGgAAO3fuRFhYWLt1arW6XWj3Vk9vaqxUKu3PZTIZANv4OlFPcA+b3F58fDzUajWKiooQExPT7hEeHm5vd/DgQfvzmpoanD9/HqNHjwYAjB49GgcOHGi33QMHDmDUqFFQKBQYN24crFYr9u3b1z8figYl7mGT2/Px8cHzzz+P5557DlarFdOnT0ddXR0OHDgAnU6HyMhIAMDLL78Mf39/BAcH409/+hMCAgIwf/58AMCKFSswefJkvPLKK1i4cCGys7Px1ltv4T/+4z8AAFFRUViyZAl++9vf4s0330RCQgIuXbqEiooKPPDAA1J9dHI3Ug+iE/UHq9UqNm/eLGJjY4VSqRSBgYFi9uzZYt++ffYDgl999ZUYM2aMUKlUYsqUKeLEiRPttrFt2zYRHx8vlEqliIiIEBs2bGi3vrm5WTz33HP2+2jGxMSI9957Twhx/aBjTU2NvX3bDW8LCwud/fHJTfCejjToZWVl4c4770RNTQ30er3U5RB1iWPYREQugoFNROQiOCRCROQiuIdNROQiGNhERC6CgU1E5CIY2ERELoKBTUTkIhjYREQugoFNROQiGNhERC7i/wOLwYOW3RSwqgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10\n",
    "lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=len(src_vocab), \n",
    "    key_size=key_size, \n",
    "    query_size=query_size, \n",
    "    value_size=value_size, \n",
    "    num_hiddens=num_hiddens, \n",
    "    norm_shape=norm_shape, \n",
    "    ffn_num_input=ffn_num_input, \n",
    "    ffn_num_hiddens=ffn_num_hiddens, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout\n",
    ")\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size=len(tgt_vocab), \n",
    "    key_size=key_size, \n",
    "    query_size=query_size, \n",
    "    value_size=value_size, \n",
    "    num_hiddens=num_hiddens, \n",
    "    norm_shape=norm_shape, \n",
    "    ffn_num_input=ffn_num_input, \n",
    "    ffn_num_hiddens=ffn_num_hiddens, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout\n",
    ")\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !,  bleu 1.000\n",
      "i lost . => j'ai perdu .,  bleu 1.000\n",
      "he's calm . => il est calme .,  bleu 1.000\n",
      "i'm home . => je suis chez moi .,  bleu 1.000\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = d2l.predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}, ',\n",
    "          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7. <a id='toc11_7_'></a>[BERT](#toc0_)\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是基于Transformer架构的一种预训练语言模型，由Google在2018年提出。BERT使用了Transformer的编码器部分，并通过双向训练来捕捉上下文信息。BERT的主要创新在于它的预训练任务（如Masked Language Model和Next Sentence Prediction），使其在各种自然语言处理任务中表现出色。\n",
    "\n",
    "- 就是Encoder部分\n",
    "\n",
    "- Base：12层，768维，12个注意力头，110M参数\n",
    "\n",
    "- Large：24层，1024维，16个注意力头，340M参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.1. <a id='toc11_7_1_'></a>[BERT encode block](#toc0_)\n",
    "\n",
    "\n",
    "![BERT](./Pytorch_Pictures/BERT/BERT.jpg)\n",
    "\n",
    "1. 词元化：将原始文本转化为词元列表（tokenization）。\n",
    "2. 添加特殊标记：在词元列表前后添加 '<cls>' 和 '<sep>'。\n",
    "3. 生成段标识：为每个词元分配相应的段标识。\n",
    "4. 转换为ID：使用词汇表（vocab）将词元转换为对应的ID。\n",
    "5. 位置编码：为每个词元添加位置编码。\n",
    "6. 输入模型：将处理后的序列输入到 BERT 模型中进行训练或推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['<cls>', 'You', 'are', 'the', 'best', '<sep>', 'You', 'are', 'the', 'worst', '<sep>']\n",
      "segments: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"\n",
    "    Get tokens of the BERT input sequence and their segment IDs.\n",
    "    Args:\n",
    "        tokens_a: List[str] 第一段文本的词元列表（即第一句话的分词结果）。\n",
    "        tokens_b: List[str] （可选）第二段文本的词元列表（即第二句话的分词结果）。在单句任务中，此参数可以省略。\n",
    "    Returns:\n",
    "        tokens: List[str] 词元列表，其中第一个词元是'<cls>'，表示序列的开始，最后一个词元是'<sep>'，表示序列的结束。\n",
    "        segments: List[int] 段标识列表，其中0表示第一段，1表示第二段。\n",
    "    \"\"\"\n",
    "    # classification (cls) and separator (sep) tokens are added\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments\n",
    "\n",
    "\n",
    "# 测试\n",
    "tokens_a = ['You', 'are', 'the', 'best']\n",
    "tokens_b = ['You', 'are', 'the', 'worst']\n",
    "\n",
    "tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "\n",
    "print(f'tokens: {tokens}')\n",
    "print(f'segments: {segments}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.shape: torch.Size([2, 8])\n",
      "segments.shape: torch.Size([2, 8])\n",
      "encoded_X.shape: torch.Size([2, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            # 此处调用TransformerEncoder中的EncoderBlock\n",
    "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        # 此处用nn.Parameter来创建一个可学习的参数，用于存储位置编码, 形状为(1, max_len, num_hiddens)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        # 词元嵌入和段嵌入相加\n",
    "        # (batch_size, seq_len, num_hiddens) + (batch_size, seq_len, num_hiddens) = (batch_size, seq_len, num_hiddens)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        # 位置编码\n",
    "        # (batch_size, seq_len, num_hiddens) + (1, seq_len, num_hiddens) = (batch_size, seq_len, num_hiddens)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        # X: (batch_size, seq_len, num_hiddens)\n",
    "        return X\n",
    "    \n",
    "\n",
    "# 测试\n",
    "#@tab pytorch\n",
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "\n",
    "#@tab pytorch\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "tokens = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "\n",
    "encoded_X = encoder(tokens, segments, None) \n",
    "# encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "\n",
    "# tokens.shape, segments.shape, encoded_X.shape\n",
    "print(f'tokens.shape: {tokens.shape}')\n",
    "print(f'segments.shape: {segments.shape}')\n",
    "print(f'encoded_X.shape: {encoded_X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.2. <a id='toc11_7_2_'></a>[Masked Language Modeling](#toc0_)\n",
    "MaskLM 类通过多层感知机（MLP）对被遮蔽的位置进行预测，输出每个被遮蔽位置的词汇表概率分布。这是 BERT 模型在预训练阶段的核心任务之一，旨在让模型学习上下文关系和词汇之间的语义联系。通过这样的设计，模型能够在处理自然语言理解任务时表现出色，因为它已经通过大量的无监督数据学习到了丰富的语言表示。\n",
    "\n",
    "- 选择一些位置进行预测，这些位置被称为被遮蔽的位置。\n",
    "- 被遮蔽的位置上的词元被替换为特殊的“<mask>”词元。\n",
    "- 模型需要预测这些被遮蔽位置上的原始词元。\n",
    "- `从经过BERTEncoder编码后的序列中，提取出被遮蔽位置（<mask>, mlm_positions）上的子序列，然后通过多层感知机（MLP）进行预测。`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_X.shape: torch.Size([2, 8, 768])\n",
      "mlm_positions.shape: torch.Size([2, 3])\n",
      "mlm_Y_hat.shape: torch.Size([2, 3, 10000])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "#@save\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size)) # 输出： (batch_size, num_pred_positions, vocab_size)\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        # pred_positions: (batch_size, num_pred_positions)\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        # 将预测位置展平成一维，用于后续的索引操作, 形状为 (batch_size * num_pred_positions), e.g., torch.tensor([1, 5, 2, 6, 1, 5])\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "\n",
    "        # 生成批次索引\n",
    "        ## X: (batch_size, seq_len, num_hiddens)\n",
    "        batch_size = X.shape[0]\n",
    "        ## 创建一个包含batch_size个元素的索引            \n",
    "        batch_idx = torch.arange(0, batch_size) \n",
    "        ## Suppose that `batch_size` = 2, `num_pred_positions` = 3, then `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "\n",
    "        # 根据batch_idx和pred_positions从X中提取出对应的子序列\n",
    "        ## masked_X: (batch_size * num_pred_positions, num_hiddens), 索引：encoded_X[[0, 0, 0, 1, 1, 1], [1, 5, 2, 6, 1, 5]]\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        ## masked_X: (batch_size, num_pred_positions, num_hiddens)\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        ## (batch_size, num_pred_positions, vocab_size)\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat\n",
    "    \n",
    "\n",
    "# 测试\n",
    "#@tab pytorch\n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])    # (batch_size, num_pred_positions)\n",
    "\n",
    "# encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "# mlm_positions: (batch_size, num_pred_positions)\n",
    "# mlm_Y_hat: (batch_size, num_pred_positions, vocab_size)\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)               \n",
    "\n",
    "print(f'encoded_X.shape: {encoded_X.shape}')\n",
    "print(f'mlm_positions.shape: {mlm_positions.shape}')\n",
    "print(f'mlm_Y_hat.shape: {mlm_Y_hat.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.3. <a id='toc11_7_3_'></a>[Next Sentence Prediction](#toc0_)\n",
    "NextSentencePred 类通常与 BERT 模型的编码器部分结合使用。在预训练 BERT 模型时，除了进行 Masked Language Modeling（MLM）任务，还会同时进行 NSP 任务。通过 NSP 任务，模型能够学习句子之间的关系，这对于诸如问答系统、自然语言推理等下游任务具有重要意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_X.shape: torch.Size([2, 6144])\n",
      "nsp_Y_hat.shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)\n",
    "    \n",
    "\n",
    "# 测试\n",
    "# PyTorch by default won't flatten the tensor as seen in mxnet where, if flatten=True, all but the first axis of input data are collapsed together\n",
    "## encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "## flattened_encoded_X: (batch_size, seq_len * num_hiddens)\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "\n",
    "# input_shape for NSP: (batch size, `num_hiddens`)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "\n",
    "# encoded_X: (batch_size, seq_len * num_hiddens)\n",
    "# nsp_Y_hat: (batch_size, 2)\n",
    "nsp_Y_hat = nsp(encoded_X) \n",
    "\n",
    "print(f'encoded_X.shape: {encoded_X.shape}')\n",
    "print(f'nsp_Y_hat.shape: {nsp_Y_hat.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.4. <a id='toc11_7_4_'></a>[BERT模型](#toc0_)\n",
    "\n",
    "![BERT模型](./Pytorch_Pictures/BERT/BERT_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768, mlm_in_features=768, nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        # tokens: (batch_size, seq_len)\n",
    "        # segments: (batch_size, seq_len)\n",
    "        # valid_lens: (batch_size,)\n",
    "        # pred_positions: (batch_size, num_pred_positions)\n",
    "        # encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            # mlm_Y_hat: (batch_size, num_pred_positions, vocab_size)\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # The hidden layer of the MLP classifier for next sentence prediction. 0 is the index of the '<cls>' token\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        # encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "        # mlm_Y_hat: (batch_size, num_pred_positions, vocab_size)\n",
    "        # nsp_Y_hat: (batch_size, 2)\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n",
    "    \n",
    "\n",
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.5. <a id='toc11_7_5_'></a>[Datasets for Pre-training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import random\n",
    "\n",
    "\n",
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    # file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    # with open(file_name, 'r') as f:\n",
    "    #     lines = f.readlines()\n",
    "    file_name = os.path.join(data_dir, 'train-00000-of-00001.parquet')\n",
    "    df = pd.read_parquet(data_dir)\n",
    "    lines = df['text'].tolist()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.1. <a id='toc11_7_5_1_'></a>[生成下一句预测任务的数据](#toc0_)\n",
    "下一句预测任务 是BERT预训练的两个主要任务之一（另一个是遮蔽语言模型）。NSP任务的目的是让模型理解句子之间的关系，判断一个句子是否是另一个句子的真实下一句。这对于下游任务如问答系统、自然语言推理等具有重要意义。  \n",
    "\n",
    "函数 _get_next_sentence 的具体作用：\n",
    "  - 正样本生成：以50%的概率，函数返回的 next_sentence 是 sentence 的真实下一句，对应标签 is_next=True。\n",
    "  - 负样本生成：以另外50%的概率，函数返回的 next_sentence 是随机选择的其他段落中的句子，对应标签 is_next=False。\n",
    "通过这种方式，模型在训练过程中能够接触到正负两类样本，从而学习句子之间的逻辑关系和上下文关联。\n",
    "\n",
    "函数 _get_nsp_data_from_paragraph 的主要作用是从给定的段落中生成用于下一句预测任务的训练数据。具体步骤包括遍历段落中的句子对、生成句子对及其标签、过滤超长的句子对、格式化词元和段落标记，并将符合条件的句子对数据收集起来。通过这种方式，模型在训练过程中能够学习到句子之间的逻辑关系和上下文关联，从而提高其在诸如问答系统、自然语言推理等下游任务中的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['句子1A'], ['句子1B'], True)\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    '''\n",
    "    生成下一句预测任务的数据\n",
    "    Args:\n",
    "        sentence: 当前句子\n",
    "        next_sentence: 下一句句子\n",
    "        paragraphs: 段落列表\n",
    "    Returns:\n",
    "        sentence: 当前句子\n",
    "        next_sentence: 下一句句子\n",
    "        is_next: 是否是下一句\n",
    "    '''\n",
    "    # 1. 随机决定是否使用真实的下一句：使用 random.random() 生成一个 [0,1) 之间的随机数，如果随机数小于 0.5，则 is_next 设置为 True，表示使用真实的下一句\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 生成负样本（不真实的下一句）：从 paragraphs 中随机选择一个段落，再从中随机选择一句作为 next_sentence。\n",
    "        ## 设置 is_next 为 False，表示 next_sentence 不是 sentence 的真实下一句。\n",
    "        # paragraphs是三重列表的嵌套\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next\n",
    "\n",
    "\n",
    "# 测试\n",
    "paragraphs = [\n",
    "    [[\"句子1A\"], [\"句子1B\"], [\"句子1C\"]],\n",
    "    [[\"句子2A\"], [\"句子2B\"], [\"句子2C\"]],\n",
    "    [[\"句子3A\"], [\"句子3B\"], [\"句子3C\"]]\n",
    "]\n",
    "sentence = [\"句子1A\"]\n",
    "next_sentence = [\"句子1B\"]\n",
    "result = _get_next_sentence(sentence, next_sentence, paragraphs)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsp_data_from_paragraph: [(['<cls>', '句子1A', '<sep>', '句子2C', '<sep>'], [0, 0, 0, 1, 1], False), (['<cls>', '句子1B', '<sep>', '句子2C', '<sep>'], [0, 0, 0, 1, 1], False)]\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    ''' \n",
    "    生成下一句预测任务的数据\n",
    "    Args:\n",
    "        paragraph: 段落, 当前处理的段落，通常是由多个句子组成的列表。\n",
    "        paragraphs: 段落列表, 所有段落的集合，用于在生成负样本时随机选择其他句子。\n",
    "        vocab: 词汇表, 用于将词元转换为对应的索引或其他形式。\n",
    "        max_len: 最大长度, 模型接受的最大句子对长度，包含了特殊词元 <cls> 和 <sep>。\n",
    "    Returns:\n",
    "        nsp_data_from_paragraph: 下一句预测任务的数据\n",
    "    '''\n",
    "    nsp_data_from_paragraph = []\n",
    "    # 遍历当前段落中的每一个句子，除了最后一个句子，因为需要成对处理句子与下一句的关系。\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(sentence=paragraph[i], next_sentence=paragraph[i + 1], paragraphs=paragraphs)\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph\n",
    "\n",
    "\n",
    "# 测试\n",
    "paragraphs = [\n",
    "    [[\"句子1A\"], [\"句子1B\"], [\"句子1C\"]],\n",
    "    [[\"句子2A\"], [\"句子2B\"], [\"句子2C\"]],\n",
    "    [[\"句子3A\"], [\"句子3B\"], [\"句子3C\"]]\n",
    "]\n",
    "result = _get_nsp_data_from_paragraph(paragraph=paragraphs[0], paragraphs=paragraphs, vocab=None, max_len=100)\n",
    "\n",
    "print(f'nsp_data_from_paragraph: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.2. <a id='toc11_7_5_2_'></a>[生成遮蔽语言模型任务的数据](#toc0_)\n",
    "函数 _replace_mlm_tokens 的主要功能是为遮蔽语言模型（Masked Language Model, MLM）任务生成新的输入词元序列，其中部分词元被替换为特殊的 <mask> 词元或随机词元。这是BERT等预训练模型在进行自监督学习时常用的策略。  \n",
    "函数 _replace_mlm_tokens 实现了MLM任务中词元的随机替换，按照BERT的策略进行：\n",
    "- 80% 的词元被替换为 <mask>。\n",
    "- 10% 的词元保持不变。\n",
    "- 10% 的词元被替换为随机词元。  \n",
    "\n",
    "通过这种方式，模型在训练过程中能够学习到预测被遮蔽词元的能力，从而理解上下文关系和词汇之间的语义联系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: \t ['<cls>', '我', '爱', '中', '国', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
      "mlm_input_tokens: \t ['<cls>', '<mask>', '<mask>', '中', '<mask>', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
      "pred_positions_and_labels: \t [(2, '爱'), (1, '我'), (4, '国')]\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    ''' \n",
    "    为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    Args:\n",
    "        tokens :list: 输入词元序列列表，通常是一个句子的词元化结果。\n",
    "        candidate_pred_positions :list: 候选预测位置列表，表示哪些词元有可能被遮蔽和预测。\n",
    "        num_mlm_preds :int: 需要遮蔽和预测的词元数量。\n",
    "        vocab :Vocab: 词汇表对象，包含词元到索引的映射（idx_to_token）。\n",
    "    Returns:\n",
    "        mlm_input_tokens: 返回修改后的词元序列，包含被替换的词元。\n",
    "        pred_positions_and_labels: 返回被替换词元的位置及其对应的原始词元，用于模型训练时的预测目标。\n",
    "    '''\n",
    "    # 创建一个输入词元的副本，准备在其中进行替换操作。\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    # 初始化一个空列表，用于存储被遮蔽词元的位置及其原始标签。\n",
    "    pred_positions_and_labels = []\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        # 如果已经替换了所需数量的词元（num_mlm_preds），则退出循环。\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        # 替换操作: 根据上述概率策略，对指定位置的词元进行替换。\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 将预测位置 及其 原始标签添 加到列表中，用于后续的损失计算和模型训练。\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "\n",
    "# 测试\n",
    "tokens = ['<cls>', '我', '爱', '中', '国', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
    "# 候选预测位置，需要遮蔽和预测的词元位置，1，2，3，4表示列表索引位置\n",
    "candidate_pred_positions = [1, 2, 3, 4]\n",
    "# 需要遮蔽和预测的词元数量\n",
    "num_mlm_preds = 3\n",
    "vocab = d2l.Vocab()\n",
    "result = _replace_mlm_tokens(tokens=tokens, candidate_pred_positions=candidate_pred_positions, num_mlm_preds=num_mlm_preds, vocab=vocab)\n",
    "\n",
    "print(f'tokens: \\t {tokens}')\n",
    "print(f'mlm_input_tokens: \\t {result[0]}')\n",
    "print(f'pred_positions_and_labels: \\t {result[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 只是替换15%的词元：\n",
    "  - 在这被替换的15%词元中，有80%被替换为`<mask>`，\n",
    "  - 有10%被替换为`其他词元`，\n",
    "  - 有10%`保持不变`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_input_tokens: \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "pred_positions: \t [1, 5]\n",
      "mlm_pred_labels: \t [0, 0]\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    ''' \n",
    "    生成遮蔽语言模型任务的数据\n",
    "    Args:\n",
    "        tokens :list: 输入词元序列列表，通常是一个句子的词元化结果。\n",
    "        vocab :Vocab: 词汇表对象，包含词元到索引的映射（idx_to_token）。\n",
    "    Returns:\n",
    "        vocab[mlm_input_tokens]：经过遮蔽处理后的词元序列，通常会被转换为词汇表中的索引。\n",
    "        pred_positions：被遮蔽词元的位置索引列表。\n",
    "        vocab[mlm_pred_labels]：被遮蔽词元的原始标签，通常也是词汇表中的索引。\n",
    "    '''\n",
    "    # 初始化候选预测位置列表\n",
    "    candidate_pred_positions = []\n",
    "    # 过滤特殊词元\n",
    "    ## tokens是一个字符串列表\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    # 生成遮蔽后的词元和标签\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    # 对被遮蔽的位置和标签进行排序,按照位置索引进行排序，确保顺序的一致性。这对于后续处理和训练时的批量操作非常重要。\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    # 分离位置索引和标签\n",
    "    ## pred_positions：仅包含被遮蔽词元的位置索引。\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    ## mlm_pred_labels：包含这些位置上被遮蔽词元的原始标签。\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
    "\n",
    "\n",
    "# 测试\n",
    "tokens = ['<cls>', '我', '爱', '中', '国', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
    "result = _get_mlm_data_from_tokens(tokens=tokens, vocab=vocab)\n",
    "\n",
    "print(f'mlm_input_tokens: \\t {result[0]}')\n",
    "print(f'pred_positions: \\t {result[1]}')\n",
    "print(f'mlm_pred_labels: \\t {result[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.3. <a id='toc11_7_5_3_'></a>[将文本转换为预训练数据集](#toc0_)\n",
    "函数 _pad_bert_inputs 的主要作用是为BERT模型的`预训练任务`（包括遮蔽语言模型任务 MLM 和下一句预测任务 NSP）准备和`填充`输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_token_ids: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "all_segments: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "valid_lens: [tensor(10.)]\n",
      "all_pred_positions: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "all_mlm_weights: [tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
      "all_mlm_labels: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "nsp_labels: [tensor(1)]\n"
     ]
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    ''' \n",
    "    为BERT模型的预训练任务（包括遮蔽语言模型任务 MLM 和下一句预测任务 NSP）准备和填充输入数据\n",
    "    Args:\n",
    "        examples :list:  包含多个样本的数据集。每个样本是一个元组，通常包含以下内容：\n",
    "                        - token_ids：词元ID列表，表示一个句子的词元序列。\n",
    "                        - pred_positions：被遮蔽词元的位置索引列表。\n",
    "                        - mlm_pred_label_ids：被遮蔽词元的原始标签列表。\n",
    "                        - segments：段落ID列表，表示句子在段落中的位置。\n",
    "                        - is_next：布尔值，表示是否为下一句预测任务的标签。\n",
    "        max_len :int: 模型接受的最大句子对长度，包含了特殊词元 <cls> 和 <sep>,所有样本将被填充或截断到这个长度。\n",
    "        vocab :Vocab: 词汇表对象，包含词元到索引的映射（idx_to_token）。\n",
    "    Returns:\n",
    "        all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels\n",
    "    '''\n",
    "    # 根据最大长度 max_len 计算出最多可以进行遮蔽预测的词元数量，通常占总长度的15%。\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    # 初始化列表，用于存储填充后的数据\n",
    "    ## all_token_ids: 存储填充后的词元索引。\n",
    "    ## all_segments: 存储填充后的段落ID。\n",
    "    ## valid_lens: 存储每个样本的有效长度，不包括'<pad>'的计数。    \n",
    "    all_token_ids, all_segments, valid_lens = [], [], []\n",
    "    ## all_pred_positions: 存储填充后的被遮蔽词元的位置索引。\n",
    "    ## all_mlm_weights: 存储填充后的被遮蔽词元的权重。\n",
    "    ## all_mlm_labels: 存储填充后的被遮蔽词元的原始标签。\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    ## nsp_labels: 存储填充后的下一句预测任务的标签。\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # 词元索引填充: 若 token_ids 的长度小于 max_len，则使用 <pad> 词元的索引进行填充，确保每个序列长度一致。\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))\n",
    "        # 段落ID填充: 若 segments 的长度小于 max_len，则使用0进行填充，确保每个序列长度一致。\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens: 记录每个样本中实际有效的词元数量，即不包括填充的 <pad> 词元的数量。\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        # 被遮蔽词元的位置索引填充: 若 pred_positions 的长度小于 max_num_mlm_preds，则使用0进行填充，确保每个序列长度一致。\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # MLM 权重填充: 对于实际的遮蔽词元位置，赋予权重 1.0；对于填充的位置，赋予权重 0.0。这样在计算损失时，填充部分不会影响结果。\n",
    "        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.float32))\n",
    "        # MLM 标签填充: 对于实际的遮蔽词元，使用其真实的词元索引作为标签；对于填充的位置，使用 0 作为占位符。\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        # NSP 标签: 记录每个样本的下一句预测标签，通常为 0 或 1，表示是否为真实的下一句。\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)\n",
    "\n",
    "\n",
    "# 测试\n",
    "examples = [\n",
    "    ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], True)\n",
    "]\n",
    "result = _pad_bert_inputs(examples=examples, max_len=10, vocab=vocab)\n",
    "\n",
    "print(f'all_token_ids: {result[0]}')\n",
    "print(f'all_segments: {result[1]}')\n",
    "print(f'valid_lens: {result[2]}')\n",
    "print(f'all_pred_positions: {result[3]}')\n",
    "print(f'all_mlm_weights: {result[4]}')\n",
    "print(f'all_mlm_labels: {result[5]}')\n",
    "print(f'nsp_labels: {result[6]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.4. <a id='toc11_7_5_4_'></a>[创建数据集](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
    "        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n",
    "        paragraphs = [d2l.tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # 获取下一句子预测任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(paragraph, paragraphs, self.vocab, max_len))\n",
    "        # 获取遮蔽语言模型任务的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        # 填充输入\n",
    "        (self.all_token_ids, \n",
    "         self.all_segments, \n",
    "         self.valid_lens,\n",
    "         self.all_pred_positions, \n",
    "         self.all_mlm_weights,\n",
    "         self.all_mlm_labels, \n",
    "         self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], \n",
    "                self.all_segments[idx],\n",
    "                self.valid_lens[idx], \n",
    "                self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], \n",
    "                self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    # data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    data_dir = './data/wikipedia_text'\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, \n",
    "          segments_X.shape, \n",
    "          valid_lens_x.shape,\n",
    "          pred_positions_X.shape, \n",
    "          mlm_weights_X.shape, \n",
    "          mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.6. <a id='toc11_7_6_'></a>[预训练BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y):\n",
    "    ''' \n",
    "    计算BERT模型的遮蔽语言模型损失和下一句预测任务损失。\n",
    "    Args:\n",
    "        net :BERTModel: BERT模型实例。\n",
    "        loss :nn.CrossEntropyLoss: 损失函数实例。\n",
    "        vocab_size :int: 词汇表大小。\n",
    "        tokens_X :torch.Tensor: 输入词元索引。\n",
    "        segments_X :torch.Tensor: 输入段落ID。\n",
    "        valid_lens_x :torch.Tensor: 有效长度。\n",
    "    Returns:\n",
    "        mlm_l :torch.Tensor: 遮蔽语言模型损失。\n",
    "        nsp_l :torch.Tensor: 下一句预测任务损失。\n",
    "        l :torch.Tensor: 总损失。\n",
    "    '''\n",
    "    # 前向传播\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X, valid_lens_x.reshape(-1), pred_positions_X)\n",
    "    # 计算遮蔽语言模型损失\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) * mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # 计算下一句子预测任务的损失\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM loss 5.616, NSP loss 0.743\n",
      "7495.3 sentence pairs/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"252.646875pt\" height=\"183.35625pt\" viewBox=\"0 0 252.646875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-11T18:27:11.282085</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 252.646875 183.35625 \n",
       "L 252.646875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "L 40.603125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 78.084943 145.8 \n",
       "L 78.084943 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m7bc6efeab6\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"78.084943\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(71.722443 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 117.539489 145.8 \n",
       "L 117.539489 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"117.539489\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(111.176989 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 156.994034 145.8 \n",
       "L 156.994034 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"156.994034\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(150.631534 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 196.44858 145.8 \n",
       "L 196.44858 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"196.44858\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(190.08608 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"235.903125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(226.359375 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(127.4375 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 40.603125 123.024186 \n",
       "L 235.903125 123.024186 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"mb078a3a8e8\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"123.024186\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(27.240625 126.823405) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 40.603125 96.910243 \n",
       "L 235.903125 96.910243 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"96.910243\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(27.240625 100.709462) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 40.603125 70.7963 \n",
       "L 235.903125 70.7963 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"70.7963\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(27.240625 74.595519) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 40.603125 44.682358 \n",
       "L 235.903125 44.682358 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"44.682358\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(27.240625 48.481576) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 40.603125 18.568415 \n",
       "L 235.903125 18.568415 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"18.568415\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 22.367634) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 40.603125 13.5 \n",
       "L 42.575852 25.661385 \n",
       "L 44.54858 30.721015 \n",
       "L 46.521307 38.846162 \n",
       "L 48.494034 45.134569 \n",
       "L 50.466761 49.840032 \n",
       "L 52.439489 53.348194 \n",
       "L 54.412216 55.998856 \n",
       "L 56.384943 58.039534 \n",
       "L 58.35767 59.523746 \n",
       "L 60.330398 60.651841 \n",
       "L 62.303125 61.620033 \n",
       "L 64.275852 62.590861 \n",
       "L 66.24858 63.471262 \n",
       "L 68.221307 64.142753 \n",
       "L 70.194034 64.739469 \n",
       "L 72.166761 65.387778 \n",
       "L 74.139489 65.88051 \n",
       "L 76.112216 66.317057 \n",
       "L 78.084943 66.786108 \n",
       "L 80.05767 67.188831 \n",
       "L 82.030398 67.576942 \n",
       "L 84.003125 67.972549 \n",
       "L 85.975852 68.30938 \n",
       "L 87.94858 68.629127 \n",
       "L 89.921307 68.862662 \n",
       "L 91.894034 69.207132 \n",
       "L 93.866761 69.55001 \n",
       "L 95.839489 69.848155 \n",
       "L 97.812216 70.058182 \n",
       "L 99.784943 70.303884 \n",
       "L 101.75767 70.507851 \n",
       "L 103.730398 70.657058 \n",
       "L 105.703125 70.841539 \n",
       "L 107.675852 71.027889 \n",
       "L 109.64858 71.233382 \n",
       "L 111.621307 71.42783 \n",
       "L 113.594034 71.613844 \n",
       "L 115.566761 71.758865 \n",
       "L 117.539489 71.90429 \n",
       "L 119.512216 72.06756 \n",
       "L 121.484943 72.210509 \n",
       "L 123.45767 72.349104 \n",
       "L 125.430398 72.467599 \n",
       "L 127.403125 72.587853 \n",
       "L 129.375852 72.709511 \n",
       "L 131.34858 72.817607 \n",
       "L 133.321307 72.933981 \n",
       "L 135.294034 72.996189 \n",
       "L 137.266761 73.102321 \n",
       "L 139.239489 73.212049 \n",
       "L 141.212216 73.30338 \n",
       "L 143.184943 73.373167 \n",
       "L 145.15767 73.443248 \n",
       "L 147.130398 73.530904 \n",
       "L 149.103125 73.605939 \n",
       "L 151.075852 73.684886 \n",
       "L 153.04858 73.776422 \n",
       "L 155.021307 73.849052 \n",
       "L 156.994034 73.927017 \n",
       "L 158.966761 73.980969 \n",
       "L 160.939489 74.070449 \n",
       "L 162.912216 74.128658 \n",
       "L 164.884943 74.19497 \n",
       "L 166.85767 74.260885 \n",
       "L 168.830398 74.310094 \n",
       "L 170.803125 74.368606 \n",
       "L 172.775852 74.442467 \n",
       "L 174.74858 74.492515 \n",
       "L 176.721307 74.548386 \n",
       "L 178.694034 74.596607 \n",
       "L 180.666761 74.642948 \n",
       "L 182.639489 74.700606 \n",
       "L 184.612216 74.772941 \n",
       "L 186.584943 74.807425 \n",
       "L 188.55767 74.86277 \n",
       "L 190.530398 74.916105 \n",
       "L 192.503125 74.951362 \n",
       "L 194.475852 75.000177 \n",
       "L 196.44858 75.035135 \n",
       "L 198.421307 75.075498 \n",
       "L 200.394034 75.120261 \n",
       "L 202.366761 75.174091 \n",
       "L 204.339489 75.214076 \n",
       "L 206.312216 75.228521 \n",
       "L 208.284943 75.256377 \n",
       "L 210.25767 75.302434 \n",
       "L 212.230398 75.350611 \n",
       "L 214.203125 75.402255 \n",
       "L 216.175852 75.445684 \n",
       "L 218.14858 75.475246 \n",
       "L 220.121307 75.513269 \n",
       "L 222.094034 75.556111 \n",
       "L 224.066761 75.599085 \n",
       "L 226.039489 75.642944 \n",
       "L 228.012216 75.678068 \n",
       "L 229.984943 75.710396 \n",
       "L 231.95767 75.74417 \n",
       "L 233.930398 75.774387 \n",
       "L 235.903125 75.811024 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_22\">\n",
       "    <path d=\"M 40.603125 139.5 \n",
       "L 42.575852 137.108483 \n",
       "L 44.54858 129.767098 \n",
       "L 46.521307 128.981284 \n",
       "L 48.494034 130.797116 \n",
       "L 50.466761 132.180952 \n",
       "L 52.439489 133.087804 \n",
       "L 54.412216 133.914726 \n",
       "L 56.384943 134.383507 \n",
       "L 58.35767 134.705106 \n",
       "L 60.330398 135.151464 \n",
       "L 62.303125 135.557287 \n",
       "L 64.275852 135.782361 \n",
       "L 66.24858 136.003891 \n",
       "L 68.221307 136.270284 \n",
       "L 70.194034 136.500998 \n",
       "L 72.166761 136.667052 \n",
       "L 74.139489 136.816249 \n",
       "L 76.112216 136.970833 \n",
       "L 78.084943 137.126161 \n",
       "L 80.05767 137.251917 \n",
       "L 82.030398 137.334579 \n",
       "L 84.003125 137.435436 \n",
       "L 85.975852 137.543338 \n",
       "L 87.94858 137.636781 \n",
       "L 89.921307 137.711951 \n",
       "L 91.894034 137.782432 \n",
       "L 93.866761 137.864181 \n",
       "L 95.839489 137.94024 \n",
       "L 97.812216 138.009755 \n",
       "L 99.784943 138.06656 \n",
       "L 101.75767 138.124856 \n",
       "L 103.730398 138.183478 \n",
       "L 105.703125 138.238981 \n",
       "L 107.675852 138.292134 \n",
       "L 109.64858 138.338831 \n",
       "L 111.621307 138.381482 \n",
       "L 113.594034 138.425971 \n",
       "L 115.566761 138.468694 \n",
       "L 117.539489 138.505792 \n",
       "L 119.512216 138.543333 \n",
       "L 121.484943 138.5782 \n",
       "L 123.45767 138.6123 \n",
       "L 125.430398 138.643668 \n",
       "L 127.403125 138.674632 \n",
       "L 129.375852 138.701133 \n",
       "L 131.34858 138.73022 \n",
       "L 133.321307 138.758786 \n",
       "L 135.294034 138.786784 \n",
       "L 137.266761 138.808848 \n",
       "L 139.239489 138.83537 \n",
       "L 141.212216 138.8598 \n",
       "L 143.184943 138.883176 \n",
       "L 145.15767 138.905363 \n",
       "L 147.130398 138.926825 \n",
       "L 149.103125 138.947044 \n",
       "L 151.075852 138.967389 \n",
       "L 153.04858 138.986126 \n",
       "L 155.021307 139.004576 \n",
       "L 156.994034 139.022287 \n",
       "L 158.966761 139.0399 \n",
       "L 160.939489 139.058224 \n",
       "L 162.912216 139.073238 \n",
       "L 164.884943 139.088934 \n",
       "L 166.85767 139.103486 \n",
       "L 168.830398 139.118382 \n",
       "L 170.803125 139.132383 \n",
       "L 172.775852 139.143206 \n",
       "L 174.74858 139.156424 \n",
       "L 176.721307 139.167468 \n",
       "L 178.694034 139.18164 \n",
       "L 180.666761 139.19177 \n",
       "L 182.639489 139.203878 \n",
       "L 184.612216 139.215785 \n",
       "L 186.584943 139.226793 \n",
       "L 188.55767 139.23733 \n",
       "L 190.530398 139.248572 \n",
       "L 192.503125 139.259306 \n",
       "L 194.475852 139.269773 \n",
       "L 196.44858 139.280112 \n",
       "L 198.421307 139.290201 \n",
       "L 200.394034 139.298813 \n",
       "L 202.366761 139.308033 \n",
       "L 204.339489 139.317115 \n",
       "L 206.312216 139.32591 \n",
       "L 208.284943 139.334348 \n",
       "L 210.25767 139.343088 \n",
       "L 212.230398 139.351701 \n",
       "L 214.203125 139.359098 \n",
       "L 216.175852 139.366249 \n",
       "L 218.14858 139.374265 \n",
       "L 220.121307 139.381281 \n",
       "L 222.094034 139.388853 \n",
       "L 224.066761 139.396269 \n",
       "L 226.039489 139.403586 \n",
       "L 228.012216 139.410363 \n",
       "L 229.984943 139.417148 \n",
       "L 231.95767 139.424252 \n",
       "L 233.930398 139.431052 \n",
       "L 235.903125 139.437225 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 40.603125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 7.2 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 174.64375 44.55625 \n",
       "L 228.903125 44.55625 \n",
       "Q 230.903125 44.55625 230.903125 42.55625 \n",
       "L 230.903125 14.2 \n",
       "Q 230.903125 12.2 228.903125 12.2 \n",
       "L 174.64375 12.2 \n",
       "Q 172.64375 12.2 172.64375 14.2 \n",
       "L 172.64375 42.55625 \n",
       "Q 172.64375 44.55625 174.64375 44.55625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_23\">\n",
       "     <path d=\"M 176.64375 20.298438 \n",
       "L 186.64375 20.298438 \n",
       "L 196.64375 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- mlm -->\n",
       "     <g transform=\"translate(204.64375 23.798438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"97.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"125.195312\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 176.64375 34.976562 \n",
       "L 186.64375 34.976562 \n",
       "L 196.64375 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- nsp -->\n",
       "     <g transform=\"translate(204.64375 38.476562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"63.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"115.478516\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p927c2fb8b1\">\n",
       "   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss', xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1, (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')\n",
    "    \n",
    "\n",
    "#@tab mxnet, pytorch\n",
    "# train_bert(train_iter, net, loss, len(vocab), devices, 100000)\n",
    "train_bert(train_iter, net, loss, len(vocab), devices, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), 'Pytorch_params/BERT/bert_5000.pt')\n",
    "# torch.save(net.state_dict(), 'Pytorch_params/BERT/bert_50000.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.7. <a id='toc11_7_7_'></a>[用BERT表示文本](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    ''' \n",
    "    获取BERT模型的文本表示。\n",
    "    Args:\n",
    "        net :BERTModel: BERT模型实例。\n",
    "        tokens_a :list: 第一个文本的词元列表。\n",
    "        tokens_b :list: 第二个文本的词元列表（可选）。\n",
    "    Returns:\n",
    "        encoded_X :torch.Tensor: 文本的BERT表示。\n",
    "    '''\n",
    "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_text.shape: torch.Size([1, 6, 128])\n",
      "encoded_text_cls.shape: torch.Size([1, 128])\n",
      "encoded_text_crane.shape: torch.Size([1, 128])\n",
      "encoded_text_crane[0][:3]: tensor([-0.6918, -0.0320,  0.0174], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# 词元：'<cls>','a','crane','is','flying','<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "\n",
    "# encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]\n",
    "print(f'encoded_text.shape: {encoded_text.shape}')\n",
    "print(f'encoded_text_cls.shape: {encoded_text_cls.shape}')\n",
    "print(f'encoded_text_crane.shape: {encoded_text_crane.shape}')\n",
    "print(f'encoded_text_crane[0][:3]: {encoded_text_crane[0][:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_pair.shape: torch.Size([1, 10, 128])\n",
      "encoded_pair_cls.shape: torch.Size([1, 128])\n",
      "encoded_pair_crane.shape: torch.Size([1, 128])\n",
      "encoded_pair_crane[0][:3]: tensor([-0.7072,  0.0097,  1.1122], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\n",
    "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
    "# 词元：'<cls>','a','crane','driver','came','<sep>','he','just','left','<sep>'\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "\n",
    "# encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]\n",
    "print(f'encoded_pair.shape: {encoded_pair.shape}')\n",
    "print(f'encoded_pair_cls.shape: {encoded_pair_cls.shape}')\n",
    "print(f'encoded_pair_crane.shape: {encoded_pair_crane.shape}')\n",
    "print(f'encoded_pair_crane[0][:3]: {encoded_pair_crane[0][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.8. <a id='toc11_8_'></a>[用BERT做微调](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.8.1. <a id='toc11_8_1_'></a>[情感分析](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['aclImdb'] = (\n",
    "    'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "    '01ada507287d82875905620988597833ad4e0903')\n",
    "\n",
    "data_dir = d2l.download_extract('aclImdb', 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainings: 25000\n",
      "label: 1 review: Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postw\n",
      "label: 1 review: Zentropa is the most original movie I've seen in years. If you like unique thrillers that are influe\n",
      "label: 1 review: Lars Von Trier is never backward in trying out new techniques. Some of them are very original while \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "#@tab all\n",
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label in ('pos', 'neg'):\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data = read_imdb(data_dir, is_train=True)\n",
    "print('# trainings:', len(train_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('label:', y, 'review:', x[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"255.828125pt\" height=\"183.35625pt\" viewBox=\"0 0 255.828125 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-12T17:52:24.823105</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 255.828125 183.35625 \n",
       "L 255.828125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 53.328125 145.8 \n",
       "L 248.628125 145.8 \n",
       "L 248.628125 7.2 \n",
       "L 53.328125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 62.205398 145.8 \n",
       "L 71.549895 145.8 \n",
       "L 71.549895 135.096774 \n",
       "L 62.205398 135.096774 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 71.549895 145.8 \n",
       "L 80.894393 145.8 \n",
       "L 80.894393 99.870968 \n",
       "L 71.549895 99.870968 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 80.894393 145.8 \n",
       "L 90.238891 145.8 \n",
       "L 90.238891 13.8 \n",
       "L 80.894393 13.8 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 90.238891 145.8 \n",
       "L 99.583388 145.8 \n",
       "L 99.583388 52.23871 \n",
       "L 90.238891 52.23871 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 99.583388 145.8 \n",
       "L 108.927886 145.8 \n",
       "L 108.927886 91.277419 \n",
       "L 99.583388 91.277419 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 108.927886 145.8 \n",
       "L 118.272383 145.8 \n",
       "L 118.272383 110.032258 \n",
       "L 108.927886 110.032258 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 118.272383 145.8 \n",
       "L 127.616881 145.8 \n",
       "L 127.616881 119.090323 \n",
       "L 118.272383 119.090323 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 127.616881 145.8 \n",
       "L 136.961379 145.8 \n",
       "L 136.961379 126.348387 \n",
       "L 127.616881 126.348387 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 136.961379 145.8 \n",
       "L 146.305876 145.8 \n",
       "L 146.305876 131.109677 \n",
       "L 136.961379 131.109677 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 146.305876 145.8 \n",
       "L 155.650374 145.8 \n",
       "L 155.650374 134.554839 \n",
       "L 146.305876 134.554839 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 155.650374 145.8 \n",
       "L 164.994871 145.8 \n",
       "L 164.994871 137.341935 \n",
       "L 155.650374 137.341935 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 164.994871 145.8 \n",
       "L 174.339369 145.8 \n",
       "L 174.339369 139.045161 \n",
       "L 164.994871 139.045161 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 174.339369 145.8 \n",
       "L 183.683867 145.8 \n",
       "L 183.683867 140.825806 \n",
       "L 174.339369 140.825806 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 183.683867 145.8 \n",
       "L 193.028364 145.8 \n",
       "L 193.028364 141.793548 \n",
       "L 183.683867 141.793548 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 193.028364 145.8 \n",
       "L 202.372862 145.8 \n",
       "L 202.372862 142.432258 \n",
       "L 193.028364 142.432258 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 202.372862 145.8 \n",
       "L 211.717359 145.8 \n",
       "L 211.717359 143.225806 \n",
       "L 202.372862 143.225806 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 211.717359 145.8 \n",
       "L 221.061857 145.8 \n",
       "L 221.061857 143.554839 \n",
       "L 211.717359 143.554839 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 221.061857 145.8 \n",
       "L 230.406355 145.8 \n",
       "L 230.406355 144.154839 \n",
       "L 221.061857 144.154839 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 230.406355 145.8 \n",
       "L 239.750852 145.8 \n",
       "L 239.750852 144.348387 \n",
       "L 230.406355 144.348387 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m14c49d08f1\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"62.205398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(59.024148 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"99.583388\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(90.039638 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"136.961379\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(127.417629 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"174.339369\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(164.795619 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"211.717359\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(202.173609 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- # tokens per review -->\n",
       "     <g transform=\"translate(100.597656 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-23\" d=\"M 3272 2816 \n",
       "L 2363 2816 \n",
       "L 2100 1772 \n",
       "L 3016 1772 \n",
       "L 3272 2816 \n",
       "z\n",
       "M 2803 4594 \n",
       "L 2478 3297 \n",
       "L 3391 3297 \n",
       "L 3719 4594 \n",
       "L 4219 4594 \n",
       "L 3897 3297 \n",
       "L 4872 3297 \n",
       "L 4872 2816 \n",
       "L 3775 2816 \n",
       "L 3519 1772 \n",
       "L 4513 1772 \n",
       "L 4513 1294 \n",
       "L 3397 1294 \n",
       "L 3072 0 \n",
       "L 2572 0 \n",
       "L 2894 1294 \n",
       "L 1978 1294 \n",
       "L 1656 0 \n",
       "L 1153 0 \n",
       "L 1478 1294 \n",
       "L 494 1294 \n",
       "L 494 1772 \n",
       "L 1594 1772 \n",
       "L 1856 2816 \n",
       "L 850 2816 \n",
       "L 850 3297 \n",
       "L 1978 3297 \n",
       "L 2297 4594 \n",
       "L 2803 4594 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n",
       "L 844 3500 \n",
       "L 1563 769 \n",
       "L 2278 3500 \n",
       "L 2956 3500 \n",
       "L 3675 769 \n",
       "L 4391 3500 \n",
       "L 4966 3500 \n",
       "L 4050 0 \n",
       "L 3372 0 \n",
       "L 2619 2869 \n",
       "L 1863 0 \n",
       "L 1184 0 \n",
       "L 269 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-23\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"83.789062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"115.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"154.785156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\" x=\"215.966797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"270.251953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"331.775391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"395.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"447.253906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"479.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"542.517578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"604.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"645.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"676.941406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"715.804688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"777.328125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"836.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"864.291016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-77\" x=\"925.814453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m62e23531bd\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(39.965625 149.599219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"107.090323\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(20.878125 110.889541) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"68.380645\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 4000 -->\n",
       "      <g transform=\"translate(20.878125 72.179864) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"29.670968\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 6000 -->\n",
       "      <g transform=\"translate(20.878125 33.470186) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- count -->\n",
       "     <g transform=\"translate(14.798437 90.60625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"179.541016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"242.919922\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 53.328125 145.8 \n",
       "L 53.328125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 248.628125 145.8 \n",
       "L 248.628125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 53.328125 145.8 \n",
       "L 248.628125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 53.328125 7.2 \n",
       "L 248.628125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p33ba718331\">\n",
       "   <rect x=\"53.328125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab all\n",
    "d2l.set_figsize(figsize=(3, 3))\n",
    "d2l.plt.xlabel('# tokens per review')\n",
    "d2l.plt.ylabel('count')\n",
    "d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 500])\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500  # sequence length\n",
    "train_features = d2l.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([64, 500]) , y: torch.Size([64])\n",
      "# batches: 391\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), 64)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X:', X.shape, ', y:', y.shape)\n",
    "    break\n",
    "print('# batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\n",
    "    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = torch.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])), batch_size, is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.1.1. <a id='toc11_8_1_1_'></a>[使用RNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.1.2. <a id='toc11_8_1_2_'></a>[使用CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.8.2. <a id='toc11_8_2_'></a>[自然语言推断](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.2.1. <a id='toc11_8_2_1_'></a>[使用Attention](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.2.2. <a id='toc11_8_2_2_'></a>[微调BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import json\n",
    "import multiprocessing\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices, bert_type='small'):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # Define an empty vocabulary to load the predefined vocabulary\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n",
    "    if bert_type == 'small':    \n",
    "        # parameters of BERT-small\n",
    "        bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=num_heads, num_layers=num_layers, dropout=dropout,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    else:\n",
    "        # parameters of BERT-base\n",
    "        bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[768],\n",
    "                         ffn_num_input=768, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=num_heads, num_layers=num_layers, dropout=dropout,\n",
    "                         max_len=max_len, key_size=768, query_size=768,\n",
    "                         value_size=768, hid_in_features=768,\n",
    "                         mlm_in_features=768, nsp_in_features=768)\n",
    "    # Load pretrained BERT parameters\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.100967 M parameters\n",
      "122.46 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509516/614722111.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "# BERT-small\n",
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    pretrained_model='bert.small', \n",
    "    num_hiddens=256, \n",
    "    ffn_num_hiddens=512, \n",
    "    num_heads=4,\n",
    "    num_layers=2, \n",
    "    dropout=0.1, \n",
    "    max_len=512, \n",
    "    devices=devices,\n",
    "    bert_type='small')\n",
    "parameter_size(bert, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509516/614722111.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.861159 M parameters\n",
      "682.30 MB\n"
     ]
    }
   ],
   "source": [
    "# BERT-base\n",
    "bert, vocab = load_pretrained_model(\n",
    "    pretrained_model='bert.base',\n",
    "    num_hiddens=768, \n",
    "    ffn_num_hiddens=3072, \n",
    "    num_heads=12,\n",
    "    num_layers=12, \n",
    "    dropout=0.1, \n",
    "    max_len=512, \n",
    "    devices=devices,\n",
    "    bert_type='base')\n",
    "parameter_size(bert, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[p_tokens, h_tokens] for p_tokens, h_tokens in zip(*[d2l.tokenize([s.lower() for s in sentences]) for sentences in dataset[:2]])]\n",
    "        \n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long), \n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n",
    "        # input\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "# Reduce `batch_size` if there is an out of memory error. In the original BERT model, `max_len` = 512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, bert_type='small'):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        if bert_type == 'small':\n",
    "            self.output = nn.Linear(256, 3)\n",
    "        else:\n",
    "            self.output = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))\n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "net = BERTClassifier(bert, bert_type='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.022, train acc 0.993, test acc 0.860\n",
      "386.6 examples/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"187.155469pt\" viewBox=\"0 0 238.965625 187.155469\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-13T14:00:07.476575</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 187.155469 \n",
       "L 238.965625 187.155469 \n",
       "L 238.965625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "L 30.103125 10.999219 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 65.974554 149.599219 \n",
       "L 65.974554 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m79ce69da6b\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"65.974554\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(59.612054 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 105.831696 149.599219 \n",
       "L 105.831696 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"105.831696\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(99.469196 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 145.688839 149.599219 \n",
       "L 145.688839 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"145.688839\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(139.326339 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 185.545982 149.599219 \n",
       "L 185.545982 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"185.545982\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(179.183482 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"225.403125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(219.040625 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(112.525 177.875781) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m190ef7832f\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(7.2 153.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 30.103125 121.879219 \n",
       "L 225.403125 121.879219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"121.879219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(7.2 125.678438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 30.103125 94.159219 \n",
       "L 225.403125 94.159219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"94.159219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 97.958438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 30.103125 66.439219 \n",
       "L 225.403125 66.439219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"66.439219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 70.238437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 30.103125 38.719219 \n",
       "L 225.403125 38.719219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"38.719219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 42.518438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 30.103125 10.999219 \n",
       "L 225.403125 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 26.912325 58.125151 \n",
       "L 27.707239 68.381335 \n",
       "L 28.502153 73.805893 \n",
       "L 29.297067 77.41677 \n",
       "L 30.103125 80.104161 \n",
       "L 30.898039 99.361541 \n",
       "L 31.692953 99.61591 \n",
       "L 32.487867 99.661185 \n",
       "L 33.282782 99.915327 \n",
       "L 34.088839 99.959084 \n",
       "L 34.883753 110.985948 \n",
       "L 35.678668 110.398307 \n",
       "L 36.473582 109.963923 \n",
       "L 38.074554 109.41106 \n",
       "L 38.869468 118.502536 \n",
       "L 39.664382 117.660466 \n",
       "L 42.060268 116.425186 \n",
       "L 42.855182 124.378462 \n",
       "L 43.650096 123.510346 \n",
       "L 45.239924 122.485848 \n",
       "L 46.045982 122.10765 \n",
       "L 46.840896 127.792179 \n",
       "L 48.430725 126.802854 \n",
       "L 50.031696 126.099038 \n",
       "L 50.826611 131.176623 \n",
       "L 52.416439 130.082259 \n",
       "L 54.017411 129.444282 \n",
       "L 54.812325 134.113213 \n",
       "L 55.607239 133.083603 \n",
       "L 56.402153 132.648314 \n",
       "L 58.003125 131.95916 \n",
       "L 58.798039 135.786756 \n",
       "L 61.988839 134.212472 \n",
       "L 62.783753 137.429235 \n",
       "L 63.578668 136.88134 \n",
       "L 65.168496 136.252734 \n",
       "L 65.974554 135.914784 \n",
       "L 66.769468 138.768584 \n",
       "L 69.960268 137.345953 \n",
       "L 70.755182 139.860581 \n",
       "L 71.550096 139.245357 \n",
       "L 73.945982 138.503516 \n",
       "L 74.740896 140.95419 \n",
       "L 77.931696 139.642648 \n",
       "L 78.726611 141.702016 \n",
       "L 79.521525 141.253081 \n",
       "L 81.111353 140.654439 \n",
       "L 81.917411 140.356366 \n",
       "L 82.712325 142.410966 \n",
       "L 84.302153 141.790849 \n",
       "L 85.903125 141.250588 \n",
       "L 86.698039 142.941165 \n",
       "L 87.492953 142.658437 \n",
       "L 88.287867 142.254109 \n",
       "L 89.888839 141.756079 \n",
       "L 90.683753 143.435878 \n",
       "L 93.874554 142.424393 \n",
       "L 94.669468 143.780791 \n",
       "L 97.860268 142.65379 \n",
       "L 98.655182 143.992376 \n",
       "L 101.039924 143.245149 \n",
       "L 101.845982 143.098493 \n",
       "L 102.640896 144.276378 \n",
       "L 104.230725 143.696571 \n",
       "L 105.831696 143.352003 \n",
       "L 106.626611 144.720697 \n",
       "L 107.421525 144.32933 \n",
       "L 109.011353 143.910828 \n",
       "L 109.817411 143.705235 \n",
       "L 110.612325 144.554295 \n",
       "L 112.202153 144.362637 \n",
       "L 113.803125 144.016578 \n",
       "L 114.598039 144.90786 \n",
       "L 117.788839 144.269661 \n",
       "L 118.583753 145.088036 \n",
       "L 120.173582 144.771726 \n",
       "L 121.774554 144.415468 \n",
       "L 122.569468 145.32131 \n",
       "L 124.159296 144.749907 \n",
       "L 125.760268 144.467221 \n",
       "L 126.555182 145.457272 \n",
       "L 129.745982 144.794953 \n",
       "L 130.540896 145.552129 \n",
       "L 131.33581 145.484043 \n",
       "L 132.130725 145.16472 \n",
       "L 133.731696 144.932733 \n",
       "L 134.526611 145.839737 \n",
       "L 137.717411 145.096789 \n",
       "L 138.512325 145.591672 \n",
       "L 140.102153 145.353143 \n",
       "L 141.703125 145.050634 \n",
       "L 142.498039 145.970592 \n",
       "L 145.688839 145.338215 \n",
       "L 146.483753 145.885664 \n",
       "L 147.278668 145.826291 \n",
       "L 148.868496 145.39645 \n",
       "L 149.674554 145.300129 \n",
       "L 150.469468 146.248235 \n",
       "L 152.85421 145.699968 \n",
       "L 153.660268 145.57984 \n",
       "L 154.455182 146.254604 \n",
       "L 156.04501 145.83425 \n",
       "L 157.645982 145.56669 \n",
       "L 158.440896 146.292321 \n",
       "L 160.030725 145.972181 \n",
       "L 161.631696 145.718664 \n",
       "L 162.426611 146.220652 \n",
       "L 165.617411 145.712153 \n",
       "L 166.412325 146.2914 \n",
       "L 168.797067 145.914004 \n",
       "L 169.603125 145.778291 \n",
       "L 170.398039 146.446668 \n",
       "L 171.192953 146.173309 \n",
       "L 173.588839 145.85444 \n",
       "L 174.383753 146.475535 \n",
       "L 175.178668 146.391609 \n",
       "L 176.768496 146.042559 \n",
       "L 177.574554 145.927707 \n",
       "L 178.369468 146.550791 \n",
       "L 181.560268 145.995037 \n",
       "L 182.355182 146.450987 \n",
       "L 185.545982 145.968765 \n",
       "L 186.340896 146.17449 \n",
       "L 187.930725 146.240451 \n",
       "L 189.531696 145.98479 \n",
       "L 190.326611 146.571855 \n",
       "L 193.517411 146.245112 \n",
       "L 194.312325 146.73362 \n",
       "L 195.902153 146.439496 \n",
       "L 197.503125 146.198527 \n",
       "L 198.298039 146.933209 \n",
       "L 199.092953 146.592397 \n",
       "L 201.488839 146.261457 \n",
       "L 202.283753 146.945829 \n",
       "L 203.078668 146.692927 \n",
       "L 205.474554 146.366696 \n",
       "L 206.269468 146.795164 \n",
       "L 208.65421 146.366878 \n",
       "L 209.460268 146.304738 \n",
       "L 210.255182 147.098318 \n",
       "L 211.050096 146.809251 \n",
       "L 213.445982 146.474802 \n",
       "L 214.240896 147.064407 \n",
       "L 217.431696 146.44592 \n",
       "L 218.226611 146.964882 \n",
       "L 221.417411 146.464633 \n",
       "L 222.212325 146.94643 \n",
       "L 225.403125 146.595698 \n",
       "L 225.403125 146.595698 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_24\">\n",
       "    <path d=\"M 26.912325 50.191718 \n",
       "L 27.707239 44.60562 \n",
       "L 28.502153 41.852629 \n",
       "L 29.297067 40.136173 \n",
       "L 30.103125 38.814231 \n",
       "L 30.898039 29.787787 \n",
       "L 34.088839 29.639692 \n",
       "L 34.883753 25.003678 \n",
       "L 36.473582 25.462861 \n",
       "L 38.074554 25.696646 \n",
       "L 38.869468 22.017089 \n",
       "L 40.459296 22.560182 \n",
       "L 42.060268 22.867967 \n",
       "L 42.855182 19.853994 \n",
       "L 43.650096 20.21957 \n",
       "L 45.239924 20.573444 \n",
       "L 46.045982 20.715678 \n",
       "L 46.840896 18.520718 \n",
       "L 48.430725 18.933941 \n",
       "L 50.031696 19.206477 \n",
       "L 50.826611 17.273459 \n",
       "L 53.211353 17.837635 \n",
       "L 54.017411 17.946538 \n",
       "L 54.812325 16.279195 \n",
       "L 55.607239 16.646668 \n",
       "L 58.003125 17.072857 \n",
       "L 58.798039 15.66442 \n",
       "L 61.988839 16.307156 \n",
       "L 62.783753 15.138194 \n",
       "L 65.96341 15.693515 \n",
       "L 65.974554 15.694847 \n",
       "L 66.769468 14.663831 \n",
       "L 69.960268 15.210449 \n",
       "L 70.755182 14.31217 \n",
       "L 72.34501 14.625038 \n",
       "L 73.945982 14.824193 \n",
       "L 74.740896 13.960509 \n",
       "L 77.931696 14.436675 \n",
       "L 78.726611 13.684746 \n",
       "L 81.111353 14.052535 \n",
       "L 81.917411 14.172779 \n",
       "L 82.712325 13.451991 \n",
       "L 85.097067 13.79543 \n",
       "L 85.903125 13.908379 \n",
       "L 86.698039 13.263511 \n",
       "L 89.888839 13.704024 \n",
       "L 90.683753 13.154724 \n",
       "L 93.874554 13.497903 \n",
       "L 94.669468 13.047202 \n",
       "L 97.860268 13.415656 \n",
       "L 98.655182 12.977629 \n",
       "L 101.845982 13.284465 \n",
       "L 102.640896 12.814448 \n",
       "L 105.025639 13.131006 \n",
       "L 105.831696 13.195911 \n",
       "L 106.626611 12.698071 \n",
       "L 109.011353 12.967825 \n",
       "L 109.817411 13.042266 \n",
       "L 110.612325 12.761319 \n",
       "L 112.202153 12.827941 \n",
       "L 113.803125 12.941602 \n",
       "L 114.598039 12.656327 \n",
       "L 116.982782 12.81002 \n",
       "L 117.788839 12.8697 \n",
       "L 118.583753 12.531095 \n",
       "L 121.774554 12.793256 \n",
       "L 122.569468 12.498206 \n",
       "L 124.95421 12.728746 \n",
       "L 125.760268 12.802086 \n",
       "L 126.555182 12.426103 \n",
       "L 129.745982 12.658532 \n",
       "L 130.540896 12.357794 \n",
       "L 133.731696 12.59874 \n",
       "L 134.526611 12.304666 \n",
       "L 137.717411 12.548786 \n",
       "L 138.512325 12.381829 \n",
       "L 140.897067 12.515283 \n",
       "L 141.703125 12.569726 \n",
       "L 142.498039 12.232562 \n",
       "L 145.688839 12.469567 \n",
       "L 146.483753 12.264187 \n",
       "L 148.868496 12.429897 \n",
       "L 149.674554 12.459475 \n",
       "L 150.469468 12.173109 \n",
       "L 153.660268 12.395394 \n",
       "L 154.455182 12.161724 \n",
       "L 157.645982 12.393123 \n",
       "L 158.440896 12.104801 \n",
       "L 161.631696 12.318193 \n",
       "L 162.426611 12.156664 \n",
       "L 165.617411 12.343674 \n",
       "L 166.412325 12.108596 \n",
       "L 169.603125 12.290693 \n",
       "L 170.398039 12.084561 \n",
       "L 173.588839 12.274294 \n",
       "L 174.383753 12.040287 \n",
       "L 177.574554 12.249065 \n",
       "L 178.369468 12.032698 \n",
       "L 181.560268 12.212988 \n",
       "L 182.355182 12.095946 \n",
       "L 186.340896 12.14275 \n",
       "L 189.531696 12.224088 \n",
       "L 190.326611 11.997278 \n",
       "L 193.517411 12.156475 \n",
       "L 194.312325 11.945415 \n",
       "L 195.902153 12.065165 \n",
       "L 197.503125 12.13831 \n",
       "L 198.298039 11.902406 \n",
       "L 200.682782 12.097211 \n",
       "L 201.488839 12.126957 \n",
       "L 202.283753 11.907466 \n",
       "L 203.873582 12.024686 \n",
       "L 205.474554 12.103998 \n",
       "L 206.269468 11.94668 \n",
       "L 209.460268 12.114594 \n",
       "L 210.255182 11.848012 \n",
       "L 212.639924 12.016253 \n",
       "L 213.445982 12.050008 \n",
       "L 214.240896 11.870782 \n",
       "L 217.431696 12.066155 \n",
       "L 218.226611 11.856867 \n",
       "L 221.417411 12.045719 \n",
       "L 222.212325 11.908731 \n",
       "L 225.403125 12.021752 \n",
       "L 225.403125 12.021752 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_25\">\n",
       "    <path d=\"M 30.103125 30.05954 \n",
       "L 34.088839 28.832118 \n",
       "L 38.074554 28.930876 \n",
       "L 42.060268 28.761576 \n",
       "L 46.045982 28.719251 \n",
       "L 50.031696 29.410558 \n",
       "L 54.017411 29.847916 \n",
       "L 58.003125 29.593966 \n",
       "L 61.988839 29.438775 \n",
       "L 65.974554 29.340017 \n",
       "L 69.960268 29.495208 \n",
       "L 73.945982 29.593966 \n",
       "L 77.931696 29.678616 \n",
       "L 81.917411 29.819699 \n",
       "L 85.903125 29.706833 \n",
       "L 89.888839 29.664508 \n",
       "L 93.874554 29.706833 \n",
       "L 97.860268 29.6504 \n",
       "L 101.845982 29.904349 \n",
       "L 105.831696 29.763266 \n",
       "L 109.817411 29.340017 \n",
       "L 113.803125 29.56575 \n",
       "L 117.788839 29.777374 \n",
       "L 121.774554 29.622183 \n",
       "L 125.760268 29.622183 \n",
       "L 129.745982 30.242948 \n",
       "L 133.731696 29.862024 \n",
       "L 137.717411 29.946674 \n",
       "L 141.703125 30.327598 \n",
       "L 145.688839 30.257057 \n",
       "L 149.674554 29.6504 \n",
       "L 153.660268 29.763266 \n",
       "L 157.645982 30.172407 \n",
       "L 161.631696 29.509316 \n",
       "L 165.617411 29.340017 \n",
       "L 169.603125 29.932566 \n",
       "L 173.588839 29.523425 \n",
       "L 177.574554 30.003107 \n",
       "L 181.560268 29.636291 \n",
       "L 185.545982 29.678616 \n",
       "L 189.531696 29.636291 \n",
       "L 193.517411 29.833808 \n",
       "L 197.503125 29.678616 \n",
       "L 201.488839 29.932566 \n",
       "L 205.474554 30.299382 \n",
       "L 209.460268 30.454573 \n",
       "L 213.445982 30.567439 \n",
       "L 217.431696 30.200623 \n",
       "L 221.417411 30.31349 \n",
       "L 225.403125 30.355815 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 30.103125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 10.999219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 140.634375 144.599219 \n",
       "L 218.403125 144.599219 \n",
       "Q 220.403125 144.599219 220.403125 142.599219 \n",
       "L 220.403125 99.564844 \n",
       "Q 220.403125 97.564844 218.403125 97.564844 \n",
       "L 140.634375 97.564844 \n",
       "Q 138.634375 97.564844 138.634375 99.564844 \n",
       "L 138.634375 142.599219 \n",
       "Q 138.634375 144.599219 140.634375 144.599219 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_26\">\n",
       "     <path d=\"M 142.634375 105.663281 \n",
       "L 152.634375 105.663281 \n",
       "L 162.634375 105.663281 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- train loss -->\n",
       "     <g transform=\"translate(170.634375 109.163281) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_27\">\n",
       "     <path d=\"M 142.634375 120.341406 \n",
       "L 152.634375 120.341406 \n",
       "L 162.634375 120.341406 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- train acc -->\n",
       "     <g transform=\"translate(170.634375 123.841406) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_28\">\n",
       "     <path d=\"M 142.634375 135.019531 \n",
       "L 152.634375 135.019531 \n",
       "L 162.634375 135.019531 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- test acc -->\n",
       "     <g transform=\"translate(170.634375 138.519531) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"100.732422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"152.832031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"192.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"223.828125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"285.107422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"340.087891\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p54fb815d60\">\n",
       "   <rect x=\"30.103125\" y=\"10.999219\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 1e-4, 50\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.125955 M parameters\n",
      "504.02 MB\n"
     ]
    }
   ],
   "source": [
    "parameter_size(net, torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9. <a id='toc11_9_'></a>[后BERT](#toc0_)\n",
    "### 11.9.1. <a id='toc11_9_1_'></a>[BERT的改进模型](#toc0_)\n",
    "\n",
    "BERT的改进模型包括但不限于以下几种：\n",
    "\n",
    "1. **RoBERTa (Robustly optimized BERT approach)**:\n",
    "    - 通过更大的数据集和更长的训练时间来改进BERT。\n",
    "    - 移除了Next Sentence Prediction (NSP)任务。\n",
    "    - 使用动态的masking策略。\n",
    "\n",
    "2. **ALBERT (A Lite BERT)**:\n",
    "    - 通过参数共享和因子分解嵌入矩阵来减少模型参数。\n",
    "    - 引入了句子顺序预测（Sentence Order Prediction, SOP）任务。\n",
    "\n",
    "3. **DistilBERT**:\n",
    "    - 通过知识蒸馏技术，将BERT模型压缩为更小、更快的版本。\n",
    "    - 保持了BERT模型的大部分性能。\n",
    "\n",
    "4. **TinyBERT**:\n",
    "    - 通过两阶段的知识蒸馏过程，将BERT模型压缩为更小的版本。\n",
    "    - 在保持性能的同时，大大减少了模型的参数量。\n",
    "\n",
    "5. **SpanBERT**:\n",
    "    - 通过预测span级别的masking来改进BERT。\n",
    "    - 更适合于处理需要理解句子内部结构的任务。\n",
    "\n",
    "6. **ERNIE (Enhanced Representation through kNowledge Integration)**:\n",
    "    - 通过引入外部知识图谱来增强BERT的表示能力。\n",
    "    - 在多个自然语言处理任务上取得了显著的性能提升。\n",
    "\n",
    "这些改进模型在不同的任务和数据集上表现出色，进一步推动了自然语言处理技术的发展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.10. <a id='toc11_10_'></a>[GPT](#toc0_)\n",
    "GPT（Generative Pre-trained Transformer）是OpenAI提出的基于Transformer架构的生成模型。GPT仅使用Transformer的解码器部分，通过单向训练（Unidirectional）进行预训练。GPT擅长文本生成任务，能够生成连贯且有意义的文本。GPT-1于2018年发布，随后是GPT-2（2019年）和GPT-3（2020年），每一代模型的生成能力都得到了显著提升。\n",
    "```python\n",
    "就是Transformer的Decoder部分。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.11. <a id='toc11_11_'></a>[T5](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.12. <a id='toc11_12_'></a>[BART](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.13. <a id='toc11_13_'></a>[mBART](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "checkpoint = \"facebook/mbart-large-50\"\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path= checkpoint)\n",
    "# get model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path= checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880.69 M parameters\n",
      "7174.25 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1880.686592, 7174.25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_size.get_parameter_size(model= model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.14. <a id='toc11_14_'></a>[MoE](#toc0_)\n",
    "\n",
    "专家混合模型（Mixture of Experts, MoE）是一种用于处理大规模数据和模型的深度学习架构。MoE模型由多个专家网络和一个门控网络组成，专家网络负责处理不同的输入数据子集，门控网络负责动态地选择合适的专家网络。MoE模型能够有效地处理大规模数据和模型，提高模型的泛化能力和性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.14.1. <a id='toc11_14_1_'></a>[基于Transformer实现MoE](#toc0_)\n",
    "\n",
    "将Transformer中FFN层，改为MoE层即可。\n",
    "\n",
    "|名称|\t作用|\n",
    "|-|-|\n",
    "|Expert\t|专家模块，单独的前馈网络|\n",
    "|Gating Network\t|根据输入决定调用哪些专家|\n",
    "|MoE\t|聚合多个专家，只激活部分|\n",
    "|TransformerBlock\t|用 MoE 替代原 FFN 层|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "    \n",
    "\n",
    "# 定义 MoE 层（带门控）\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts=4, k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k  # Top-k 激活的专家\n",
    "\n",
    "        # 初始化多个专家\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "\n",
    "        # 门控网络：根据输入选择专家\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, input_dim]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        x_flat = x.view(-1, input_dim)  # [B * S, D]\n",
    "\n",
    "        # 门控分数，softmax 后得到每个专家的概率\n",
    "        gate_scores = F.softmax(self.gate(x_flat), dim=-1)  # [B * S, num_experts]\n",
    "\n",
    "        # 选出 top-k 的专家索引和分数\n",
    "        topk_scores, topk_indices = torch.topk(gate_scores, self.k, dim=-1)  # [B*S, k]\n",
    "\n",
    "        # 初始化输出\n",
    "        output = torch.zeros_like(x_flat)\n",
    "\n",
    "        for i in range(self.k):\n",
    "            expert_idx = topk_indices[:, i]  # 第 i 个专家索引\n",
    "            expert_weight = topk_scores[:, i].unsqueeze(1)  # [B*S, 1]\n",
    "\n",
    "            # 遍历每个专家\n",
    "            for eid in range(self.num_experts):\n",
    "                mask = (expert_idx == eid)  # [B*S] 哪些 token 被分配给该专家\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                selected_input = x_flat[mask]  # 挑出这部分输入\n",
    "                expert_output = self.experts[eid](selected_input)  # 喂给专家\n",
    "                output[mask] += expert_output * expert_weight[mask]  # 加权组合输出\n",
    "\n",
    "        return output.view(batch_size, seq_len, input_dim)\n",
    "\n",
    "    \n",
    "\n",
    "# 定义 Transformer Block（MoE 替代 FFN）\n",
    "class MoETransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, num_experts=4, top_k=2):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.moe = MoE(embed_dim, hidden_dim, num_experts, top_k)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head Self Attention\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # MoE Feedforward\n",
    "        moe_output = self.moe(x)\n",
    "        x = self.norm2(x + moe_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test \n",
    "# 模拟一批输入：batch=2, seq_len=5, embedding_dim=32\n",
    "x = torch.randn(2, 5, 32)\n",
    "\n",
    "# 构建 Transformer 块\n",
    "block = MoETransformerBlock(embed_dim=32, num_heads=4, hidden_dim=64, num_experts=6, top_k=2)\n",
    "\n",
    "# 前向传播\n",
    "out = block(x)\n",
    "print(out.shape)  # -> torch.Size([2, 5, 32])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.14.2. <a id='toc11_14_2_'></a>[小项目](#toc0_)\n",
    "\n",
    "利用基于MoE架构的模型，研究细菌基因组特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.15. <a id='toc11_15_'></a>[Mamba](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义状态空间模型（SSM）模块\n",
    "class SSM(nn.Module):\n",
    "    def __init__(self, input_size, state_size):\n",
    "        super(SSM, self).__init__()\n",
    "        # 状态空间模型的参数矩阵 A, B, C, D\n",
    "        self.A = nn.Parameter(torch.randn(state_size, state_size))\n",
    "        self.B = nn.Parameter(torch.randn(state_size, input_size))\n",
    "        self.C = nn.Parameter(torch.randn(input_size, state_size))\n",
    "        self.D = nn.Parameter(torch.randn(input_size, input_size))\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        # 状态更新: h_t = A * h_{t-1} + B * x_t\n",
    "        h_next = torch.tanh(self.A @ h_prev + self.B @ x)\n",
    "        # 输出: y_t = C * h_t + D * x_t\n",
    "        y = self.C @ h_next + self.D @ x\n",
    "        return y, h_next\n",
    "\n",
    "# 定义自注意力机制模块\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x 的形状: [seq_len, batch_size, embed_size]\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "# 定义 Mamba 模型\n",
    "class MambaModel(nn.Module):\n",
    "    def __init__(self, input_size, state_size, embed_size, num_heads, num_layers, output_size, select_threshold=0.5):\n",
    "        super(MambaModel, self).__init__()\n",
    "        self.ssm = SSM(input_size, state_size)\n",
    "        self.attention_module = AttentionModule(embed_size, num_heads)\n",
    "        self.num_layers = num_layers\n",
    "        self.fc = nn.Linear(embed_size, output_size)\n",
    "        self.select_threshold = select_threshold\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状: [seq_len, batch_size, input_size]\n",
    "        seq_len, batch_size, input_size = x.size()\n",
    "        h = torch.zeros(batch_size, input_size).to(x.device)  # 初始化状态\n",
    "        \n",
    "        # 用于选择是否应用状态空间模型的掩码\n",
    "        select_mask = (torch.rand(seq_len) > self.select_threshold).to(x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            if select_mask[t]:\n",
    "                # 使用状态空间模型\n",
    "                y, h = self.ssm(x[t], h)\n",
    "            else:\n",
    "                # 使用自注意力机制\n",
    "                y = self.attention_module(x[t].unsqueeze(0)).squeeze(0)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        \n",
    "        # 将输出送入全连接层\n",
    "        outputs = self.fc(outputs.mean(dim=0))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# 模型参数配置\n",
    "input_size = 512     # 输入特征维度\n",
    "state_size = 256     # 状态空间模型的状态维度\n",
    "embed_size = 512     # 自注意力机制的嵌入维度\n",
    "num_heads = 8        # 注意力头的数量\n",
    "num_layers = 6       # 模型的层数\n",
    "output_size = 10     # 输出类别的数量\n",
    "select_threshold = 0.5  # 选择机制的阈值\n",
    "\n",
    "# 创建模型实例\n",
    "model = MambaModel(input_size, state_size, embed_size, num_heads, num_layers, output_size, select_threshold)\n",
    "\n",
    "# 输入示例 (假设序列长度为 30，batch size 为 16)\n",
    "input_data = torch.rand(30, 16, input_size)\n",
    "output = model(input_data)\n",
    "\n",
    "print(output.shape)  # 输出形状: [batch_size, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_path = 'kuleshov-group/PlantCaduceus_l24'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path, trust_remote_code=True, device_map=device)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "sequence = \"ATGCGTACGATCGTAG\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(input_ids=input_ids, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. <a id='toc12_'></a>[==============](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. <a id='toc13_'></a>[炼丹心得](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1. <a id='toc13_1_'></a>[关于调参](#toc0_)\n",
    "1. Pytorch没有变量、常量之分，不需要定义说明什么是变量，全部都是张量；\n",
    "\n",
    "2. 因为变量定义后需要初始化，就相当于常量；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7580])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    '''带参数的，自定义层'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(2, requires_grad=True))  # 变量，立即初始化，相当于常量\n",
    "        self.bias = nn.Parameter(torch.zeros(1, requires_grad=True))    # 同上\n",
    "    \n",
    "    def forward(self, X):\n",
    "        y_hat = self.weight.data@X + self.bias.data\n",
    "        # y_hat = torch.matmul(self.weight.data, X) + self.bias.data    # 同上\n",
    "        return F.relu(y_hat)\n",
    "\n",
    "\n",
    "# Test\n",
    "myLayer = MyLayer()\n",
    "\n",
    "X = torch.ones(2)\n",
    "\n",
    "myLayer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([ 1.4272, -0.6692])), ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer.state_dict() # 访问神经网络参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2. <a id='toc13_2_'></a>[模型选择](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的复杂度和数据的复杂度应该相适应，不能太大，也不能太小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3. <a id='toc13_3_'></a>[离散数据](#toc0_)\n",
    "- 离散数据：数据是离散的，不能连续的，比如性别，颜色，类别等。\n",
    "- 连续数据：数据是连续的，可以连续的，比如身高，体重，温度等。\n",
    "\n",
    "\n",
    "在 PyTorch 中，one-hot 编码和embedding（嵌入）是两种常见的用于处理离散数据的方法，尤其是在自然语言处理（NLP）任务中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.1. <a id='toc13_3_1_'></a>[one-hot](#toc0_)\n",
    "One-hot 编码将离散的分类变量（如单词、字符等）转换为高维稀疏向量，其中一个位置为 1，其余位置为 0。  \n",
    "有向量无偏差表示；  \n",
    "One-hot 编码生成的张量是稀疏的，随着类别数量增加，存储效率较低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3993157/1598271952.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(col_raw == raw) # 只是bool\n",
      "/tmp/ipykernel_3993157/1598271952.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  one_hot = torch.tensor(col_raw == raw, dtype=torch.float32) # bool -> torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4]]),\n",
       " tensor([[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# 先做广播，后比较即可\n",
    "raw = [0, 1, 2, 3, 4]\n",
    "raw = torch.tensor(raw)\n",
    "col_raw = raw.reshape(5, 1)\n",
    "col_raw == raw # （5， 1） 和 （1， 5）先广播后比较\n",
    "torch.tensor(col_raw == raw) # 只是bool\n",
    "one_hot = torch.tensor(col_raw == raw, dtype=torch.float32) # bool -> torch.float32\n",
    "\n",
    "col_raw, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# 先做广播，后比较即可\n",
    "raw = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long)\n",
    "\n",
    "# help(F.one_hot)\n",
    "F.one_hot(raw, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.2. <a id='toc13_3_2_'></a>[embedding](#toc0_)\n",
    "Embedding 是一种将离散的索引映射为稠密向量的方式，用于学习类别之间的语义关系。  \n",
    "相比 One-Hot 编码，Embedding 提供了一个低维的稠密向量表示（比如词向量）。  \n",
    "\n",
    "Embedding 的特点：\n",
    "- 输入是类别索引（通常是整数）。\n",
    "- 输出是一个稠密向量，其维度由用户定义。\n",
    "- 通常作为模型的一部分，通过反向传播自动更新。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.3.2.1. <a id='toc13_3_2_1_'></a>[使用 torch.nn.Embedding](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4047,  0.0436,  1.6518],\n",
      "        [ 0.3203, -0.2485,  1.1492],\n",
      "        [ 0.7009, -1.1020,  0.0516]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "# 定义类别数量和嵌入维度\n",
    "num_classes = 5 \n",
    "embedding_dim = 3 \n",
    "\n",
    "# 创建嵌入层\n",
    "embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "# 输入类别索引\n",
    "indices = torch.tensor([0, 2, 4])\n",
    "\n",
    "# 获取嵌入向量\n",
    "embedded = embedding(indices)\n",
    "\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.3.2.2. <a id='toc13_3_2_2_'></a>[初始化 Embedding 层](#toc0_)\n",
    "嵌入层可以通过预训练向量（如 Word2Vec、GloVe）初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Tensor:\n",
      "tensor([[0.1000, 0.2000, 0.3000],\n",
      "        [0.7000, 0.8000, 0.9000],\n",
      "        [1.3000, 1.4000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn  \n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "# 假设预训练向量\n",
    "pretrained_weights = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],  # 类别 0\n",
    "    [0.4, 0.5, 0.6],  # 类别 1\n",
    "    [0.7, 0.8, 0.9],  # 类别 2\n",
    "    [1.0, 1.1, 1.2],  # 类别 3\n",
    "    [1.3, 1.4, 1.5]   # 类别 4\n",
    "])\n",
    "\n",
    "# 创建嵌入层并加载权重\n",
    "embedding = nn.Embedding.from_pretrained(pretrained_weights)\n",
    "\n",
    "# 输入类别索引\n",
    "indices = torch.tensor([0, 2, 4])\n",
    "embedded = embedding(indices)\n",
    "print(\"Embedded Tensor:\")\n",
    "print(embedded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4. <a id='toc13_4_'></a>[BN和LN](#toc0_)\n",
    "Batch norm和Layer norm之间的区别  \n",
    "\n",
    "* BatchNorm：在同一特征（同一列），不同样品之间（不同行）之间做的normalization？ standerlization？\n",
    "\n",
    "* LayerNorm：在同一样品（同一行），不同特征（不同列）之间做的normalization？ standerlization？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "torch.nn.BatchNorm1d()\n",
    "torch.nn.LayerNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2., 3., 4.],\n",
       "         [5., 6., 7., 8., 9.]]),\n",
       " tensor([[-1.5667, -1.2185, -0.8704, -0.5222, -0.1741],\n",
       "         [ 0.1741,  0.5222,  0.8704,  1.2185,  1.5667]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "x = torch.arange(10, dtype=torch.float32).reshape(2, 5)\n",
    "\n",
    "ln = nn.LayerNorm(normalized_shape=x.shape)\n",
    "\n",
    "\n",
    "x, ln(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5. <a id='toc13_5_'></a>[掩码 (mask)](#toc0_)\n",
    "在深度学习中，掩码（mask） 是一种用于选择性地处理或忽略数据元素的机制。掩码的常见形式是一个`布尔张量`或`数值张量`，与`目标张量的形状相匹配`，用来标记哪些元素应被关注或忽略。\n",
    "掩码机制广泛应用于自然语言处理（NLP）、计算机视觉（CV）和其他深度学习任务中。  \n",
    "```python \n",
    "scores: (3 x 5)\n",
    "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
    "        [ 5.,  6.,  7.,  8.,  9.],\n",
    "        [10., 11., 12., 13., 14.]])\n",
    "        \n",
    "mask: (3 x 5)\n",
    "tensor([[ True,  True,  True, False, False],\n",
    "        [ True,  True, False, False, False],\n",
    "        [ True,  True,  True,  True,  True]])\n",
    "```\n",
    "\n",
    "掩码的作用和常见场景\n",
    "1. 忽略无效数据  \n",
    "在 NLP 中，序列可能具有不同的长度，为了对齐这些序列，通常会填充（padding）较短的序列。\n",
    "掩码可以帮助模型忽略填充的部分，使得它们不会影响计算。\n",
    "2. 选择性操作  \n",
    "在计算损失或应用注意力机制时，掩码可以用来只关注有效的部分。\n",
    "3. 实现自定义操作  \n",
    "掩码还可以用于筛选数据、执行条件更新等操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.1. <a id='toc13_5_1_'></a>[简单演示](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.1.1. <a id='toc13_5_1_1_'></a>[忽略填充](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 序列张量，填充部分为 0\n",
    "sequences = torch.tensor([\n",
    "    [1, 2, 3, 0, 0],\n",
    "    [4, 5, 0, 0, 0],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "\n",
    "# 创建掩码，标记非填充部分\n",
    "# True 表示非填充部分，False 表示填充部分\n",
    "mask = sequences != 0\n",
    "print(\"Mask:\", mask, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于布尔掩码索引数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用掩码过滤张量，此处过滤掉0元素\n",
    "sequences[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6,  0,  0],\n",
       "        [ 8, 10,  0,  0,  0],\n",
       "        [12, 14, 16, 18, 20]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用掩码来选择指定张量，并进行替换操作\n",
    "sequences[mask] = sequences[mask] * 2 \n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于掩码SoftMax计算\n",
    "\n",
    "<div style=\"display:flex;justify-content:center\">\n",
    "<img src='./Pytorch_Pictures/Mask/Masked_SoftMax.png' height=600, width=900>\n",
    "</div>\n",
    "\n",
    "\n",
    "<!-- ![基于掩码SoftMax计算](./Pytorch_Pictures/Mask/Masked_SoftMax.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.]])\n",
      "valid_length:\n",
      "tensor([[1., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "scores:\n",
      "tensor([[ 0.,  1.,  2., -inf, -inf],\n",
      "        [ 5.,  6., -inf, -inf, -inf],\n",
      "        [10., 11., 12., 13., 14.]])\n",
      "probabilities:\n",
      "tensor([[0.0900, 0.2447, 0.6652, 0.0000, 0.0000],\n",
      "        [0.2689, 0.7311, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]])\n"
     ]
    }
   ],
   "source": [
    "# 基于掩码SoftMax计算\n",
    "# 掩码通常与 PyTorch 的模块结合使用，例如注意力机制\n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "# 假设注意力分数\n",
    "scores = torch.arange(15, dtype=torch.float32).reshape(3, 5)\n",
    "print('scores:', scores, sep='\\n')\n",
    "\n",
    "# 创建掩码\n",
    "## 数值掩码：序列张量，填充部分为 0\n",
    "valid_length = torch.tensor([[1, 1, 1, 0, 0], \n",
    "                             [1, 1, 0, 0, 0], \n",
    "                             [1, 1, 1, 1, 1]], dtype=torch.float32)\n",
    "print('valid_length:', valid_length, sep='\\n')\n",
    "\n",
    "## bool掩码：序列张量，填充部分为 False\n",
    "mask = (valid_length != 0)\n",
    "print('mask:', mask, sep='\\n')\n",
    "\n",
    "# 使用掩码填充分数\n",
    "scores = scores.masked_fill(~mask, float('-inf')) # 填充部分设置为 -inf \n",
    "print('scores:', scores, sep='\\n')\n",
    "\n",
    "# 计算概率，-inf 会变成 0\n",
    "probabilities = F.softmax(scores, dim=-1) # 计算概率\n",
    "print('probabilities:', probabilities, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.1.2. <a id='toc13_5_1_2_'></a>[加权忽略](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "loss:\n",
      "tensor([[ 0.8100,  3.2400,  7.2900,  0.1600,  0.2500],\n",
      "        [12.2500, 19.3600,  0.4900,  0.6400,  0.8100],\n",
      "        [26.0100, 38.4400, 53.2900, 70.5600, 90.2500]])\n",
      "masked_loss:\n",
      "tensor([[ 0.8100,  3.2400,  7.2900,  0.0000,  0.0000],\n",
      "        [12.2500, 19.3600,  0.0000,  0.0000,  0.0000],\n",
      "        [26.0100, 38.4400, 53.2900, 70.5600, 90.2500]])\n",
      "Final loss:\n",
      "tensor(32.1500)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# 假设模型输出和目标值\n",
    "output = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "])\n",
    "\n",
    "target = torch.tensor([\n",
    "    [1, 2, 3, 0, 0],\n",
    "    [4, 5, 0, 0, 0],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "\n",
    "mask = (target !=0)\n",
    "print('mask:', mask, sep='\\n')\n",
    "\n",
    "# 损失函数\n",
    "loss = (output - target.float())**2\n",
    "print('loss:', loss, sep='\\n')\n",
    "\n",
    "# 使用掩码忽略填充部分\n",
    "masked_loss = loss * mask\n",
    "print('masked_loss:', masked_loss, sep='\\n')\n",
    "\n",
    "final_loss = masked_loss.sum() / mask.sum()  # 仅对非填充部分求平均\n",
    "print(\"Final loss:\", final_loss, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4],\n",
       "         [5, 6, 7, 8, 9]]),\n",
       " tensor(45))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10).reshape(2, 5)\n",
    "\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.2. <a id='toc13_5_2_'></a>[注意力机制中的掩码](#toc0_)\n",
    "在 Transformer 中，掩码常用于：\n",
    "  1. Padding Mask：防止模型关注填充的部分。\n",
    "  2. Causal Mask（未来掩码）：防止模型在自回归任务中关注未来的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.2.1. <a id='toc13_5_2_1_'></a>[Padding Mask](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "~mask:\n",
      "tensor([[False, False, False,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False]])\n",
      "Masked Attention:\n",
      "tensor([[0.1000, 0.2000, 0.3000,   -inf,   -inf],\n",
      "        [0.5000, 0.6000,   -inf,   -inf,   -inf],\n",
      "        [0.9000, 0.8000, 0.7000, 0.6000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# 0表示填充，1表示有效\n",
    "valid_length = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "mask = (valid_length !=0)\n",
    "print('mask:', mask, sep='\\n')\n",
    "# 取反操作\n",
    "print('~mask:', ~mask, sep='\\n')\n",
    "\n",
    "\n",
    "# 假设一批序列的注意力权重\n",
    "attention_weights = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "])\n",
    "\n",
    "# 利用掩码设置填充部分的权重为 -inf\n",
    "masked_attention = attention_weights.masked_fill(~mask, float('-inf'))\n",
    "print(\"Masked Attention:\", masked_attention, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.2.2. <a id='toc13_5_2_2_'></a>[Causal Mask](#toc0_)\n",
    "用于防止模型在解码时看到未来的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask:\n",
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "\n",
    "# True 表示需要屏蔽的部分\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "print(\"Causal Mask:\", causal_mask, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.3. <a id='toc13_5_3_'></a>[掩码注意力计算](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3, 0, 0],\n",
       "         [4, 5, 0, 0, 0],\n",
       "         [6, 7, 8, 9, 0]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F  \n",
    "\n",
    "\n",
    "# 假设有一个批次的序列，长度为5\n",
    "sequences = torch.tensor([[1, 2, 3, 0, 0],  # 0 表示填充\n",
    "                          [4, 5, 0, 0, 0],\n",
    "                          [6, 7, 8, 9, 0]])\n",
    "sequences, sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ True,  True,  True, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True, False]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个mask，1表示有效部分，0表示填充部分\n",
    "mask = (sequences != 0)\n",
    "mask, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1088, 0.1637, 0.7025, 0.6790, 0.9155],\n",
       "          [0.2418, 0.1591, 0.7653, 0.2979, 0.8035],\n",
       "          [0.3813, 0.7860, 0.1115, 0.2477, 0.6524],\n",
       "          [0.6057, 0.3725, 0.7980, 0.8399, 0.1374],\n",
       "          [0.2331, 0.9578, 0.3313, 0.3227, 0.0162]],\n",
       " \n",
       "         [[0.2137, 0.6249, 0.4340, 0.1371, 0.5117],\n",
       "          [0.1585, 0.0758, 0.2247, 0.0624, 0.1816],\n",
       "          [0.9998, 0.5944, 0.6541, 0.0337, 0.1716],\n",
       "          [0.3336, 0.5782, 0.0600, 0.2846, 0.2007],\n",
       "          [0.5014, 0.3139, 0.4654, 0.1612, 0.1568]],\n",
       " \n",
       "         [[0.2083, 0.3289, 0.1054, 0.9192, 0.4008],\n",
       "          [0.9302, 0.6558, 0.0766, 0.8460, 0.3624],\n",
       "          [0.3083, 0.0850, 0.0029, 0.6431, 0.3908],\n",
       "          [0.6947, 0.0897, 0.8712, 0.1330, 0.4137],\n",
       "          [0.6044, 0.7581, 0.9037, 0.9555, 0.1035]]]),\n",
       " torch.Size([3, 5, 5]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设有一个注意力得分矩阵\n",
    "attention_scores = torch.rand(3, 5, 5)  # (batch_size, seq_length, seq_length)\n",
    "attention_scores, attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False]],\n",
       " \n",
       "         [[ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False]],\n",
       " \n",
       "         [[ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False]]]),\n",
       " torch.Size([3, 5, 5]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用masked_fill来忽略填充部分\n",
    "# mask需要扩展到与attention_scores相同的形状\n",
    "mask = mask.unsqueeze(1).expand(-1, sequences.size(1), -1)\n",
    "mask, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True]],\n",
       "\n",
       "        [[False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7284, 0.8325, 0.7335,   -inf,   -inf],\n",
       "         [0.4551, 0.3431, 0.4233,   -inf,   -inf],\n",
       "         [0.6648, 0.7469, 0.0081,   -inf,   -inf],\n",
       "         [0.1827, 0.9752, 0.4536,   -inf,   -inf],\n",
       "         [0.2960, 0.3019, 0.9114,   -inf,   -inf]],\n",
       "\n",
       "        [[0.7884, 0.0216,   -inf,   -inf,   -inf],\n",
       "         [0.7880, 0.1560,   -inf,   -inf,   -inf],\n",
       "         [0.8611, 0.5480,   -inf,   -inf,   -inf],\n",
       "         [0.0806, 0.9418,   -inf,   -inf,   -inf],\n",
       "         [0.2446, 0.0949,   -inf,   -inf,   -inf]],\n",
       "\n",
       "        [[0.0106, 0.8559, 0.8870, 0.3322,   -inf],\n",
       "         [0.2074, 0.1152, 0.6055, 0.7251,   -inf],\n",
       "         [0.3443, 0.9339, 0.3960, 0.9770,   -inf],\n",
       "         [0.1821, 0.4533, 0.3604, 0.4188,   -inf],\n",
       "         [0.3007, 0.6403, 0.9883, 0.0820,   -inf]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将填充部分的注意力得分设置为一个非常小的值\n",
    "attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重:\n",
      " tensor([[[0.3210, 0.3563, 0.3227, 0.0000, 0.0000],\n",
      "         [0.3493, 0.3123, 0.3384, 0.0000, 0.0000],\n",
      "         [0.3840, 0.4169, 0.1991, 0.0000, 0.0000],\n",
      "         [0.2212, 0.4887, 0.2901, 0.0000, 0.0000],\n",
      "         [0.2593, 0.2608, 0.4798, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6828, 0.3172, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6529, 0.3471, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5776, 0.4224, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2971, 0.7029, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5373, 0.4627, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1406, 0.3275, 0.3378, 0.1940, 0.0000],\n",
      "         [0.1969, 0.1795, 0.2932, 0.3304, 0.0000],\n",
      "         [0.1743, 0.3142, 0.1835, 0.3281, 0.0000],\n",
      "         [0.2095, 0.2747, 0.2504, 0.2654, 0.0000],\n",
      "         [0.1924, 0.2702, 0.3827, 0.1546, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# 计算注意力权重\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "print(\"注意力权重:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.6. <a id='toc13_6_'></a>[MLP、FC、FNN、CNN、RNN](#toc0_)\n",
    "Linear()：线性网络，即没有非线性激活函数  \n",
    "MLP()：多层感知机，有非线性激活函数  \n",
    "FNN()：前馈神经网络，同MLP（）  \n",
    "CNN()：卷积神经网络    \n",
    "RNN()：循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.7. <a id='toc13_7_'></a>[优化显存使用](#toc0_)\n",
    "PyTorch 在进行深度学习训练的时候，有 4 大部分的显存开销，分别是`模型参数(parameters)`，`模型参数的梯度(gradients)`，`优化器状态(optimizer states)` 以及 `中间激活值(intermediate activations) 或者叫中间结果(intermediate results)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.1. <a id='toc13_7_1_'></a>[删除中间暂时不用的变量](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.randn(1000, 1000)\n",
    "\n",
    "# 删除变量\n",
    "del x \n",
    "\n",
    "# 释放显存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.2. <a id='toc13_7_2_'></a>[混合精度训练(Mixed Precision Training)](#toc0_)\n",
    "使用半精度（FP16）或混合精度（AMP）可以减少显存占用，提高训练速度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1140599250793457\n",
      "Epoch 1, Loss: 1.1137373447418213\n",
      "Epoch 2, Loss: 1.1134288311004639\n",
      "Epoch 3, Loss: 1.1131079196929932\n",
      "Epoch 4, Loss: 1.1127952337265015\n",
      "Epoch 5, Loss: 1.1124804019927979\n",
      "Epoch 6, Loss: 1.112164855003357\n",
      "Epoch 7, Loss: 1.11184823513031\n",
      "Epoch 8, Loss: 1.111536979675293\n",
      "Epoch 9, Loss: 1.1112210750579834\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# 定义一个简单的模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(1000, 1000)\n",
    "        self.layer2 = nn.Linear(1000, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 创建模型、损失函数和优化器\n",
    "model = SimpleModel().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# 创建输入数据和目标\n",
    "input_data = torch.randn(10, 1000).cuda()\n",
    "target = torch.randn(10, 1000).cuda()\n",
    "\n",
    "# 创建GradScaler用于缩放梯度\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 混合精度训练循环\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 使用autocast进行前向传播\n",
    "    with autocast(device_type='cuda'):\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output, target)\n",
    "    \n",
    "    # 使用GradScaler进行反向传播和优化\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.3. <a id='toc13_7_3_'></a>[梯度检查点（Gradient Checkpointing）](#toc0_)\n",
    "\n",
    "- 时间换空间：通过在前向传播过程中保存较少的中间激活值，AlphaFold2可以在反向传播时重新计算这些值，从而减少显存占用。  \n",
    "\n",
    "- 具体地来说，在前向传递中，传入的function将以torch.no_grad的方式运行，即不保存中间激活值。取而代之的是，前向传递保存了输入元组以及function参数。在反向传递中，保存下来的输入元组与function参数将会被重新取回，并且前向传递将会在function上重新计算，此时会追踪中间激活值，然后梯度将会根据这些中间激活值计算得到。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数说明：\n",
    "\n",
    "```python\n",
    "torch.utils.checkpoint.checkpoint(function, *args, use_reentrant: Optional[bool] = None)\n",
    "# function：在前向传播时调用的函数（通常是模型的某一部分）。\n",
    "# *args：传递给 function 的输入参数。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1000]), torch.Size([10, 1000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils import checkpoint\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(1000, 1000)\n",
    "        self.layer2 = nn.Linear(1000, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 使用梯度检查点来减少内存使用\n",
    "        x = checkpoint.checkpoint(self.layer1, x)\n",
    "        x = checkpoint.checkpoint(self.layer2, x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 创建模型和输入数据\n",
    "model = SimpleModel().cuda()\n",
    "input_data = torch.randn(10, 1000).cuda()\n",
    "\n",
    "# 前向传播\n",
    "output = model(input_data)\n",
    "\n",
    "input_data.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.4. <a id='toc13_7_4_'></a>[分块计算 (Chunking)](#toc0_)\n",
    "\n",
    "AlphaFold2将计算过程分成多个较小的块来处理。这种方法可以减少一次性需要加载到显存中的数据量，从而降低显存的使用。\n",
    "\n",
    "在 PyTorch 中，chunk 是一种用于将张量沿指定维度分割为多个小张量的操作。其主要功能是将一个大的张量分成多个`小块（chunk）`，以便于并行处理或其他需要分割数据的场景。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数说明：\n",
    "\n",
    "```python\n",
    "torch.chunk(input, chunks, dim=0)\n",
    "# input: 要分割的张量\n",
    "# chunks: 分割的块数\n",
    "# dim: 沿哪个维度进行分割，默认为0\n",
    "# 返回：一个包含分割后的小张量的元组\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.7.4.1. <a id='toc13_7_4_1_'></a>[简单演示](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "\n",
      "Chunks:\n",
      "Chunk 0:\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "Chunk 1:\n",
      "tensor([[ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "# 示例 1：按行分割张量\n",
    "import torch\n",
    "\n",
    "\n",
    "# 创建一个 4x4 的张量\n",
    "x = torch.arange(16).view(4, 4)\n",
    "print(\"Original Tensor:\")\n",
    "print(x)\n",
    "\n",
    "# 按行分割为 2 块\n",
    "chunks = torch.chunk(x, chunks=2, dim=0)\n",
    "print(\"\\nChunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks by Columns:\n",
      "Chunk 0:\n",
      "tensor([[ 0],\n",
      "        [ 4],\n",
      "        [ 8],\n",
      "        [12]])\n",
      "Chunk 1:\n",
      "tensor([[ 1],\n",
      "        [ 5],\n",
      "        [ 9],\n",
      "        [13]])\n",
      "Chunk 2:\n",
      "tensor([[ 2],\n",
      "        [ 6],\n",
      "        [10],\n",
      "        [14]])\n",
      "Chunk 3:\n",
      "tensor([[ 3],\n",
      "        [ 7],\n",
      "        [11],\n",
      "        [15]])\n"
     ]
    }
   ],
   "source": [
    "# 示例 2：按列分割张量\n",
    "# 按列分割为 4 块\n",
    "chunks = torch.chunk(x, chunks=4, dim=1)\n",
    "print(\"\\nChunks by Columns:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.7.4.2. <a id='toc13_7_4_2_'></a>[重要演示](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def chunked_attention(query, key, value, chunk_size):\n",
    "    \"\"\"\n",
    "    计算分块的自注意力。\n",
    "    \"\"\"\n",
    "    num_chunks = query.size(0) // chunk_size\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # 获取当前块的query, key, value\n",
    "        q_chunk = query[i * chunk_size:(i + 1) * chunk_size]\n",
    "        k_chunk = key[i * chunk_size:(i + 1) * chunk_size]\n",
    "        v_chunk = value[i * chunk_size:(i + 1) * chunk_size]\n",
    "\n",
    "        # 计算注意力得分\n",
    "        scores = torch.matmul(q_chunk, k_chunk.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        output = torch.matmul(attn_weights, v_chunk)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # 将所有块的输出拼接在一起\n",
    "    return torch.cat(outputs, dim=0)\n",
    "\n",
    "\n",
    "# 示例输入\n",
    "seq_length = 1024\n",
    "d_model = 64\n",
    "chunk_size = 256\n",
    "\n",
    "query = torch.randn(seq_length, d_model)\n",
    "key = torch.randn(seq_length, d_model)\n",
    "value = torch.randn(seq_length, d_model)\n",
    "\n",
    "# 使用分块注意力计算\n",
    "output = chunked_attention(query, key, value, chunk_size)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.8. <a id='toc13_8_'></a>[模型参数量](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数总数: 0.20 M\n",
      "参数占用内存: 0.78 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def parameter_size(model, dtype=torch.float32):\n",
    "    bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "    total_params = count_parameters(model)\n",
    "    total_size = total_params * bytes_per_param\n",
    "    return total_params, total_size\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = SampleModel()\n",
    "    params, size = parameter_size(model, dtype=torch.float32)\n",
    "    print(f\"参数总数: {params / 1000000:.2f} M\")\n",
    "    print(f\"参数占用内存: {size / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.9. <a id='toc13_9_'></a>[大模型微调](#toc0_)\n",
    "\n",
    "|特性|全模型微调（SFT）|参数高效微调（PEFT）|指令微调（Instruction Tuning）|强化学习微调（RLHF）|\n",
    "|---|---|---|---|---|\n",
    "|训练目标|优化单一任务|优化少量参数，提高训练效率|提高多任务能力，适应自然语言指令|生成符合人类反馈的内容|\n",
    "|计算开销|高|低（训练少量参数）|中等，取决于任务复杂度|高，训练奖励模型和强化学习|\n",
    "|数据需求|大量带标注的训练数据|训练数据较少，但任务数据需要多样|需要多样化的指令数据|需要人类反馈数据|\n",
    "|适用场景|单一任务的优化|资源有限的场景，快速微调|多任务学习，灵活的指令处理|开放式任务生成，基于人类偏好的优化|\n",
    "|优点|可以大幅提升单一任务性能|节省资源，减少训练成本|提升多任务能力和灵活性|增强生成质量，符合人类期望|\n",
    "|缺点|计算资源消耗大，容易过拟合|微调效果可能不如全模型微调|数据准备复杂，训练时间长|实现复杂，资源需求高|\n",
    "\n",
    "\n",
    "不需要更新参数：\n",
    " - 提示词微调（Promot tuning）\n",
    " \n",
    " 更新参数：\n",
    " - 全量微调（Full Fine-Tuning）\n",
    " - 高效参数微调（Efficient Fine-Tuning）\n",
    " - 强化学习微调（RLHF）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.10. <a id='toc13_10_'></a>[加速器](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据并行、模型并行、混合并行与工具包的关系比较\n",
    "\n",
    "\n",
    "|特性|数据并行|模型并行|混合并行|\n",
    "|---|---|---|---|\n",
    "|定义|将训练数据划分到多个设备（GPU/节点）上，并进行并行计算，最后合并梯度。|将模型划分为多个部分，并分配到多个设备（GPU/节点）上进行并行计算。|结合数据并行和模型并行，既拆分数据也拆分模型以提高训练效率。|\n",
    "|适用场景|数据量大，但模型相对较小。适用于大规模数据训练，适合处理常见的深度学习任务。|模型大，单个设备无法存下整个模型，适用于非常大的模型（如Transformer、BERT类模型）。|适用于需要同时处理超大模型和大规模数据的训练任务，如训练GPT类模型等。|\n",
    "|工具包支持|DeepSpeed（通过DistributedDataParallel和ZeRO），Horovod（基于Ring-AllReduce），TensorFlow（tf.distribute.Strategy）|Megatron-LM（张量并行），DeepSpeed（ZeRO阶段3），FairScale（FSDP）|DeepSpeed（结合数据并行和ZeRO），Megatron-LM（结合数据并行和模型并行）|\n",
    "|性能优化|高效的梯度同步和数据拆分，适用于多GPU/多节点训练。|模型拆分，适合超大规模模型，减少单个设备内存压力。|综合了数据并行和模型并行的优势，适用于超大规模数据和模型训练。|\n",
    "|内存优化|通过数据拆分来减少每个设备的内存需求。|通过模型拆分来减轻设备的内存压力。|结合了数据和模型并行的内存优化，特别适用于超大模型和数据的训练。|\n",
    "|分布式训练支持|DeepSpeed（支持NCCL后端），Horovod（Ring-AllReduce），TensorFlow（MirroredStrategy）|Megatron-LM（张量并行），FairScale（FSDP），DeepSpeed（ZeRO阶段3）|DeepSpeed（支持两者的混合模式），Megatron-LM（数据和模型并行结合）|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|特性/工具|Hugging Face Trainer|DeepSpeed|Horovod|Megatron-LM|FairScale|\n",
    "|---|---|---|---|---|---|\n",
    "|功能|高层封装的训练API，简化训练过程（适用于NLP任务）|低层次优化，支持大规模模型训练和数据并行，提供多种内存优化方案|分布式训练框架，适用于大规模数据并行训练，基于Horovod接口|专注于超大规模模型（如GPT-3），通过张量并行进行大规模训练|专注于大模型训练的内存优化，支持模型并行和数据并行|\n",
    "|适用场景|主要用于NLP任务，适用于Transformer类模型训练，方便与 Hugging Face 数据集和模型库集成|适用于大规模模型和数据的分布式训练，尤其适合超大模型（如GPT、BERT）|适用于大规模分布式训练，尤其是多节点环境中的数据并行|适用于需要极大计算和内存资源的超大规模模型训练（如GPT-3）|适用于内存受限的情况下进行大规模分布式训练（如BERT、GPT模型）|\n",
    "|易用性|非常易用，高层API，少量代码即可完成训练、微调、评估等任务|需要较多配置，适合需要高度定制化训练的高级用户|需要较高的分布式训练经验，配置较为复杂|需要深入了解模型并行、数据并行的概念，配置复杂|相对复杂，需要开发者了解内存优化和并行训练技术|\n",
    "|分布式训练支持|支持数据并行，集成了 Accelerate 库，支持多GPU训练|支持数据并行、模型并行和混合并行，尤其在大规模模型训练中表现优异|支持数据并行，分布式梯度同步（基于 Ring-AllReduce 或 NCCL）|支持模型并行和数据并行的结合，专门针对超大规模模型（如GPT-3）|支持数据并行和模型并行，内存优化，适合大规模训练\n",
    "|模型并行支持|不支持复杂的模型并行，主要聚焦于数据并行和微调|支持模型并行，尤其是在 ZeRO 和混合并行模式下支持大规模模型训练|不直接支持模型并行，专注于数据并行|通过张量并行（Tensor Parallelism）支持模型并行|支持模型并行，尤其是通过 FSDP（Fully Sharded Data Parallel）模式优化内存\n",
    "|内存优化|提供微调、自动混合精度（AMP）等基本优化|ZeRO（Zero Redundancy Optimizer）优化，支持多种内存优化技术|主要通过数据并行和全局梯度同步优化内存|张量并行和模型分片，通过分布式内存管理优化超大规模模型训练|通过 FSDP 和混合并行优化内存，减少训练时内存占用\n",
    "|性能|性能主要依赖于配置，适用于中小规模模型训练和微调|在大规模模型和数据训练中提供显著性能提升，特别是在分布式环境下|在多节点环境下性能较强，尤其是在数据并行模式下|适合极大规模的模型训练，提供高效的张量并行支持|在模型并行和内存优化方面提供较好性能，适合内存受限的场景\n",
    "|集成度|与 Hugging Face 模型库和数据集无缝集成，极大简化了训练过程|可以与 Hugging Face 集成，但需要更多配置和自定义|需要与 PyTorch 集成，配置较为复杂|可以与 Hugging Face 集成，但适用于大规模训练和模型开发者|适用于与 PyTorch 结合，专注于内存优化和并行训练\n",
    "|自动化功能|自动保存、评估、调优、日志记录、早期停止等功能|提供 ZeRO、FP16、混合精度等自动优化功能，但配置较为复杂|通过 Horovod 提供分布式训练的自动化控制|支持大规模模型的自动优化，尤其是通过模型并行与数据并行的结合|提供分布式训练的内存优化和自动化控制，尤其是 FSDP 优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.10.1. <a id='toc13_10_1_'></a>[deepspeed](#toc0_)\n",
    "DeepSpeed 是一个由 Microsoft 提供的深度学习优化库，旨在提高深度学习模型训练的效率，特别是对于超大规模模型的训练。它提供了多种性能优化技术，包括内存优化、分布式训练、混合精度训练和模型并行等。DeepSpeed 的目标是让研究人员和开发者能够训练更大规模的模型，同时保持高效的内存利用和计算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.10.1.1. <a id='toc13_10_1_1_'></a>[数据并行](#toc0_)\n",
    "在 DeepSpeed 中，数据并行使用 DeepSpeed 和 torch.nn.DataParallel 的结合来加速训练，特别是在多GPU环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-16 15:01:45,498] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=056f307, git-branch=HEAD\n",
      "[2025-01-16 15:01:45,499] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1\n",
      "[2025-01-16 15:01:45,501] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-01-16 15:01:45,502] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-01-16 15:01:45,502] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-01-16 15:01:45,503] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = SGD\n",
      "[2025-01-16 15:01:45,503] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=SGD type=<class 'torch.optim.sgd.SGD'>\n",
      "[2025-01-16 15:01:45,504] [WARNING] [engine.py:1244:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-01-16 15:01:45,504] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2025-01-16 15:01:45,505] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-01-16 15:01:45,505] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-01-16 15:01:45,505] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-01-16 15:01:45,506] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-01-16 15:01:45,649] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-01-16 15:01:45,650] [INFO] [utils.py:782:see_memory_usage] MA 0.36 GB         Max_MA 0.36 GB         CA 0.36 GB         Max_CA 0 GB \n",
      "[2025-01-16 15:01:45,651] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 148.31 GB, percent = 14.7%\n",
      "[2025-01-16 15:01:45,786] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-01-16 15:01:45,787] [INFO] [utils.py:782:see_memory_usage] MA 0.36 GB         Max_MA 0.36 GB         CA 0.36 GB         Max_CA 0 GB \n",
      "[2025-01-16 15:01:45,787] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 148.32 GB, percent = 14.7%\n",
      "[2025-01-16 15:01:45,788] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized\n",
      "[2025-01-16 15:01:45,918] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-01-16 15:01:45,919] [INFO] [utils.py:782:see_memory_usage] MA 0.36 GB         Max_MA 0.36 GB         CA 0.36 GB         Max_CA 0 GB \n",
      "[2025-01-16 15:01:45,920] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 148.28 GB, percent = 14.7%\n",
      "[2025-01-16 15:01:45,921] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-01-16 15:01:45,921] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-01-16 15:01:45,922] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-01-16 15:01:45,922] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.01], mom=[0]\n",
      "[2025-01-16 15:01:45,923] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-01-16 15:01:45,923] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-01-16 15:01:45,924] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-01-16 15:01:45,924] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-01-16 15:01:45,924] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-01-16 15:01:45,925] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-01-16 15:01:45,925] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-01-16 15:01:45,926] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-01-16 15:01:45,926] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-01-16 15:01:45,927] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-01-16 15:01:45,927] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-01-16 15:01:45,927] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdf1987b190>\n",
      "[2025-01-16 15:01:45,928] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-01-16 15:01:45,928] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-01-16 15:01:45,928] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-01-16 15:01:45,929] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-01-16 15:01:45,929] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-01-16 15:01:45,929] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-01-16 15:01:45,931] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-01-16 15:01:45,931] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-01-16 15:01:45,932] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-01-16 15:01:45,932] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-01-16 15:01:45,932] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-01-16 15:01:45,933] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-01-16 15:01:45,933] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-01-16 15:01:45,933] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-01-16 15:01:45,934] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-01-16 15:01:45,934] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-01-16 15:01:45,935] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-01-16 15:01:45,935] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-01-16 15:01:45,935] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-01-16 15:01:45,936] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-01-16 15:01:45,936] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2025-01-16 15:01:45,936] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2025-01-16 15:01:45,937] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-01-16 15:01:45,937] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-01-16 15:01:45,937] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-01-16 15:01:45,938] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-01-16 15:01:45,938] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-01-16 15:01:45,938] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-01-16 15:01:45,939] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-01-16 15:01:45,939] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-01-16 15:01:45,939] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-01-16 15:01:45,940] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-01-16 15:01:45,940] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-01-16 15:01:45,940] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-01-16 15:01:45,941] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-01-16 15:01:45,941] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-01-16 15:01:45,941] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-01-16 15:01:45,942] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-01-16 15:01:45,942] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-01-16 15:01:45,945] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2025-01-16 15:01:45,945] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2025-01-16 15:01:45,946] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-01-16 15:01:45,946] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-01-16 15:01:45,946] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-01-16 15:01:45,947] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-01-16 15:01:45,947] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-01-16 15:01:45,947] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-01-16 15:01:45,948] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-01-16 15:01:45,948] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-01-16 15:01:45,948] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-01-16 15:01:45,949] [INFO] [config.py:1003:print]   steps_per_print .............. 200\n",
      "[2025-01-16 15:01:45,949] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-01-16 15:01:45,949] [INFO] [config.py:1003:print]   train_batch_size ............. 32\n",
      "[2025-01-16 15:01:45,950] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  32\n",
      "[2025-01-16 15:01:45,950] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-01-16 15:01:45,950] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-01-16 15:01:45,951] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-01-16 15:01:45,951] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-01-16 15:01:45,951] [INFO] [config.py:1003:print]   world_size ................... 1\n",
      "[2025-01-16 15:01:45,952] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True\n",
      "[2025-01-16 15:01:45,952] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-01-16 15:01:45,952] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-01-16 15:01:45,953] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-01-16 15:01:45,953] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2\n",
      "[2025-01-16 15:01:45,953] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 32, \n",
      "    \"steps_per_print\": 200, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 启动DeepSpeed训练\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 62\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# 创建数据集\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data_size):\n",
    "        self.data = torch.randn(data_size, 10)\n",
    "        self.labels = torch.randint(0, 2, (data_size,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# 分布式训练配置\n",
    "def train():\n",
    "    model = SimpleModel()\n",
    "\n",
    "    # DeepSpeed配置\n",
    "    config = {\n",
    "        \"train_batch_size\": 32,\n",
    "        \"steps_per_print\": 200,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2\n",
    "        },\n",
    "        # \"fp16\": {\n",
    "        #     \"enabled\": True\n",
    "        # },\n",
    "        # \"cpu_offload\": False\n",
    "        \"zero_allow_untested_optimizer\": True\n",
    "    }\n",
    "\n",
    "    # 创建数据加载器\n",
    "    dataset = SimpleDataset(1000)\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    # 初始化DeepSpeed\n",
    "    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optim.SGD(model.parameters(), lr=0.01), config_params=config)\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            model.backward(loss)\n",
    "            model.step()\n",
    "\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# 启动DeepSpeed训练\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.10.1.2. <a id='toc13_10_1_2_'></a>[模型并行](#toc0_)\n",
    "在 DeepSpeed 中，模型并行允许将模型划分为多个部分并分配到不同的设备。通过这种方式，我们能够训练超大模型，超出单个GPU内存限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-16 14:34:38,491] [WARNING] [real_accelerator.py:181:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-16 14:34:38,493] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3340546/4292986897.py\", line 1, in <module>\n",
      "    import deepspeed\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/__init__.py\", line 26, in <module>\n",
      "    from . import module_inject\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/module_inject/__init__.py\", line 6, in <module>\n",
      "    from .replace_module import replace_transformer_layer, revert_transformer_layer, ReplaceWithTensorSlicing, GroupQuantizer, generic_injection\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/module_inject/replace_module.py\", line 652, in <module>\n",
      "    from ..pipe import PipelineModule\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/pipe/__init__.py\", line 6, in <module>\n",
      "    from ..runtime.pipe import PipelineModule, LayerSpec, TiedLayerSpec\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/pipe/__init__.py\", line 6, in <module>\n",
      "    from .module import PipelineModule, LayerSpec, TiedLayerSpec\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/pipe/module.py\", line 19, in <module>\n",
      "    from ..activation_checkpointing import checkpointing\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py\", line 26, in <module>\n",
      "    from deepspeed.runtime.config import DeepSpeedConfig\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/config.py\", line 29, in <module>\n",
      "    from .zero.config import get_zero_config, ZeroStageEnum\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/zero/__init__.py\", line 15, in <module>\n",
      "    from .mics import MiCS_Init\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/zero/mics.py\", line 19, in <module>\n",
      "    from deepspeed.runtime.zero.stage3 import DeepSpeedZeroOptimizer_Stage3\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py\", line 33, in <module>\n",
      "    from deepspeed.checkpoint.constants import OPTIMIZER_STATE_DICT, FP32_FLAT_GROUPS, PARTITION_COUNT, ZERO_STAGE, LOSS_SCALER\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/checkpoint/__init__.py\", line 10, in <module>\n",
      "    from .utils import (get_layer_ckpt_name_for_rank, get_model_ckpt_name_for_rank, get_zero_ckpt_name_for_rank)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/checkpoint/utils.py\", line 41, in <module>\n",
      "    def clone_tensors_for_torch_save(item, device=torch.device('cpu')):\n",
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/checkpoint/utils.py:41: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1682343904035/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  def clone_tensors_for_torch_save(item, device=torch.device('cpu')):\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 启动DeepSpeed训练\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m():\n\u001b[0;32m---> 40\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# DeepSpeed配置\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_print\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         }\n\u001b[1;32m     52\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart1 \u001b[38;5;241m=\u001b[39m \u001b[43mModelPart1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart2 \u001b[38;5;241m=\u001b[39m ModelPart2()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 定义模型的多个部分\n",
    "class ModelPart1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart1, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.fc1(x))\n",
    "\n",
    "\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# 模型组合\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.part1 = ModelPart1().to('cuda:0')\n",
    "        self.part2 = ModelPart2().to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.part1(x)\n",
    "        return self.part2(x)\n",
    "\n",
    "\n",
    "# 分布式训练配置\n",
    "def train():\n",
    "    model = Model()\n",
    "\n",
    "    # DeepSpeed配置\n",
    "    config = {\n",
    "        \"train_batch_size\": 32,\n",
    "        \"steps_per_print\": 200,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 初始化DeepSpeed\n",
    "    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optim.SGD(model.parameters(), lr=0.01), config_params=config)\n",
    "\n",
    "    # 开始训练\n",
    "    data = torch.randn(32, 10).to('cuda:0')  # 模拟输入数据\n",
    "    labels = torch.randint(0, 2, (32,)).to('cuda:1')  # 标签\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.backward(loss)\n",
    "        model.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 启动DeepSpeed训练\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.10.1.3. <a id='toc13_10_1_3_'></a>[混合并行](#toc0_)\n",
    "混合并行结合了数据并行和模型并行的优势。数据被拆分到多个设备上，同时每个设备上存储模型的不同部分。DeepSpeed 提供了一个简单的API来实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 定义模型的多个部分\n",
    "class ModelPart1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart1, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.fc1(x))\n",
    "\n",
    "\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# 模型组合\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.part1 = ModelPart1().to('cuda:0')\n",
    "        self.part2 = ModelPart2().to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.part1(x)\n",
    "        return self.part2(x)\n",
    "\n",
    "\n",
    "# 分布式训练配置\n",
    "def train():\n",
    "    model = Model()\n",
    "\n",
    "    # DeepSpeed配置\n",
    "    config = {\n",
    "        \"train_batch_size\": 32,\n",
    "        \"steps_per_print\": 200,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 初始化DeepSpeed\n",
    "    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optim.SGD(model.parameters(), lr=0.01), config_params=config)\n",
    "\n",
    "    # 创建数据\n",
    "    data = torch.randn(32, 10).to('cuda:0')\n",
    "    labels = torch.randint(0, 2, (32,)).to('cuda:1')\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.backward(loss)\n",
    "        model.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 启动DeepSpeed训练\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.10.2. <a id='toc13_10_2_'></a>[huggingface trainer and accelerate](#toc0_)\n",
    "#### 13.10.2.1. <a id='toc13_10_2_1_'></a>[数据并行](#toc0_)\n",
    "#### 13.10.2.2. <a id='toc13_10_2_2_'></a>[模型并行](#toc0_)\n",
    "#### 13.10.2.3. <a id='toc13_10_2_3_'></a>[混合并行](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. <a id='toc14_'></a>[PyTorch做迁移学习](#toc0_)\n",
    "- 在今后的很长时间，深度学习的模型创新上会有很大的难度，基于已有的模型的微调（Fine-tuning）应用于新的可解决的问题是趋势。\n",
    "\n",
    "- Fine-tuning in CV：\n",
    "\n",
    "    - 1.用Pre-trained的参数初始化特征提取器如Encoder的参数，而不是随机初始化；\n",
    "\n",
    "    - 2.用小的lerning-rate和小的epochs；\n",
    "\n",
    "    - 3.固定模型层的（其实就是learning-rate为0）。\n",
    "\n",
    "- 如何找到Pre-trained model？\n",
    "\n",
    "    - TIMM（pytorch）-一个叫Ross的小哥自己维护的；\n",
    "\n",
    "    - HugginFace - 一个早期只是东抄抄西抄抄的公司，逐渐发展为比较好的社区公司。\n",
    "\n",
    "- Fine-tuning in NLP：\n",
    "\n",
    "    - 1.Self-supervised pre-training;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1. <a id='toc14_1_'></a>[Fine-tuning](#toc0_)\n",
    "- 目前已知两种方式进行Fine-tuning:\n",
    "    - 设置非常小的lr\n",
    "    - param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.1. <a id='toc14_1_1_'></a>[小的lr](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 1)\n",
    "        self.hidden = nn.Linear(1, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.fc = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.input(X)\n",
    "        y = self.hidden(y)\n",
    "        y = self.output(y)\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "param_1x = [param for name, param in net.named_parameters() if name not in ['fc.weight', 'fc.bias']]    # 提取出fc以外的所有参数\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "opt = optim.SGD(\n",
    "    params=[\n",
    "        {'params': param_1x},                                           # lr不变\n",
    "        {'params': net.fc.parameters(), 'lr': learning_rate * 0.001}    # lr缩小\n",
    "    ], \n",
    "    lr=learning_rate, \n",
    "    weight_decay=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.2. <a id='toc14_1_2_'></a>[停止计算梯度](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认参数信息：\n",
      "input.weight >>> True\n",
      "input.bias >>> True\n",
      "hidden.weight >>> True\n",
      "hidden.bias >>> True\n",
      "output.weight >>> True\n",
      "output.bias >>> True\n",
      "fc.weight >>> True\n",
      "fc.bias >>> True\n",
      "========== \n",
      " 修改后参数信息：\n",
      "input.weight >>> False\n",
      "input.bias >>> False\n",
      "hidden.weight >>> False\n",
      "hidden.bias >>> False\n",
      "output.weight >>> False\n",
      "output.bias >>> False\n",
      "fc.weight >>> True\n",
      "fc.bias >>> True\n"
     ]
    }
   ],
   "source": [
    "from torch import optim \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 1)\n",
    "        self.hidden = nn.Linear(1, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.fc = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.input(X)\n",
    "        y = self.hidden(y)\n",
    "        y = self.output(y)\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print('默认参数信息：')\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, '>>>', param.requires_grad)\n",
    "\n",
    "print('='*10, '\\n', '修改后参数信息：')\n",
    "for name, param in net.named_parameters():\n",
    "    if name not in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = False\n",
    "    print(name, '>>>', param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2. <a id='toc14_2_'></a>[torchvision的应用案例](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3. <a id='toc14_3_'></a>[迁移学习案例](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    '''重载torch.utils.data.Dataset类'''\n",
    "    def __init__(self, dirname, transform=None):\n",
    "        super(MyDataset, self).__init__() # 要不要都行\n",
    "        self.classes = os.listdir(dirname)\n",
    "        self.images = []\n",
    "        self.transform = transform\n",
    "        for i, classes in enumerate(self.classes):\n",
    "            classes_path = os.path.join(dirname, classes)\n",
    "            for image_name in os.listdir(classes_path):\n",
    "                self.images.append((os.path.join(classes_path, image_name), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        '''改写__len__()方法'''\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''改写__getitem__()方法'''\n",
    "        image_name, classes = self.images[idx]\n",
    "        image = Image.open(image_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, classes\n",
    "    \n",
    "    def get_claesses(self):\n",
    "        return self.classes\n",
    "    \n",
    "# 分布实现训练和预测的transform\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.RandomResizedCrop(224), #随机裁剪一个area然后再resize\n",
    "        transforms.RandomHorizontalFlip(), #随机水平翻转\n",
    "        transforms.Resize(size=(256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.Resize(size=(256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 分别实现loader\n",
    "# ws = 'Pytorch_datasets/hymenoptera_data/'\n",
    "train_dataset = MyDataset('Pytorch_datasets/hymenoptera_data/train/', train_transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "\n",
    "val_dataset = MyDataset('Pytorch_datasets/hymenoptera_data/val/', val_transform)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择预训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练的模型\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0350, -0.0355,  ...,  0.0068,  0.0349,  0.0407],\n",
      "        [-0.0257,  0.0340, -0.0237,  ..., -0.0052, -0.0351,  0.0249]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0364,  0.0310], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 停止权重更新，并将model最后一层替换掉\n",
    "only_train_fc = True\n",
    "if only_train_fc:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "fc_in_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(fc_in_features, 2, bias=True)\n",
    "\n",
    "# 查看\n",
    "for i in model.parameters():\n",
    "    if i.requires_grad:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练主体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.31711972 0.88244045\n",
      "1 0.30389076 0.85200006\n",
      "0 0.33484977 0.858817\n",
      "1 0.4615616 0.80550003\n",
      "0.85200006\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "device = 'cuda:0'\n",
    "\n",
    "epochs = 2\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(lr=0.01, params=model.parameters())\n",
    "opt_step = torch.optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.1)\n",
    "max_acc = 0\n",
    "epoch_acc = []\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for type_id, loader in enumerate([train_loader, val_loader]):\n",
    "        # print('type_id:',type_id)\n",
    "        mean_loss = []\n",
    "        mean_acc = []\n",
    "        for images, labels in loader:\n",
    "            if type_id == 0:\n",
    "                # opt_step.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            opt.zero_grad()\n",
    "            with torch.set_grad_enabled(type_id==0):\n",
    "                outputs = model(images)\n",
    "                _, pre_labels = torch.max(outputs, 1)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            if type_id == 0:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            acc = torch.sum(pre_labels==labels) / torch.tensor(labels.shape[0], dtype=torch.float32)        \n",
    "            mean_loss.append(loss.detach().cpu().numpy())\n",
    "            mean_acc.append(acc.detach().cpu().numpy())\n",
    "        if type_id == 1:\n",
    "            epoch_acc.append(np.mean(mean_acc))\n",
    "            epoch_loss.append(np.mean(mean_loss))\n",
    "            if max_acc < np.mean(mean_acc):\n",
    "                max_acc = np.mean(mean_acc)\n",
    "        print(type_id, np.mean(mean_loss),np.mean(mean_acc))\n",
    "print(max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. <a id='toc15_'></a>[Metrics](#toc0_)\n",
    "\n",
    "- 图像分类任务中，需要计算各种评估指标，如准确率、精确率、召回率等。\n",
    "\n",
    "- 文本分类任务中，需要计算评估指标，如 F1 分数。\n",
    "\n",
    "- 生成对抗网络（GAN）的训练中，需要计算生成图片的质量指标，如 Frechet Inception Distance（FID）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1. <a id='toc15_1_'></a>[TorchMetrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.1. <a id='toc15_1_1_'></a>[准确率、精确率、召回率和F1分数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: tensor(0.5000)\n",
      "prec: tensor(0.5000)\n",
      "rec: tensor(0.5000)\n",
      "F1 score: tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchmetrics \n",
    "\n",
    "\n",
    "# 模拟预测和真实标签\n",
    "preds = torch.tensor([0, 2, 1, 3])\n",
    "target = torch.tensor([0, 1, 2, 3])\n",
    "\n",
    "# 准确率： Accuracy\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
    "acc = accuracy(preds, target)\n",
    "print('acc:', acc)\n",
    "\n",
    "# 精确率： precision\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=4)\n",
    "prec = precision(preds, target)\n",
    "print('prec:', prec)\n",
    "\n",
    "# 召回率： recall\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=4)\n",
    "rec = recall(preds, target)\n",
    "print('rec:', rec)\n",
    "\n",
    "# F1分数\n",
    "f1_score = torchmetrics.F1Score(task='multiclass', num_classes=4)\n",
    "f1 = f1_score(preds, target)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.2. <a id='toc15_1_2_'></a>[自定义计算指标](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchmetrics \n",
    "\n",
    "\n",
    "class CustomMetrics(torchmetrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def compute(self):\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.3. <a id='toc15_1_3_'></a>[于PyTorch Lightning联合使用](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pytorch_lightning as L \n",
    "import torchmetrics  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2903, 0.3722, 0.0199, 0.3067, 0.0109],\n",
       "         [0.4634, 0.0822, 0.1153, 0.1082, 0.2309],\n",
       "         [0.2371, 0.4400, 0.0101, 0.0359, 0.2770],\n",
       "         [0.1717, 0.0242, 0.0344, 0.6931, 0.0766],\n",
       "         [0.1555, 0.2979, 0.2756, 0.0383, 0.2328],\n",
       "         [0.2484, 0.1408, 0.1883, 0.0630, 0.3596],\n",
       "         [0.3829, 0.0814, 0.1469, 0.3578, 0.0310],\n",
       "         [0.0218, 0.2064, 0.3804, 0.2019, 0.1896],\n",
       "         [0.1798, 0.3152, 0.1579, 0.1464, 0.2007],\n",
       "         [0.0955, 0.0309, 0.1288, 0.2167, 0.5280]]),\n",
       " tensor([0, 2, 1, 1, 1, 3, 1, 0, 2, 2]),\n",
       " tensor(0.2000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# import our library\n",
    "import torchmetrics\n",
    "\n",
    "# simulate a classification problem\n",
    "preds = torch.randn(10, 5).softmax(dim=-1)\n",
    "target = torch.randint(5, (10,))\n",
    "# target = torch.randn(5, (10,))\n",
    "\n",
    "acc = torchmetrics.functional.accuracy(preds, target, task=\"multiclass\", num_classes=5)\n",
    "preds, target, acc\n",
    "# preds.dtype, target.dtype, acc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2. <a id='toc15_2_'></a>[分类问题的评估指标](#toc0_)\n",
    "分类问题的目标是将输入数据分配到预定义的类别中。评估指标主要基于模型预测的类别与实际类别的匹配程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.1. <a id='toc15_2_1_'></a>[混淆矩阵](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.1. <a id='toc15_2_1_1_'></a>[二分类混淆矩阵](#toc0_)\n",
    "1. 对于二分类问题，混淆矩阵可以表示为：\n",
    "\n",
    "    |实际正例 (Positive)|实际负例 (Negative)|\n",
    "    |:---:|:---:|\n",
    "    |预测正例|真正例 (TP)|假正例 (FP)|\n",
    "    |预测负例|假负例 (FN)|真负例 (TN)|\n",
    "\n",
    "    - TP（True Positive）: 实际是正例，且预测为正例。\n",
    "\n",
    "    - FP（False Positive）: 实际是负例，但预测为正例（误报）。\n",
    "\n",
    "    - FN（False Negative）: 实际是正例，但预测为负例（漏报）。\n",
    "\n",
    "    - TN（True Negative）: 实际是负例，且预测为负例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.2. <a id='toc15_2_1_2_'></a>[多分类混淆矩阵](#toc0_)\n",
    "2. 对于多分类任务，混淆矩阵会扩展为𝐶×𝐶的结构，其中𝐶是类别数。\n",
    "\n",
    "    |预测\\实际|类别 1|类别 2|类别 3|...|类别 𝐶|\n",
    "    |:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "    |类别 1|TP|FP|FP|...|FP|\n",
    "    |类别 2|FN|TP|FP|...|FP|\n",
    "    |类别 3|FN|FN|TP|...|FP|\n",
    "    |...|...|...|...|...|...|\n",
    "    |类别 𝐶|FN|FN|FN|...|TP|\n",
    "\n",
    "    - 对角线上的值为正确分类的数量。\n",
    "    - 非对角线上的值为分类错误的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.3. <a id='toc15_2_1_3_'></a>[可视化混淆矩阵](#toc0_)\n",
    "通过热力图或颜色编码的矩阵图可以直观展示分类器的性能。Python 中可以使用 sklearn.metrics 模块生成混淆矩阵，并用 seaborn 可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 示例: 真实标签和预测标签\n",
    "y_true = [0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(2, 2))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.4. <a id='toc15_2_1_4_'></a>[混淆矩阵的优点与局限性](#toc0_)\n",
    "优点：\n",
    "- 提供了分类结果的详细信息（TP、FP、FN、TN），帮助分析分类器的性能。\n",
    "- 可用于计算多种指标（如 Precision、Recall、F1-Score 等）。\n",
    "- 易于扩展到多分类任务。\n",
    "\n",
    "局限性：\n",
    "- 难以直接提供一个全局的性能评分（需要其他指标辅助）。\n",
    "- 当类别过多或类别不平衡时，可能难以直观理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.2. <a id='toc15_2_2_'></a>[准确率 (Accuracy)](#toc0_)\n",
    "准确率是最基本的评估指标，表示模型预测正确的样本占总样本的比例。\n",
    "\n",
    "$\\text{公式: Accuracy}=\\frac{\\text{正确预测样本数}}{\\text{总样本数}}$\n",
    "\n",
    "适用场景: 数据类别分布均衡时效果较好。\n",
    "\n",
    "缺点: 对类别不平衡的数据不敏感（如正例远多于负例时）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "y_hat = torch.tensor(y_pred) \n",
    "y = torch.tensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    correct = (y_hat == y).sum().item()\n",
    "    total = y.size(0)   # batch总数\n",
    "    return correct / total \n",
    "\n",
    "accuracy(y_hat= y_hat, y= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.3. <a id='toc15_2_3_'></a>[精确率 (Precision)](#toc0_)\n",
    "\n",
    "$\\text{公式: Precision}=\\frac{\\text{真正例 }(\\mathrm{TP})}{\\text{真正例 }(\\mathrm{TP})+\\text{假正例 }(\\mathrm{FP})}$\n",
    "\n",
    "适用场景: 关注减少误报的场景（如垃圾邮件检测）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8750)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    " \n",
    "\n",
    "def precision(preds: torch.Tensor, \n",
    "                        targets: torch.Tensor, \n",
    "                        average: str = 'macro', \n",
    "                        num_classes: int = None, \n",
    "                        eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算精确率（Precision）的PyTorch函数 \n",
    "    参数：\n",
    "        - preds: 预测类别标签（形状为[N,]的整数张量）\n",
    "        - targets: 真实类别标签（形状为[N,]的整数张量）\n",
    "        - average: 平均方式，可选'macro'（类平均）、'micro'（全局统计）、'weighted'（加权平均）或None（返回各类别值）\n",
    "        - num_classes: 手动指定类别数（若未指定，则自动推断）\n",
    "        - eps: 防止除零的小量 \n",
    "    返回：\n",
    "        - 精确率数值（float）或各类别精确率（Tensor）\n",
    "    \"\"\"\n",
    "    assert preds.shape  == targets.shape,  \"预测值与真实标签形状需一致\"\n",
    "    \n",
    "    # 自动推断类别数 \n",
    "    if num_classes is None:\n",
    "        num_classes = max(preds.max().item(),  targets.max().item())  + 1 \n",
    "    \n",
    "    # 生成混淆矩阵 \n",
    "    confusion_matrix = torch.zeros(num_classes,  num_classes, dtype=torch.int64) \n",
    "    for p, t in zip(preds.flatten(),  targets.flatten()): \n",
    "        confusion_matrix[p.long(), t.long()]  += 1 \n",
    "    \n",
    "    # 计算TP和FP \n",
    "    tp = confusion_matrix.diag()                # 对角线为各类别TP \n",
    "    fp = confusion_matrix.sum(dim=1)  - tp      # 每行总和-TP=FP \n",
    "    \n",
    "    # 计算各类别精确率 \n",
    "    precision_per_class = tp.float()  / (tp + fp + eps)\n",
    "    \n",
    "    # 根据平均方式处理结果 \n",
    "    if average == 'macro':\n",
    "        result = precision_per_class.nanmean()   # 忽略NaN（如0/0情况）\n",
    "    elif average == 'micro':\n",
    "        total_tp = tp.sum().float() \n",
    "        total_fp = fp.sum().float() \n",
    "        result = total_tp / (total_tp + total_fp + eps)\n",
    "    elif average == 'weighted':\n",
    "        weights = targets.bincount(minlength=num_classes).float()  / targets.numel() \n",
    "        result = (precision_per_class * weights).sum()\n",
    "    elif average is None:\n",
    "        result = precision_per_class \n",
    "    else:\n",
    "        raise ValueError(f\"无效的average参数：{average}\")\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "precision(preds= y_hat, targets= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.4. <a id='toc15_2_4_'></a>[召回率 (Recall)](#toc0_)\n",
    "\n",
    "$\\text{公式: Recall}=\\frac{\\text{真正例 }(\\mathrm{TP})}{\\text{真正例 }(\\mathrm{TP})+\\text{假负例 }(\\mathrm{FN})}$\n",
    "\n",
    "适用场景: 关注尽可能找到所有正例的场景（如疾病筛查）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8750)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    " \n",
    "def recall(preds: torch.Tensor,\n",
    "                     targets: torch.Tensor,\n",
    "                     average: str = 'macro',\n",
    "                     num_classes: int = None,\n",
    "                     eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算召回率（Recall）的PyTorch函数 \n",
    " \n",
    "    参数：\n",
    "    - preds: 预测类别标签（形状为[N,]的整数张量）\n",
    "    - targets: 真实类别标签（形状为[N,]的整数张量）\n",
    "    - average: 平均方式，可选'macro'（类平均）、'micro'（全局统计）、'weighted'（加权平均）或None（返回各类别值）\n",
    "    - num_classes: 手动指定类别数（若未指定，则自动推断）\n",
    "    - eps: 防止除零的小量 \n",
    " \n",
    "    返回：\n",
    "    - 召回率数值（float）或各类别召回率（Tensor）\n",
    "    \"\"\"\n",
    "    assert preds.shape  == targets.shape,  \"预测值与真实标签形状需一致\"\n",
    "    \n",
    "    # 自动推断类别数 \n",
    "    if num_classes is None:\n",
    "        num_classes = max(preds.max().item(),  targets.max().item())  + 1 \n",
    "    \n",
    "    # 生成混淆矩阵（行为预测类别，列为真实类别）\n",
    "    confusion_matrix = torch.zeros(num_classes,  num_classes, dtype=torch.int64) \n",
    "    for p, t in zip(preds.flatten(),  targets.flatten()): \n",
    "        confusion_matrix[p.long(), t.long()]  += 1 \n",
    "    \n",
    "    # 计算TP和FN \n",
    "    tp = confusion_matrix.diag()                # 对角线为各类别TP \n",
    "    fn = confusion_matrix.sum(dim=0)  - tp      # 每列总和-TP=FN \n",
    "    \n",
    "    # 计算各类别召回率 \n",
    "    recall_per_class = tp.float()  / (tp + fn + eps)\n",
    "    \n",
    "    # 根据平均方式处理结果 \n",
    "    if average == 'macro':\n",
    "        result = recall_per_class.nanmean()     # 忽略NaN（如0/0情况）\n",
    "    elif average == 'micro':\n",
    "        total_tp = tp.sum().float() \n",
    "        total_fn = fn.sum().float() \n",
    "        result = total_tp / (total_tp + total_fn + eps)\n",
    "    elif average == 'weighted':\n",
    "        weights = targets.bincount(minlength=num_classes).float()  / targets.numel() \n",
    "        result = (recall_per_class * weights).sum()\n",
    "    elif average is None:\n",
    "        result = recall_per_class \n",
    "    else:\n",
    "        raise ValueError(f\"无效的average参数：{average}\")\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "recall(preds= y_hat, targets= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.5. <a id='toc15_2_5_'></a>[F1-Score](#toc0_)\n",
    "\n",
    "$\\text{公式: }F1=2\\times\\frac{\\mathrm{Precision}\\times\\mathrm{Recall}}{\\mathrm{Precision}+\\mathrm{Recall}}$\n",
    "\n",
    "适用场景: 在精确率和召回率之间需要平衡时。\n",
    "\n",
    "特点: F1-Score 是精确率和召回率的调和平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8571)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    " \n",
    "\n",
    "def f1(preds: torch.Tensor,\n",
    "                 targets: torch.Tensor,\n",
    "                 average: str = 'macro',\n",
    "                 num_classes: int = None,\n",
    "                 eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算F1值的PyTorch函数 \n",
    " \n",
    "    参数：\n",
    "    - preds: 预测类别标签（形状为[N,]的整数张量）\n",
    "    - targets: 真实类别标签（形状为[N,]的整数张量）\n",
    "    - average: 平均方式，可选'macro'（类平均）、'micro'（全局统计）、'weighted'（加权平均）或None（返回各类别值）\n",
    "    - num_classes: 手动指定类别数（若未指定，则自动推断）\n",
    "    - eps: 防止除零的小量 \n",
    " \n",
    "    返回：\n",
    "    - F1值（float）或各类别F1值（Tensor）\n",
    "    \"\"\"\n",
    "    assert preds.shape  == targets.shape,  \"预测值与真实标签形状需一致\"\n",
    "    \n",
    "    # 自动推断类别数 \n",
    "    if num_classes is None:\n",
    "        num_classes = max(preds.max().item(),  targets.max().item())  + 1 \n",
    "    \n",
    "    # 生成混淆矩阵（行=预测类别，列=真实类别）\n",
    "    confusion_matrix = torch.zeros(num_classes,  num_classes, dtype=torch.int64) \n",
    "    for p, t in zip(preds.flatten(),  targets.flatten()): \n",
    "        confusion_matrix[p.long(), t.long()]  += 1 \n",
    "    \n",
    "    # 计算TP、FP、FN \n",
    "    tp = confusion_matrix.diag()                    # 对角线为各类别TP \n",
    "    fp = confusion_matrix.sum(dim=1)  - tp          # 每行总和-TP=FP \n",
    "    fn = confusion_matrix.sum(dim=0)  - tp         # 每列总和-TP=FN \n",
    "    \n",
    "    # 计算各类别精确率和召回率 \n",
    "    precision_per_class = tp.float()  / (tp + fp + eps)\n",
    "    recall_per_class = tp.float()  / (tp + fn + eps)\n",
    "    \n",
    "    # 计算各类别F1值（调和平均）\n",
    "    f1_per_class = 2 * (precision_per_class * recall_per_class) / (precision_per_class + recall_per_class + eps)\n",
    "    \n",
    "    # 根据平均方式处理结果 \n",
    "    if average == 'macro':\n",
    "        result = f1_per_class.nanmean()            # 忽略NaN（如0/0情况）\n",
    "    elif average == 'micro':\n",
    "        total_tp = tp.sum().float() \n",
    "        total_fp = fp.sum().float() \n",
    "        total_fn = fn.sum().float() \n",
    "        micro_precision = total_tp / (total_tp + total_fp + eps)\n",
    "        micro_recall = total_tp / (total_tp + total_fn + eps)\n",
    "        result = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall + eps)\n",
    "    elif average == 'weighted':\n",
    "        weights = targets.bincount(minlength=num_classes).float()  / targets.numel() \n",
    "        result = (f1_per_class * weights).sum()\n",
    "    elif average is None:\n",
    "        result = f1_per_class \n",
    "    else:\n",
    "        raise ValueError(f\"无效的average参数：{average}\")\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "f1(preds= y_hat, targets= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.6. <a id='toc15_2_6_'></a>[ROC 曲线和 AUC (Area Under Curve)](#toc0_)\n",
    "ROC 曲线: 通过绘制不同阈值下的 假正例率 (FPR) 和 真正例率 (TPR) 来评估分类器性能。\n",
    "\n",
    "AUC: 曲线下面积，表示模型区分正负例的能力。\n",
    "\n",
    "适用场景: 用于评估二分类模型，尤其是在类别分布不平衡的情况下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def roc_auc(\n",
    "    scores: Tensor,  # 模型输出的概率值（形状[N]）\n",
    "    targets: Tensor,  # 真实标签（0或1，形状[N]）\n",
    "    num_thresholds: int = 100  # 阈值采样数\n",
    ") -> tuple[list, list, float]:\n",
    "    \"\"\"\n",
    "    计算ROC曲线坐标点及AUC值\n",
    "    \"\"\"\n",
    "    # 按概率降序排序\n",
    "    sorted_indices = torch.argsort(scores,  descending=True)\n",
    "    sorted_scores = scores[sorted_indices]\n",
    "    sorted_targets = targets[sorted_indices]\n",
    "\n",
    "    # 初始化变量\n",
    "    tpr_list, fpr_list = [], []\n",
    "    tp = fp = 0\n",
    "    total_p = sorted_targets.sum().item() \n",
    "    total_n = len(sorted_targets) - total_p\n",
    "\n",
    "    # 遍历所有样本作为阈值分割点\n",
    "    for i in range(len(sorted_scores)):\n",
    "        if sorted_targets[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        tpr = tp / (total_p + 1e-6)\n",
    "        fpr = fp / (total_n + 1e-6)\n",
    "        tpr_list.append(tpr) \n",
    "        fpr_list.append(fpr) \n",
    "\n",
    "    # 计算AUC（梯形法则积分）\n",
    "    auc = 0.0\n",
    "    for i in range(1, len(fpr_list)):\n",
    "        delta_fpr = fpr_list[i] - fpr_list[i-1]\n",
    "        avg_tpr = (tpr_list[i] + tpr_list[i-1]) / 2\n",
    "        auc += delta_fpr * avg_tpr\n",
    "\n",
    "    return fpr_list, tpr_list, auc\n",
    "\n",
    "\n",
    "# roc_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.7. <a id='toc15_2_7_'></a>[多分类问题指标](#toc0_)\n",
    "Top-k Accuracy: 预测中真实标签出现在模型输出概率前 k 个类别中。\n",
    "\n",
    "分类报告: 包含每个类别的 Precision、Recall 和 F1-Score，适用于多分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3. <a id='toc15_3_'></a>[回归问题的评估指标](#toc0_)\n",
    "回归问题的目标是预测连续数值。评估指标衡量预测值与真实值之间的偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.1. <a id='toc15_3_1_'></a>[平均绝对误差 (MAE)](#toc0_)\n",
    "公式: MAE = $\\frac{1}{N}\\sum_{i=1}^{N}|y_i-\\hat{y}_i|$\n",
    "\n",
    "特点: 衡量预测值与真实值之间的平均绝对差异。\n",
    "\n",
    "优点: 对异常值较为鲁棒。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1429)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    " \n",
    "\n",
    "def mae(preds: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        dim: int or tuple = None,\n",
    "        keepdim: bool = False,\n",
    "        reduction: str = 'mean') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算平均绝对误差（Mean Absolute Error）的PyTorch函数 \n",
    " \n",
    "    参数：\n",
    "    - preds: 预测值张量（形状需与targets一致）\n",
    "    - targets: 真实值张量（形状需与preds一致）\n",
    "    - dim: 指定计算维度（默认全局计算）\n",
    "    - keepdim: 是否保留维度（仅当指定dim时生效）\n",
    "    - reduction: 聚合方式，可选'mean'（平均）、'sum'（求和）或'none'（保留原始形状）\n",
    " \n",
    "    返回：\n",
    "    - MAE数值（标量）或未聚合的绝对误差（张量）\n",
    "    \"\"\"\n",
    "    assert preds.shape  == targets.shape,  \"预测值与真实值形状需一致\"\n",
    "    \n",
    "    # 计算绝对误差 \n",
    "    abs_errors = torch.abs(preds  - targets)\n",
    "    \n",
    "    # 选择聚合方式 \n",
    "    if reduction == 'mean':\n",
    "        result = torch.mean(abs_errors,  dim=dim, keepdim=keepdim)\n",
    "    elif reduction == 'sum':\n",
    "        result = torch.sum(abs_errors,  dim=dim, keepdim=keepdim)\n",
    "    elif reduction == 'none':\n",
    "        result = abs_errors \n",
    "    else:\n",
    "        raise ValueError(f\"无效的reduction参数：{reduction}\")\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "mae(preds= y_hat.float(), targets= y.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.2. <a id='toc15_3_2_'></a>[均方误差 (MSE)](#toc0_)\n",
    "公式: MSE = $\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2$\n",
    "\n",
    "特点: 衡量预测值与真实值之间的平均平方差异。\n",
    "\n",
    "优点: 对异常值较为敏感。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1429)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    " \n",
    "\n",
    "def mse(preds: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        dim: int or tuple = None,\n",
    "        keepdim: bool = False,\n",
    "        reduction: str = 'mean') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算均方误差（Mean Squared Error）的PyTorch函数 \n",
    " \n",
    "    参数：\n",
    "    - preds: 预测值张量（形状需与targets一致）\n",
    "    - targets: 真实值张量（形状需与preds一致）\n",
    "    - dim: 指定计算维度（默认全局计算）\n",
    "    - keepdim: 是否保留维度（仅当指定dim时生效）\n",
    "    - reduction: 聚合方式，可选'mean'（平均）、'sum'（求和）或'none'（保留原始形状）\n",
    " \n",
    "    返回：\n",
    "    - MSE数值（标量）或未聚合的平方误差（张量）\n",
    "    \"\"\"\n",
    "    assert preds.shape  == targets.shape,  f\"形状不一致：预测值{preds.shape}  vs 真实值{targets.shape}\" \n",
    "    \n",
    "    # 计算平方误差 \n",
    "    squared_errors = torch.square(preds  - targets)\n",
    "    \n",
    "    # 选择聚合方式 \n",
    "    if reduction == 'mean':\n",
    "        result = torch.mean(squared_errors,  dim=dim, keepdim=keepdim)\n",
    "    elif reduction == 'sum':\n",
    "        result = torch.sum(squared_errors,  dim=dim, keepdim=keepdim)\n",
    "    elif reduction == 'none':\n",
    "        result = squared_errors \n",
    "    else:\n",
    "        raise ValueError(f\"无效的reduction参数：{reduction}，可选'mean'/'sum'/'none'\")\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "mse(preds= y_hat.float(), targets= y.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.3. <a id='toc15_3_3_'></a>[均方根误差 (RMSE)](#toc0_)\n",
    "公式: RMSE = $\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}$\n",
    "\n",
    "特点: 是 MSE 的平方根，用于衡量预测值与真实值之间的平均误差。\n",
    "\n",
    "优点: 对异常值较为敏感。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3780)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    " \n",
    "def rmse(preds: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        dim: int or tuple = None,\n",
    "        keepdim: bool = False,\n",
    "        eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算均方根误差（Root Mean Squared Error）\n",
    " \n",
    "    参数说明：\n",
    "    - preds: 预测值张量（需与targets同维度）\n",
    "    - targets: 真实值张量 \n",
    "    - dim: 聚合维度（如设为(1,2)表示在H,W维度计算）\n",
    "    - keepdim: 是否保留计算维度 \n",
    "    - eps: 数值稳定系数（避免零除）\n",
    "    \n",
    "    返回：\n",
    "    - 指定维度的RMSE标量或张量 \n",
    "    \"\"\"\n",
    "    assert preds.shape  == targets.shape,  f\"形状不匹配：预测值{preds.shape} ，真实值{targets.shape}\" \n",
    "    \n",
    "    # 计算平方误差均值（沿指定维度）\n",
    "    mse = torch.mean((preds  - targets)**2, dim=dim, keepdim=keepdim)\n",
    "    \n",
    "    # 开平方获得RMSE \n",
    "    rmse = torch.sqrt(mse  + eps)\n",
    "    \n",
    "    return rmse \n",
    "\n",
    "\n",
    "rmse(preds= y_hat.float(), targets= y.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.4. <a id='toc15_3_4_'></a>[R² (决定系数)](#toc0_)\n",
    "\n",
    "$\\text{公式: }R^2=1-\\frac{\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{N}(y_i-\\bar{y})^2}$\n",
    "\n",
    "特点: 衡量模型解释数据变异的能力。\n",
    "\n",
    "优点: 易于理解和解释。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4167)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    " \n",
    "def R2(preds: torch.Tensor, \n",
    "        targets: torch.Tensor,\n",
    "        dim: int = None,\n",
    "        adjusted: bool = False,\n",
    "        n_features: int = None,\n",
    "        eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算决定系数R²（支持多维度计算与自由度调整）\n",
    "    \n",
    "    参数：\n",
    "    - preds: 预测值张量（形状需与targets一致）\n",
    "    - targets: 真实值张量 \n",
    "    - dim: 计算维度（None表示全局计算）\n",
    "    - adjusted: 是否计算调整后R² \n",
    "    - n_features: 特征数（仅调整R²需要）\n",
    "    - eps: 数值稳定系数 \n",
    "    \n",
    "    返回：\n",
    "    - R²值（范围可能为[-∞,1]）\n",
    "    \"\"\"\n",
    "    # 计算总平方和（SST）\n",
    "    mean_target = torch.mean(targets,  dim=dim, keepdim=True)\n",
    "    ss_total = torch.sum((targets  - mean_target)**2, dim=dim)\n",
    "    \n",
    "    # 计算残差平方和（SSE）\n",
    "    ss_res = torch.sum((targets  - preds)**2, dim=dim)\n",
    "    \n",
    "    # 基础R²计算 \n",
    "    r2 = 1 - (ss_res + eps) / (ss_total + eps)\n",
    "    \n",
    "    # 调整R²计算 \n",
    "    if adjusted:\n",
    "        assert n_features is not None, \"调整R²需指定特征数\"\n",
    "        n_samples = targets.shape[0]  if dim is None else targets.shape[dim] \n",
    "        adj_factor = (n_samples - 1) / (n_samples - n_features - 1 + eps)\n",
    "        r2 = 1 - (1 - r2) * adj_factor \n",
    "        \n",
    "    return r2 \n",
    "\n",
    "\n",
    "R2(preds= y_hat.float(), targets= y.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4. <a id='toc15_4_'></a>[Metrics tracker](#toc0_)\n",
    "\n",
    "自己手写指标追踪器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        # 示例输出结构\n",
    "        history = {\n",
    "            'epoch': {\n",
    "                'train_loss': [0.5, 0.4, 0.3],          # 每个epoch的指标\n",
    "                'train_acc': [0.8, 0.85, 0.9],\n",
    "                'val_loss': [0.6, 0.5, 0.4],\n",
    "                'val_acc': [0.7, 0.75, 0.8]\n",
    "            },\n",
    "            'step': {\n",
    "                'train_loss': [0.55, 0.45, 0.35, ...],  # 每个step的指标\n",
    "                'train_acc': [0.78, 0.83, 0.88, ...],\n",
    "                'val_loss': [0.62, 0.52, ...],          # 验证阶段一般只在epoch结束时计算\n",
    "                'val_acc': [0.72, 0.77, ...]\n",
    "            }\n",
    "        }     \n",
    "        '''\n",
    "        self._metrics = {}                          # 存储指标计算函数\n",
    "        self._epoch_buffer = defaultdict(list)      # Epoch级别累积\n",
    "        self._step_buffer = defaultdict(list)       # Step级别累积\n",
    "        self._history ={\n",
    "            'epoch': defaultdict(list),             # 按阶段和指标名存储epoch指标\n",
    "            \"step\": defaultdict(list)               # 按阶段和指标名存储step指标\n",
    "        }\n",
    "        self.current_stage = 'train'                # 当前阶段标识\n",
    "\n",
    "    def add_metric(self, name, metric_fn):\n",
    "        \"\"\"注册指标（如损失、准确率）\"\"\"\n",
    "        self._metrics[name] = metric_fn\n",
    "\n",
    "    def set_stage(self, stage):\n",
    "        \"\"\"设置当前阶段（train/val/test）\"\"\"\n",
    "        self.current_stage = stage\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"更新缓冲区（需传入指标函数所需的参数），紧邻每个batch之后计算。\"\"\"\n",
    "        for name, fn in self._metrics.items():\n",
    "            value = fn(**kwargs)\n",
    "            self._epoch_buffer[name].append(value)  # 累积到epoch\n",
    "            self._step_buffer[name].append(value)   # 累积到step\n",
    "\n",
    "    def compute_epoch_metrics(self):\n",
    "        \"\"\"计算并返回当前阶段的Epoch平均指标\"\"\"\n",
    "        epoch_metrics = {}\n",
    "        for name, values in self._epoch_buffer.items():\n",
    "            avg_value = self._compute_avg(values)\n",
    "            epoch_metrics[name] = avg_value\n",
    "            self._history['epoch'][f\"{self.current_stage}_{name}\"].append(avg_value)\n",
    "        self._epoch_buffer.clear()  # 清空Epoch缓冲区\n",
    "        return epoch_metrics\n",
    "\n",
    "    def compute_step_metrics(self):\n",
    "        \"\"\"计算并返回当前阶段的Step平均指标（自动清空Step缓冲区）\"\"\"\n",
    "        step_metrics = {}\n",
    "        for name, values in self._step_buffer.items():\n",
    "            avg_value = self._compute_avg(values)\n",
    "            step_metrics[name] = avg_value\n",
    "            self._history['step'][f\"{self.current_stage}_{name}_step\"].append(avg_value)\n",
    "        self._step_buffer.clear()  # 清空Step缓冲区\n",
    "        return step_metrics\n",
    "\n",
    "    def _compute_avg(self, values):\n",
    "        \"\"\"通用平均值计算（支持标量和张量）\"\"\"\n",
    "        if not values:\n",
    "            return 0.0  # 避免空列表\n",
    "        if isinstance(values[0], (int, float)):\n",
    "            return sum(values) / len(values)\n",
    "        elif isinstance(values[0], torch.Tensor):\n",
    "            return torch.stack(values).mean(dim=0)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type: {type(values[0])}\")\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"获取所有历史记录（用于可视化）\"\"\"\n",
    "        return self._history\n",
    "    \n",
    "\n",
    "def f1_score(**kwargs):\n",
    "    outputs = kwargs['y_hat']\n",
    "    labels = kwargs['y']\n",
    "    preds = outputs.argmax(1)\n",
    "    tp = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    fp = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    fn = ((preds == 0) & (labels == 1)).sum().item()\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test\n",
    "    tracker = MetricTracker()\n",
    "    tracker.add_metric('loss', lambda **kw: kw['loss'])  # 直接从kwargs获取loss\n",
    "    tracker.add_metric('acc', lambda **kw: (kw['y_hat'].argmax(1) == kw['y']).float().mean().item())\n",
    "    tracker.add_metric('f1', f1_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`伪代码演示：`\n",
    "\n",
    "```python\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs, step_interval=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    tracker = MetricTracker()\n",
    "    tracker.add_metric('loss', lambda **kw: kw['loss'])  # 直接从kwargs获取loss\n",
    "    tracker.add_metric('acc', lambda **kw: (kw['outputs'].argmax(1) == kw['labels']).float().mean().item())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- 训练阶段 ----\n",
    "        tracker.set_stage('train')\n",
    "        model.train()\n",
    "        for step, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 更新指标（需传递指标函数所需的参数）\n",
    "            tracker.update(outputs=outputs, labels=labels, loss=loss.item())\n",
    "\n",
    "            # Step级别记录（每step_interval个batch）\n",
    "            if step % step_interval == 0:\n",
    "                step_metrics = tracker.compute_step_metrics()\n",
    "                print(f\"Epoch {epoch} | Step {step} | Train Step Metrics: {step_metrics}\")\n",
    "\n",
    "        # Epoch级别记录\n",
    "        train_metrics = tracker.compute_epoch_metrics()\n",
    "        print(f\"Epoch {epoch} | Train Epoch Metrics: {train_metrics}\")\n",
    "\n",
    "        # ---- 验证阶段 ----\n",
    "        tracker.set_stage('val')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                tracker.update(outputs=outputs, labels=labels, loss=torch.nn.functional.cross_entropy(outputs, labels).item())\n",
    "        \n",
    "        val_metrics = tracker.compute_epoch_metrics()\n",
    "        print(f\"Epoch {epoch} | Val Metrics: {val_metrics}\")\n",
    "\n",
    "    return tracker.get_history()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. <a id='toc16_'></a>[Benchmark](#toc0_)\n",
    "Benchmark 是评估算法、模型、系统或硬件性能的关键步骤，通常用于比较不同方法的效率、准确性或资源消耗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1. <a id='toc16_1_'></a>[确定 Benchmark 目标](#toc0_)\n",
    "明确要评估的内容和目标。可能的目标包括：\n",
    "  - 模型性能：例如准确率、召回率、F1-score 等。\n",
    "  - 运行效率：如训练时间、推理时间、内存占用。\n",
    "  - 可扩展性：系统对大规模数据或复杂任务的适应性。\n",
    "  - 能耗：如 GPU 或 CPU 使用率。\n",
    "\n",
    "Benchmark 注意事项\n",
    "  - 公平性: 确保所有方法在相同条件下运行，消除外部干扰因素。\n",
    "  - 可靠性: 结果需稳定，避免因随机性导致的显著偏差。\n",
    "  - 多样性: 在不同数据集、不同任务上进行 Benchmark，提高结果的通用性。\n",
    "  - 资源消耗: 注意方法的时间和硬件需求，避免忽略效率问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2. <a id='toc16_2_'></a>[Benchmark模板](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking ModelA...\n",
      "Benchmarking ModelB...\n",
      "\n",
      "Benchmark Results:\n",
      "{'Model': 'ModelA', 'Train Time (s)': 0.17575478553771973, 'Inference Time (s)': 0.01085042953491211, 'Current Memory (MB)': 0.00067, 'Peak Memory (MB)': 0.042011, 'Accuracy': 0.218}\n",
      "{'Model': 'ModelB', 'Train Time (s)': 0.20154285430908203, 'Inference Time (s)': 0.012841463088989258, 'Current Memory (MB)': 0.00067, 'Peak Memory (MB)': 0.042011, 'Accuracy': 0.479}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 数据集定义\n",
    "# -----------------------------\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, input_dim=100, num_classes=10):\n",
    "        self.num_samples = num_samples\n",
    "        self.data = torch.randn(num_samples, input_dim)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 模型定义\n",
    "# -----------------------------\n",
    "class ModelA(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ModelA, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ModelB(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ModelB, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "# -----------------------------\n",
    "# 3. Benchmark 工具函数\n",
    "# -----------------------------\n",
    "def benchmark_training(model, dataloader, criterion, optimizer, device, num_epochs=5):\n",
    "    \"\"\" 测量模型训练时间 \"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return total_time\n",
    "\n",
    "\n",
    "def benchmark_inference(model, dataloader, device):\n",
    "    \"\"\" 测量模型推理时间 \"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "    total_time = time.time() - start_time\n",
    "    return total_time\n",
    "\n",
    "\n",
    "def measure_memory(model, dataloader, device):\n",
    "    \"\"\" 测量模型内存占用 \"\"\"\n",
    "    tracemalloc.start()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "            break  # 只测量一次前向传播\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    return current / 10**6, peak / 10**6  # 转换为 MB\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device):\n",
    "    \"\"\" 测量模型准确性 \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Benchmark 主流程\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    INPUT_DIM = 100\n",
    "    NUM_CLASSES = 10\n",
    "    NUM_SAMPLES = 1000\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 5\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 数据集和加载器\n",
    "    dataset = DummyDataset(num_samples=NUM_SAMPLES, input_dim=INPUT_DIM, num_classes=NUM_CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # 模型列表\n",
    "    models = {\n",
    "        \"ModelA\": ModelA(INPUT_DIM, NUM_CLASSES),\n",
    "        \"ModelB\": ModelB(INPUT_DIM, NUM_CLASSES)\n",
    "    }\n",
    "\n",
    "    # 损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Benchmark 结果\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Benchmarking {model_name}...\")\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        # 优化器\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        # 测试训练时间\n",
    "        train_time = benchmark_training(model, dataloader, criterion, optimizer, DEVICE, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "        # 测试推理时间\n",
    "        inference_time = benchmark_inference(model, dataloader, DEVICE)\n",
    "\n",
    "        # 测试内存占用\n",
    "        current_memory, peak_memory = measure_memory(model, dataloader, DEVICE)\n",
    "\n",
    "        # 测试准确性\n",
    "        accuracy = evaluate_accuracy(model, dataloader, DEVICE)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Train Time (s)\": train_time,\n",
    "            \"Inference Time (s)\": inference_time,\n",
    "            \"Current Memory (MB)\": current_memory,\n",
    "            \"Peak Memory (MB)\": peak_memory,\n",
    "            \"Accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "    # 打印 Benchmark 结果\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    for result in results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. <a id='toc17_'></a>[PyTorch lightning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.1. <a id='toc17_1_'></a>[训练逻辑](#toc0_)\n",
    "- PyTorch lightning官方给的`PyTorch lightning`教程：[https://lightning.ai/docs/pytorch/stable/expertise_levels.html](https://lightning.ai/docs/pytorch/stable/expertise_levels.html)\n",
    "\n",
    "- PyTorch lightning给的`PyTorch`教程：[https://lightning.ai/docs/pytorch/stable/tutorials.html](https://lightning.ai/docs/pytorch/stable/tutorials.html)\n",
    "\n",
    "- PuTorch lightning给的`PyTorch code to PyTorchLightning`：[https://lightning.ai/docs/pytorch/stable/starter/converting.html](https://lightning.ai/docs/pytorch/stable/starter/converting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![Frame](./Pytorch_Pictures/PyTorch_lightning/Frame1.jpg)\n",
    "![Frame2](./Pytorch_Pictures/PyTorch_lightning/Frame2.jpg)\n",
    "![Frame3](./Pytorch_Pictures/PyTorch_lightning/Frame3.jpg)\n",
    "![Frame4](./Pytorch_Pictures/PyTorch_lightning/Frame4.jpg)\n",
    "![Frame5](./Pytorch_Pictures/PyTorch_lightning/Frame5.jpg) -->\n",
    "\n",
    "layout by html and CSS style, with display:flex\n",
    "<div style=\"display:flex;justify-content:center;gap:10px\">\n",
    "    <img src=\"./Pytorch_Pictures/PyTorch_lightning/Frame1.jpg\" alt=\"PyTorch lightning\" width=800 height=600>\n",
    "    <img src=\"./Pytorch_Pictures/PyTorch_lightning/Frame2.jpg\" alt=\"PyTorch lightning\" width=800 height=600>\n",
    "</div>\n",
    "\n",
    "<div style=\"display:flex;justify-content:center;gap:10px\">\n",
    "    <img src=\"./Pytorch_Pictures/PyTorch_lightning/Frame3.jpg\" alt=\"PyTorch lightning\" width=800 height=600>\n",
    "    <img src=\"./Pytorch_Pictures/PyTorch_lightning/Frame4.jpg\" alt=\"PyTorch lightning\" width=800 height=600>\n",
    "</div>\n",
    "\n",
    "<div style=\"display:flex;justify-content:center;gap:10px\">\n",
    "    <img src=\"./Pytorch_Pictures/PyTorch_lightning/Frame5.jpg\" alt=\"PyTorch lightning\" width=800 height=600>\n",
    "    <div ></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch lightning version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as L\n",
    "\n",
    "\n",
    "print(f'Pytorch lightning version: {L.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.2. <a id='toc17_2_'></a>[Data.py](#toc0_)\n",
    "数据部分应该单独处理，便于以后管理和维护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 2]),\n",
       " torch.Size([10000, 1]),\n",
       " tensor([0.1184, 0.0257]),\n",
       " tensor([3.5712]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 虚拟出一些数据\n",
    "# 仅供参考\n",
    "\n",
    "import torch \n",
    "\n",
    "\n",
    "def syn_datas(\n",
    "    w = torch.tensor([2.0, -3.0]),\n",
    "    b = torch.tensor([3.4]), nums = 10000):\n",
    "    X = torch.normal(mean=0, std=0.1, size=(nums, w.shape[0]), dtype=torch.float32)\n",
    "    y = X @ w + b\n",
    "    y += torch.normal(mean=0, std=0.1, size=y.shape, dtype=torch.float)\n",
    "    return X, y \n",
    "\n",
    "\n",
    "# 预设参数，注意 形状/维度\n",
    "preset_weight = torch.tensor([2.0, -3.0], dtype=torch.float32).reshape(2, 1)\n",
    "preset_bias = torch.tensor([3.4], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 虚拟数据\n",
    "features, labels = syn_datas(w=preset_weight, b=preset_bias, nums=10000)\n",
    "\n",
    "\n",
    "# 初步查看\n",
    "features.shape, labels.shape, features[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data \n",
    "\n",
    "\n",
    "datasets = data.TensorDataset(features, labels)\n",
    "\n",
    "train_iter = data.DataLoader(dataset= datasets, shuffle= True, batch_size= 128, num_workers= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.3. <a id='toc17_3_'></a>[Model.py](#toc0_)\n",
    "模型部分应该用纯PyTorch编写以便于理解和后续的维护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用纯PyTorch构建模型的网络结构\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AlphaFold2(nn.Module):\n",
    "    def __init__(self, in_features=2, out_features=1):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Sequential(nn.Linear(in_features, out_features))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.hidden(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.4. <a id='toc17_4_'></a>[ModelWrapper.py](#toc0_)\n",
    "利用PyTorch lightning训练框架进行训练只是方便调用，最后进行模型`魔改`后最好还是用纯PyTorch进行学习。  \n",
    "都是固定框架（API信息如下）\n",
    "- [L.LightningModule API](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#backward)\n",
    "- [Trainer API](https://lightning.ai/docs/pytorch/stable/common/trainer.html)\n",
    "    - Automatically enabling/disabling grads\n",
    "    - Running the training, validation and test dataloaders\n",
    "    - Calling the Callbacks at the appropriate times\n",
    "    - Putting batches and computations on the correct devices\n",
    "    \n",
    "![Trainer_API](./Pytorch_Pictures/PyTorch_lightning/Trainer_API.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/bmp/backup/zhaosy/miniconda3/envs/deeplearning/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# 用PyTorch lightning构建训练步骤\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "\n",
    "import pytorch_lightning as L\n",
    "\n",
    "\n",
    "class AlphaFold2Wrapper(L.LightningModule):\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        ## save hyperparameters\n",
    "        self.save_hyperparameters()                                 # 超参数保存\n",
    "        self.learning_rate = learning_rate                          # 超参数\n",
    "        ## model initiate from model constructed by pure PyTorch\n",
    "        self.demo_model = AlphaFold2(in_features=2, out_features=1)      ## 模型\n",
    "        ## loss_fn\n",
    "        self.loss_fn = torch.nn.MSELoss()                           ## 损失函数\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.demo_model(X)\n",
    "\n",
    "    def configure_optimizers(self):                                 ## 优化函数\n",
    "        opt = optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        return opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''训练步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)                 # 在进度条上显示出来\n",
    "\n",
    "        self.training_step_outputs.append(loss)                     # 保存结果，以备后续使用 (on_train_epoch_end(self))\n",
    "        return loss\n",
    "    \n",
    "    # def on_train_batch_start(self, batch, batch_idx):\n",
    "    #     '''\n",
    "    #     Called in the training loop before anything happens for that batch.\n",
    "    #     If you return -1 here, you will skip training for the rest of the current epoch.\n",
    "    #     '''\n",
    "    #     pass\n",
    "\n",
    "    # def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "    #     '''\n",
    "    #     Called in the training loop after the batch.\n",
    "    #     Parameters:\n",
    "    #             outputs (Union[Tensor, Mapping[str, Any], None]) – The outputs of training_step(x)\n",
    "    #             batch (Any) – The batched data as it is returned by the training DataLoader.\n",
    "    #             batch_idx (int) – the index of the batch\n",
    "    #     '''\n",
    "    #     pass\n",
    "\n",
    "    # def on_train_epoch_start(self):\n",
    "    #     '''Called in the training loop at the very beginning of the epoch.'''\n",
    "    #     pass\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Called in the training loop at the very end of the epoch.\n",
    "        To access all batch outputs at the end of the epoch, \n",
    "        you can cache step outputs as an attribute of the LightningModule and access them in this hook:\n",
    "        '''\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_outputs).mean() # 来自于training_step()计算结果\n",
    "        self.log(\"training_epoch_mean\", epoch_mean)\n",
    "        # free up the memory\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''验证步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X) \n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    # def on_validation_batch_start(self, batch, batch_idx, dataloader_idx=0):\n",
    "    #     '''Called in the validation loop before anything happens for that batch.'''\n",
    "    #     pass \n",
    "\n",
    "    # def on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "    #     '''Called in the validation loop after the batch.\n",
    "    #     Parameters:\n",
    "    #             outputs (Union[Tensor, Mapping[str, Any], None]) – The outputs of validation_step(x)\n",
    "    #             batch (Any) – The batched data as it is returned by the validation DataLoader.\n",
    "    #             batch_idx (int) – the index of the batch\n",
    "    #             dataloader_idx (int) – the index of the dataloader\n",
    "    #     '''\n",
    "    #     pass\n",
    "\n",
    "    # def on_validation_epoch_start(self):\n",
    "    #     '''Called in the validation loop at the very beginning of the epoch.'''\n",
    "    #     pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        '''Called in the validation loop at the very end of the epoch.'''\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''测试步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X) \n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self, batch, batch_idx):\n",
    "        '''预测步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X) \n",
    "        self.log('y_hat', y_hat)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "## 实例化一个对象\n",
    "alphafold2 = AlphaFold2Wrapper(learning_rate=0.01)\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator= \"gpu\",              # cpu, gpu, tpu, auto\n",
    "    devices= 1, \n",
    "    # strategy= \"ddp\",                 # ddp, ddp_spawn, ddp_notebook\n",
    "    # num_nodes= 1,                    # Number of GPU nodes for distributed training.\n",
    "\n",
    "    # precision=\"32-true\",            # There are two different techniques to set the mixed precision. “True” precision and “Mixed” precision.\n",
    "\n",
    "    # callbacks = ,\n",
    "    \n",
    "    # min_epochs= 1,\n",
    "    max_epochs= 10, \n",
    "    # min_steps=None,                 # Force training for at least this number of global steps. Trainer will train model for at least min_steps or min_epochs (latest).\n",
    "    max_steps= -1,                   # Stop training after this number of global steps. Training will stop if max_steps or max_epochs have reached (earliest).\n",
    "    log_every_n_steps= 1,           ## How often to add logging rows (does not write to disk)\n",
    "    check_val_every_n_epoch= 1,      # default used by the Trainer\n",
    "\n",
    "    # default_root_dir=os.getcwd(),   # os.getcwd()\n",
    "    # enable_progress_bar=True,       # Whether to enable or disable the progress bar. Defaults to True.\n",
    "    # enable_model_summary=True,      # Whether to enable or disable the model summarization. Defaults to True.\n",
    "\n",
    "    profiler=None,                  # simple, advanced, None: To profile individual steps during training and assist in identifying bottlenecks.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.1. <a id='toc17_4_1_'></a>[Training and vlidation](#toc0_)\n",
    "Training的同时进行Validation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model= alphafold2, \n",
    "    train_dataloaders= train_iter, \n",
    "    val_dataloaders= train_iter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.2. <a id='toc17_4_2_'></a>[Validation](#toc0_)\n",
    "只进行validation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a78057d51964bd08f8f114ee0bdc263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            12.767409324645996\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 12.767409324645996}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model=alphafold2, dataloaders=train_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.3. <a id='toc17_4_3_'></a>[Test](#toc0_)\n",
    "只进行test。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/bmp/backup/zhaosy/miniconda3/envs/deeplearning/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44cfe00bdd44d5587ca9d2c78dd0ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           12.767407417297363\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 12.767407417297363}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=alphafold2, dataloaders=train_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.4. <a id='toc17_4_4_'></a>[Prediction](#toc0_)\n",
    "\n",
    "进行预测。\n",
    "\n",
    "<!-- ![Prediction summary](./Pytorch_Pictures/PyTorch_lightning/Frame4.jpg) -->\n",
    "\n",
    "<div style=\"display:flex;justify-content:center\">\n",
    "<img src=\"./Pytorch_Pictures/PyTorch_lightning/Frame4.jpg\" alt=\"Prediction\" width=800 height=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.4.4.1. <a id='toc17_4_4_1_'></a>[PyTorch lightning自身Trainer直接predict](#toc0_)\n",
    "调用PyTorch lightning自身Trainer的predict，程序会自动使用：  \n",
    "- model.eval()\n",
    "- with torch.no_grad():\n",
    "- 或 torch.set_grad_enable(True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367bfa7a81ad4ae39d910edbc61beca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor([-0.2127]),\n",
       " tensor([-0.0477]),\n",
       " tensor([-0.1609]),\n",
       " tensor([-0.1269]),\n",
       " tensor([-0.1909]),\n",
       " tensor([-0.1667]),\n",
       " tensor([-0.2182]),\n",
       " tensor([-0.2330]),\n",
       " tensor([-0.2138]),\n",
       " tensor([-0.1298]),\n",
       " tensor([-0.2620]),\n",
       " tensor([-0.0983]),\n",
       " tensor([-0.1618]),\n",
       " tensor([-0.1909]),\n",
       " tensor([-0.1667]),\n",
       " tensor([-0.1917]),\n",
       " tensor([-0.1002]),\n",
       " tensor([-0.1618]),\n",
       " tensor([-0.0907]),\n",
       " tensor([-0.2054]),\n",
       " tensor([-0.0621]),\n",
       " tensor([-0.0999]),\n",
       " tensor([-0.1571]),\n",
       " tensor([-0.0900]),\n",
       " tensor([-0.1526]),\n",
       " tensor([-0.1472]),\n",
       " tensor([-0.0359]),\n",
       " tensor([-0.2769]),\n",
       " tensor([-0.2004]),\n",
       " tensor([-0.1007]),\n",
       " tensor([-0.0914]),\n",
       " tensor([-0.2074]),\n",
       " tensor([-0.1197]),\n",
       " tensor([-0.0566]),\n",
       " tensor([-0.0941]),\n",
       " tensor([-0.0092]),\n",
       " tensor([-0.1733]),\n",
       " tensor([-0.1429]),\n",
       " tensor([-0.1621]),\n",
       " tensor([-0.0307]),\n",
       " tensor([-0.0528]),\n",
       " tensor([-0.2584]),\n",
       " tensor([-0.1461]),\n",
       " tensor([-0.1884]),\n",
       " tensor([-0.0604]),\n",
       " tensor([-0.1951]),\n",
       " tensor([-0.1595]),\n",
       " tensor([-0.2809]),\n",
       " tensor([-0.2254]),\n",
       " tensor([-0.1168]),\n",
       " tensor([-0.3140]),\n",
       " tensor([-0.1856]),\n",
       " tensor([-0.1477]),\n",
       " tensor([-0.1551]),\n",
       " tensor([-0.1312]),\n",
       " tensor([-0.1699]),\n",
       " tensor([-0.2011]),\n",
       " tensor([-0.1280]),\n",
       " tensor([-0.0805]),\n",
       " tensor([-0.0522]),\n",
       " tensor([-0.1997]),\n",
       " tensor([-0.0024]),\n",
       " tensor([-0.1609]),\n",
       " tensor([-0.1561]),\n",
       " tensor([-0.2258]),\n",
       " tensor([-0.1416]),\n",
       " tensor([-0.2286]),\n",
       " tensor([-0.1494]),\n",
       " tensor([-0.1822]),\n",
       " tensor([-0.1046]),\n",
       " tensor([-0.0605]),\n",
       " tensor([-0.2355]),\n",
       " tensor([-0.0680]),\n",
       " tensor([-0.0521]),\n",
       " tensor([-0.0371]),\n",
       " tensor([-0.1811]),\n",
       " tensor([-0.1611]),\n",
       " tensor([-0.1196]),\n",
       " tensor([-0.1782]),\n",
       " tensor([-0.1197]),\n",
       " tensor([-0.1408]),\n",
       " tensor([-0.1717]),\n",
       " tensor([-0.2065]),\n",
       " tensor([-0.0982]),\n",
       " tensor([-0.0024]),\n",
       " tensor([-0.2176]),\n",
       " tensor([-0.1245]),\n",
       " tensor([-0.1741]),\n",
       " tensor([-0.1603]),\n",
       " tensor([-0.1167]),\n",
       " tensor([-0.2755]),\n",
       " tensor([-0.1702]),\n",
       " tensor([-0.0378]),\n",
       " tensor([-0.1729]),\n",
       " tensor([-0.1213]),\n",
       " tensor([-0.2606]),\n",
       " tensor([-0.1024]),\n",
       " tensor([-0.1451]),\n",
       " tensor([-0.1140]),\n",
       " tensor([-0.0993]),\n",
       " tensor([-0.0644]),\n",
       " tensor([-0.1849]),\n",
       " tensor([-0.1381]),\n",
       " tensor([-0.1182]),\n",
       " tensor([-0.2791]),\n",
       " tensor([-0.2344]),\n",
       " tensor([-0.1461]),\n",
       " tensor([-0.0821]),\n",
       " tensor([-0.0456]),\n",
       " tensor([-0.1833]),\n",
       " tensor([-0.1372]),\n",
       " tensor([-0.1515]),\n",
       " tensor([-0.1519]),\n",
       " tensor([-0.0909]),\n",
       " tensor([-0.2727]),\n",
       " tensor([-0.0271]),\n",
       " tensor([-0.0205]),\n",
       " tensor([-0.2096]),\n",
       " tensor([-0.1157]),\n",
       " tensor([-0.1714]),\n",
       " tensor([-0.0623]),\n",
       " tensor([-0.1544]),\n",
       " tensor([-0.2104]),\n",
       " tensor([-0.1424]),\n",
       " tensor([-0.1922]),\n",
       " tensor([-0.1229]),\n",
       " tensor([-0.2438]),\n",
       " tensor([-0.1253]),\n",
       " tensor([-0.1133]),\n",
       " tensor([-0.0817]),\n",
       " tensor([-0.0187]),\n",
       " tensor([-0.0866]),\n",
       " tensor([-0.1755]),\n",
       " tensor([-0.0394]),\n",
       " tensor([-0.0735]),\n",
       " tensor([-0.1023]),\n",
       " tensor([-0.1335]),\n",
       " tensor([-0.0841]),\n",
       " tensor([-0.1668]),\n",
       " tensor([-0.1182]),\n",
       " tensor([-0.0516]),\n",
       " tensor([-0.2238]),\n",
       " tensor([-0.1737]),\n",
       " tensor([-0.0930]),\n",
       " tensor([-0.2468]),\n",
       " tensor([-0.2073]),\n",
       " tensor([-0.1186]),\n",
       " tensor([-0.1423]),\n",
       " tensor([-0.1477]),\n",
       " tensor([-0.1184]),\n",
       " tensor([-0.1657]),\n",
       " tensor([-0.1744]),\n",
       " tensor([-0.0431]),\n",
       " tensor([-0.2033]),\n",
       " tensor([-0.1019]),\n",
       " tensor([-0.2257]),\n",
       " tensor([-0.2233]),\n",
       " tensor([-0.2118]),\n",
       " tensor([-0.2100]),\n",
       " tensor([-0.1427]),\n",
       " tensor([-0.1087]),\n",
       " tensor([-0.1554]),\n",
       " tensor([-0.1500]),\n",
       " tensor([-0.1060]),\n",
       " tensor([-0.0965]),\n",
       " tensor([-0.1929]),\n",
       " tensor([-0.1108]),\n",
       " tensor([-0.2517]),\n",
       " tensor([-0.2180]),\n",
       " tensor([-0.1163]),\n",
       " tensor([-0.0920]),\n",
       " tensor([-0.2298]),\n",
       " tensor([-0.1440]),\n",
       " tensor([-0.1822]),\n",
       " tensor([-0.1191]),\n",
       " tensor([-0.1512]),\n",
       " tensor([-0.0826]),\n",
       " tensor([-0.2158]),\n",
       " tensor([-0.2562]),\n",
       " tensor([-0.1805]),\n",
       " tensor([-0.1763]),\n",
       " tensor([-0.2769]),\n",
       " tensor([-0.1759]),\n",
       " tensor([-0.1153]),\n",
       " tensor([-0.1710]),\n",
       " tensor([-0.1401]),\n",
       " tensor([0.0002]),\n",
       " tensor([-0.2454]),\n",
       " tensor([-0.1370]),\n",
       " tensor([-0.2208]),\n",
       " tensor([-0.1038]),\n",
       " tensor([-0.0699]),\n",
       " tensor([-0.1276]),\n",
       " tensor([-0.1605]),\n",
       " tensor([-0.1983]),\n",
       " tensor([-0.1071]),\n",
       " tensor([-0.1228]),\n",
       " tensor([-0.1746]),\n",
       " tensor([-0.1009]),\n",
       " tensor([-0.1880]),\n",
       " tensor([-0.1882]),\n",
       " tensor([-0.2028]),\n",
       " tensor([-0.0896]),\n",
       " tensor([-0.1716]),\n",
       " tensor([-0.1865]),\n",
       " tensor([-0.0976]),\n",
       " tensor([-0.1166]),\n",
       " tensor([-0.1207]),\n",
       " tensor([-0.0622]),\n",
       " tensor([-0.0666]),\n",
       " tensor([-0.2405]),\n",
       " tensor([-0.1281]),\n",
       " tensor([-0.1640]),\n",
       " tensor([-0.1295]),\n",
       " tensor([-0.2654]),\n",
       " tensor([-0.1415]),\n",
       " tensor([-0.0899]),\n",
       " tensor([-0.1238]),\n",
       " tensor([-0.1664]),\n",
       " tensor([-0.1289]),\n",
       " tensor([-0.1062]),\n",
       " tensor([-0.0926]),\n",
       " tensor([-0.1393]),\n",
       " tensor([-0.1396]),\n",
       " tensor([-0.2144]),\n",
       " tensor([-0.2326]),\n",
       " tensor([-0.0635]),\n",
       " tensor([-0.1975]),\n",
       " tensor([-0.1486]),\n",
       " tensor([-0.1948]),\n",
       " tensor([-0.1604]),\n",
       " tensor([-0.0810]),\n",
       " tensor([-0.1338]),\n",
       " tensor([0.0275]),\n",
       " tensor([-0.0775]),\n",
       " tensor([-0.1271]),\n",
       " tensor([-0.1248]),\n",
       " tensor([-0.0494]),\n",
       " tensor([-0.2868]),\n",
       " tensor([-0.0687]),\n",
       " tensor([-0.1481]),\n",
       " tensor([-0.1365]),\n",
       " tensor([-0.0853]),\n",
       " tensor([-0.1636]),\n",
       " tensor([-0.1817]),\n",
       " tensor([-0.1492]),\n",
       " tensor([-0.1991]),\n",
       " tensor([-0.1556]),\n",
       " tensor([-0.2380]),\n",
       " tensor([-0.1761]),\n",
       " tensor([-0.2368]),\n",
       " tensor([-0.1702]),\n",
       " tensor([-0.1091]),\n",
       " tensor([-0.1628]),\n",
       " tensor([-0.0145]),\n",
       " tensor([-0.1817]),\n",
       " tensor([-0.0238]),\n",
       " tensor([-0.1382]),\n",
       " tensor([-0.0205]),\n",
       " tensor([-0.1784]),\n",
       " tensor([-0.3076]),\n",
       " tensor([-0.2343]),\n",
       " tensor([-0.1369]),\n",
       " tensor([-0.1630]),\n",
       " tensor([-0.2276]),\n",
       " tensor([-0.0478]),\n",
       " tensor([-0.2550]),\n",
       " tensor([-0.2243]),\n",
       " tensor([-0.1274]),\n",
       " tensor([-0.0714]),\n",
       " tensor([0.0261]),\n",
       " tensor([-0.2129]),\n",
       " tensor([-0.1833]),\n",
       " tensor([-0.1950]),\n",
       " tensor([-0.1634]),\n",
       " tensor([-0.0515]),\n",
       " tensor([-0.0797]),\n",
       " tensor([-0.0966]),\n",
       " tensor([-0.1885]),\n",
       " tensor([-0.2603]),\n",
       " tensor([-0.1182]),\n",
       " tensor([-0.1534]),\n",
       " tensor([-0.0965]),\n",
       " tensor([-0.0843]),\n",
       " tensor([-0.0922]),\n",
       " tensor([-0.1213]),\n",
       " tensor([-0.0940]),\n",
       " tensor([-0.1566]),\n",
       " tensor([-0.0925]),\n",
       " tensor([-0.1342]),\n",
       " tensor([-0.1258]),\n",
       " tensor([-0.0771]),\n",
       " tensor([-0.1096]),\n",
       " tensor([-0.2606]),\n",
       " tensor([-0.0255]),\n",
       " tensor([-0.1528]),\n",
       " tensor([-0.2333]),\n",
       " tensor([0.0040]),\n",
       " tensor([-0.1306]),\n",
       " tensor([-0.0633]),\n",
       " tensor([0.0011]),\n",
       " tensor([-0.1580]),\n",
       " tensor([-0.0756]),\n",
       " tensor([-0.1576]),\n",
       " tensor([-0.1540]),\n",
       " tensor([-0.1856]),\n",
       " tensor([-0.0053]),\n",
       " tensor([-0.1082]),\n",
       " tensor([-0.1433]),\n",
       " tensor([-0.1144]),\n",
       " tensor([-0.1190]),\n",
       " tensor([-0.1395]),\n",
       " tensor([-0.1978]),\n",
       " tensor([-0.1345]),\n",
       " tensor([-0.0688]),\n",
       " tensor([-0.3219]),\n",
       " tensor([-0.1360]),\n",
       " tensor([-0.1908]),\n",
       " tensor([-0.1561]),\n",
       " tensor([-0.1149]),\n",
       " tensor([-0.0945]),\n",
       " tensor([-0.0707]),\n",
       " tensor([-0.2258]),\n",
       " tensor([-0.1461]),\n",
       " tensor([-0.0427]),\n",
       " tensor([-0.1933]),\n",
       " tensor([-0.1581]),\n",
       " tensor([-0.0819]),\n",
       " tensor([-0.2360]),\n",
       " tensor([-0.1973]),\n",
       " tensor([-0.1499]),\n",
       " tensor([-0.2204]),\n",
       " tensor([-0.0966]),\n",
       " tensor([-0.1925]),\n",
       " tensor([-0.1684]),\n",
       " tensor([-0.1445]),\n",
       " tensor([-0.0712]),\n",
       " tensor([-0.1289]),\n",
       " tensor([-0.1386]),\n",
       " tensor([-0.1236]),\n",
       " tensor([-0.2462]),\n",
       " tensor([-0.0503]),\n",
       " tensor([-0.1303]),\n",
       " tensor([-0.0147]),\n",
       " tensor([-0.1785]),\n",
       " tensor([-0.1706]),\n",
       " tensor([-0.1771]),\n",
       " tensor([-0.1949]),\n",
       " tensor([-0.1247]),\n",
       " tensor([-0.0967]),\n",
       " tensor([-0.0386]),\n",
       " tensor([-0.2043]),\n",
       " tensor([-0.0024]),\n",
       " tensor([-0.2396]),\n",
       " tensor([-0.1351]),\n",
       " tensor([-0.1579]),\n",
       " tensor([-0.2927]),\n",
       " tensor([-0.2028]),\n",
       " tensor([-0.1362]),\n",
       " tensor([-0.2754]),\n",
       " tensor([-0.1226]),\n",
       " tensor([-0.1897]),\n",
       " tensor([-0.3025]),\n",
       " tensor([-0.1722]),\n",
       " tensor([-0.1366]),\n",
       " tensor([-0.0093]),\n",
       " tensor([-0.0893]),\n",
       " tensor([-0.1728]),\n",
       " tensor([-0.1187]),\n",
       " tensor([-0.1225]),\n",
       " tensor([-0.0695]),\n",
       " tensor([-0.1563]),\n",
       " tensor([-0.1324]),\n",
       " tensor([-0.0682]),\n",
       " tensor([-0.1422]),\n",
       " tensor([-0.1299]),\n",
       " tensor([-0.1754]),\n",
       " tensor([-0.1522]),\n",
       " tensor([-0.2283]),\n",
       " tensor([-0.1590]),\n",
       " tensor([-0.2436]),\n",
       " tensor([-0.1234]),\n",
       " tensor([-0.0982]),\n",
       " tensor([-0.0484]),\n",
       " tensor([-0.0644]),\n",
       " tensor([-0.1942]),\n",
       " tensor([-0.2100]),\n",
       " tensor([-0.1891]),\n",
       " tensor([-0.0429]),\n",
       " tensor([-0.1021]),\n",
       " tensor([-0.1443]),\n",
       " tensor([-0.0663]),\n",
       " tensor([-0.1990]),\n",
       " tensor([-0.1163]),\n",
       " tensor([-0.0154]),\n",
       " tensor([-0.1974]),\n",
       " tensor([-0.2098]),\n",
       " tensor([-0.1516]),\n",
       " tensor([0.0134]),\n",
       " tensor([-0.0324]),\n",
       " tensor([-0.0530]),\n",
       " tensor([-0.2352]),\n",
       " tensor([-0.0857]),\n",
       " tensor([-0.0521]),\n",
       " tensor([-0.1094]),\n",
       " tensor([-0.1562]),\n",
       " tensor([-0.1657]),\n",
       " tensor([-0.1744]),\n",
       " tensor([-0.1005]),\n",
       " tensor([0.0121]),\n",
       " tensor([-0.1601]),\n",
       " tensor([-0.0875]),\n",
       " tensor([-0.1541]),\n",
       " tensor([-0.1342]),\n",
       " tensor([-0.1235]),\n",
       " tensor([-0.0492]),\n",
       " tensor([-0.1680]),\n",
       " tensor([0.0344]),\n",
       " tensor([-0.1597]),\n",
       " tensor([-0.1343]),\n",
       " tensor([-0.1912]),\n",
       " tensor([-0.0916]),\n",
       " tensor([-0.1593]),\n",
       " tensor([-0.2570]),\n",
       " tensor([-0.1111]),\n",
       " tensor([-0.2561]),\n",
       " tensor([-0.1078]),\n",
       " tensor([-0.0665]),\n",
       " tensor([-0.1824]),\n",
       " tensor([-0.1593]),\n",
       " tensor([-0.1444]),\n",
       " tensor([-0.1756]),\n",
       " tensor([-0.1953]),\n",
       " tensor([-0.1853]),\n",
       " tensor([-0.2533]),\n",
       " tensor([-0.2425]),\n",
       " tensor([-0.1707]),\n",
       " tensor([-0.0808]),\n",
       " tensor([-0.2532]),\n",
       " tensor([-0.1063]),\n",
       " tensor([-0.1397]),\n",
       " tensor([-0.0560]),\n",
       " tensor([-0.1805]),\n",
       " tensor([-0.1874]),\n",
       " tensor([-0.1123]),\n",
       " tensor([-0.1577]),\n",
       " tensor([-0.0989]),\n",
       " tensor([-0.1666]),\n",
       " tensor([-0.1300]),\n",
       " tensor([-0.0028]),\n",
       " tensor([-0.2084]),\n",
       " tensor([-0.0056]),\n",
       " tensor([-0.1809]),\n",
       " tensor([-0.0136]),\n",
       " tensor([-0.1520]),\n",
       " tensor([-0.2657]),\n",
       " tensor([-0.1801]),\n",
       " tensor([-0.1701]),\n",
       " tensor([-0.0196]),\n",
       " tensor([-0.0924]),\n",
       " tensor([-0.2173]),\n",
       " tensor([-0.1315]),\n",
       " tensor([-0.1005]),\n",
       " tensor([-0.1662]),\n",
       " tensor([-0.2219]),\n",
       " tensor([-0.0877]),\n",
       " tensor([-0.2312]),\n",
       " tensor([-0.1287]),\n",
       " tensor([-0.1451]),\n",
       " tensor([-0.2703]),\n",
       " tensor([-0.2023]),\n",
       " tensor([-0.1786]),\n",
       " tensor([-0.1957]),\n",
       " tensor([-0.1571]),\n",
       " tensor([-0.1571]),\n",
       " tensor([-0.1354]),\n",
       " tensor([-0.0624]),\n",
       " tensor([-0.1682]),\n",
       " tensor([-0.1690]),\n",
       " tensor([-0.1579]),\n",
       " tensor([-0.1683]),\n",
       " tensor([-0.0963]),\n",
       " tensor([-0.2430]),\n",
       " tensor([-0.1079]),\n",
       " tensor([-0.2376]),\n",
       " tensor([-0.1708]),\n",
       " tensor([-0.1032]),\n",
       " tensor([-0.2063]),\n",
       " tensor([-0.0883]),\n",
       " tensor([-0.0855]),\n",
       " tensor([-0.1381]),\n",
       " tensor([-0.2445]),\n",
       " tensor([-0.1707]),\n",
       " tensor([-0.2541]),\n",
       " tensor([-0.1699]),\n",
       " tensor([-0.2370]),\n",
       " tensor([-0.1419]),\n",
       " tensor([-0.2460]),\n",
       " tensor([-0.1667]),\n",
       " tensor([-0.1174]),\n",
       " tensor([-0.1555]),\n",
       " tensor([-0.1758]),\n",
       " tensor([-0.0659]),\n",
       " tensor([-0.2598]),\n",
       " tensor([-0.1754]),\n",
       " tensor([0.0034]),\n",
       " tensor([-0.0778]),\n",
       " tensor([-0.1455]),\n",
       " tensor([-0.0098]),\n",
       " tensor([-0.0690]),\n",
       " tensor([-0.2418]),\n",
       " tensor([-0.0668]),\n",
       " tensor([-0.2252]),\n",
       " tensor([-0.1180]),\n",
       " tensor([-0.3583]),\n",
       " tensor([-0.1807]),\n",
       " tensor([-0.0943]),\n",
       " tensor([-0.0936]),\n",
       " tensor([-0.2305]),\n",
       " tensor([-0.2236]),\n",
       " tensor([-0.0380]),\n",
       " tensor([-0.2988]),\n",
       " tensor([-0.1559]),\n",
       " tensor([-0.1244]),\n",
       " tensor([-0.1278]),\n",
       " tensor([-0.0922]),\n",
       " tensor([-0.0208]),\n",
       " tensor([-0.0494]),\n",
       " tensor([-0.1601]),\n",
       " tensor([-0.1546]),\n",
       " tensor([-0.1495]),\n",
       " tensor([-0.1572]),\n",
       " tensor([-0.1340]),\n",
       " tensor([-0.2577]),\n",
       " tensor([-0.2547]),\n",
       " tensor([-0.0791]),\n",
       " tensor([-0.0457]),\n",
       " tensor([-0.2032]),\n",
       " tensor([-0.2178]),\n",
       " tensor([-0.1905]),\n",
       " tensor([-0.1569]),\n",
       " tensor([-0.1710]),\n",
       " tensor([-0.1999]),\n",
       " tensor([-0.1789]),\n",
       " tensor([-0.2026]),\n",
       " tensor([-0.0777]),\n",
       " tensor([-0.1484]),\n",
       " tensor([-0.1475]),\n",
       " tensor([-0.1223]),\n",
       " tensor([-0.1979]),\n",
       " tensor([-0.1490]),\n",
       " tensor([-0.2013]),\n",
       " tensor([-0.1551]),\n",
       " tensor([-0.1055]),\n",
       " tensor([-0.1027]),\n",
       " tensor([-0.1678]),\n",
       " tensor([-0.0726]),\n",
       " tensor([-0.0747]),\n",
       " tensor([-0.1622]),\n",
       " tensor([-0.1496]),\n",
       " tensor([-0.1506]),\n",
       " tensor([-0.1222]),\n",
       " tensor([-0.2377]),\n",
       " tensor([-0.0409]),\n",
       " tensor([-0.1389]),\n",
       " tensor([-0.0950]),\n",
       " tensor([-0.2040]),\n",
       " tensor([-0.0684]),\n",
       " tensor([-0.1659]),\n",
       " tensor([-0.1222]),\n",
       " tensor([-0.1615]),\n",
       " tensor([-0.0933]),\n",
       " tensor([-0.1767]),\n",
       " tensor([-0.0088]),\n",
       " tensor([-0.2152]),\n",
       " tensor([-0.0758]),\n",
       " tensor([-0.0821]),\n",
       " tensor([-0.2702]),\n",
       " tensor([-0.2167]),\n",
       " tensor([-0.2180]),\n",
       " tensor([-0.1280]),\n",
       " tensor([-0.0795]),\n",
       " tensor([-0.2142]),\n",
       " tensor([-0.2020]),\n",
       " tensor([-0.1337]),\n",
       " tensor([-0.1740]),\n",
       " tensor([-0.0719]),\n",
       " tensor([-0.1373]),\n",
       " tensor([-0.1545]),\n",
       " tensor([-0.2161]),\n",
       " tensor([-0.1398]),\n",
       " tensor([-0.1692]),\n",
       " tensor([-0.1738]),\n",
       " tensor([-0.2052]),\n",
       " tensor([-0.2276]),\n",
       " tensor([-0.1914]),\n",
       " tensor([-0.1619]),\n",
       " tensor([-0.1918]),\n",
       " tensor([-0.1489]),\n",
       " tensor([-0.0870]),\n",
       " tensor([-0.0632]),\n",
       " tensor([0.0159]),\n",
       " tensor([-0.1366]),\n",
       " tensor([-0.1512]),\n",
       " tensor([-0.1986]),\n",
       " tensor([-0.2491]),\n",
       " tensor([-0.0806]),\n",
       " tensor([-0.2148]),\n",
       " tensor([-0.2727]),\n",
       " tensor([-0.1177]),\n",
       " tensor([-0.1665]),\n",
       " tensor([-0.1969]),\n",
       " tensor([-0.1978]),\n",
       " tensor([-0.1956]),\n",
       " tensor([-0.0736]),\n",
       " tensor([-0.1842]),\n",
       " tensor([-0.1521]),\n",
       " tensor([-0.1454]),\n",
       " tensor([-0.0565]),\n",
       " tensor([-0.2396]),\n",
       " tensor([-0.0517]),\n",
       " tensor([-0.1781]),\n",
       " tensor([-0.1828]),\n",
       " tensor([-0.1324]),\n",
       " tensor([-0.1390]),\n",
       " tensor([-0.3119]),\n",
       " tensor([-0.2345]),\n",
       " tensor([-0.0206]),\n",
       " tensor([-0.0430]),\n",
       " tensor([-0.0555]),\n",
       " tensor([-0.2079]),\n",
       " tensor([-0.1280]),\n",
       " tensor([-0.2659]),\n",
       " tensor([-0.2303]),\n",
       " tensor([-0.0745]),\n",
       " tensor([-0.1655]),\n",
       " tensor([-0.0992]),\n",
       " tensor([-0.1701]),\n",
       " tensor([-0.0992]),\n",
       " tensor([-0.1644]),\n",
       " tensor([-0.1241]),\n",
       " tensor([-0.1140]),\n",
       " tensor([-0.1596]),\n",
       " tensor([-0.2375]),\n",
       " tensor([-0.1840]),\n",
       " tensor([-0.1280]),\n",
       " tensor([-0.0761]),\n",
       " tensor([-0.0038]),\n",
       " tensor([-0.1433]),\n",
       " tensor([-0.1780]),\n",
       " tensor([-0.1500]),\n",
       " tensor([-0.1640]),\n",
       " tensor([-0.0459]),\n",
       " tensor([-0.2439]),\n",
       " tensor([-0.0850]),\n",
       " tensor([-0.2467]),\n",
       " tensor([-0.1663]),\n",
       " tensor([-0.0935]),\n",
       " tensor([-0.1019]),\n",
       " tensor([-0.1726]),\n",
       " tensor([-0.1276]),\n",
       " tensor([-0.1203]),\n",
       " tensor([-0.2796]),\n",
       " tensor([-0.1322]),\n",
       " tensor([-0.1138]),\n",
       " tensor([-0.0892]),\n",
       " tensor([-0.1372]),\n",
       " tensor([-0.1077]),\n",
       " tensor([-0.0538]),\n",
       " tensor([-0.1365]),\n",
       " tensor([-0.1239]),\n",
       " tensor([-0.0818]),\n",
       " tensor([-0.0924]),\n",
       " tensor([-0.1687]),\n",
       " tensor([-0.1057]),\n",
       " tensor([-0.0331]),\n",
       " tensor([-0.2368]),\n",
       " tensor([-0.0966]),\n",
       " tensor([-0.0027]),\n",
       " tensor([-0.0624]),\n",
       " tensor([-0.0528]),\n",
       " tensor([-0.1154]),\n",
       " tensor([-0.1682]),\n",
       " tensor([-0.2264]),\n",
       " tensor([-0.1073]),\n",
       " tensor([-0.1998]),\n",
       " tensor([0.0266]),\n",
       " tensor([-0.1117]),\n",
       " tensor([-0.2003]),\n",
       " tensor([-0.1361]),\n",
       " tensor([-0.1731]),\n",
       " tensor([-0.0152]),\n",
       " tensor([-0.0520]),\n",
       " tensor([-0.2625]),\n",
       " tensor([-0.0991]),\n",
       " tensor([0.0111]),\n",
       " tensor([-0.1199]),\n",
       " tensor([-0.1101]),\n",
       " tensor([-0.1120]),\n",
       " tensor([-0.2278]),\n",
       " tensor([-0.1128]),\n",
       " tensor([-0.2671]),\n",
       " tensor([-0.1222]),\n",
       " tensor([-0.2296]),\n",
       " tensor([-0.2253]),\n",
       " tensor([-0.2052]),\n",
       " tensor([-0.1313]),\n",
       " tensor([-0.1390]),\n",
       " tensor([-0.1014]),\n",
       " tensor([-0.2790]),\n",
       " tensor([-0.0841]),\n",
       " tensor([-0.1855]),\n",
       " tensor([-0.1677]),\n",
       " tensor([-0.2101]),\n",
       " tensor([-0.0557]),\n",
       " tensor([-0.2607]),\n",
       " tensor([-0.2249]),\n",
       " tensor([-0.0891]),\n",
       " tensor([-0.0409]),\n",
       " tensor([-0.1879]),\n",
       " tensor([-0.1409]),\n",
       " tensor([-0.1525]),\n",
       " tensor([-0.1755]),\n",
       " tensor([-0.1311]),\n",
       " tensor([-0.1594]),\n",
       " tensor([-0.0217]),\n",
       " tensor([-0.1538]),\n",
       " tensor([-0.1757]),\n",
       " tensor([-0.2347]),\n",
       " tensor([-0.1250]),\n",
       " tensor([-0.1247]),\n",
       " tensor([-0.1740]),\n",
       " tensor([-0.1417]),\n",
       " tensor([-0.1124]),\n",
       " tensor([-0.1986]),\n",
       " tensor([-0.1281]),\n",
       " tensor([-0.1470]),\n",
       " tensor([-0.1119]),\n",
       " tensor([-0.1194]),\n",
       " tensor([-0.0843]),\n",
       " tensor([-0.2134]),\n",
       " tensor([-0.2265]),\n",
       " tensor([-0.0174]),\n",
       " tensor([-0.1098]),\n",
       " tensor([-0.2008]),\n",
       " tensor([-0.2104]),\n",
       " tensor([0.0144]),\n",
       " tensor([-0.1705]),\n",
       " tensor([-0.1292]),\n",
       " tensor([-0.2574]),\n",
       " tensor([-0.2260]),\n",
       " tensor([-0.0491]),\n",
       " tensor([-0.0815]),\n",
       " tensor([-0.1157]),\n",
       " tensor([-0.2017]),\n",
       " tensor([-0.0848]),\n",
       " tensor([-0.0396]),\n",
       " tensor([-0.2852]),\n",
       " tensor([-0.1545]),\n",
       " tensor([-0.1879]),\n",
       " tensor([-0.1846]),\n",
       " tensor([-0.1535]),\n",
       " tensor([-0.0966]),\n",
       " tensor([-0.1508]),\n",
       " tensor([-0.1855]),\n",
       " tensor([-0.1517]),\n",
       " tensor([-0.1315]),\n",
       " tensor([-0.1306]),\n",
       " tensor([-0.2044]),\n",
       " tensor([-0.1066]),\n",
       " tensor([-0.1701]),\n",
       " tensor([-0.0938]),\n",
       " tensor([-0.0911]),\n",
       " tensor([-0.1199]),\n",
       " tensor([-0.1272]),\n",
       " tensor([-0.1298]),\n",
       " tensor([-0.1588]),\n",
       " tensor([-0.1918]),\n",
       " tensor([-0.2673]),\n",
       " tensor([-0.0753]),\n",
       " tensor([-0.0350]),\n",
       " tensor([-0.1125]),\n",
       " tensor([-0.0852]),\n",
       " tensor([-0.0794]),\n",
       " tensor([-0.3585]),\n",
       " tensor([-0.0794]),\n",
       " tensor([-0.2489]),\n",
       " tensor([-0.0800]),\n",
       " tensor([-0.1548]),\n",
       " tensor([-0.1351]),\n",
       " tensor([-0.1242]),\n",
       " tensor([-0.1186]),\n",
       " tensor([-0.1315]),\n",
       " tensor([-0.1478]),\n",
       " tensor([-0.1826]),\n",
       " tensor([-0.2000]),\n",
       " tensor([-0.1526]),\n",
       " tensor([-0.2146]),\n",
       " tensor([-0.2480]),\n",
       " tensor([-0.1491]),\n",
       " tensor([-0.1443]),\n",
       " tensor([-0.1429]),\n",
       " tensor([-0.1549]),\n",
       " tensor([-0.2128]),\n",
       " tensor([-0.0264]),\n",
       " tensor([-0.1987]),\n",
       " tensor([-0.0993]),\n",
       " tensor([-0.2049]),\n",
       " tensor([-0.0481]),\n",
       " tensor([-0.2155]),\n",
       " tensor([-0.1124]),\n",
       " tensor([-0.2108]),\n",
       " tensor([-0.3414]),\n",
       " tensor([-0.1803]),\n",
       " tensor([-0.2355]),\n",
       " tensor([-0.1428]),\n",
       " tensor([-0.0763]),\n",
       " tensor([-0.1437]),\n",
       " tensor([-0.2510]),\n",
       " tensor([-0.1694]),\n",
       " tensor([-0.0563]),\n",
       " tensor([-0.2010]),\n",
       " tensor([-0.1790]),\n",
       " tensor([-0.1698]),\n",
       " tensor([-0.1503]),\n",
       " tensor([-0.1088]),\n",
       " tensor([-0.0873]),\n",
       " tensor([-0.1341]),\n",
       " tensor([-0.0218]),\n",
       " tensor([-0.1844]),\n",
       " tensor([-0.1979]),\n",
       " tensor([-0.0879]),\n",
       " tensor([-0.0590]),\n",
       " tensor([-0.0564]),\n",
       " tensor([-0.0763]),\n",
       " tensor([-0.1104]),\n",
       " tensor([-0.1623]),\n",
       " tensor([-0.0705]),\n",
       " tensor([-0.2061]),\n",
       " tensor([-0.2687]),\n",
       " tensor([-0.2252]),\n",
       " tensor([-0.2269]),\n",
       " tensor([-0.1117]),\n",
       " tensor([-0.0785]),\n",
       " tensor([-0.2209]),\n",
       " tensor([-0.1621]),\n",
       " tensor([-0.1071]),\n",
       " tensor([-0.0763]),\n",
       " tensor([-0.0570]),\n",
       " tensor([-0.0844]),\n",
       " tensor([-0.1015]),\n",
       " tensor([-0.1939]),\n",
       " tensor([-0.0524]),\n",
       " tensor([-0.0417]),\n",
       " tensor([-0.0983]),\n",
       " tensor([0.0062]),\n",
       " tensor([-0.1632]),\n",
       " tensor([-0.1431]),\n",
       " tensor([-0.1934]),\n",
       " tensor([-0.0769]),\n",
       " tensor([-0.2413]),\n",
       " tensor([-0.0972]),\n",
       " tensor([-0.2396]),\n",
       " tensor([-0.2518]),\n",
       " tensor([-0.2056]),\n",
       " tensor([-0.1089]),\n",
       " tensor([-0.1654]),\n",
       " tensor([-0.0804]),\n",
       " tensor([-0.2107]),\n",
       " tensor([-0.1718]),\n",
       " tensor([-0.0995]),\n",
       " tensor([-0.2030]),\n",
       " tensor([-0.0867]),\n",
       " tensor([-0.0973]),\n",
       " tensor([-0.1440]),\n",
       " tensor([-0.0866]),\n",
       " tensor([-0.0371]),\n",
       " tensor([0.0041]),\n",
       " tensor([-0.1006]),\n",
       " tensor([-0.0604]),\n",
       " tensor([-0.1809]),\n",
       " tensor([-0.0389]),\n",
       " tensor([-0.1448]),\n",
       " tensor([-0.1588]),\n",
       " tensor([-0.1108]),\n",
       " tensor([-0.0823]),\n",
       " tensor([-0.1401]),\n",
       " tensor([-0.2627]),\n",
       " tensor([-0.2265]),\n",
       " tensor([-0.1489]),\n",
       " tensor([-0.0670]),\n",
       " tensor([-0.1796]),\n",
       " tensor([-0.0728]),\n",
       " tensor([-0.2445]),\n",
       " tensor([-0.1099]),\n",
       " tensor([-0.1335]),\n",
       " tensor([-0.0731]),\n",
       " tensor([-0.1342]),\n",
       " tensor([-0.2181]),\n",
       " tensor([-0.2247]),\n",
       " tensor([-0.1274]),\n",
       " tensor([-0.0629]),\n",
       " tensor([-0.1708]),\n",
       " tensor([-0.1100]),\n",
       " tensor([-0.1735]),\n",
       " tensor([-0.0504]),\n",
       " tensor([-0.0995]),\n",
       " tensor([-0.1829]),\n",
       " tensor([-0.0804]),\n",
       " tensor([-0.2055]),\n",
       " tensor([-0.2258]),\n",
       " tensor([-0.1563]),\n",
       " tensor([-0.1475]),\n",
       " tensor([-0.1196]),\n",
       " tensor([-0.0253]),\n",
       " tensor([-0.1014]),\n",
       " tensor([-0.0763]),\n",
       " tensor([-0.1563]),\n",
       " tensor([-0.1714]),\n",
       " tensor([-0.2447]),\n",
       " tensor([-0.1278]),\n",
       " tensor([-0.0352]),\n",
       " tensor([-0.2221]),\n",
       " tensor([-0.0369]),\n",
       " tensor([-0.0821]),\n",
       " tensor([-0.2002]),\n",
       " tensor([-0.1795]),\n",
       " tensor([-0.0971]),\n",
       " tensor([-0.0686]),\n",
       " tensor([-0.2179]),\n",
       " tensor([-0.1158]),\n",
       " tensor([-0.1371]),\n",
       " tensor([-0.0793]),\n",
       " tensor([-0.1273]),\n",
       " tensor([-0.0954]),\n",
       " tensor([-0.0216]),\n",
       " tensor([-0.1543]),\n",
       " tensor([-0.1311]),\n",
       " tensor([-0.1543]),\n",
       " tensor([-0.1420]),\n",
       " tensor([-0.0626]),\n",
       " tensor([-0.0910]),\n",
       " tensor([-0.1850]),\n",
       " tensor([-0.1595]),\n",
       " tensor([-0.3082]),\n",
       " tensor([-0.1959]),\n",
       " tensor([-0.1599]),\n",
       " tensor([-0.0555]),\n",
       " tensor([-0.1675]),\n",
       " tensor([-0.0901]),\n",
       " tensor([-0.0675]),\n",
       " tensor([-0.1597]),\n",
       " tensor([-0.0402]),\n",
       " tensor([-0.2372]),\n",
       " tensor([-0.1551]),\n",
       " tensor([-0.1357]),\n",
       " tensor([-0.0991]),\n",
       " tensor([-0.1487]),\n",
       " tensor([-0.1555]),\n",
       " tensor([-0.1728]),\n",
       " tensor([-0.1709]),\n",
       " tensor([-0.0469]),\n",
       " tensor([-0.1253]),\n",
       " tensor([-0.1176]),\n",
       " tensor([-0.1689]),\n",
       " tensor([-0.1679]),\n",
       " tensor([-0.1875]),\n",
       " tensor([-0.1443]),\n",
       " tensor([-0.1461]),\n",
       " tensor([-0.3130]),\n",
       " tensor([-0.0199]),\n",
       " tensor([-0.0839]),\n",
       " tensor([-0.1276]),\n",
       " tensor([-0.1692]),\n",
       " tensor([-0.1323]),\n",
       " tensor([-0.1016]),\n",
       " tensor([-0.0979]),\n",
       " tensor([-0.2104]),\n",
       " tensor([-0.1885]),\n",
       " tensor([-0.2139]),\n",
       " tensor([-0.1308]),\n",
       " tensor([-0.2210]),\n",
       " tensor([-0.1384]),\n",
       " tensor([-0.2552]),\n",
       " tensor([-0.0966]),\n",
       " tensor([-0.1885]),\n",
       " tensor([-0.1493]),\n",
       " tensor([-0.2295]),\n",
       " tensor([-0.3119]),\n",
       " tensor([-0.2106]),\n",
       " tensor([-0.1673]),\n",
       " tensor([-0.0943]),\n",
       " tensor([-0.1314]),\n",
       " tensor([-0.0952]),\n",
       " tensor([-0.1721]),\n",
       " tensor([-0.2361]),\n",
       " tensor([-0.1351]),\n",
       " tensor([-0.0304]),\n",
       " tensor([-0.0993]),\n",
       " tensor([-0.1015]),\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(model=alphafold2, dataloaders=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.4.4.2. <a id='toc17_4_4_2_'></a>[PyTorch lightning加载权重后预测](#toc0_)\n",
    "需要手动：\n",
    "- model.eval()\n",
    "- with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_alphafold2 = AlphaFold2Wrapper.load_from_checkpoint('./lightning_logs/version_0/checkpoints/epoch=9-step=790.ckpt')\n",
    "\n",
    "# 进行预测/推理\n",
    "pretrained_alphafold2.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = pretrained_alphafold2(features.to('cuda:0'))\n",
    "y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.4.4.3. <a id='toc17_4_4_3_'></a>[提取权重后加载至纯PyTorch模型](#toc0_)\n",
    "从checkpoint中`提取模型的权重参数`，`修改相关格式`后再加载到纯PyTorch的模型中，就是普通又熟悉的PyTorch的预测方式了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268120/3329860571.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 9,\n",
       " 'global_step': 790,\n",
       " 'pytorch-lightning_version': '2.4.0',\n",
       " 'state_dict': OrderedDict([('demo_model.hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('demo_model.hidden.0.bias',\n",
       "               tensor([3.3954], device='cuda:0'))]),\n",
       " 'loops': {'fit_loop': {'state_dict': {},\n",
       "   'epoch_loop.state_dict': {'_batches_that_stepped': 790},\n",
       "   'epoch_loop.batch_progress': {'total': {'ready': 790,\n",
       "     'completed': 790,\n",
       "     'started': 790,\n",
       "     'processed': 790},\n",
       "    'current': {'ready': 79, 'completed': 79, 'started': 79, 'processed': 79},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_loop.scheduler_progress': {'total': {'ready': 0, 'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.automatic_optimization.state_dict': {},\n",
       "   'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 790,\n",
       "       'completed': 790},\n",
       "      'current': {'ready': 79, 'completed': 79}},\n",
       "     'zero_grad': {'total': {'ready': 790, 'completed': 790, 'started': 790},\n",
       "      'current': {'ready': 79, 'completed': 79, 'started': 79}}}},\n",
       "   'epoch_loop.manual_optimization.state_dict': {},\n",
       "   'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0,\n",
       "     'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.val_loop.state_dict': {},\n",
       "   'epoch_loop.val_loop.batch_progress': {'total': {'ready': 79,\n",
       "     'completed': 79,\n",
       "     'started': 79,\n",
       "     'processed': 79},\n",
       "    'current': {'ready': 79, 'completed': 79, 'started': 79, 'processed': 79},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_progress': {'total': {'ready': 10,\n",
       "     'completed': 9,\n",
       "     'started': 10,\n",
       "     'processed': 10},\n",
       "    'current': {'ready': 10, 'completed': 9, 'started': 10, 'processed': 10}}},\n",
       "  'validate_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'test_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'predict_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}},\n",
       " 'callbacks': {\"ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': None,\n",
       "   'best_model_score': None,\n",
       "   'best_model_path': '/bmp/backup/zhaosy/ws/PyTorch_learning/lightning_logs/version_0/checkpoints/epoch=9-step=790.ckpt',\n",
       "   'current_score': None,\n",
       "   'dirpath': '/bmp/backup/zhaosy/ws/PyTorch_learning/lightning_logs/version_0/checkpoints',\n",
       "   'best_k_models': {},\n",
       "   'kth_best_model_path': '',\n",
       "   'kth_value': tensor(inf),\n",
       "   'last_model_path': ''}},\n",
       " 'optimizer_states': [{'state': {},\n",
       "   'param_groups': [{'lr': 0.01,\n",
       "     'momentum': 0,\n",
       "     'dampening': 0,\n",
       "     'weight_decay': 0,\n",
       "     'nesterov': False,\n",
       "     'maximize': False,\n",
       "     'foreach': None,\n",
       "     'differentiable': False,\n",
       "     'fused': None,\n",
       "     'params': [0, 1]}]}],\n",
       " 'lr_schedulers': [],\n",
       " 'hparams_name': 'kwargs',\n",
       " 'hyper_parameters': {'learning_rate': 0.01}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = './lightning_logs/version_0/checkpoints/epoch=9-step=790.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "checkpoint  # checkpoint的贮存格式，其中 'state_dict'就是模型权重信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AlphaFold2Wrapper(\n",
       "   (demo_model): AlphaFold2(\n",
       "     (hidden): Sequential(\n",
       "       (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (loss_fn): MSELoss()\n",
       " ),\n",
       " OrderedDict([('demo_model.hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('demo_model.hidden.0.bias',\n",
       "               tensor([3.3954], device='cuda:0'))]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphafold2, checkpoint['state_dict']    # with AlphaFold2Wrapper, 多了demo_model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_model.hidden.0.weight\n",
      "demo_model.hidden.0.bias\n"
     ]
    }
   ],
   "source": [
    "for param in checkpoint['state_dict']:\n",
    "    print(param)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `纯PyTorch的state_dict`如下，`如上的checkpoint`中的state_dict`不符合`相应格式，需要进行更改："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AlphaFold2(\n",
       "   (hidden): Sequential(\n",
       "     (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " OrderedDict([('hidden.0.weight', tensor([[-0.5207,  0.0861]])),\n",
       "              ('hidden.0.bias', tensor([0.0467]))]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphafold_with_pure_pytorch = AlphaFold2()\n",
    "alphafold_with_pure_pytorch, alphafold_with_pure_pytorch.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 更改操作如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = checkpoint['state_dict']\n",
    "\n",
    "# 把demo_model.删除即可\n",
    "for key in model_weights:\n",
    "    model_weights[key.replace(\"demo_model.\", \"\")] = model_weights.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('hidden.0.bias', tensor([3.3954], device='cuda:0'))]),\n",
       " OrderedDict([('hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('hidden.0.bias', tensor([3.3954], device='cuda:0'))]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['state_dict'], model_weights # 都更改了，什么鬼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4124],\n",
       "        [3.3868],\n",
       "        [3.3909],\n",
       "        ...,\n",
       "        [3.4022],\n",
       "        [3.3898],\n",
       "        [3.3934]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新实例化一个新的对象\n",
    "alphafold_with_pure_pytorch = AlphaFold2()\n",
    "\n",
    "# 加载修改后的权重\n",
    "alphafold_with_pure_pytorch.load_state_dict(model_weights)  # 加载修改后的model_weights\n",
    "\n",
    "# 进行预测/推理\n",
    "alphafold_with_pure_pytorch.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = alphafold_with_pure_pytorch(features)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. <a id='toc18_'></a>[Torchvision](#toc0_)\n",
    "Torchvision Docs: [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision version: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision \n",
    "\n",
    "\n",
    "print('torchvision version:', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1. <a id='toc18_1_'></a>[Models](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.1. <a id='toc18_1_1_'></a>[可用模型](#toc0_)\n",
    "可用`模型`见：[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'deeplabv3_mobilenet_v3_large',\n",
       " 'deeplabv3_resnet101',\n",
       " 'deeplabv3_resnet50',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'fasterrcnn_mobilenet_v3_large_320_fpn',\n",
       " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
       " 'fasterrcnn_resnet50_fpn',\n",
       " 'fasterrcnn_resnet50_fpn_v2',\n",
       " 'fcn_resnet101',\n",
       " 'fcn_resnet50',\n",
       " 'fcos_resnet50_fpn',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'keypointrcnn_resnet50_fpn',\n",
       " 'lraspp_mobilenet_v3_large',\n",
       " 'maskrcnn_resnet50_fpn',\n",
       " 'maskrcnn_resnet50_fpn_v2',\n",
       " 'maxvit_t',\n",
       " 'mc3_18',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mvit_v1_b',\n",
       " 'mvit_v2_s',\n",
       " 'quantized_googlenet',\n",
       " 'quantized_inception_v3',\n",
       " 'quantized_mobilenet_v2',\n",
       " 'quantized_mobilenet_v3_large',\n",
       " 'quantized_resnet18',\n",
       " 'quantized_resnet50',\n",
       " 'quantized_resnext101_32x8d',\n",
       " 'quantized_resnext101_64x4d',\n",
       " 'quantized_shufflenet_v2_x0_5',\n",
       " 'quantized_shufflenet_v2_x1_0',\n",
       " 'quantized_shufflenet_v2_x1_5',\n",
       " 'quantized_shufflenet_v2_x2_0',\n",
       " 'r2plus1d_18',\n",
       " 'r3d_18',\n",
       " 'raft_large',\n",
       " 'raft_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'retinanet_resnet50_fpn',\n",
       " 'retinanet_resnet50_fpn_v2',\n",
       " 's3d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'ssd300_vgg16',\n",
       " 'ssdlite320_mobilenet_v3_large',\n",
       " 'swin3d_b',\n",
       " 'swin3d_s',\n",
       " 'swin3d_t',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models \n",
    "\n",
    "\n",
    "models.list_models()    # List all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.2. <a id='toc18_1_2_'></a>[下载模型和权重](#toc0_)\n",
    "可用`权重`见：[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "alexnet = models.get_model(name='alexnet')\n",
    "\n",
    "# 1. Get weight\n",
    "weights = models.get_weight('AlexNet_Weights.IMAGENET1K_V1')\n",
    "# weights = models.get_weight('ResNet50_Weights.IMAGENET1K_V1')\n",
    "# weights = models.get_weight('ResNet50_Weights.IMAGENET1K_V2')\n",
    "\n",
    "# 2. (Recommendation) Get weight with model name\n",
    "weights = models.get_model_weights(name='alexnet')\n",
    "\n",
    "# Get the state_dict parameters from loaded weights wrapper\n",
    "state_dict = weights.IMAGENET1K_V1.get_state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.3. <a id='toc18_1_3_'></a>[模型加载权重](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "alexnet.load_state_dict(state_dict=state_dict)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.4. <a id='toc18_1_4_'></a>[总结](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrapper to the following:\n",
    "def get_pretrained_model(model_name:str)-> torch.nn.Module:\n",
    "    '''Default to get: IMAGENET1K_V1'''\n",
    "    model = models.get_model(name=model_name)\n",
    "    weight_wrapper = models.get_model_weights(name=model_name)\n",
    "    state_dict = weight_wrapper.IMAGENET1K_V1.get_state_dict()\n",
    "    model.load_state_dict(state_dict=state_dict)\n",
    "    return model\n",
    "\n",
    "# pretrained_model = get_pretrained_model(model_name='resnet50')\n",
    "pretrained_model = get_pretrained_model(model_name='alexnet')\n",
    "pretrained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2. <a id='toc18_2_'></a>[Dataset](#toc0_)\n",
    "torchvision Docs: [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)  \n",
    "\n",
    "1. torchvision的datasets有很多，如：\n",
    "  \n",
    "    - Image classification\n",
    "        - FashionMNIST(root[, train, transform, ...])\n",
    "        - MNIST(root[, train, transform, ...])\n",
    "    - Image detection or segmentation\n",
    "        - CocoDetection(root, annFile[, transform, ...])\n",
    "    - Video classification\n",
    "        - HMDB51(root, annotation_path, frames_per_clip)\n",
    "    - Video prediction\n",
    "        - MovingMNIST(root[, split, split_ratio, ...])\n",
    "\n",
    "2. 另外，还可以自定义数据集，函数如下：\n",
    "\n",
    "    - Base classes for custom datasets\n",
    "        - `DatasetFolder`(root, loader[, extensions, ...])    # A generic data loader.\n",
    "        - `ImageFolder`(root, transform, ...)                 # A generic data loader where the images are arranged in this way by default: .\n",
    "        - `VisionDataset`([root, transforms, transform, ...]) # Base Class For making datasets which are compatible with torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.FashionMNIST,\n",
       " torchvision.datasets.mnist.FashionMNIST)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets \n",
    "from torchvision import transforms \n",
    "\n",
    "\n",
    "dbs = './data/'\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),                                  # PIL转换为tensor格式\n",
    "    transforms.Normalize(mean=(0.5,), std=(1.0,))           # 标准化\n",
    "])\n",
    "\n",
    "\n",
    "train_datasets = datasets.FashionMNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=trans\n",
    ")\n",
    "\n",
    "test_datasets = datasets.FashionMNIST(\n",
    "    root=dbs,  \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=trans\n",
    ")\n",
    "\n",
    "type(train_datasets), type(test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. <a id='toc19_'></a>[多模态 (ML, MultiModal Learning)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.1. <a id='toc19_1_'></a>[特征融合](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1.1. <a id='toc19_1_1_'></a>[concatenate融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1880,  0.7078],\n",
       "        [ 0.1063, -0.2480],\n",
       "        [-0.8586, -1.1807],\n",
       "        [ 0.0430, -0.5358],\n",
       "        [-0.1850,  0.2209],\n",
       "        [-0.3914, -0.1025],\n",
       "        [-0.1250,  0.0920],\n",
       "        [-0.1198, -0.2499],\n",
       "        [ 0.3329, -0.4342],\n",
       "        [ 0.3291, -0.1004],\n",
       "        [ 0.1111, -0.3002],\n",
       "        [ 0.0725, -0.6321],\n",
       "        [ 0.1208, -0.0259],\n",
       "        [ 0.1791,  0.1676],\n",
       "        [-0.0622,  0.1658],\n",
       "        [-0.3677,  0.0363],\n",
       "        [ 0.1491,  0.3804],\n",
       "        [ 0.2784, -0.0216],\n",
       "        [ 0.4644,  0.2349],\n",
       "        [-0.2483, -0.2576]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class ConcatenationFusion(nn.Module):\n",
    "    def __init__(self, text_dim, hidden_dim, image_dim, num_classes): \n",
    "        super().__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, text, image):\n",
    "        text_embed = self.text_fc(text)\n",
    "        image_embed = self.image_fc(image)\n",
    "        embed = torch.cat((text_embed, image_embed), dim=0)  # 拼接融合\n",
    "        return self.classifier(embed)\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "text = torch.randn(batch_size, text_dim)\n",
    "image = torch.randn(batch_size, image_dim)\n",
    "\n",
    "model = ConcatenationFusion(text_dim, hidden_dim, image_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1.2. <a id='toc19_1_2_'></a>[加权融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0170,  0.1555],\n",
       "        [ 0.0385, -0.1914],\n",
       "        [ 0.1036, -0.0659],\n",
       "        [-0.1466,  0.0531],\n",
       "        [-0.6206, -0.1544],\n",
       "        [-0.3399, -0.2346],\n",
       "        [-0.4070, -0.4799],\n",
       "        [-0.5645,  0.0331],\n",
       "        [-0.1366,  0.0349],\n",
       "        [-0.7428,  0.2844]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class WeightedFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(WeightedFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.text_weight = nn.Parameter(torch.randn(1))\n",
    "        self.image_weight = nn.Parameter(torch.randn(1))\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))\n",
    "        image_feat = torch.relu(self.image_fc(image))\n",
    "        combined = self.text_weight * text_feat + self.image_weight * image_feat  # 加权融合    \n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = WeightedFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1.3. <a id='toc19_1_3_'></a>[元素级融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1509,  0.3871],\n",
       "        [-0.8885, -0.3169],\n",
       "        [-0.8079, -0.5716],\n",
       "        [-0.7261, -0.1003],\n",
       "        [-0.0701, -0.5493],\n",
       "        [-0.8524, -0.3561],\n",
       "        [-1.1294, -0.3606],\n",
       "        [-0.8689, -0.3579],\n",
       "        [-0.9081, -0.6309],\n",
       "        [-1.0464, -0.4657]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ElementWiseFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(ElementWiseFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))\n",
    "        image_feat = torch.relu(self.image_fc(image))\n",
    "        combined = text_feat + image_feat  # 元素级融合\n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = ElementWiseFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1.4. <a id='toc19_1_4_'></a>[张量融合](#toc0_)\n",
    "通过构建高纬度张量来表示不同模态之间的交互关系，捕捉多阶的特征交互信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6352, -0.3399],\n",
       "        [-0.5262, -0.7934],\n",
       "        [-0.8739,  0.2307],\n",
       "        [ 0.1405,  0.0721],\n",
       "        [-0.4297, -0.4880],\n",
       "        [-1.2108, -0.8075],\n",
       "        [ 0.0606,  0.8979],\n",
       "        [ 0.2691,  0.4548],\n",
       "        [-0.1921, -0.7102],\n",
       "        [-0.3202,  0.6673]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class TensorFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(TensorFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        # 双线性层进行张量融合\n",
    "        self.bilinear = nn.Bilinear(hidden_dim, hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))  # (batch, hidden_dim)\n",
    "        image_feat = torch.relu(self.image_fc(image))  # (batch, hidden_dim)\n",
    "        # 张量融合\n",
    "        fused_feat = self.bilinear(text_feat, image_feat)\n",
    "        output = self.classifier(fused_feat)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = TensorFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1.5. <a id='toc19_1_5_'></a>[注意力机制融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2901, -0.0059],\n",
       "        [-0.0250,  0.1197],\n",
       "        [-0.2061, -0.1256],\n",
       "        [-0.3189,  0.2029],\n",
       "        [-0.0152,  0.1354],\n",
       "        [ 0.0839,  0.0148],\n",
       "        [-0.1621,  0.0316],\n",
       "        [ 0.0181,  0.2491],\n",
       "        [-0.0624,  0.0125],\n",
       "        [-0.2978, -0.0618]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models/attention_fusion.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AttentionFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(AttentionFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))  # (batch, hidden_dim)\n",
    "        image_feat = torch.relu(self.image_fc(image))  # (batch, hidden_dim)\n",
    "        # 拼接特征用于计算注意力\n",
    "        combined = torch.cat((text_feat, image_feat), dim=1)\n",
    "        attention_weights = torch.sigmoid(self.attention(combined))  # (batch, 1)\n",
    "        # 加权融合\n",
    "        fused_feat = attention_weights * text_feat + (1 - attention_weights) * image_feat\n",
    "        output = self.classifier(fused_feat)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = AttentionFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1.6. <a id='toc19_1_6_'></a>[高阶融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0248, -0.0359],\n",
       "        [ 0.0652, -0.0690],\n",
       "        [ 0.0805, -0.0663],\n",
       "        [ 0.0584, -0.0093],\n",
       "        [ 0.0452, -0.0590],\n",
       "        [ 0.0372, -0.0490],\n",
       "        [ 0.0078, -0.0247],\n",
       "        [ 0.0474, -0.0621],\n",
       "        [ 0.0738, -0.0500],\n",
       "        [ 0.0369, -0.0351]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models/high_order_fusion.py\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class HighOrderFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim, order=2):\n",
    "        super(HighOrderFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.order = order\n",
    "        # 高阶特征交互\n",
    "        self.high_order_fc = nn.Linear(hidden_dim ** order, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))  # (batch, hidden_dim)\n",
    "        image_feat = torch.relu(self.image_fc(image))  # (batch, hidden_dim)\n",
    "        # 高阶交互，通过外积实现\n",
    "        if self.order == 2:\n",
    "            fused_feat = torch.bmm(text_feat.unsqueeze(2), image_feat.unsqueeze(1))  # (batch, hidden_dim, hidden_dim)\n",
    "            fused_feat = fused_feat.view(fused_feat.size(0), -1)  # (batch, hidden_dim^2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"当前仅支持二阶融合\")\n",
    "        fused_feat = torch.relu(self.high_order_fc(fused_feat))\n",
    "        output = self.classifier(fused_feat)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = HighOrderFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.2. <a id='toc19_2_'></a>[简单示例](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# 模拟的多模态数据，假设基因组、转录组、代谢组的数据分别有不同维度\n",
    "np.random.seed(42)\n",
    "genomics_data = np.random.rand(1000, 500)  # 基因组数据 (1000 samples, 500 features)\n",
    "transcriptomics_data = np.random.rand(1000, 300)  # 转录组数据 (1000 samples, 300 features)\n",
    "metabolomics_data = np.random.rand(1000, 100)  # 代谢组数据 (1000 samples, 100 features)\n",
    "\n",
    "# 标签 (假设为二分类问题：健康或疾病)\n",
    "labels = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# 归一化数据\n",
    "scaler = StandardScaler()\n",
    "genomics_data = scaler.fit_transform(genomics_data)\n",
    "transcriptomics_data = scaler.fit_transform(transcriptomics_data)\n",
    "metabolomics_data = scaler.fit_transform(metabolomics_data)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train_genomics, X_test_genomics, X_train_transcript, X_test_transcript, X_train_metabol, X_test_metabol, y_train, y_test = train_test_split(\n",
    "    genomics_data, \n",
    "    transcriptomics_data, \n",
    "    metabolomics_data, \n",
    "    labels, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# 转换为Tensor\n",
    "X_train_genomics = torch.tensor(X_train_genomics, dtype=torch.float32)\n",
    "X_test_genomics = torch.tensor(X_test_genomics, dtype=torch.float32)\n",
    "X_train_transcript = torch.tensor(X_train_transcript, dtype=torch.float32)\n",
    "X_test_transcript = torch.tensor(X_test_transcript, dtype=torch.float32)\n",
    "X_train_metabol = torch.tensor(X_train_metabol, dtype=torch.float32)\n",
    "X_test_metabol = torch.tensor(X_test_metabol, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, genomics, transcriptomics, metabolomics, labels):\n",
    "        self.genomics = genomics\n",
    "        self.transcriptomics = transcriptomics\n",
    "        self.metabolomics = metabolomics\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.genomics[idx], self.transcriptomics[idx], self.metabolomics[idx], self.labels[idx])\n",
    "\n",
    "\n",
    "# 创建训练集和测试集的DataLoader\n",
    "train_dataset = MultiModalDataset(X_train_genomics, X_train_transcript, X_train_metabol, y_train)\n",
    "test_dataset = MultiModalDataset(X_test_genomics, X_test_transcript, X_test_metabol, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "        \n",
    "        # 基因组数据的网络分支\n",
    "        self.genomics_fc1 = nn.Linear(500, 256)\n",
    "        self.genomics_fc2 = nn.Linear(256, 128)\n",
    "\n",
    "        # 转录组数据的网络分支\n",
    "        self.transcript_fc1 = nn.Linear(300, 256)\n",
    "        self.transcript_fc2 = nn.Linear(256, 128)\n",
    "\n",
    "        # 代谢组数据的网络分支\n",
    "        self.metabol_fc1 = nn.Linear(100, 128)\n",
    "        self.metabol_fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # 融合层\n",
    "        self.fc1 = nn.Linear(128 + 128 + 64, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)  # 假设二分类问题\n",
    "\n",
    "    def forward(self, genomics, transcriptomics, metabolomics):\n",
    "        # 基因组网络\n",
    "        x1 = F.relu(self.genomics_fc1(genomics))\n",
    "        x1 = F.relu(self.genomics_fc2(x1))\n",
    "        \n",
    "        # 转录组网络\n",
    "        x2 = F.relu(self.transcript_fc1(transcriptomics))\n",
    "        x2 = F.relu(self.transcript_fc2(x2))\n",
    "        \n",
    "        # 代谢组网络\n",
    "        x3 = F.relu(self.metabol_fc1(metabolomics))\n",
    "        x3 = F.relu(self.metabol_fc2(x3))\n",
    "\n",
    "        # 融合三种模态数据\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "\n",
    "        # 融合后的全连接层\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # 输出层 (softmax 在 loss 中计算)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 20.94676155090332\n",
      "Epoch 2/100, Loss: 54.83552772283554\n",
      "Epoch 3/100, Loss: 10.057543358802796\n",
      "Epoch 4/100, Loss: 0.7026899170875549\n",
      "Epoch 5/100, Loss: 0.6973190450668335\n",
      "Epoch 6/100, Loss: 0.6928165507316589\n",
      "Epoch 7/100, Loss: 0.6944693756103516\n",
      "Epoch 8/100, Loss: 0.6974156284332276\n",
      "Epoch 9/100, Loss: 0.6922244310379029\n",
      "Epoch 10/100, Loss: 0.695613100528717\n",
      "Epoch 11/100, Loss: 0.6915403628349304\n",
      "Epoch 12/100, Loss: 0.6957681703567505\n",
      "Epoch 13/100, Loss: 0.6990368723869324\n",
      "Epoch 14/100, Loss: 0.7004756379127502\n",
      "Epoch 15/100, Loss: 0.7100483918190003\n",
      "Epoch 16/100, Loss: 0.7087678623199463\n",
      "Epoch 17/100, Loss: 0.6971363520622254\n",
      "Epoch 18/100, Loss: 0.6948053312301635\n",
      "Epoch 19/100, Loss: 0.6938735198974609\n",
      "Epoch 20/100, Loss: 0.695681836605072\n",
      "Epoch 21/100, Loss: 0.6923375487327575\n",
      "Epoch 22/100, Loss: 0.700504195690155\n",
      "Epoch 23/100, Loss: 0.6940534663200378\n",
      "Epoch 24/100, Loss: 0.6968497586250305\n",
      "Epoch 25/100, Loss: 0.6928295707702636\n",
      "Epoch 26/100, Loss: 0.6989257454872131\n",
      "Epoch 27/100, Loss: 0.6959793329238891\n",
      "Epoch 28/100, Loss: 0.6933456635475159\n",
      "Epoch 29/100, Loss: 0.6992210912704467\n",
      "Epoch 30/100, Loss: 0.6968233442306518\n",
      "Epoch 31/100, Loss: 0.6959753060340881\n",
      "Epoch 32/100, Loss: 0.696601676940918\n",
      "Epoch 33/100, Loss: 0.692946445941925\n",
      "Epoch 34/100, Loss: 0.702085223197937\n",
      "Epoch 35/100, Loss: 0.6969559621810913\n",
      "Epoch 36/100, Loss: 0.6938698530197144\n",
      "Epoch 37/100, Loss: 0.6991508340835572\n",
      "Epoch 38/100, Loss: 0.6942765116691589\n",
      "Epoch 39/100, Loss: 0.6967732954025269\n",
      "Epoch 40/100, Loss: 0.6980652523040771\n",
      "Epoch 41/100, Loss: 0.6964621186256409\n",
      "Epoch 42/100, Loss: 0.7023824501037598\n",
      "Epoch 43/100, Loss: 0.6996036982536316\n",
      "Epoch 44/100, Loss: 0.6996751117706299\n",
      "Epoch 45/100, Loss: 0.6942368912696838\n",
      "Epoch 46/100, Loss: 0.6935684394836426\n",
      "Epoch 47/100, Loss: 0.6933724498748779\n",
      "Epoch 48/100, Loss: 0.6991984438896179\n",
      "Epoch 49/100, Loss: 0.6959720516204834\n",
      "Epoch 50/100, Loss: 0.6930283093452454\n",
      "Epoch 51/100, Loss: 0.6962665939331054\n",
      "Epoch 52/100, Loss: 0.6955298185348511\n",
      "Epoch 53/100, Loss: 0.6974543595314026\n",
      "Epoch 54/100, Loss: 0.6964155435562134\n",
      "Epoch 55/100, Loss: 0.6923963618278504\n",
      "Epoch 56/100, Loss: 0.6940584421157837\n",
      "Epoch 57/100, Loss: 0.6940929794311523\n",
      "Epoch 58/100, Loss: 0.692244930267334\n",
      "Epoch 59/100, Loss: 0.6955211091041565\n",
      "Epoch 60/100, Loss: 0.6940890693664551\n",
      "Epoch 61/100, Loss: 0.6935715794563293\n",
      "Epoch 62/100, Loss: 0.6945450139045716\n",
      "Epoch 63/100, Loss: 0.6960492849349975\n",
      "Epoch 64/100, Loss: 0.6982914662361145\n",
      "Epoch 65/100, Loss: 0.700158109664917\n",
      "Epoch 66/100, Loss: 0.6945721244812012\n",
      "Epoch 67/100, Loss: 0.7055608606338502\n",
      "Epoch 68/100, Loss: 0.7001825904846192\n",
      "Epoch 69/100, Loss: 0.6948332619667054\n",
      "Epoch 70/100, Loss: 0.6945928001403808\n",
      "Epoch 71/100, Loss: 0.6959373688697815\n",
      "Epoch 72/100, Loss: 0.6945960283279419\n",
      "Epoch 73/100, Loss: 0.6948668599128723\n",
      "Epoch 74/100, Loss: 0.6942522001266479\n",
      "Epoch 75/100, Loss: 0.6976808142662049\n",
      "Epoch 76/100, Loss: 0.6988477826118469\n",
      "Epoch 77/100, Loss: 0.6944482898712159\n",
      "Epoch 78/100, Loss: 0.6940049934387207\n",
      "Epoch 79/100, Loss: 0.6933387780189514\n",
      "Epoch 80/100, Loss: 0.7015858173370362\n",
      "Epoch 81/100, Loss: 0.6948337483406067\n",
      "Epoch 82/100, Loss: 0.6959353876113892\n",
      "Epoch 83/100, Loss: 0.6962678265571595\n",
      "Epoch 84/100, Loss: 0.6959707140922546\n",
      "Epoch 85/100, Loss: 0.7037600779533386\n",
      "Epoch 86/100, Loss: 0.7078559255599975\n",
      "Epoch 87/100, Loss: 0.6986056971549988\n",
      "Epoch 88/100, Loss: 0.6954821085929871\n",
      "Epoch 89/100, Loss: 0.6937740540504456\n",
      "Epoch 90/100, Loss: 0.6924272108078003\n",
      "Epoch 91/100, Loss: 0.6956948852539062\n",
      "Epoch 92/100, Loss: 0.7021110033988953\n",
      "Epoch 93/100, Loss: 0.6964201259613038\n",
      "Epoch 94/100, Loss: 0.6948275518417358\n",
      "Epoch 95/100, Loss: 0.6972855257987977\n",
      "Epoch 96/100, Loss: 0.6960871601104737\n",
      "Epoch 97/100, Loss: 0.696740026473999\n",
      "Epoch 98/100, Loss: 0.6918070650100708\n",
      "Epoch 99/100, Loss: 0.694259696006775\n",
      "Epoch 100/100, Loss: 0.6952045392990113\n",
      "Accuracy: 49.5%\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = MultiModalNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for genomics, transcriptomics, metabolomics, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(genomics, transcriptomics, metabolomics)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# 测试函数\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for genomics, transcriptomics, metabolomics, labels in test_loader:\n",
    "            outputs = model(genomics, transcriptomics, metabolomics)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# 测试模型\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. <a id='toc20_'></a>[Few-shot learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.1. <a id='toc20_1_'></a>[Siamese Network](#toc0_)\n",
    "\n",
    "孪生Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. <a id='toc21_'></a>[matplotlib](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.1. <a id='toc21_1_'></a>[字体](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all font list get from matplotlib.font_manager:\n",
      "\tAbyssinica SIL\n",
      "\tAdobe Arabic\n",
      "\tAdobe Caslon Pro\n",
      "\tAdobe Devanagari\n",
      "\tAdobe Fan Heiti Std\n",
      "\tAdobe Fangsong Std\n",
      "\tAdobe Garamond Pro\n",
      "\tAdobe Gothic Std\n",
      "\tAdobe Hebrew\n",
      "\tAdobe Heiti Std\n",
      "\tAdobe Kaiti Std\n",
      "\tAdobe Ming Std\n",
      "\tAdobe Myungjo Std\n",
      "\tAdobe Naskh\n",
      "\tAdobe Song Std\n",
      "\tArial\n",
      "\tBahnschrift\n",
      "\tBirch Std\n",
      "\tBlackoak Std\n",
      "\tBook Antiqua\n",
      "\tBookman Old Style\n",
      "\tBookshelf Symbol 7\n",
      "\tBradley Hand ITC\n",
      "\tBrush Script Std\n",
      "\tC059\n",
      "\tCalibri\n",
      "\tCambria\n",
      "\tCandara\n",
      "\tCantarell\n",
      "\tCentury\n",
      "\tCentury Gothic\n",
      "\tChaparral Pro\n",
      "\tCharlemagne Std\n",
      "\tComic Sans MS\n",
      "\tConsolas\n",
      "\tConstantia\n",
      "\tCooper Std\n",
      "\tCorbel\n",
      "\tCourier New\n",
      "\tD050000L\n",
      "\tDejaVu Math TeX Gyre\n",
      "\tDejaVu Sans\n",
      "\tDejaVu Sans Display\n",
      "\tDejaVu Sans Mono\n",
      "\tDejaVu Serif\n",
      "\tDejaVu Serif Display\n",
      "\tDengXian\n",
      "\tDroid Sans\n",
      "\tDroid Sans Arabic\n",
      "\tDroid Sans Armenian\n",
      "\tDroid Sans Devanagari\n",
      "\tDroid Sans Ethiopic\n",
      "\tDroid Sans Fallback\n",
      "\tDroid Sans Georgian\n",
      "\tDroid Sans Hebrew\n",
      "\tDroid Sans Japanese\n",
      "\tDroid Sans Tamil\n",
      "\tDroid Sans Thai\n",
      "\tDubai\n",
      "\tEbrima\n",
      "\tFZShuTi\n",
      "\tFZYaoTi\n",
      "\tFangSong\n",
      "\tFontAwesome\n",
      "\tFranklin Gothic Medium\n",
      "\tFreeMono\n",
      "\tFreestyle Script\n",
      "\tFrench Script MT\n",
      "\tGabriola\n",
      "\tGadugi\n",
      "\tGaramond\n",
      "\tGeorgia\n",
      "\tGiddyup Std\n",
      "\tHarmonyOS Sans SC\n",
      "\tHobo Std\n",
      "\tImpact\n",
      "\tInk Free\n",
      "\tJavanese Text\n",
      "\tJomolhari\n",
      "\tJuice ITC\n",
      "\tKaiTi\n",
      "\tKhmer OS\n",
      "\tKhmer OS Content\n",
      "\tKhmer OS System\n",
      "\tKingsoft UE\n",
      "\tKozuka Gothic Pr6N\n",
      "\tKozuka Gothic Pro\n",
      "\tKozuka Mincho Pr6N\n",
      "\tKozuka Mincho Pro\n",
      "\tKristen ITC\n",
      "\tLeelawadee UI\n",
      "\tLetter Gothic Std\n",
      "\tLiSu\n",
      "\tLiberation Mono\n",
      "\tLiberation Sans\n",
      "\tLithos Pro\n",
      "\tLohit Assamese\n",
      "\tLohit Bengali\n",
      "\tLohit Devanagari\n",
      "\tLohit Gujarati\n",
      "\tLohit Gurmukhi\n",
      "\tLohit Kannada\n",
      "\tLohit Odia\n",
      "\tLohit Tamil\n",
      "\tLohit Telugu\n",
      "\tLucida Console\n",
      "\tLucida Handwriting\n",
      "\tLucida Sans Unicode\n",
      "\tMS Gothic\n",
      "\tMS Reference Sans Serif\n",
      "\tMS Reference Specialty\n",
      "\tMT Extra\n",
      "\tMV Boli\n",
      "\tMalgun Gothic\n",
      "\tMeera\n",
      "\tMesquite Std\n",
      "\tMicrosoft Himalaya\n",
      "\tMicrosoft JhengHei\n",
      "\tMicrosoft New Tai Lue\n",
      "\tMicrosoft PhagsPa\n",
      "\tMicrosoft Sans Serif\n",
      "\tMicrosoft Tai Le\n",
      "\tMicrosoft YaHei\n",
      "\tMicrosoft Yi Baiti\n",
      "\tMingLiU-ExtB\n",
      "\tMinion Pro\n",
      "\tMistral\n",
      "\tMongolian Baiti\n",
      "\tMonotype Corsiva\n",
      "\tMontserrat\n",
      "\tMontserrat Alternates\n",
      "\tMyanmar Text\n",
      "\tMyriad Arabic\n",
      "\tMyriad Hebrew\n",
      "\tMyriad Pro\n",
      "\tNimbus Mono PS\n",
      "\tNimbus Roman\n",
      "\tNimbus Sans\n",
      "\tNimbus Sans Narrow\n",
      "\tNirmala UI\n",
      "\tNoto Sans CJK JP\n",
      "\tNoto Sans Lisu\n",
      "\tNoto Sans Mandaic\n",
      "\tNoto Sans Meetei Mayek\n",
      "\tNoto Sans SC\n",
      "\tNoto Sans Sinhala\n",
      "\tNoto Sans Tagalog\n",
      "\tNoto Sans Tai Tham\n",
      "\tNoto Sans Tai Viet\n",
      "\tNoto Serif CJK JP\n",
      "\tNoto Serif SC\n",
      "\tNueva Std\n",
      "\tNuosu SIL\n",
      "\tOCR A Std\n",
      "\tOrator Std\n",
      "\tP052\n",
      "\tPT Sans\n",
      "\tPT Sans Narrow\n",
      "\tPadauk\n",
      "\tPakType Naskh Basic\n",
      "\tPalatino Linotype\n",
      "\tPapyrus\n",
      "\tPoplar Std\n",
      "\tPrestige Elite Std\n",
      "\tPristina\n",
      "\tRosewood Std\n",
      "\tSTCaiyun\n",
      "\tSTFangsong\n",
      "\tSTHupo\n",
      "\tSTIX\n",
      "\tSTIXGeneral\n",
      "\tSTIXNonUnicode\n",
      "\tSTIXSizeFiveSym\n",
      "\tSTIXSizeFourSym\n",
      "\tSTIXSizeOneSym\n",
      "\tSTIXSizeThreeSym\n",
      "\tSTIXSizeTwoSym\n",
      "\tSTKaiti\n",
      "\tSTLiti\n",
      "\tSTSong\n",
      "\tSTXihei\n",
      "\tSTXingkai\n",
      "\tSTXinwei\n",
      "\tSTZhongsong\n",
      "\tSans Serif Collection\n",
      "\tSegoe Fluent Icons\n",
      "\tSegoe MDL2 Assets\n",
      "\tSegoe Print\n",
      "\tSegoe Script\n",
      "\tSegoe UI\n",
      "\tSegoe UI Emoji\n",
      "\tSegoe UI Historic\n",
      "\tSegoe UI Symbol\n",
      "\tSegoe UI Variable\n",
      "\tSimHei\n",
      "\tSimSun\n",
      "\tSimSun-ExtB\n",
      "\tSimSun-ExtG\n",
      "\tSitka\n",
      "\tStencil Std\n",
      "\tSylfaen\n",
      "\tSymbol\n",
      "\tTahoma\n",
      "\tTekton Pro\n",
      "\tTempus Sans ITC\n",
      "\tTimes New Roman\n",
      "\tTrajan Pro\n",
      "\tTrebuchet MS\n",
      "\tURW Bookman\n",
      "\tURW Gothic\n",
      "\tVerdana\n",
      "\tWaree\n",
      "\tWebdings\n",
      "\tWingdings\n",
      "\tWingdings 2\n",
      "\tWingdings 3\n",
      "\tYouYuan\n",
      "\tYu Gothic\n",
      "\tZ003\n",
      "\tZWAdobeF\n",
      "\tcmb10\n",
      "\tcmex10\n",
      "\tcmmi10\n",
      "\tcmr10\n",
      "\tcmss10\n",
      "\tcmsy10\n",
      "\tcmtt10\n"
     ]
    }
   ],
   "source": [
    "# 查询当前系统所有字体\n",
    "from matplotlib.font_manager import FontManager\n",
    "\n",
    "\n",
    "mpl_fonts = set(f.name for f in FontManager().ttflist)\n",
    "\n",
    "print('all font list get from matplotlib.font_manager:')\n",
    "for f in sorted(mpl_fonts):\n",
    "    print('\\t' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abyssinica SIL',\n",
       " 'Adobe Arabic',\n",
       " 'Adobe Caslon Pro',\n",
       " 'Adobe Devanagari',\n",
       " 'Adobe Fan Heiti Std',\n",
       " 'Adobe Fangsong Std',\n",
       " 'Adobe Garamond Pro',\n",
       " 'Adobe Gothic Std',\n",
       " 'Adobe Hebrew',\n",
       " 'Adobe Heiti Std',\n",
       " 'Adobe Kaiti Std',\n",
       " 'Adobe Ming Std',\n",
       " 'Adobe Myungjo Std',\n",
       " 'Adobe Naskh',\n",
       " 'Adobe Song Std',\n",
       " 'Arial',\n",
       " 'Bahnschrift',\n",
       " 'Birch Std',\n",
       " 'Blackoak Std',\n",
       " 'Book Antiqua',\n",
       " 'Bookman Old Style',\n",
       " 'Bookshelf Symbol 7',\n",
       " 'Bradley Hand ITC',\n",
       " 'Brush Script Std',\n",
       " 'C059',\n",
       " 'Calibri',\n",
       " 'Cambria',\n",
       " 'Candara',\n",
       " 'Cantarell',\n",
       " 'Century',\n",
       " 'Century Gothic',\n",
       " 'Chaparral Pro',\n",
       " 'Charlemagne Std',\n",
       " 'Comic Sans MS',\n",
       " 'Consolas',\n",
       " 'Constantia',\n",
       " 'Cooper Std',\n",
       " 'Corbel',\n",
       " 'Courier New',\n",
       " 'D050000L',\n",
       " 'DejaVu Math TeX Gyre',\n",
       " 'DejaVu Sans',\n",
       " 'DejaVu Sans Display',\n",
       " 'DejaVu Sans Mono',\n",
       " 'DejaVu Serif',\n",
       " 'DejaVu Serif Display',\n",
       " 'DengXian',\n",
       " 'Droid Sans',\n",
       " 'Droid Sans Arabic',\n",
       " 'Droid Sans Armenian',\n",
       " 'Droid Sans Devanagari',\n",
       " 'Droid Sans Ethiopic',\n",
       " 'Droid Sans Fallback',\n",
       " 'Droid Sans Georgian',\n",
       " 'Droid Sans Hebrew',\n",
       " 'Droid Sans Japanese',\n",
       " 'Droid Sans Tamil',\n",
       " 'Droid Sans Thai',\n",
       " 'Dubai',\n",
       " 'Ebrima',\n",
       " 'FZShuTi',\n",
       " 'FZYaoTi',\n",
       " 'FangSong',\n",
       " 'FontAwesome',\n",
       " 'Franklin Gothic Medium',\n",
       " 'FreeMono',\n",
       " 'Freestyle Script',\n",
       " 'French Script MT',\n",
       " 'Gabriola',\n",
       " 'Gadugi',\n",
       " 'Garamond',\n",
       " 'Georgia',\n",
       " 'Giddyup Std',\n",
       " 'HarmonyOS Sans SC',\n",
       " 'Hobo Std',\n",
       " 'Impact',\n",
       " 'Ink Free',\n",
       " 'Javanese Text',\n",
       " 'Jomolhari',\n",
       " 'Juice ITC',\n",
       " 'KaiTi',\n",
       " 'Khmer OS',\n",
       " 'Khmer OS Content',\n",
       " 'Khmer OS System',\n",
       " 'Kingsoft UE',\n",
       " 'Kozuka Gothic Pr6N',\n",
       " 'Kozuka Gothic Pro',\n",
       " 'Kozuka Mincho Pr6N',\n",
       " 'Kozuka Mincho Pro',\n",
       " 'Kristen ITC',\n",
       " 'Leelawadee UI',\n",
       " 'Letter Gothic Std',\n",
       " 'LiSu',\n",
       " 'Liberation Mono',\n",
       " 'Liberation Sans',\n",
       " 'Lithos Pro',\n",
       " 'Lohit Assamese',\n",
       " 'Lohit Bengali',\n",
       " 'Lohit Devanagari',\n",
       " 'Lohit Gujarati',\n",
       " 'Lohit Gurmukhi',\n",
       " 'Lohit Kannada',\n",
       " 'Lohit Odia',\n",
       " 'Lohit Tamil',\n",
       " 'Lohit Telugu',\n",
       " 'Lucida Console',\n",
       " 'Lucida Handwriting',\n",
       " 'Lucida Sans Unicode',\n",
       " 'MS Gothic',\n",
       " 'MS Reference Sans Serif',\n",
       " 'MS Reference Specialty',\n",
       " 'MT Extra',\n",
       " 'MV Boli',\n",
       " 'Malgun Gothic',\n",
       " 'Meera',\n",
       " 'Mesquite Std',\n",
       " 'Microsoft Himalaya',\n",
       " 'Microsoft JhengHei',\n",
       " 'Microsoft New Tai Lue',\n",
       " 'Microsoft PhagsPa',\n",
       " 'Microsoft Sans Serif',\n",
       " 'Microsoft Tai Le',\n",
       " 'Microsoft YaHei',\n",
       " 'Microsoft Yi Baiti',\n",
       " 'MingLiU-ExtB',\n",
       " 'Minion Pro',\n",
       " 'Mistral',\n",
       " 'Mongolian Baiti',\n",
       " 'Monotype Corsiva',\n",
       " 'Montserrat',\n",
       " 'Montserrat Alternates',\n",
       " 'Myanmar Text',\n",
       " 'Myriad Arabic',\n",
       " 'Myriad Hebrew',\n",
       " 'Myriad Pro',\n",
       " 'Nimbus Mono PS',\n",
       " 'Nimbus Roman',\n",
       " 'Nimbus Sans',\n",
       " 'Nimbus Sans Narrow',\n",
       " 'Nirmala UI',\n",
       " 'Noto Sans CJK JP',\n",
       " 'Noto Sans Lisu',\n",
       " 'Noto Sans Mandaic',\n",
       " 'Noto Sans Meetei Mayek',\n",
       " 'Noto Sans SC',\n",
       " 'Noto Sans Sinhala',\n",
       " 'Noto Sans Tagalog',\n",
       " 'Noto Sans Tai Tham',\n",
       " 'Noto Sans Tai Viet',\n",
       " 'Noto Serif CJK JP',\n",
       " 'Noto Serif SC',\n",
       " 'Nueva Std',\n",
       " 'Nuosu SIL',\n",
       " 'OCR A Std',\n",
       " 'Orator Std',\n",
       " 'P052',\n",
       " 'PT Sans',\n",
       " 'PT Sans Narrow',\n",
       " 'Padauk',\n",
       " 'PakType Naskh Basic',\n",
       " 'Palatino Linotype',\n",
       " 'Papyrus',\n",
       " 'Poplar Std',\n",
       " 'Prestige Elite Std',\n",
       " 'Pristina',\n",
       " 'Rosewood Std',\n",
       " 'STCaiyun',\n",
       " 'STFangsong',\n",
       " 'STHupo',\n",
       " 'STIX',\n",
       " 'STIXGeneral',\n",
       " 'STIXNonUnicode',\n",
       " 'STIXSizeFiveSym',\n",
       " 'STIXSizeFourSym',\n",
       " 'STIXSizeOneSym',\n",
       " 'STIXSizeThreeSym',\n",
       " 'STIXSizeTwoSym',\n",
       " 'STKaiti',\n",
       " 'STLiti',\n",
       " 'STSong',\n",
       " 'STXihei',\n",
       " 'STXingkai',\n",
       " 'STXinwei',\n",
       " 'STZhongsong',\n",
       " 'Sans Serif Collection',\n",
       " 'Segoe Fluent Icons',\n",
       " 'Segoe MDL2 Assets',\n",
       " 'Segoe Print',\n",
       " 'Segoe Script',\n",
       " 'Segoe UI',\n",
       " 'Segoe UI Emoji',\n",
       " 'Segoe UI Historic',\n",
       " 'Segoe UI Symbol',\n",
       " 'Segoe UI Variable',\n",
       " 'SimHei',\n",
       " 'SimSun',\n",
       " 'SimSun-ExtB',\n",
       " 'SimSun-ExtG',\n",
       " 'Sitka',\n",
       " 'Stencil Std',\n",
       " 'Sylfaen',\n",
       " 'Symbol',\n",
       " 'Tahoma',\n",
       " 'Tekton Pro',\n",
       " 'Tempus Sans ITC',\n",
       " 'Times New Roman',\n",
       " 'Trajan Pro',\n",
       " 'Trebuchet MS',\n",
       " 'URW Bookman',\n",
       " 'URW Gothic',\n",
       " 'Verdana',\n",
       " 'Waree',\n",
       " 'Webdings',\n",
       " 'Wingdings',\n",
       " 'Wingdings 2',\n",
       " 'Wingdings 3',\n",
       " 'YouYuan',\n",
       " 'Yu Gothic',\n",
       " 'Z003',\n",
       " 'ZWAdobeF',\n",
       " 'cmb10',\n",
       " 'cmex10',\n",
       " 'cmmi10',\n",
       " 'cmr10',\n",
       " 'cmss10',\n",
       " 'cmsy10',\n",
       " 'cmtt10'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpl_fonts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.2. <a id='toc21_2_'></a>[显示中文](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def set_plt_default():\n",
    "    plt.rcdefaults()                    # 重置成默认的参数\n",
    "\n",
    "\n",
    "def set_plt_func(font_name: str= 'Times new roman', font_size: int= 12, **kwargs):\n",
    "    plt.rc('font', family=font_name)  # 设置字体\n",
    "    plt.rc('axes', titlesize=font_size)  # Axes标题字体大小\n",
    "    plt.rc('axes', labelsize=font_size)  # Axes标签字体大小\n",
    "    plt.rc('xtick', labelsize=font_size)  # x轴刻度字体大小\n",
    "    plt.rc('ytick', labelsize=font_size)  # y轴刻度字体大小\n",
    "    plt.rc('legend', fontsize=font_size)  # 图例字体大小\n",
    "    plt.rc('figure', titlesize=font_size)  # Figure标题字体大小\n",
    "\n",
    "\n",
    "def set_plt_dict(**kswargs):\n",
    "    # 设置字体栈（优先级从高到低）\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.sans-serif'] = [\n",
    "        'Times New Roman',   # 英文优先使用\n",
    "        'SimSun',            # 中文宋体\n",
    "        # 'SimHei',            # 备用中文字体黑体\n",
    "        # 'Noto Sans CJK SC'   # 最后回退\n",
    "    ]\n",
    "    plt.rcParams['font.serif'] = \"Arial\"\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "    plt.rcParams['pdf.fonttype'] = 42           # ai可编辑的字体格式\n",
    "    plt.rcParams['figure.figsize'] = (3, 3)     # figsize\n",
    "    plt.rcParams['savefig.format'] = \"svg\"      # svg格式\n",
    "    plt.rcParams['savefig.transparent'] = True  # 背景是否透明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from deepspore.matplotlib_config import set_plt_rcParams\n",
    "\n",
    "\n",
    "set_plt_rcParams()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(0,10))\n",
    "plt.xlabel('xlabel')\n",
    "plt.ylabel('ylabel')\n",
    "plt.title('SCI papers')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. <a id='toc22_'></a>[argparse](#toc0_)\n",
    "\n",
    "接受命令参数并传递进行程序中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "\n",
    "\n",
    "# 创建ArgumentParser对象\n",
    "parser = argparse.ArgumentParser(\n",
    "    prog= \"argparse demo in PyTorch\",\n",
    "    description= \"demo for argparse which is used to parse command-line arguments\",\n",
    "    usage= \"python argparse_demo.py [options]\",\n",
    "    epilog= \"End of ArgumentParser demo\",\n",
    "    add_help= True   # 是否显示帮助信息\n",
    ")\n",
    "\n",
    "# 添加参数\n",
    "parser.add_argument(\n",
    "    '-b',                                                   # 短选项   \n",
    "    '--batch_size',                                         # 长选项\n",
    "    type= int,                                              # 参数类型\n",
    "    default= 32,                                            # 默认值\n",
    "    required= True,                                         # 是否必须\n",
    "    choices= [4, 8, 16, 32, 64, 128, 256, 512],             # 可选值\n",
    "    help= 'batch size for training or inference',           # 帮助信息\n",
    "    action= 'store'                                         # 当调用参数时候，返回bool类型数据，可以通过default定义。\n",
    ")\n",
    "\n",
    "# 解析参数\n",
    "args = parser.parse_args()\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. <a id='toc23_'></a>[ml_collections](#toc0_)\n",
    "\n",
    "配置参数字典。\n",
    "\n",
    "官方教程1：[https://github.com/google/ml_collections](https://github.com/google/ml_collections)  \n",
    "\n",
    "官方教程2：[https://ml-collections.readthedocs.io/en/latest/](https://ml-collections.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.1. <a id='toc23_1_'></a>[概述](#toc0_)\n",
    "\n",
    "安装和演示\n",
    "\n",
    "```python\n",
    "# Install\n",
    "pip install ml-collections \n",
    "\n",
    "# Module structure\n",
    "mlc.ConfigDict()                # 可读写\n",
    "mlc.FrozenConfigDict()          # 只读，不可改\n",
    "mlc.FieldReference()            # 变量声明 (占位符)\n",
    "mlc.config_dict()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.2. <a id='toc23_2_'></a>[详细使用](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 100\n",
       "learning_rate: 0.01\n",
       "save_dir:\n",
       "  dir_base: ./bs/train/checkpoints\n",
       "  prefix: demo\n",
       "  suffix: .ckpt\n",
       "steps_counter: 0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ml_collections as mlc\n",
    "\n",
    "\n",
    "# 1. ConfigDict: 以字典方式传参\n",
    "config = mlc.ConfigDict(\n",
    "    {\n",
    "        'learning_rate': 0.01,\n",
    "        'epochs': 100,\n",
    "        'save_dir': {\n",
    "            'dir_base': './bs/train/checkpoints',\n",
    "            'prefix': 'demo', \n",
    "            'suffix': '.ckpt'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. 直接添加\n",
    "config.steps_counter = 0\n",
    "\n",
    "# 以x.x的方式取值\n",
    "config.learning_rate\n",
    "config.epochs\n",
    "config.save_dir\n",
    "config.save_dir.dir_base\n",
    "config.save_dir.prefix\n",
    "config.save_dir.suffix\n",
    "config.steps_counter\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.learning_rate = 0    # 可修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning_rate: 0.001\n",
       "optm: optm"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "FrozenConfigDict不可改动\n",
    "'''\n",
    "\n",
    "\n",
    "fc = mlc.FrozenConfigDict({\n",
    "    'learning_rate': 0.001,\n",
    "    'optm': 'optm'\n",
    "})\n",
    "\n",
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.learning_rate = 0    # 不可修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning_rate: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "  _field_type: !!python/name:builtins.float ''\n",
       "  _ops: []\n",
       "  _required: false\n",
       "  _value: 1.0e-05"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "FieldReference的使用演示\n",
    "'''\n",
    "\n",
    "\n",
    "lr = mlc.FieldReference(default=0.001, field_type=float)\n",
    "gama = mlc.FieldReference(default=0.01, field_type=float)\n",
    "\n",
    "c = mlc.ConfigDict({\n",
    "    'learning_rate': lr * gama  # 只保存逻辑\n",
    "})\n",
    "\n",
    "c.learning_rate\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. <a id='toc24_'></a>[functools](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24.1. <a id='toc24_1_'></a>[partial](#toc0_)\n",
    "\n",
    "将函数的部分参数进行锁定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 6)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def add_fn(a, b):\n",
    "    return a + b\n",
    "\n",
    "a_add = partial(add_fn, a= 1)    # 固定了a的值\n",
    "\n",
    "a_add(b= 2), a_add(b= 3), a_add(b= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 6)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用lambda可以实现如上功能。\n",
    "a_add = lambda x: add_fn(a=1, b=x)\n",
    "\n",
    "a_add(x=2), a_add(x=3), a_add(x=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. <a id='toc25_'></a>[copy](#toc0_)\n",
    "直接赋值：其实就是对象的引用（别名）。\n",
    "\n",
    "浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象。\n",
    "\n",
    "深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。\n",
    "\n",
    "总结：\n",
    "\n",
    "|对象|拷贝类型|\n",
    "|-|-|\n",
    "|列表|copy|\n",
    "|字典|deepcopy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.1. <a id='toc25_1_'></a>[列表类型的拷贝](#toc0_)\n",
    "\n",
    "列表 **浅拷贝**即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [1, 2, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 浅拷贝\n",
    "import copy \n",
    "\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = a.copy()\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 100], [1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.append(100)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.2. <a id='toc25_2_'></a>[字典类型的拷贝](#toc0_)\n",
    "\n",
    "字典必须 **深拷贝**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'k1': [1, 2, 3]}, {'k1': [1, 2, 3]}, {'k1': [1, 2, 3]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 浅拷贝\n",
    "import copy\n",
    "\n",
    "\n",
    "a_dict = {\"k1\": [1, 2, 3]}\n",
    "\n",
    "\n",
    "## 浅拷贝\n",
    "b_dict = a_dict.copy()                      # copy.copy(a_dict)     浅拷贝\n",
    "\n",
    "## 深拷贝\n",
    "c_dict = copy.deepcopy(a_dict)              # copy.deepcopy(a_dict) 深拷贝\n",
    "\n",
    "a_dict, b_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'k1': [1, 2, 3, 100]}, {'k1': [1, 2, 3, 100]}, {'k1': [1, 2, 3]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dict['k1'].append(100)    # 只改a_dict\n",
    "\n",
    "a_dict, b_dict, c_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. <a id='toc26_'></a>[tqdm](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3536512.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "# 基础用法（文本进度条）\n",
    "for i in tqdm(range(1000)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6477de608f724bf1aff4abb93c9f7b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto  import tqdm \n",
    "\n",
    "\n",
    "# 自动适配环境（Notebook 中显示 HTML 进度条）\n",
    "for i in tqdm(range(1000)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26.1. <a id='toc26_1_'></a>[基础循环封装](#toc0_)\n",
    "\n",
    "- 直接封装可迭代对象（如列表、range），自动显示进度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 100/100 [00:10<00:00,  9.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import time \n",
    "\n",
    "\n",
    "for i in tqdm(range(100), desc=\"训练进度\"):\n",
    "    time.sleep(0.1)   # 模拟耗时操作 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26.2. <a id='toc26_2_'></a>[手动控制进度](#toc0_)\n",
    "\n",
    "- 适用于无法直接迭代的场景（如分批次处理文件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "自定义进度: 100%|██████████| 100/100 [00:05<00:00, 19.95it/s,  状态=第9次迭代]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=100, desc=\"自定义进度\") as pbar:\n",
    "    for i in range(10):\n",
    "        time.sleep(0.5) \n",
    "        pbar.update(10)   # 手动更新步长 \n",
    "        pbar.set_postfix({' 状态': f'第{i}次迭代'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `自动跟新（推荐）`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simple demo: 100%|██████████| 100/100 [00:10<00:00,  9.93epoch/s, show1=99, show2=99, show3=99]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import time \n",
    "\n",
    "\n",
    "with tqdm(range(100), desc= 'Simple demo', unit= \"epoch\") as pbar:\n",
    "    for epoch in pbar:\n",
    "        time.sleep(0.1)\n",
    "        pbar.set_postfix({'show1': epoch, \"show2\": epoch, \"show3\": epoch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26.3. <a id='toc26_3_'></a>[多进度条嵌套](#toc0_)\n",
    "\n",
    "- leave=False 确保内层进度条完成后自动消失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "外层循环: 100%|██████████| 3/3 [00:07<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "outer = tqdm(range(3), desc=\"外层循环\")\n",
    "for i in outer:\n",
    "    inner = tqdm(range(5), desc=\"内层循环\", leave= False)\n",
    "    for j in inner:\n",
    "        time.sleep(0.51) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26.4. <a id='toc26_4_'></a>[进阶功能与优化](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.4.1. <a id='toc26_4_1_'></a>[动态调整参数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 阶段 4: 100%|\u001b[31m███████████████████████████████████████████████████\u001b[0m| 50/50 [00:00<00:00, 19732.33it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bar = tqdm(range(50), ncols=100, mininterval=0.5)\n",
    "for i in bar:\n",
    "    if i % 10 == 0:\n",
    "        bar.set_description(f\" 阶段 {i//10}\")  # 动态修改描述 \n",
    "        bar.colour  = 'red'  # 动态改变颜色（需tqdm>=4.62.0）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.4.2. <a id='toc26_4_2_'></a>[与Pandas结合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 676064.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()   # 激活Pandas支持 \n",
    "\n",
    "\n",
    "df = pd.DataFrame({'data': range(1000)})\n",
    "df['processed'] = df['data'].progress_apply(lambda x: x**2)  # 显示apply进度 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.4.3. <a id='toc26_4_3_'></a>[Jupyter Notebook适配](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee939e2e46364a7ea35018c7d7d7d002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Notebook进度:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook  import tqdm  # 专用Jupyter版本 \n",
    "import time \n",
    "\n",
    "\n",
    "for i in tqdm(range(100), desc=\"Notebook进度\", disable= False):\n",
    "    time.sleep(0.05) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.4.4. <a id='toc26_4_4_'></a>[多线程/多进程支持](#toc0_)\n",
    "\n",
    "- 使用tqdm包裹map结果以显示并行任务进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 461267.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures  import ThreadPoolExecutor \n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "def func(num):\n",
    "    return num\n",
    "\n",
    "data = [ i for i in range(1000)]\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    results = list(tqdm(pool.map(func,  data), total=len(data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26.4.5. <a id='toc26_4_5_'></a>[自定义进度条格式](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                    | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7f55b8998950>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar_format = '{l_bar}{bar:20}{r_bar}'  # 调整左右部分宽度 \n",
    "tqdm(range(100), bar_format= bar_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. <a id='toc27_'></a>[callback](#toc0_)\n",
    "\n",
    "回调（callback）：简单来说就是用函数标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27.1. <a id='toc27_1_'></a>[基于getattr实现](#toc0_)\n",
    "\n",
    "利用getattr `显示` 获取类的 `方法` 和 `属性。`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Help on built-in function getattr in module builtins:\n",
    "\n",
    "getattr(...)\n",
    "    getattr(object, name[, default]) -> value\n",
    "    \n",
    "    Get a named attribute from an object; getattr(x, 'y') is equivalent to x.y.\n",
    "    When a default argument is given, it is returned when the attribute doesn't\n",
    "    exist; without it, an exception is raised in that case.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "No exist!\n",
      "on_train_begin_demo.\n"
     ]
    }
   ],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.x = 10 \n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        print(\"on_train_begin_demo.\")\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        pass \n",
    "\n",
    "\n",
    "# test\n",
    "test = Test()\n",
    "\n",
    "\n",
    "print(getattr(test, \"x\"))   # 获取属性\n",
    "print(getattr(test, \"xx\", \"No exist!\"))   # 获取属性失败，返回\"No exist!\"\n",
    "\n",
    "\n",
    "# 同样，可以用于获取方法\n",
    "method = getattr(test,\"on_train_begin\", \"No exist!\")\n",
    "\n",
    "method()    # 等同于，test.on_train_begin()。于是基于此通过说明 方法名称 即可实现调用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在相应位置插入标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_train_begin ...\n",
      "++++++++++++++\n",
      "on_train_end ...\n",
      "on_train_begin ...\n",
      "++++++++++++++\n",
      "on_train_end ...\n",
      "on_train_begin ...\n",
      "++++++++++++++\n",
      "on_train_end ...\n"
     ]
    }
   ],
   "source": [
    "class Callback:\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        print(\"on_train_begin ...\")\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        print(\"on_train_end ...\")\n",
    "\n",
    "callback = Callback()\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            getattr(callback, \"on_train_begin\", \"No exist!\")()  # 用()调用方法\n",
    "            print('++++++++++++++')\n",
    "            getattr(callback, \"on_train_end\", \"No exist!\")()\n",
    "            \n",
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "trainer.train(epochs= 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高级用法：循环执行所有含有指定method_name的callback类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_train_begin ...\n",
      "on_train_begin ...\n",
      "on_train_begin ...\n",
      "on_train_begin ...\n",
      "on_train_begin ...\n"
     ]
    }
   ],
   "source": [
    "callbacks = [callback, callback, callback, callback, callback]\n",
    "\n",
    "class Trainer:\n",
    "    def train(self):\n",
    "        self._call_callback(callbacks, \"on_train_begin\")\n",
    "\n",
    "    def _call_callback(self, callbacks, method_name):\n",
    "        for callback in callbacks:\n",
    "            method = getattr(callback, method_name, \"No exist!\")\n",
    "            method()\n",
    "\n",
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. <a id='toc28_'></a>[typing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typing 库是 Python 3.5+ 引入的类型注解支持工具，它允许开发者通过类型提示（Type Hints）来标注变量、函数参数和返回值的预期类型，从而提高代码的可读性、可维护性，并配合静态类型检查工具（如 mypy）进行类型验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.1. <a id='toc28_1_'></a>[基础类型注释](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.1. <a id='toc28_1_1_'></a>[变量注解](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union\n",
    "\n",
    "\n",
    "# 基本类型\n",
    "name: str = \"Alice\"\n",
    "age: int = 30\n",
    "is_active: bool = True\n",
    "\n",
    "# 特殊类型\n",
    "data: Any = None  # 任意类型\n",
    "maybe_int: Union[int, None] = 42  # 可选类型(Python 3.10+可用 int | None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.2. <a id='toc28_1_2_'></a>[函数注解](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hellow , Alice! You are 30 years old.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greet(name: str, age: int) -> str:\n",
    "    return f\"Hellow , {name}! You are {age} years old.\"\n",
    "\n",
    "\n",
    "# 类型检查会捕获错误\n",
    "greet(\"Alice\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function greet in module __main__:\n",
      "\n",
      "greet(name: str, age: int) -> str\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(greet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.2. <a id='toc28_2_'></a>[容器类型注解](#toc0_)\n",
    "\n",
    "Union: 表示“或”（如， Union[int, str]），等价于[int | str]。\n",
    "\n",
    "Optional: 等价于 Union[T, None]，可能为None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.2.1. <a id='toc28_2_1_'></a>[标准容器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "\n",
    "names: List[str] = [\"Alice\", \"Bob\"]  # list[str] (Python 3.9+)\n",
    "scores: Dict[str, float] = {\"math\": 90.5}  # dict[str, float]\n",
    "coords: Tuple[int, int, float] = (10, 20, 5.5)  # 固定长度元组\n",
    "unique_ids: Set[int] = {1, 2, 3}  # set[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union, Optional, List, Dict \n",
    "\n",
    "\n",
    "def process_data(\n",
    "        data: Union[str, List[int]],                # 接受字符串或证书列表\n",
    "        config: Optional[Dict[str, int]] = None     # 可选字典\n",
    ") -> float:\n",
    "    if isinstance(data, str):\n",
    "        return len(data)\n",
    "    else:\n",
    "        return sum(data) / len(data)\n",
    "    \n",
    "\n",
    "process_data(\"hello\")   # 5\n",
    "process_data([1, 2, 3], {'max': 10})    # 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.2.2. <a id='toc28_2_2_'></a>[嵌套容器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import DefaultDict\n",
    "\n",
    "\n",
    "# 嵌套字典示例\n",
    "graph: Dict[str, List[Tuple[str, float]]] = {\n",
    "    \"A\": [(\"B\", 1.2), (\"C\", 3.4)]\n",
    "}\n",
    "\n",
    "# 默认字典\n",
    "counts: DefaultDict[str, int] = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.3. <a id='toc28_3_'></a>[高级类型](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.3.1. <a id='toc28_3_1_'></a>[泛型与类型变量](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Generic, Sequence\n",
    "\n",
    "\n",
    "T = TypeVar('T')  # 泛型类型变量\n",
    "\n",
    "class Stack(Generic[T]):\n",
    "    def __init__(self) -> None:\n",
    "        self.items: List[T] = []\n",
    "    \n",
    "    def push(self, item: T) -> None:\n",
    "        self.items.append(item)\n",
    "\n",
    "# 使用泛型类\n",
    "int_stack: Stack[int] = Stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.3.2. <a id='toc28_3_2_'></a>[回调函数类型](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "\n",
    "# 接受两个int参数，返回bool的函数\n",
    "Predicate = Callable[[int, int], bool]\n",
    "\n",
    "def filter_numbers(\n",
    "    nums: Sequence[int],\n",
    "    predicate: Predicate,\n",
    "    limit: Optional[int] = None\n",
    ") -> List[int]:\n",
    "    return [n for n in nums if predicate(n, limit or 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.4. <a id='toc28_4_'></a>[结构化类型](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.4.1. <a id='toc28_4_1_'></a>[类型别名](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias\n",
    "\n",
    "\n",
    "# Python 3.10+\n",
    "JsonValue: TypeAlias = Union[str, int, float, bool, None, 'JsonDict', List['JsonValue']]\n",
    "JsonDict: TypeAlias = Dict[str, JsonValue]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29. <a id='toc29_'></a>[collections](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该库提供8种增强型容器数据类型，针对基础数据结构的性能缺陷和功能缺失进行优化：\n",
    "\n",
    "- 解决元组可读性问题（namedtuple）\n",
    "- 优化列表的插入/删除效率（deque）\n",
    "- 扩展字典的键值处理逻辑（defaultdict/OrderedDict）\n",
    "- 提供专业的数据统计工具（Counter）\n",
    "- 实现复杂数据结构组合（ChainMap/UserDict）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29.1. <a id='toc29_1_'></a>[namedtuple（具名元组）](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用场景：CSV数据解析、数据库记录处理、科学计算坐标存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.demo_namedtuple'>\n",
      "demo_namedtuple(x=0, y=0, z=0)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple \n",
    "\n",
    "\n",
    "Vector3D = namedtuple(\"demo_namedtuple\", ['x', 'y', 'z'], defaults= [0, 0, 0])\n",
    "\n",
    "print(Vector3D)\n",
    "print(Vector3D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_namedtuple(x=1.2, y=2.2, z=0)\n",
      "{'x': 1.2, 'y': 2.2, 'z': 0}\n",
      "demo_namedtuple(x=1.2, y=2.2, z=5.6)\n"
     ]
    }
   ],
   "source": [
    "v = Vector3D(1.2, 2.2) # 默认z = 0\n",
    "\n",
    "print(v)\n",
    "print(v._asdict())\n",
    "print(v._replace(z= 5.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29.2. <a id='toc29_2_'></a>[deque（双端队列）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([2, 3, 4], maxlen=3)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque \n",
    " \n",
    "\n",
    "# 创建带最大长度的队列 \n",
    "d = deque(maxlen= 3)\n",
    "d.extend([1,  2, 3])\n",
    "d.append(4)         # 自动弹出最左端元素 \n",
    "print(d)            # 输出：deque([2, 3, 4], maxlen=3)\n",
    " \n",
    "# 线程安全操作 \n",
    "d.appendleft(0)     # 输出：deque([0, 2, 3], maxlen=3)\n",
    "d.rotate(1)         # 循环右移：deque([3, 0, 2], maxlen=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29.3. <a id='toc29_3_'></a>[ defaultdict（默认字典）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {})\n",
      "defaultdict(<class 'list'>, {'科幻': ['三体']})\n"
     ]
    }
   ],
   "source": [
    "# 对应代码示例 \n",
    "from collections import defaultdict \n",
    " \n",
    "\n",
    "library = defaultdict(list)  # 每个分类默认用列表装书 \n",
    "print(library)\n",
    "\n",
    "library['科幻'].append('三体')  # 自动创建'科幻'书架 \n",
    "print(library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29.4. <a id='toc29_4_'></a>[ OrderedDict（有序字典）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    " \n",
    "\n",
    "# 保持插入顺序 \n",
    "config = OrderedDict()\n",
    "config['batch_size'] = 32 \n",
    "config['learning_rate'] = 0.001 \n",
    "config.move_to_end('batch_size')   # 移动键位置 \n",
    " \n",
    " \n",
    "# 最近最少使用缓存 \n",
    "class LRUCache:\n",
    "    def __init__(self, capacity):\n",
    "        self.cache  = OrderedDict()\n",
    "        self.capacity  = capacity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29.5. <a id='toc29_5_'></a>[Counter（计数器）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 16), ('orange', 10)]\n",
      "32\n",
      "Counter({'apple': 16, 'orange': 10})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    " \n",
    " \n",
    "# 多维度统计分析 \n",
    "sales = Counter(apple=15, orange=10, banana=5)\n",
    "sales.update(['apple',  'pear'])  # 动态更新 \n",
    " \n",
    "print(sales.most_common(2))   # [('apple', 16), ('orange', 10)]\n",
    "print(sales.total())   # 33（总数量）\n",
    " \n",
    "# 集合运算演示 \n",
    "inventory = Counter(apple=20, orange=15)\n",
    "print(sales & inventory)  # 交集：apple:16 → 取较小值 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30. <a id='toc30_'></a>[multiprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|method|feature|way|方式|\n",
    "|-|-|-|-|\n",
    "|map(func, iterable)| 同一个函数多次调用，顺序一致|`单函数`批量任务|阻塞|\n",
    "|map_async(func, iterable)| 同一个函数多次调用，顺序一致|`单函数`批量任务|异步|\n",
    "|starmap(func, iter)|同一个函数多次调用，多参数展开|`多参数`批量任务|阻塞|\n",
    "|starmap_async(func, iter)|同一个函数多次调用，多参数展开|`多参数`批量任务|异步|\n",
    "|apply(func, args)|异步提交任意函数和参数|不同函数/参数混合任务|阻塞|\n",
    "|apply_async(func, args)|异步提交任意函数和参数|不同函数/参数混合任务|异步|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|执行|with语句|直接调用|\n",
    "|-|-|-|\n",
    "|进入|with Pool(processes) as pool:|pool = Pool(processes)|\n",
    "|退出|自动pool.close()和pool.join()|手动pool.close()和pool.join()|\n",
    "|保险|1. 当_async()提交的任务返回的是一个 AsyncResult 对象，代表“将来会有结果的任务”，但不会阻塞主进程；pool.join() 只是等待任务队列为空，但并不会自动强制所有 AsyncResult 完成或处理异常；2. 如果主进程提前退出（即没有执行 get() 或 wait()），那么有些子进程任务可能还在运行中，就会被销毁（尤其在非 UNIX 平台，如 Windows）；3. 而 with 语句退出后，会清理资源，但不会知道你提交的_async() 中是否还有未完成的任务。||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种写法演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主程序开始 ...\n",
      "中间程序 ...\n",
      "success:  0\n",
      "success:  1\n",
      "success:  4\n",
      "success:  9\n",
      "success:  16\n",
      "success:  25\n",
      "success:  36\n",
      "success:  64\n",
      "success:  49\n",
      "success:  81\n",
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "主程序结束\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count \n",
    "import time \n",
    "\n",
    "\n",
    "def func_demo(x):\n",
    "    time.sleep(1)\n",
    "    return x*x\n",
    "\n",
    "def on_success(success):\n",
    "    print(\"success: \", success)\n",
    "\n",
    "\n",
    "def on_error(error):\n",
    "    print(\"error: \", error)\n",
    "\n",
    "\n",
    "# 1. 定义任务列表\n",
    "# [\n",
    "#   (函数, (参数1, 参数2, ...)), \n",
    "#   (函数, (参数1, 参数2, ...)), \n",
    "#   (函数, (参数1, 参数2, ...)), \n",
    "# ]\n",
    "tasks = [(func_demo, (i,)) for i in range(10)]\n",
    "\n",
    "\n",
    "# 2. 使用with自动回收资源, pool.close() pool.join()\n",
    "with Pool(processes= cpu_count()) as pool:\n",
    "    print(\"主程序开始 ...\")\n",
    "    results = []\n",
    "    for func, params in tasks:\n",
    "        result = pool.apply_async(func, params, callback= on_success, error_callback= on_error)\n",
    "        results.append(result)\n",
    "\n",
    "    print(\"中间程序 ...\")\n",
    "\n",
    "    # 3. 执行AsyncResult的.get()或.wait()。\n",
    "    # 否则随着with结束，子程序也会被销毁的。\n",
    "    for r in results:\n",
    "        print(r.get())\n",
    "    print(\"主程序结束\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主程序开始 ...\n",
      "success:  0\n",
      "success:  1\n",
      "success:  4\n",
      "success:  9\n",
      "success:  16\n",
      "success:  25\n",
      "success:  36\n",
      "success:  49\n",
      "success:  64\n",
      "success:  81\n",
      "中间程序 ...\n",
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "主程序结束\n"
     ]
    }
   ],
   "source": [
    "# 2. 手动pool\n",
    "pool = Pool(processes= cpu_count())\n",
    "print(\"主程序开始 ...\")\n",
    "results = []\n",
    "for func, params in tasks:\n",
    "    result = pool.apply_async(func, params, callback= on_success, error_callback= on_error)\n",
    "    results.append(result)\n",
    "# 3. 手动close和join\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"中间程序 ...\")\n",
    "\n",
    "# 查看结果\n",
    "for r in results:\n",
    "    print(r.get())\n",
    "print(\"主程序结束\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30.1. <a id='toc30_1_'></a>[map and map_async](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count \n",
    "\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 获取CPU核心数\n",
    "    num_cores = cpu_count()\n",
    "    print(f\"Number of CPU cores: {num_cores}\")\n",
    "\n",
    "    # 创建进程池\n",
    "    with Pool(processes= num_cores) as pool:\n",
    "        results = pool.map(square, range(10)) # 并行映射任务, the same function, put all params into one parameter list.\n",
    "    \n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30.2. <a id='toc30_2_'></a>[starmap and starmap_async](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 方法     | 参数传递方式         | 适用场景                     | 示例调用                                  |\n",
    "|----------|----------------------|------------------------------|-------------------------------------------|\n",
    "| `map`    | 单参数（`func(arg)`） | 函数只需一个参数               | `pool.map(func, [1, 2, 3])`               |\n",
    "| `starmap`| 多参数（`func(*args)`）| 函数需要多个参数（如 `x, y`） | `pool.starmap(func, [(1, 2), (3, 4)])`    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30.2.1. <a id='toc30_2_1_'></a>[starmap（同步阻塞）](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- starmap 是 map 的增强版，适用于函数需要多个参数的情况。\n",
    "\n",
    "- 特点:\n",
    "  - 同步执行：主进程会阻塞，直到所有任务完成。\n",
    "  - 直接返回结果：任务完成后，starmap 直接返回结果列表。\n",
    "  - 简单但不够灵活：适合需要立即获取结果的场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def power(x, y):\n",
    "    return x ** y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 参数列表：每个元素是一个元组 (x, y)\n",
    "    params = [(2, 3), (3, 2), (4, 2)]\n",
    "\n",
    "    with Pool(2) as pool:\n",
    "        results = pool.starmap(power, params)\n",
    "        print(results)  # 输出: [8, 9, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：\n",
    "  - starmap 会遍历 params，并将每个元组解包为 power(*args)。\n",
    "  - 同步阻塞：主进程会等待所有任务完成。\n",
    "\n",
    "- 适用场景:\n",
    "  - 需要简单并行计算，且主进程可以等待所有任务完成。\n",
    "  - 不需要在任务运行时执行其他操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30.2.2. <a id='toc30_2_2_'></a>[starmap_async（异步非阻塞）](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- starmap_async 是非阻塞的，返回一个 AsyncResult 对象，可以通过 get() 获取结果。\n",
    "\n",
    "- 特点:\n",
    "  - 异步执行：主进程不会被阻塞，可以继续执行其他代码。\n",
    "  - 返回 AsyncResult 对象：通过 .get() 获取结果（会阻塞，但可以控制时机）。\n",
    "  - 支持回调：任务完成后自动触发回调函数（如日志记录、结果聚合）。\n",
    "  - 更灵活：适合需要并行计算 + 主进程交互的场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主进程继续执行...\n",
      "所有任务完成，结果: [2, 12, 30]\n",
      "最终结果: [2, 12, 30]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "\n",
    "def task(x, y):\n",
    "    time.sleep(1)\n",
    "    return x * y\n",
    "\n",
    "def callback(results):\n",
    "    print(\"所有任务完成，结果:\", results)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    params = [(1, 2), (3, 4), (5, 6)]\n",
    "    with Pool(2) as pool:\n",
    "        async_result = pool.starmap_async(task, params, callback=callback)\n",
    "        print(\"主进程继续执行...\")  # 不会阻塞\n",
    "        # 可以在这里做其他事情\n",
    "        async_result.wait()  # 等待任务完成（非必须）\n",
    "        print(\"最终结果:\", async_result.get())  # 输出: [2, 12, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 特性             | `starmap`                          | `starmap_async`                                      |\n",
    "|------------------|------------------------------------|------------------------------------------------------|\n",
    "| **阻塞行为**     | 同步阻塞，主进程等待所有任务完成   | 异步非阻塞，主进程可继续执行其他操作                 |\n",
    "| **返回值**       | 直接返回结果列表                   | 返回 `AsyncResult` 对象，需调用 `.get()` 获取结果    |\n",
    "| **回调支持**     | 不支持                             | 支持 `callback` 和 `error_callback`                  |\n",
    "| **灵活性**       | 低                                 | 高（可控制结果获取时机、超时等）                     |\n",
    "| **典型场景**     | 简单批处理任务                     | 交互式程序、需要回调的任务链                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何选择？\n",
    "  - 用 starmap：需要简单并行计算，且主进程可以等待所有任务完成（如脚本批处理）。\n",
    "  - 用 starmap_async：需要异步控制、回调、或主进程与任务并行执行（如 GUI、服务端程序）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30.3. <a id='toc30_3_'></a>[apply and apply_aysnc](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<function func_demo at 0x7f3c4d2079c0>, (0,)), (<function func_demo at 0x7f3c4d2079c0>, (1,)), (<function func_demo at 0x7f3c4d2079c0>, (2,)), (<function func_demo at 0x7f3c4d2079c0>, (3,)), (<function func_demo at 0x7f3c4d2079c0>, (4,)), (<function func_demo at 0x7f3c4d2079c0>, (5,)), (<function func_demo at 0x7f3c4d2079c0>, (6,)), (<function func_demo at 0x7f3c4d2079c0>, (7,)), (<function func_demo at 0x7f3c4d2079c0>, (8,)), (<function func_demo at 0x7f3c4d2079c0>, (9,))]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "\n",
    "def func_demo(x):\n",
    "    time.sleep(1)\n",
    "    return x * x\n",
    "\n",
    "\n",
    "# Prepare the taks list constant of tuple containing func_demo and its paramters.\n",
    "# (func_demo, (arg1, arg2, ...))\n",
    "tasks = [(func_demo, (i,)) for i in range(10)]\n",
    "\n",
    "print(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30.3.1. <a id='toc30_3_1_'></a>[apply](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同步阻塞：调用apply会阻塞主进程，直到任务执行完成并返回结果。\n",
    "\n",
    "用法：类似普通函数调用，直接返回结果。\n",
    "\n",
    "适用场景：需要按顺序执行任务且每次只处理一个任务的简单场景（但通常效率较低，不推荐常用）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# apply\n",
    "with Pool(3) as pool:\n",
    "    for func, param in tasks:\n",
    "        result = pool.apply(func_demo, param)  # 阻塞，直到任务完成\n",
    "        print(result)  # 输出: 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30.3.2. <a id='toc30_3_2_'></a>[apply_async](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异步非阻塞：调用后立即返回一个AsyncResult对象，主进程可以继续执行其他操作。\n",
    "\n",
    "需要手动获取结果：通过get()方法获取结果（会阻塞直到任务完成）。\n",
    "\n",
    "适用场景：并行处理多个任务，提高效率；适合需要异步执行的场景。\n",
    "\n",
    "Pool.apply_async() 是异步非阻塞方式，可以提交任意函数 + 参数组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# 阻塞，直到任务完成\n",
    "with Pool(3) as pool:\n",
    "    for func, param in tasks:\n",
    "        result = pool.apply_async(func_demo, param)  # 阻塞，直到任务完成\n",
    "        print(result.get()) # 通过get()方法获取结果（会阻塞直到任务完成）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# 异步非阻塞,调用后立即返回一个AsyncResult对象，主进程可以继续执行其他操作\n",
    "results = []\n",
    "with Pool(3) as pool:\n",
    "    for func, param in tasks:\n",
    "        result = pool.apply_async(func_demo, param)  # 阻塞，直到任务完成\n",
    "        results.append(result)\n",
    "\n",
    "    # must be in the with content.\n",
    "    for result in results:\n",
    "        try:\n",
    "            print(result.get())\n",
    "        except Exception as e:\n",
    "            print(\"发生异常:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "Insert a test, this step don't block above.\n",
      "success:  0\n",
      "0\n",
      "success:  4\n",
      "success:  1\n",
      "1\n",
      "4\n",
      "success:  9\n",
      "9\n",
      "success:  16\n",
      "success:  25\n",
      "16\n",
      "25\n",
      "success:  36\n",
      "36\n",
      "success:  64\n",
      "success:  49\n",
      "49\n",
      "64\n",
      "success:  81\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "def on_success(result):\n",
    "    print(\"success: \", result)\n",
    "\n",
    "\n",
    "def on_error(result):\n",
    "    print(\"error: \", result)\n",
    "\n",
    "\n",
    "# 异步非阻塞,调用后立即返回一个AsyncResult对象，主进程可以继续执行其他操作\n",
    "results = []\n",
    "with Pool(3) as pool:\n",
    "    for func, param in tasks:\n",
    "        result = pool.apply_async(func_demo, param, callback=on_success, error_callback=on_error)  # 阻塞，直到任务完成\n",
    "        results.append(result)\n",
    "        print(\"Insert a test, this step don't block above.\")\n",
    "\n",
    "    for r in results:\n",
    "        print(r.get())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结\n",
    "- 确保主进程等待子进程完成（pool.close() + pool.join()）。\n",
    "- 检查 func_demo 是否正确返回或抛出异常。\n",
    "- apply_async 参数要用 args=(param,)（必须是元组）。\n",
    "- 确保 error_callback 能捕获所有异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. <a id='toc31_'></a>[itertools](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (2, 3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations \n",
    "\n",
    "\n",
    "elements = [1, 2, 3]\n",
    "pair = combinations(elements, 2)\n",
    "list(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements = [1, 2, 3, 4]\n",
    "trip = combinations(elements, 3)\n",
    "list(trip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32. <a id='toc32_'></a>[pip打包](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "my_package/                  # 项目根目录\n",
    "├── my_package/              # 主包目录（名字与项目同名）\n",
    "│   ├── __init__.py          # 必须存在（可为空文件）\n",
    "│   ├── core.py              # 你的主要代码\n",
    "│   └── utils.py             # 辅助工具\n",
    "├── tests/                   # 单元测试（可选）\n",
    "│   └── test_core.py\n",
    "├── setup.py                 # 打包配置文件（核心）\n",
    "├── pyproject.toml           # 构建系统要求（Python 3.10+）\n",
    "├── README.md                # 项目说明\n",
    "└── requirements.txt         # 依赖列表（可选）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.1. <a id='toc32_1_'></a>[README.md](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple tool for Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.2. <a id='toc32_2_'></a>[setup.py](#toc0_)\n",
    "\n",
    "这是打包的核心配置文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages\n",
    "\n",
    "\n",
    "# 打包\n",
    "setup(\n",
    "    name= \"bmp\",                                        # 包名（PyPI唯一标识）\n",
    "    version= \"0.1.0\",                                   # 版本号（遵循语义化版本）\n",
    "    author= \"Yu Zhao\",\n",
    "    author_email= \"zhao_sy@126.com\",\n",
    "    description= \"A simple tools for Deep Learning\",\n",
    "    long_description= open(\"README.md\").read(),\n",
    "    long_description_content_type= \"text/markdown\",\n",
    "    packages= find_packages(),                          # 自动发现所有包\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.3. <a id='toc32_3_'></a>[方式一：本地安装（开发模式）](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在项目根目录运行：\n",
    "\n",
    "- -e 表示可编辑模式，代码修改实时生效\n",
    "\n",
    "- 安装后可通过 import my_package 调用\n",
    "\n",
    "`pip install -e . `这种安装方式会在Python环境中创建到源代码目录的符号链接，而非传统的复制文件到site-packages目录。特别适合在开发阶段使用，因为任何代码修改都会立即生效，无需重新安装包。\n",
    "\n",
    "相比普通的`pip install . `命令，`-e`模式不会将包文件复制到Python的site-packages目录，而是保留源代码在原位置。这样既保持了开发环境的整洁，又允许版本控制系统正常追踪代码变更。当需要最终发布时，仍应使用常规安装方式打包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install .\n",
    "\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.4. <a id='toc32_4_'></a>[方式二：打包为分发文件，再安装](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成可用于分发的 .whl 和 .tar.gz 文件：\n",
    "\n",
    "生成文件位于 dist/ 目录：\n",
    "\n",
    "```python\n",
    "dist/\n",
    "├── my_package-0.1.0-py3-none-any.whl\n",
    "└── my_package-0.1.0.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wheel          # 确保wheel已安装\n",
    "\n",
    "# !python setup.py sdist bdist_wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地测试安装\n",
    "# !pip install dist/bmp-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.5. <a id='toc32_5_'></a>[上传到 PyPI（可选）](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注册 PyPI 账号：[https://pypi.org/](https://pypi.org/)\n",
    "\n",
    "创建账号并获取 API Token\n",
    "\n",
    "安装上传工具: ```pip install twine```\n",
    "\n",
    "上传到 PyPI：```twine upload dist/*```\n",
    "\n",
    "`twine upload -u __token__ -p pypi-your-api-token dist/*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twine upload -u __token__ -p pypi-your-api-token dist/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.6. <a id='toc32_6_'></a>[维护与更新](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32.6.1. <a id='toc32_6_1_'></a>[依赖更新](#toc0_)\n",
    "\n",
    "使用`pip install -e .`安装，只需要修改源码即可。\n",
    "\n",
    "使用`pip install .`安装后需要如下方式进行升级：\n",
    "\n",
    "- pip install -U .  # 更新本地安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U .  # 更新本地安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32.6.2. <a id='toc32_6_2_'></a>[卸载旧版](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall bmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33. <a id='toc33_'></a>[转格式](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook learn_PyTorch.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 44 image(s).\n",
      "[NbConvertApp] Writing 5800157 bytes to Format/learn_PyTorch/learn_PyTorch.html\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# ipynb to html\n",
    "# --NbConvertApp.log_level=ERROR\n",
    "jupyter nbconvert --to html --output-dir ./Format/learn_PyTorch learn_PyTorch.ipynb \n",
    "\n",
    "cp -rf Pytorch_Pictures ./Format/learn_PyTorch\n",
    "# browse translate html to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook learn_PyTorch.ipynb to markdown\n",
      "[NbConvertApp] Support files will be in learn_PyTorch_files/\n",
      "[NbConvertApp] Writing 1162767 bytes to Format/learn_PyTorch/learn_PyTorch.md\n"
     ]
    }
   ],
   "source": [
    "# ipynb to markdown\n",
    "!jupyter nbconvert --to markdown --output-dir ./Format/learn_PyTorch learn_PyTorch.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
