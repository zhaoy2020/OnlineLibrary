{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [HuggingFace](#toc1_)    \n",
    "- 2. [Transformer](#toc2_)    \n",
    "- 3. [BERT](#toc3_)    \n",
    "  - 3.1. [tokenizer](#toc3_1_)    \n",
    "    - 3.1.1. [train a new tokenizer](#toc3_1_1_)    \n",
    "    - 3.1.2. [make tokenizer to be used in transformers with AutoTokenizer](#toc3_1_2_)    \n",
    "  - 3.2. [model](#toc3_2_)    \n",
    "  - 3.3. [datas](#toc3_3_)    \n",
    "  - 3.4. [trainer](#toc3_4_)    \n",
    "- 4. [GPT](#toc4_)    \n",
    "- 5. [T5](#toc5_)    \n",
    "- 6. [BART](#toc6_)    \n",
    "- 7. [LLaMa](#toc7_)    \n",
    "  - 7.1. [Tokenizer](#toc7_1_)    \n",
    "- 8. [DeepSeek](#toc8_)    \n",
    "  - 8.1. [R1](#toc8_1_)    \n",
    "- 9. [ä»€ä¹ˆæ˜¯RAGï¼Ÿ](#toc9_)    \n",
    "  - 9.1. [æ–‡æœ¬çŸ¥è¯†æ£€ç´¢](#toc9_1_)    \n",
    "    - 9.1.1. [çŸ¥è¯†åº“æ„å»º](#toc9_1_1_)    \n",
    "    - 9.1.2. [æŸ¥è¯¢æ„å»º](#toc9_1_2_)    \n",
    "    - 9.1.3. [å¦‚ä½•æ£€ç´¢ï¼Ÿ-æ–‡æœ¬æ£€ç´¢](#toc9_1_3_)    \n",
    "    - 9.1.4. [å¦‚ä½•å–‚ç»™å¤§æ¨¡å‹ï¼Ÿ-ç”Ÿæˆå¢å¼º](#toc9_1_4_)    \n",
    "  - 9.2. [å¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢](#toc9_2_)    \n",
    "  - 9.3. [åº”ç”¨](#toc9_3_)    \n",
    "- 10. [è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹](#toc10_)    \n",
    "  - 10.1. [OpenAI Python SDK](#toc10_1_)    \n",
    "    - 10.1.1. [æ–‡æœ¬ç”Ÿæˆ](#toc10_1_1_)    \n",
    "    - 10.1.2. [ä»£ç è¡¥å…¨](#toc10_1_2_)    \n",
    "    - 10.1.3. [å›¾åƒç”Ÿæˆï¼ˆDALL-Eï¼‰](#toc10_1_3_)    \n",
    "    - 10.1.4. [å›¾åƒè¯†åˆ«](#toc10_1_4_)    \n",
    "    - 10.1.5. [è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆWhisperï¼‰](#toc10_1_5_)    \n",
    "    - 10.1.6. [é”™è¯¯å¤„ç†ä¸æœ€ä½³å®è·µ](#toc10_1_6_)    \n",
    "    - 10.1.7. [openaiåº“çš„é«˜çº§ç”¨æ³•](#toc10_1_7_)    \n",
    "      - 10.1.7.1. [å¼‚æ­¥æ”¯æŒ](#toc10_1_7_1_)    \n",
    "      - 10.1.7.2. [å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼š](#toc10_1_7_2_)    \n",
    "      - 10.1.7.3. [æµå¼å“åº”ï¼š](#toc10_1_7_3_)    \n",
    "  - 10.2. [deepseek-aiçš„SDK](#toc10_2_)    \n",
    "  - 10.3. [curlæ¥å£](#toc10_3_)    \n",
    "- 11. [éƒ¨ç½²å¤§æ¨¡å‹](#toc11_)    \n",
    "  - 11.1. [ä¸‹è½½æ¨¡å‹](#toc11_1_)    \n",
    "  - 11.2. [ollama](#toc11_2_)    \n",
    "    - 11.2.1. [Install and run model](#toc11_2_1_)    \n",
    "    - 11.2.2. [API on web port](#toc11_2_2_)    \n",
    "    - 11.2.3. [Python ollama module](#toc11_2_3_)    \n",
    "      - 11.2.3.1. [demoï¼šç¿»è¯‘ä¸­æ–‡ä¸ºè‹±æ–‡](#toc11_2_3_1_)    \n",
    "  - 11.3. [ktransformers](#toc11_3_)    \n",
    "    - 11.3.1. [Dockerå®‰è£…](#toc11_3_1_)    \n",
    "    - 11.3.2. [ç¼–è¯‘å®‰è£…](#toc11_3_2_)    \n",
    "      - 11.3.2.1. [prepare](#toc11_3_2_1_)    \n",
    "      - 11.3.2.2. [æ–¹å¼ä¸€ï¼špip3 install whl](#toc11_3_2_2_)    \n",
    "      - 11.3.2.3. [æ–¹å¼äºŒï¼šç¼–è¯‘](#toc11_3_2_3_)    \n",
    "    - 11.3.3. [è™šæ‹Ÿæœºä¸­ç¼–è¯‘å®‰è£…](#toc11_3_3_)    \n",
    "    - 11.3.4. [ä½¿ç”¨](#toc11_3_4_)    \n",
    "- 12. [å¯åŠ¨å­é¢„æµ‹](#toc12_)    \n",
    "- 13. [è½¬æ ¼å¼](#toc13_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[HuggingFace](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export HF_ENDPOINT=\"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[Transformer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. <a id='toc3_1_1_'></a>[train a new tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ WordPiece æ¨¡å‹\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# è®¾ç½®è®­ç»ƒå‚æ•°\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30000,        # è¯æ±‡è¡¨å¤§å°\n",
    "    min_frequency=2,         # æœ€å°è¯é¢‘\n",
    "    show_progress=True,      # æ˜¾ç¤ºè¿›åº¦\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# è®­ç»ƒ\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/huggingface/dna_1g.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ä¿å­˜\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/huggingface/dna_wordpiece_dict.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ\n",
    "tokenizer.train(files=[\"data/huggingface/dna_1g.txt\"], trainer=trainer)\n",
    "\n",
    "# ä¿å­˜\n",
    "tokenizer.save(\"data/huggingface/dna_wordpiece_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. <a id='toc3_1_2_'></a>[make tokenizer to be used in transformers with AutoTokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/huggingface/dna_wordpiece_dict/tokenizer_config.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/special_tokens_map.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer = Tokenizer.from_file(\"data/huggingface/dna_wordpiece_dict.json\")\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=new_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# ä¿å­˜\n",
    "wrapped_tokenizer.save_pretrained(\"data/huggingface/dna_wordpiece_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# åŠ è½½\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/huggingface/dna_wordpiece_dict\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6, 766, 22, 10], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç¼–ç \n",
    "tokenizer(\"ATCGGATCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='data/huggingface/dna_wordpiece_dict', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM \n",
    "\n",
    "\n",
    "# é…ç½®\n",
    "max_len = 1024 \n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size = len(tokenizer),\n",
    "    max_position_embeddings=max_len, \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ") \n",
    "\n",
    "# æ¨¡å‹\n",
    "model = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[datas](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1079595\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "\n",
    "raw_dataset = load_dataset('text', data_files='data/huggingface/dna_1g.txt')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 971635\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 107960\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6371b499b174ff89c4e7bb54315a9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/971635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065019d3ba1c46c598849b00771e8a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/107960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer._tokenizer.model.max_input_chars_per_word = 10000\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_len)\n",
    "\n",
    "\n",
    "# å¯¹æ•°æ®é›†åº”ç”¨åˆ†è¯å‡½æ•°\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=False, remove_columns=['text'], num_proc=50)  # è®¾ç½®ä¸ºä½ çš„ CPU æ ¸å¿ƒæ•°æˆ–æ ¹æ®éœ€è¦è°ƒæ•´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ•°æ®æ”¶é›†å™¨ï¼Œç”¨äºåŠ¨æ€å¡«å……å’Œé®è”½,æ³¨æ„mlm=true\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GAATATTTGTCTATTCTTCTTAACTTTCTCCACTGTAAATTAAATTGCTCCTCAGGGTGCTATATGGCATCCCTTGCTATTTTTGGAGCAAATCTTAAATTCTTCAACAATTTTATCAAGACAAACACAACTTTCAGTAAATTCATTGTTTAAATTTGGTGAAAAGTCAGATTTCTTTACACATAGTAAAGCAAATGTAAAATAATATATCAATGTGATTCTTTTAATAAAATACCATTATTGCCAATGGTTTTTAATAGTTCACTGTTTGAAAGAGACCACAAAATTCATGTGCAAAAATCACAAGCATTCTTATACAACAGTGACAGACAAACAGAGAGCCAAATCAGGAATGAACTTCCATTCACAATTGCTTCAAAGAGAATCAAATACCTAGGAATCCAACTTACAAGGGATGTAAAGGACCTCTTCAAGGAGAACTACAAACCACTGCTCAGTGAAATAAAAGAGGACACAAACAAATGGAAGAACATACCATGCTCATGGATAGGAAGAATCAATATCGTGAAAATGGCCATACTGCCCAAGGTAATTTATAGATTCAATGCCATCCCCATCAAGCTACCAATGAGTTTCTTCACAGAATTGGAAAAAACTGTTTTAAAGTTCATATGGAACCAAAAAAGAACCCACATTGCCAAGACAATCCTAAGTCAAATGAACAAAGCTGGAGGGATCATGCTACCTGACTTCAAACTATACTACAAGGCTACAGTAACCAAAATAGCATGGTACTGGTACCAAAACAGAAATATAGACCAATGGAACAGCATAGAGTCCTCAGAAATAATACCACACATCTACATCTTTGATAAATCTGACAAAAACAAGAAATGGGGAAAGGATTCTCTATATAATAAATGGTGCTGGGAAAATTGGCTAGCCATAAGTAGAAAGCTGAAACTGGATCCTTTCCTTACTCTTTATACGAAAATTAATTCAAGATGGAGTAGAGACTTAAATGTTAGACCTAATACCA'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GAA',\n",
       " '##TATTTG',\n",
       " '##TCTATT',\n",
       " '##CTTCTTAA',\n",
       " '##CTTTCTCC',\n",
       " '##A',\n",
       " '##CTGTAAATT',\n",
       " '##AAATT',\n",
       " '##GCTCC',\n",
       " '##TCAGG',\n",
       " '##GTGCTA',\n",
       " '##TATGGCA',\n",
       " '##TCCCTT',\n",
       " '##GCTATTTT',\n",
       " '##TGGAGCAA',\n",
       " '##A',\n",
       " '##TCTTAAA',\n",
       " '##T']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(dataset[\"train\"][0][\"text\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "run_path = \"cache/bert_run\"\n",
    "train_epoches = 5\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=run_path,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=train_epoches,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        save_steps=2000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        fp16=True, #v100æ²¡æ³•ç”¨\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"cache/dna_bert_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[GPT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[T5](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[BART](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[LLaMa](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. <a id='toc7_1_'></a>[Tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"data/huggingface/dna_1g.txt,data/huggingface/protein_1g.txt\", \n",
    "    model_prefix=\"dna_llama\", \n",
    "    vocab_size=60000, \n",
    "    model_type=\"bpe\", \n",
    "    # max_sentence_length=1000000,\n",
    "    num_threads=50, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file=\"dna_llama.model\")\n",
    "\n",
    "tokenizer.encode(\"ATCGGATCG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[DeepSeek](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. <a id='toc8_1_'></a>[R1](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model deepseek-ai/DeepSeek-R1 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3589, in from_pretrained\n    if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/utils/hub.py\", line 655, in has_file\n    response = get_session().head(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 624, in head\n    return self.request(\"HEAD\", url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 93, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 682, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 787d5459-e5a7-4279-8e49-e516f6e6aa22)')\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      5\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m pipe(messages)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    295\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model deepseek-ai/DeepSeek-R1 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3589, in from_pretrained\n    if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/utils/hub.py\", line 655, in has_file\n    response = get_session().head(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 624, in head\n    return self.request(\"HEAD\", url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 93, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 682, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 787d5459-e5a7-4279-8e49-e516f6e6aa22)')\n\n\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)\n",
    "\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. <a id='toc9_'></a>[ä»€ä¹ˆæ˜¯RAGï¼Ÿ](#toc0_)\n",
    "\n",
    "RAGçš„åˆ†ç±»ï¼š\n",
    "\n",
    "|Model | æ£€ç´¢å™¨å¾®è°ƒ | å¤§é¢„è¨€æ¨¡å‹å¾®è°ƒ| ä¾‹å¦‚ |\n",
    "|---|---|---| --- |\n",
    "| é»‘ç›’ | - | - | e.g. In-context ralm |\n",
    "| é»‘ç›’ | æ˜¯ | - | e.g. Rplug |\n",
    "| ç™½ç›’ | - | æ˜¯ | e.g. realm, self-rag |\n",
    "| ç™½ç›’ | æ˜¯ | æ˜¯ | e.g. altas |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. <a id='toc9_1_'></a>[æ–‡æœ¬çŸ¥è¯†æ£€ç´¢](#toc0_)\n",
    "å¦‚ä½•æ£€ç´¢å‡ºç›¸å…³ä¿¡æ¯æ¥è¾…åŠ©æ”¹å–„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè´¨é‡çš„ç³»ç»Ÿã€‚çŸ¥è¯†æ£€ç´¢é€šå¸¸åŒ…æ‹¬çŸ¥è¯†åº“æ„å»ºã€æŸ¥è¯¢æ„å»ºã€æ–‡æœ¬æ£€ç´¢å’Œæ£€ç´¢ç»“æœé‡æ’å››éƒ¨åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1. <a id='toc9_1_1_'></a>[çŸ¥è¯†åº“æ„å»º](#toc0_)\n",
    "æ–‡æœ¬å—çš„çŸ¥è¯†åº“æ„å»ºï¼Œå¦‚ç»´åŸºç™¾ç§‘ã€æ–°é—»ã€è®ºæ–‡ç­‰ã€‚\n",
    "\n",
    "æ–‡æœ¬åˆ†å—ï¼šå°†æ–‡æœ¬åˆ†æˆå¤šä¸ªå—ï¼Œæ¯ä¸ªå—åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå¥å­ã€‚\n",
    "- å›ºå®šå¤§å°å—ï¼šå°†æ–‡æœ¬åˆ†æˆå›ºå®šå¤§å°çš„å—ï¼Œå¦‚æ¯ä¸ªå—åŒ…å«512ä¸ªå­—ç¬¦ã€‚\n",
    "- åŸºäºå†…å®¹å—ï¼šå°†æ–‡æœ¬åˆ†æˆåŸºäºå†…å®¹çš„å—ï¼Œå¦‚æ¯ä¸ªå—åŒ…å«ä¸€ä¸ªå¥å­ã€‚\n",
    "  - é€šè¿‡å¥å­åˆ†å‰²ç¬¦åˆ†å‰²å¥å­ã€‚\n",
    "  - ç”¨LLMè¿›è¡Œåˆ†å‰²\n",
    "\n",
    "çŸ¥è¯†åº“å¢å¼ºï¼šçŸ¥è¯†åº“å¢å¼ºæ˜¯é€šè¿‡æ”¹è¿›å’Œä¸°å¯ŒçŸ¥è¯†åº“çš„å†…å®¹å’Œç»“æ„ï¼Œä¸ºæŸ¥è¯¢æä¾›\"æŠ“æ‰‹â€ï¼ŒåŒ…æ‹¬æŸ¥è¯¢ç”Ÿæˆä¸æ ‡é¢˜ç”Ÿæˆä¸¤ç§æ–¹æ³•ã€‚\n",
    "- ä¼ªæŸ¥è¯¢ç”Ÿæˆ\n",
    "- æ ‡é¢˜ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2. <a id='toc9_1_2_'></a>[æŸ¥è¯¢æ„å»º](#toc0_)\n",
    "æŸ¥è¯¢æ„å»ºï¼šæ—¨åœ¨é€šè¿‡æŸ¥è¯¢å¢å¼ºçš„æ–¹å¼ï¼Œæ‰©å±•å’Œä¸°å¯Œç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰å’Œå†…å®¹ï¼Œæé«˜æ£€ç´¢ç»“æœçš„å‡†ç¡®æ€§å’Œå…¨é¢æ€§ï¼Œâ€œé’©\"å‡ºç›¸åº”å†…å®¹ã€‚å¢å¼ºæ–¹å¼å¯åˆ†ä¸ºè¯­ä¹‰å¢å¼ºä¸å†…å®¹å¢å¼ºã€‚\n",
    "- è¯­ä¹‰å¢å¼ºï¼šåŒä¸€å¥è¯å¤šç§è¡¨è¾¾æ–¹å¼\n",
    "- å†…å®¹å¢å¼ºï¼šå¢åŠ èƒŒæ™¯çŸ¥è¯†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3. <a id='toc9_1_3_'></a>[å¦‚ä½•æ£€ç´¢ï¼Ÿ-æ–‡æœ¬æ£€ç´¢](#toc0_)\n",
    "`æ£€ç´¢å™¨`ï¼šç»™å®šçŸ¥è¯†åº“å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œæ–‡æœ¬æ£€ç´¢æ—¨åœ¨æ‰¾åˆ°çŸ¥è¯†åº“ä¸­ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„çŸ¥è¯†æ–‡æœ¬;æ£€ç´¢æ•ˆç‡å¢å¼ºæ—¨åœ¨è§£å†³æ£€ç´¢æ—¶çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚æ‰€ä»¥æ£€ç´¢è´¨é‡ã€æ£€ç´¢æ•ˆç‡å¾ˆé‡è¦ã€‚å¸¸è§æ£€ç´¢å™¨æœ‰ä¸‰ç±»ï¼š\n",
    "- åˆ¤åˆ«å¼æ£€ç´¢å™¨ï¼š\n",
    "  - ç¨€ç–æ£€ç´¢å™¨ï¼Œe.g. TF-IDF\n",
    "  - åŒå‘ç¼–ç æ£€ç´¢å™¨ï¼Œe.g. ç”¨berté¢„å…ˆå°†æ–‡æœ¬å—è¿›è¡Œç¼–ç æˆå‘é‡\n",
    "  - äº¤å‰ç¼–ç æ£€ç´¢å™¨ï¼Œe.g. \n",
    "- ç”Ÿæˆå¼æ£€ç´¢å™¨ï¼šå™¨ç›´æ¥å°†çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ä¿¡æ¯è®°å¿†åœ¨æ¨¡å‹å‚æ•°ä¸­ã€‚ç„¶åï¼Œåœ¨æ¥æ”¶åˆ°æŸ¥è¯¢è¯·æ±‚æ—¶ï¼Œèƒ½å¤Ÿç›´æ¥ç”Ÿæˆç›¸å…³æ–‡æ¡£çš„æ ‡è¯†ç¬¦å¤ºï¼ˆå³Doc IDï¼‰ï¼Œä»¥å®Œæˆæ£€ç´¢ã€‚\n",
    "- å›¾æ£€ç´¢å™¨ï¼šå›¾æ£€ç´¢å™¨çš„çŸ¥è¯†åº“ä¸ºå›¾æ•°æ®åº“ï¼ŒåŒ…æ‹¬å¼€æ”¾çŸ¥è¯†å›¾è°±å’Œè‡ªå»ºå›¾ä¸¤ç§ï¼Œå®ƒä»¬ä¸€èˆ¬ç”±<ä¸»ä½“ã€è°“è¯å’Œå®¢ä½“>ä¸‰å…ƒç»„æ„æˆã€‚è¿™æ ·åšä¸ä»…å¯ä»¥æ•æ‰æ¦‚å¿µé—´çš„è¯­ä¹‰å…³ç³»ï¼Œè¿˜å…è®¸äººç±»å’Œæœºå™¨å¯ä»¥å…±åŒå¯¹çŸ¥è¯†è¿›è¡Œç†è§£ä¸æ¨ç†ã€‚\n",
    "\n",
    "`é‡æ’å™¨`ï¼šæ£€ç´¢é˜¶æ®µä¸ºäº†ä¿è¯æ£€ç´¢é€Ÿåº¦é€šå¸¸ä¼šæŸå¤±ä¸€å®šçš„æ€§èƒ½ï¼Œå¯èƒ½æ£€ç´¢åˆ°è´¨é‡è¾ƒä½çš„æ–‡æ¡£ã€‚é‡æ’çš„ç›®çš„æ˜¯å¯¹æ£€ç´¢åˆ°çš„æ®µè½è¿›è¡Œè¿›ä¸€æ­¥çš„æ’åºç²¾é€‰ã€‚é‡æ’å¯ä»¥åˆ†ä¸ºåŸºäºäº¤å‰ç¼–ç çš„æ–¹æ³•å’ŒåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.4. <a id='toc9_1_4_'></a>[å¦‚ä½•å–‚ç»™å¤§æ¨¡å‹ï¼Ÿ-ç”Ÿæˆå¢å¼º](#toc0_)\n",
    "RAGå¢å¼ºæ¯”è¾ƒï¼š\n",
    "\n",
    "|æ¶æ„åˆ†ç±»|ä¼˜ç‚¹|ç¼ºç‚¹|\n",
    "|-|-|-|\n",
    "|è¾“å…¥ç«¯prompt|ç®€å•|tokenså¤ªå¤š|\n",
    "|ä¸­é—´å±‚|é«˜æ•ˆ|è€—GPUèµ„æº|\n",
    "|è¾“å‡ºç«¯|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. <a id='toc9_2_'></a>[å¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢](#toc0_)\n",
    "## 9.3. <a id='toc9_3_'></a>[åº”ç”¨](#toc0_)\n",
    "å¯¹è¯æœºå™¨äººã€çŸ¥è¯†åº“æ–‡ç­”..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. <a id='toc10_'></a>[è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. <a id='toc10_1_'></a>[OpenAI Python SDK](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1. <a id='toc10_1_1_'></a>[æ–‡æœ¬ç”Ÿæˆ](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \n\u001b[0;32m----> 7\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://api.deepseek.com/v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/openai/_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "import os\n",
    "import openai \n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\"), \n",
    "    base_url = \"https://api.deepseek.com/v1\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ],\n",
    "    stream=False, \n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# # stream=Trueçš„æ—¶å€™ï¼Œå¯ç”¨æµç¤ºè¿”å›\n",
    "# for chunk in response:\n",
    "#     print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2. <a id='toc10_1_2_'></a>[ä»£ç è¡¥å…¨](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': \"1 + 1\"},\n",
    "    ],\n",
    "    model = \"gpt-3.5-turbo\", \n",
    "    stream = False, \n",
    "    max_tokens = 2048,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3. <a id='toc10_1_3_'></a>[å›¾åƒç”Ÿæˆï¼ˆDALL-Eï¼‰](#toc0_)\n",
    "\n",
    "è°ƒç”¨images.generateç”Ÿæˆå›¾åƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.generate(\n",
    "    prompt=\"ä¸€åªç©¿ç€å®‡èˆªæœçš„çŒ«\",\n",
    "    n=1,\n",
    "    size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "print(response.data.url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.4. <a id='toc10_1_4_'></a>[å›¾åƒè¯†åˆ«](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"è¿™æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                {\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"data:image/png;base64,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\"\n",
    "                    },\n",
    "                    \"type\": \"image_url\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    model='gpt-4o-2024-05-13',\n",
    "    stream=False,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.5. <a id='toc10_1_5_'></a>[è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆWhisperï¼‰](#toc0_)\n",
    "\n",
    "ä½¿ç”¨audio.transcriptionså¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "print(transcript.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.6. <a id='toc10_1_6_'></a>[é”™è¯¯å¤„ç†ä¸æœ€ä½³å®è·µ](#toc0_)\n",
    "\n",
    "å¼‚å¸¸æ•è·ï¼šå¤„ç†APIè¯·æ±‚ä¸­çš„å¸¸è§é”™è¯¯ï¼ˆå¦‚è®¤è¯å¤±è´¥ã€è¶…æ—¶ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import APIError\n",
    "try:\n",
    "    response = client.chat.completions.create(...)\n",
    "except APIError as e:\n",
    "    print(f\"APIè¯·æ±‚å¤±è´¥: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.7. <a id='toc10_1_7_'></a>[openaiåº“çš„é«˜çº§ç”¨æ³•](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1.7.1. <a id='toc10_1_7_1_'></a>[å¼‚æ­¥æ”¯æŒ](#toc0_)\n",
    "\n",
    "SDKæä¾›äº†å¼‚æ­¥å®¢æˆ·ç«¯AsyncOpenAI,ä½¿ç”¨æ–¹æ³•ä¸åŒæ­¥å®¢æˆ·ç«¯ç±»ä¼¼,åªéœ€åœ¨APIè°ƒç”¨å‰åŠ ä¸Šawait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1.7.2. <a id='toc10_1_7_2_'></a>[å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼š](#toc0_)\n",
    "\n",
    "æ”¯æŒå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå®šåˆ¶åŒ–è®­ç»ƒï¼Œéœ€å‡†å¤‡æ•°æ®é›†å¹¶è°ƒç”¨fine_tuning.jobs.create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1.7.3. <a id='toc10_1_7_3_'></a>[æµå¼å“åº”ï¼š](#toc0_)\n",
    "\n",
    "é€šè¿‡stream=Trueå®ç°é€è¯å®æ—¶è¾“å‡ºï¼Œé€‚ç”¨äºäº¤äº’å¼åœºæ™¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[...],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices.delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. <a id='toc10_2_'></a>[deepseek-aiçš„SDK](#toc0_)\n",
    "\n",
    "- Temperature è®¾ç½®ï¼Œå‚æ•°é»˜è®¤ä¸º 1.0ã€‚\n",
    "\n",
    "|åœºæ™¯\t|æ¸©åº¦|\n",
    "|-|-|\n",
    "|ä»£ç ç”Ÿæˆ/æ•°å­¦è§£é¢˜â€ƒâ€ƒâ€ƒ\t|0.0|\n",
    "|æ•°æ®æŠ½å–/åˆ†æ\t|1.0|\n",
    "|é€šç”¨å¯¹è¯\t|1.3|\n",
    "|ç¿»è¯‘\t|1.3|\n",
    "|åˆ›æ„ç±»å†™ä½œ/è¯—æ­Œåˆ›ä½œ\t|1.5|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "import os \n",
    "from openai import OpenAI \n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = \"sk-\", \n",
    "    base_url = \"https://api.deepseek.com/v1\",            # å‡ºäºä¸ OpenAI å…¼å®¹è€ƒè™‘ï¼Œæ­¤å¤„ v1 ä¸æ¨¡å‹ç‰ˆæœ¬æ— å…³ã€‚\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncPage[Model](data=[Model(id='deepseek-chat', created=None, object='model', owned_by='deepseek'), Model(id='deepseek-reasoner', created=None, object='model', owned_by='deepseek')], object='list')\n"
     ]
    }
   ],
   "source": [
    "print(client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model=\"deepseek-chat\",        # DeepSeek-V3\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-reasoner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# DeepSeek æœ€æ–°æ¨å‡ºçš„æ¨ç†æ¨¡å‹ DeepSeek-R1\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/openai/resources/chat/completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    # model=\"deepseek-chat\",        # DeepSeek-V3\n",
    "    model = \"deepseek-reasoner\",    # DeepSeek æœ€æ–°æ¨å‡ºçš„æ¨ç†æ¨¡å‹ DeepSeek-R1\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. <a id='toc10_3_'></a>[curlæ¥å£](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "curl https://api.deepseek.com/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer <DeepSeek API Key>\" \\\n",
    "  -d '{\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [\n",
    "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "          {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "        ],\n",
    "        \"stream\": false\n",
    "      }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. <a id='toc11_'></a>[éƒ¨ç½²å¤§æ¨¡å‹](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. <a id='toc11_1_'></a>[ä¸‹è½½æ¨¡å‹](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
    "hfd.sh deepseek-ai/DeepSeek-R1 -x 10 -j 10 \n",
    "\n",
    "# https://huggingface.co/unsloth/DeepSeek-R1-GGUF\n",
    "hfd.sh unsloth/DeepSeek-R1-GGUF -x 10 -j 10 --include DeepSeek-R1-Q8_0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. <a id='toc11_2_'></a>[ollama](#toc0_)\n",
    "### 11.2.1. <a id='toc11_2_1_'></a>[Install and run model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the serve\n",
    "ollama serve\n",
    "\n",
    "# list all model images\n",
    "ollama list \n",
    "\n",
    "# run model from image\n",
    "ollama run model_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2. <a id='toc11_2_2_'></a>[API on web port](#toc0_)\n",
    "communicatation with local model via web port.\n",
    "\n",
    "`generate` and `chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   456  100   315  100   141    142     64  0:00:02  0:00:02 --:--:--   206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"deepseek-r1:7b\",\"created_at\":\"2025-02-18T02:49:32.744934023Z\",\"response\":\"{\\\"}\\u003cthink\\u003e{\\\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n:\\n\\n{\\n\\n}\\n\\n}\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\",\"done\":false}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"deepseek-r1:7b\",\n",
    "  \"prompt\": \"Who are you?\",\n",
    "  \"stream\": false,\n",
    "  \"options\": {\n",
    "    \"temperature\": 0.6\n",
    "  },\n",
    "  \"format\": \"json\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5034    0  4905  100   129    326      8  0:00:16  0:00:15  0:00:01   719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"deepseek-r1:7b\",\"created_at\":\"2025-02-18T02:49:26.56791501Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\u003cthink\\u003e\\nOkay, so I just read that \\\"Why is the sky blue?\\\" and now I'm trying to figure it out myself. Let me think through this step by step.\\n\\nFirst off, when you look at the sky on a clear day, it's usually blue, especially during the day when the sun is out. But sometimes I've seen it turn other colors too, like red in the evening or during sunrise. So why is it mostly blue?\\n\\nI know that light travels through the atmosphere, but how does it get colored? I remember learning about something called Rayleigh scattering from my science class. Let me try to recall what that was about. Rayleigh scattering involves light interacting with particles much smaller than the wavelength of light itself. When sunlight enters the Earth's atmosphere, it reaches tiny molecules in the air, like nitrogen and oxygen.\\n\\nWait, so these small particles scatter the sunlight in all directions. But why does this result in a blue sky? I think it has something to do with the wavelengths of light. Visible light ranges from violet to red, right? And I remember that violet light has a shorter wavelength than blue. So maybe the shorter wavelengths are scattered more.\\n\\nBut if blue is scattered more, wouldn't it be easier to see at night when there's less atmosphere overhead? Hmm, but then why does the sky turn red in the evening or during sunrise?\\n\\nOh right! During sunrise and sunset, the light has to pass through a much thicker layer of atmosphere. That makes sense because we're looking at the light after it's been scattered through more particles. The longer path means that all the shorter wavelengths (like violet) are scattered out, leaving red to dominate because it has a longer wavelength.\\n\\nSo during the day, when the sun is directly overhead, blue and green wavelengths get scattered away by Rayleigh scattering, making the sky appear blue. But in the early morning or late afternoon, as the sun is near the horizon, the light has to pass through more atmosphere, so red comes through because it's not scattered as much.\\n\\nWait, but isn't there also something called Mie scattering? I think that happens when particles are larger than the wavelength of light. Does that affect the color of the sky too?\\n\\nI believe Mie scattering is more significant for larger particles, like dust or droplets in clouds. So it might cause some effects we see during sunrise and sunset, but not as much as Rayleigh does for small molecules.\\n\\nSo to summarize: The sky appears blue on a clear day because blue light scatters more in the atmosphere due to Rayleigh scattering by tiny gas molecules. During sunrise and sunset, the longer path allows red light to dominate, giving the sky its reddish hues.\\n\\nBut wait, what about when we see other colors? I mean, sometimes during the day it's not just blue; I've seen green or yellow in some places. Is that because of Rayleigh scattering changing as the atmosphere gets thicker?\\n\\nOr maybe it's due to other factors like pollution or particles in the air affecting light differently. That might complicate things.\\n\\nAlso, does humidity play a role? Sometimes when it's humid, does the sky appear clearer instead of blue? I think higher humidity can affect how light scatters because more water vapor means more molecules to scatter off.\\n\\nSo maybe on days with high humidity, the Rayleigh scattering is less effective, making the sky look different. But that's probably a secondary effect compared to the basic reason for the color being blue.\\n\\nIn any case, I think the primary reason is Rayleigh scattering by nitrogen and oxygen in the atmosphere causing shorter wavelengths like blue to scatter more, resulting in a blue sky during clear days.\\n\\u003c/think\\u003e\\n\\nThe sky appears blue primarily due to a phenomenon known as Rayleigh scattering. When sunlight enters Earth's atmosphere, it interacts with tiny molecules of nitrogen and oxygen. These particles scatter shorter wavelengths of light, such as blue and violet, which have shorter wavelengths than red or orange. This scattering is more effective for shorter wavelengths, causing the sky to appear blue during the day.\\n\\nDuring sunrise and sunset, the light passes through a thicker layer of atmosphere, where all shorter wavelengths are scattered away, leaving longer wavelengths like red to dominate, resulting in the reddish hues observed at these times.\\n\\nOther factors such as humidity, pollution, or atmospheric particles can influence how light scatters, but the primary reason for the sky's blue color remains Rayleigh scattering by nitrogen and oxygen molecules.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":15024265818,\"load_duration\":22057058,\"prompt_eval_count\":9,\"prompt_eval_duration\":7000000,\"eval_count\":901,\"eval_duration\":14993000000}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"deepseek-r1:7b\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ],\n",
    "  \"stream\": false\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3. <a id='toc11_2_3_'></a>[Python ollama module](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "texts = '''\n",
    "è¯¦ç»†æ¯”è¾ƒdeepseekæ¯å…¬å¸å’ŒopenAIå…¬å¸çš„åŒºåˆ«\n",
    "'''\n",
    "\n",
    "# model_card = \"deepseek-r1:7b\"\n",
    "model_card = \"modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF\"\n",
    "\n",
    "# æ–¹å¼ä¸€ï¼ˆéæµå¼è¾“å‡ºï¼‰ï¼š\n",
    "# outputs = ollama.generate(model_card, inputs)\n",
    "# print(f'{outputs['response']}')\n",
    "\n",
    "# æ–¹å¼äºŒï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š\n",
    "outputs = ollama.generate(\n",
    "    stream=True,\n",
    "    model=model_card,\n",
    "    prompt=texts,\n",
    ")\n",
    "for chunk in outputs:\n",
    "    if not chunk['done']:\n",
    "        print(f'{chunk['response']}', end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.1. <a id='toc11_2_3_1_'></a>[demoï¼šç¿»è¯‘ä¸­æ–‡ä¸ºè‹±æ–‡](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "å—¯ï¼Œé¦–å…ˆæˆ‘è¦ç†è§£è¿™ä¸ªé¢˜ç›®çš„æ„æ€ã€‚â€œåŸºäºæ·±åº¦å­¦ä¹ â€æŒ‡çš„æ˜¯ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥è¿›è¡Œç ”ç©¶ã€‚è€Œâ€œå¯¹æ¯è‰èŠ½èƒæ†èŒèŠ½èƒå½¢æˆç›¸å…³åŸºå› çš„ç ”ç©¶â€åˆ™æ˜¯å…·ä½“çš„ç ”ç©¶å†…å®¹ï¼Œæ¶‰åŠåˆ°æ¯è‰èŠ½èƒæ†èŒåœ¨å½¢æˆèŠ½èƒè¿‡ç¨‹ä¸­ç›¸å…³çš„åŸºå› ã€‚\n",
      "\n",
      "æˆ‘éœ€è¦æŠŠè¿™æ•´ä¸ªå¥å­å‡†ç¡®åœ°ç¿»è¯‘æˆè‹±æ–‡ã€‚é¦–å…ˆï¼Œâ€œåŸºäºæ·±åº¦å­¦ä¹ â€å¯ä»¥ç›´æ¥ç¿»è¯‘ä¸ºâ€œBased on deep learningâ€ã€‚æ¥ä¸‹æ¥æ˜¯â€œç ”ç©¶â€ï¼Œå¯¹åº”çš„è‹±æ–‡æ˜¯â€œstudyâ€ã€‚ç„¶åæ˜¯â€œæ¯è‰èŠ½èƒæ†èŒâ€ï¼Œè¿™ä¸ªåº”è¯¥æ˜¯ä¸€ä¸ªä¸“æœ‰åè¯ï¼Œå¯èƒ½éœ€è¦æŸ¥ä¸€ä¸‹æ­£ç¡®çš„è‹±è¯‘åç§°ï¼Œæ¯”å¦‚â€œBacillus subtilisâ€ã€‚\n",
      "\n",
      "æ¥ç€æ˜¯â€œèŠ½èƒå½¢æˆç›¸å…³åŸºå› â€ï¼Œè¿™éƒ¨åˆ†å¯ä»¥ç¿»è¯‘ä¸ºâ€œgenes related to spore formationâ€ã€‚æœ€åï¼ŒæŠŠæ•´ä¸ªå¥å­è¿è´¯èµ·æ¥ï¼Œå°±æ˜¯â€œBased on deep learning study of genes related to spore formation in Bacillus subtilis.â€\n",
      "\n",
      "è¿™æ ·ç»„åˆèµ·æ¥ï¼Œæ—¢å‡†ç¡®ä¼ è¾¾äº†åŸæ„ï¼Œåˆç¬¦åˆè‹±æ–‡çš„è¡¨è¾¾ä¹ æƒ¯ã€‚æˆ‘è§‰å¾—è¿™ä¸ªç¿»è¯‘åº”è¯¥æ˜¯æ¯”è¾ƒä¸“ä¸šå’Œå‡†ç¡®çš„ã€‚\n",
      "</think>\n",
      "\n",
      "Study of Genes Related to Spore Formation in *Bacillus subtilis* Based on Deep Learningâš¡\n",
      "<think>\n",
      "å¥½çš„ï¼Œé¦–å…ˆæˆ‘è¦ç†è§£ç”¨æˆ·çš„éœ€æ±‚ã€‚ä»–ç»™äº†ä¸€ä¸ªä¸­è‹±å¯¹ç…§çš„å¥å­ï¼Œè¦æ±‚ä¸“ä¸šç¿»è¯‘ï¼Œå¹¶ä¸”éœ€è¦å°†ä¸­æ–‡å¥å­â€œé€šè¿‡å®åŸºå› ç»„ç ”ç©¶å¾®ç”Ÿç‰©ä¸æ¤ç‰©ç›¸äº’ä½œç”¨çš„æœºåˆ¶ã€‚â€å‡†ç¡®åœ°ç¿»è¯‘æˆè‹±æ–‡ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦åˆ†æåŸæ–‡çš„æ„æ€ã€‚å¥å­çš„ä¸»å¹²æ˜¯â€œé€šè¿‡å®åŸºå› ç»„ç ”ç©¶â€¦â€ï¼Œè¿™é‡Œçš„å…³é”®è¯æœ‰â€œå®åŸºå› ç»„â€ã€â€œå¾®ç”Ÿç‰©â€ã€â€œæ¤ç‰©â€ä»¥åŠâ€œç›¸äº’ä½œç”¨çš„æœºåˆ¶â€ã€‚æ‰€ä»¥ï¼Œé¦–å…ˆè¦ç¡®å®šè¿™äº›æœ¯è¯­åœ¨è‹±æ–‡ä¸­çš„å‡†ç¡®å¯¹åº”è¯ã€‚\n",
      "\n",
      "â€œå®åŸºå› ç»„â€é€šå¸¸ç¿»è¯‘ä¸ºâ€œmetagenomeâ€æˆ–è€…â€œmeta-genomicsâ€ï¼Œä½†æ›´å¸¸è§çš„æ˜¯ä½¿ç”¨â€œmetagenomicsâ€æ¥è¡¨ç¤ºè¿™ä¸€ç ”ç©¶é¢†åŸŸã€‚å› æ­¤ï¼Œè¿™é‡Œé€‰æ‹©â€œmetagenomicsâ€ä½œä¸ºç¿»è¯‘ã€‚\n",
      "\n",
      "ç„¶åï¼Œâ€œé€šè¿‡â€¦ç ”ç©¶â€¦â€çš„ç»“æ„åœ¨è‹±æ–‡ä¸­å¯ä»¥ç”¨â€œthroughâ€æˆ–è€…â€œby means ofâ€æ¥è¡¨è¾¾ï¼Œä½†ä¸ºäº†ç®€æ´å’Œä¸“ä¸šï¼Œç›´æ¥ä½¿ç”¨â€œThroughâ€æ¯”è¾ƒåˆé€‚ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥æ˜¯â€œå¾®ç”Ÿç‰©ä¸æ¤ç‰©ç›¸äº’ä½œç”¨çš„æœºåˆ¶â€ã€‚è¿™é‡Œéœ€è¦æ³¨æ„è¯­åºå’Œç”¨è¯ã€‚æ•´ä½“ç»“æ„åº”è¯¥æ˜¯â€œthe mechanisms underlying the interactions between microorganisms and plants.â€ è¿™æ ·ä¸ä»…æ¸…æ™°ï¼Œè€Œä¸”ç¬¦åˆå­¦æœ¯å†™ä½œçš„è§„èŒƒã€‚\n",
      "\n",
      "æœ€åï¼ŒæŠŠè¿™äº›éƒ¨åˆ†ç»„åˆèµ·æ¥ï¼Œç¡®ä¿å¥å­é€šé¡ºä¸”å‡†ç¡®ã€‚æ‰€ä»¥ï¼Œæœ€ç»ˆç¿»è¯‘ä¸ºï¼šâ€œThrough metagenomics research on the mechanisms of interaction between microorganisms and plants.â€\n",
      "\n",
      "åœ¨æ•´ä¸ªæ€è€ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘æ³¨æ„åˆ°ç”¨æˆ·å¯èƒ½æ˜¯åœ¨æ’°å†™å­¦æœ¯è®ºæ–‡æˆ–è€…å‡†å¤‡ç ”ç©¶æŠ¥å‘Šï¼Œå› æ­¤å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§æ˜¯å…³é”®ã€‚æ­¤å¤–ï¼Œä¿æŒå¥å­çš„ç®€æ´ä¹Ÿæ˜¯å¿…è¦çš„ï¼Œä»¥ä¾¿è¯»è€…èƒ½å¤Ÿå¿«é€Ÿç†è§£å†…å®¹ã€‚\n",
      "\n",
      "æœ€åï¼Œå†æ£€æŸ¥ä¸€éç¿»è¯‘æ˜¯å¦å¿ å®äºåŸæ–‡ï¼Œå¹¶ä¸”ç¬¦åˆè‹±è¯­è¡¨è¾¾ä¹ æƒ¯ã€‚ç¡®è®¤æ— è¯¯åï¼Œå°±å¯ä»¥å°†è¿™ä¸ªç¿»è¯‘ç»“æœæä¾›ç»™ç”¨æˆ·äº†ã€‚\n",
      "</think>\n",
      "\n",
      "Through metagenomics research on the mechanisms of interaction between microorganisms and plants.âš¡\n"
     ]
    }
   ],
   "source": [
    "import ollama \n",
    "\n",
    "\n",
    "class zh2en():\n",
    "    def __init__(self, model_card):\n",
    "        self.model_card = model_card\n",
    "        \n",
    "    def build_prompt(self, texts):\n",
    "        # with open(prompt_template_path, 'r') as f:\n",
    "        #     prompt_template = f.read()\n",
    "        #     # str with replace function\n",
    "        #     prompt = prompt_template.replace(var, texts)\n",
    "        prompt_template = \"\"\"\n",
    "        ä¸“ä¸šç¿»è¯‘ï¼š\\n\n",
    "        ---\\n\n",
    "        {Chinese_words} \\n\n",
    "        --- \\n\n",
    "        ä½œä¸ºç¿»è¯‘ä¸“å®¶ï¼Œå°†ä¸Šè¿°ä¸­æ–‡å‡†ç¡®ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚ \\n\n",
    "        \"\"\"\n",
    "        prompt = prompt_template.replace(\"{Chinese_words}\", texts)\n",
    "        return prompt\n",
    "\n",
    "    def translate(self, texts):\n",
    "        prompt = self.build_prompt(texts = texts)\n",
    "        # key step\n",
    "        outputs = ollama.generate(\n",
    "            stream=True,\n",
    "            model=self.model_card,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "        for chunk in outputs:\n",
    "            if not chunk['done']:\n",
    "                print(f'{chunk['response']}', end='', flush=True)\n",
    "            else:\n",
    "                print('âš¡')\n",
    "\n",
    "\n",
    "translater = zh2en(model_card='modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF')\n",
    "\n",
    "translater.translate('åŸºäºæ·±åº¦å­¦ä¹ çš„å¯¹æ¯è‰èŠ½èƒæ†èŒèŠ½èƒå½¢æˆç›¸å…³åŸºå› çš„ç ”ç©¶ã€‚')\n",
    "translater.translate('é€šè¿‡å®åŸºå› ç»„ç ”ç©¶å¾®ç”Ÿç‰©ä¸æ¤ç‰©ç›¸äº’ä½œç”¨çš„æœºåˆ¶ã€‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3. <a id='toc11_3_'></a>[ktransformers](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1. <a id='toc11_3_1_'></a>[Dockerå®‰è£…](#toc0_)\n",
    "\n",
    "[https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md](https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the image from docker hub \n",
    "# about 19 GB\n",
    "# docker pull approachingai/ktransformers:0.1.1\n",
    "docker pull approachingai/ktransformers:0.2.1\n",
    "\n",
    "# docker run \\\n",
    "#     --gpus all \\\n",
    "#     -v /path/to/models:/models \\\n",
    "#     -p 10002:10002 \\\n",
    "#     approachingai/ktransformers:v0.1.1 \\\n",
    "#     --port 10002 \\\n",
    "#     --gguf_path /models/path/to/gguf_path \\\n",
    "#     --model_path /models/path/to/model_path \\\n",
    "#     --web True\n",
    "\n",
    "# Directly run\n",
    "docker run  \\\n",
    "    --gpus all \\\n",
    "    -v /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai:/models \\\n",
    "    -p 10002:10002 \\\n",
    "    approachingai/ktransformers:0.1.1 \\\n",
    "    --port 10002 \\\n",
    "    --model_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1 \\\n",
    "    --gguf_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1-Q4_K_M_GGUF \\\n",
    "    --web True\n",
    "\n",
    "# or\n",
    "docker run \\\n",
    "    # -d \\\n",
    "    --gpus all \\\n",
    "    -it \\\n",
    "    -p 10002:10002 \\\n",
    "    -v /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai:/models \\\n",
    "    approachingai/ktransformers:0.1.1 \\\n",
    "    /bin/bash \n",
    "\n",
    "## and then\n",
    "docker exec -it container_ID /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA:\n",
    "\n",
    "- Q: Dockerè¿è¡Œåå‡ºç° Illegal instruction (core dumped)æŠ¥é”™\n",
    "  - [https://github.com/kvcache-ai/ktransformers/issues/356](https://github.com/kvcache-ai/ktransformers/issues/356)\n",
    "  - é‡æ–°ç¼–è¯‘ä»¥ä¸‹:\n",
    "    ```bash\n",
    "    USE_NUMA=1\n",
    "    bash install.sh\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2. <a id='toc11_3_2_'></a>[ç¼–è¯‘å®‰è£…](#toc0_)\n",
    "\n",
    "[https://kvcache-ai.github.io/ktransformers/en/install.html](https://kvcache-ai.github.io/ktransformers/en/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.1. <a id='toc11_3_2_1_'></a>[prepare](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"ktransformers\"\n",
    "\n",
    "conda create -n $name python=3.11 -y \n",
    "conda activate $name\n",
    "\n",
    "\n",
    "# Install CudaToolkit and nvcc ...\n",
    "conda install nvidia/label/cuda-12.4.0::cuda -y --channel nvidia/label/cuda-12.4.0\n",
    "\n",
    "# Anaconda provides a package called `libstdcxx-ng` that includes a newer version of `libstdc++`, which can be installed via `conda-forge`.\n",
    "conda install -c conda-forge libstdcxx-ng -y \n",
    "\n",
    "strings ~/miniconda3/envs/${name}/lib/libstdc++.so.6 | grep GLIBCXX\n",
    "\n",
    "\n",
    "# Install PyTorch via pip ...\n",
    "# pip3 install torch torchvision torchaudio\n",
    "conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -c pytorch -y\n",
    "\n",
    "# pip3 install packaging ninja cpufeature numpy\n",
    "conda install conda-forge::ninja conda-forge::packaging anaconda::numpy -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.2. <a id='toc11_3_2_2_'></a>[æ–¹å¼ä¸€ï¼špip3 install whl](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ktransformers\n",
    "pip3 install ktransformers-0.2.1.post1+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n",
    "\n",
    "# flash_attn\n",
    "# pip3 install flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp311-cp311-linux_x86_64.whl\n",
    "pip3 install flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
    "\n",
    "# flashinfer\n",
    "pip3 install flashinfer_python-0.2.2+cu124torch2.4-cp38-abi3-linux_x86_64.whl\n",
    "python -c \"import torch; print(torch.cuda.get_device_capability())\"\n",
    "export TORCH_CUDA_ARCH_LIST=\"8.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.3. <a id='toc11_3_2_3_'></a>[æ–¹å¼äºŒï¼šç¼–è¯‘](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your system has dual sockets and double size RAM than the model's size (e.g. 1T RAM for 512G model)\n",
    "export USE_NUMA=1\n",
    "bash install.sh # or `make dev_install`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3. <a id='toc11_3_3_'></a>[è™šæ‹Ÿæœºä¸­ç¼–è¯‘å®‰è£…](#toc0_)\n",
    "\n",
    "ç”±äº`GLIBCxxx`æŠ¥é”™ï¼Œæ”¹ç”¨è™šæ‹Ÿæœºä¸­ubuntuå®‰è£…ktransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‹‰å–åŒ…å«GLIBC 2.29+çš„é•œåƒï¼ˆå¦‚Ubuntu 20.04ï¼‰\n",
    "docker pull ubuntu:20.04\n",
    "\n",
    "# å¯åŠ¨å®¹å™¨å¹¶æŒ‚è½½é¡¹ç›®ç›®å½• \n",
    "# docker run -it -v /path/to/your/code:/app ubuntu:20.04 /bin/bash \n",
    "docker run \\\n",
    "    --gpus all \\\n",
    "    -p 10002:10002\\\n",
    "    -it \\\n",
    "    ubuntu:20.04 /bin/bash \n",
    "\n",
    "# Install wget and then \n",
    "apt-get update \n",
    "apt-get install wget \n",
    "\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh\n",
    "\n",
    "docker exec --gpus all -it ubuntu:20.04 /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.4. <a id='toc11_3_4_'></a>[ä½¿ç”¨](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†ä¸»æœºå’Œå®¹å™¨çš„10002ç«¯å£åšæ˜ å°„\n",
    "docker run --gpus all -p 10002:10002 -it \\\n",
    "    ubuntu_2004:ktransformers /bin/bash\n",
    "\n",
    "\n",
    "# æ£€æŸ¥æ˜ å°„\n",
    "docker port [å®¹å™¨ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we use dual socket, we set cpu_infer to 65\n",
    "python -m ktransformers.local_chat \\\n",
    "    --model_path DeepSeek-R1 \\\n",
    "    --gguf_path DeepSeek-R1-Q4_K_M_GGUF \\\n",
    "    --cpu_infer 60 \\\n",
    "    --max_new_tokens 10000 \\\n",
    "    --cache_lens 50000 \\\n",
    "    --total_context 50000 \\\n",
    "    --cache_q4 true \\\n",
    "    --temperature 0.6 \\\n",
    "    --top_p 0.95 \\\n",
    "    --force_think \\\n",
    "    --use_cuda_graph \\\n",
    "    --port 10002\n",
    "    # --optimize_config_path ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml \\\n",
    "    # --host 127.0.0.1 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32K\n",
    "# 64K\n",
    "32768 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://localhost:10002/web/index.html#/chat\n",
    "# using multi-GPU with DeepSeek-V3-configures (V3 is the same as R1)\n",
    "# increace cache_lens to take up more GPU memory, when get long context\n",
    "ktransformers \\\n",
    "    --model_path /models/DeepSeek-R1/ \\\n",
    "    --gguf_path /models/DeepSeek-R1-Q4_K_M_GGUF/ \\\n",
    "    --use_cuda_graph \\\n",
    "    --temperature 0.6 \\\n",
    "    --port 10002 \\\n",
    "    --cpu_infer 62 \\\n",
    "    --cache_lens 32768 \\\n",
    "    --total_context 32768 \\\n",
    "    --optimize_config_path /workspace/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml \\\n",
    "    # --optimize_config_path /workspace/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml \\\n",
    "    # --web True \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   213  100    81  100   132  27000  44000 --:--:-- --:--:-- --:--:-- 71000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"models\":[{\"name\":\"DeepSeek-Coder-V2-Instruct\",\"modified_at\":\"123\",\"size\":123}]}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X 'GET' \\\n",
    "  'http://localhost:10002/api/tags' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": \"tell a joke\",\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"DeepSeek-R1\",\n",
    "  \"stream\": true\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"<think>\\n\\n\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"</think>\\n\\n\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"Sure! \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"Here's \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"a \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"light-hearted \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"joke \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"for \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"you:\\n\\n\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"Why \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"donâ€™t \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"skeletons \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"fight \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"each \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"other?  \\n\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"â€¦Because \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"they \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"donâ€™t \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"have \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"the \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"*guts*!  \\n\\n\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"ğŸ˜„ \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"Got \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"any \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"more \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100 20102    0 19970  100   132    737      4  0:00:33  0:00:27  0:00:06  1213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"requests? \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"Iâ€™m \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"here \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"to \",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"help!\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null}],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "\n",
      "data: {\"id\":\"6d273293-d06b-4ae0-acdb-574d38242696\",\"choices\":[],\"created\":1742001772,\"model\":\"DeepSeek-Coder-V2-Instruct\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":53,\"prompt_tokens\":7,\"total_tokens\":60,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}\n",
      "\n",
      "data: [DONE]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:10002/v1/chat/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": \"tell a joke\",\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"DeepSeek-R1\",\n",
    "  \"stream\": true\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:10002/api/generate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{\n",
    "  \"model\": \"DeepSeek-R1\",\n",
    "  \"prompt\": \"tell me a joke\",\n",
    "  \"stream\": true\n",
    "}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. <a id='toc12_'></a>[å¯åŠ¨å­é¢„æµ‹](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. <a id='toc13_'></a>[è½¬æ ¼å¼](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LLMs.ipynb to html\n",
      "[NbConvertApp] Writing 482390 bytes to Format/LLMs/LLMs.html\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# ipynb to html\n",
    "jupyter nbconvert \\\n",
    "    --to html LLMs.ipynb \\\n",
    "    --output-dir=./Format/LLMs \\\n",
    "    # --NbConvertApp.log_level=ERROR\n",
    "\n",
    "cp -rf Pytorch_Pictures ./Format/LLMs/\n",
    "# browse translate html to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LLMs.ipynb to markdown\n",
      "[NbConvertApp] Writing 56470 bytes to Format/LLMs/LLMs.md\n"
     ]
    }
   ],
   "source": [
    "# ipynb to markdown\n",
    "!jupyter nbconvert --to markdown LLMs.ipynb --output-dir=./Format/LLMs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
