{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [HuggingFace](#toc1_)    \n",
    "- 2. [Transformer](#toc2_)    \n",
    "- 3. [BERT](#toc3_)    \n",
    "  - 3.1. [tokenizer](#toc3_1_)    \n",
    "    - 3.1.1. [train a new tokenizer](#toc3_1_1_)    \n",
    "    - 3.1.2. [make tokenizer to be used in transformers with AutoTokenizer](#toc3_1_2_)    \n",
    "  - 3.2. [model](#toc3_2_)    \n",
    "  - 3.3. [datas](#toc3_3_)    \n",
    "  - 3.4. [trainer](#toc3_4_)    \n",
    "- 4. [GPT](#toc4_)    \n",
    "- 5. [T5](#toc5_)    \n",
    "- 6. [BART](#toc6_)    \n",
    "- 7. [LLaMa](#toc7_)    \n",
    "  - 7.1. [Tokenizer](#toc7_1_)    \n",
    "- 8. [DeepSeek](#toc8_)    \n",
    "  - 8.1. [R1](#toc8_1_)    \n",
    "- 9. [‰ªÄ‰πàÊòØRAGÔºü](#toc9_)    \n",
    "  - 9.1. [ÊñáÊú¨Áü•ËØÜÊ£ÄÁ¥¢](#toc9_1_)    \n",
    "    - 9.1.1. [Áü•ËØÜÂ∫ìÊûÑÂª∫](#toc9_1_1_)    \n",
    "    - 9.1.2. [Êü•ËØ¢ÊûÑÂª∫](#toc9_1_2_)    \n",
    "    - 9.1.3. [Â¶Ç‰ΩïÊ£ÄÁ¥¢Ôºü-ÊñáÊú¨Ê£ÄÁ¥¢](#toc9_1_3_)    \n",
    "    - 9.1.4. [Â¶Ç‰ΩïÂñÇÁªôÂ§ßÊ®°ÂûãÔºü-ÁîüÊàêÂ¢ûÂº∫](#toc9_1_4_)    \n",
    "  - 9.2. [Â§öÊ®°ÊÄÅÁü•ËØÜÊ£ÄÁ¥¢](#toc9_2_)    \n",
    "  - 9.3. [Â∫îÁî®](#toc9_3_)    \n",
    "- 10. [ÈÉ®ÁΩ≤Â§ßÊ®°Âûã](#toc10_)    \n",
    "  - 10.1. [‰∏ãËΩΩÊ®°Âûã](#toc10_1_)    \n",
    "  - 10.2. [ollama](#toc10_2_)    \n",
    "    - 10.2.1. [Install and run model](#toc10_2_1_)    \n",
    "    - 10.2.2. [API on web port](#toc10_2_2_)    \n",
    "    - 10.2.3. [Python ollama module](#toc10_2_3_)    \n",
    "      - 10.2.3.1. [demoÔºöÁøªËØë‰∏≠Êñá‰∏∫Ëã±Êñá](#toc10_2_3_1_)    \n",
    "  - 10.3. [ktransformers](#toc10_3_)    \n",
    "    - 10.3.1. [DockerÂÆâË£Ö](#toc10_3_1_)    \n",
    "    - 10.3.2. [ÁºñËØëÂÆâË£Ö](#toc10_3_2_)    \n",
    "      - 10.3.2.1. [prepare](#toc10_3_2_1_)    \n",
    "      - 10.3.2.2. [ÊñπÂºè‰∏ÄÔºöpip3 install whl](#toc10_3_2_2_)    \n",
    "      - 10.3.2.3. [ÊñπÂºè‰∫åÔºöÁºñËØë](#toc10_3_2_3_)    \n",
    "    - 10.3.3. [ËôöÊãüÊú∫‰∏≠ÁºñËØëÂÆâË£Ö](#toc10_3_3_)    \n",
    "    - 10.3.4. [‰ΩøÁî®](#toc10_3_4_)    \n",
    "- 11. [ÂêØÂä®Â≠êÈ¢ÑÊµã](#toc11_)    \n",
    "- 12. [ËΩ¨Ê†ºÂºè](#toc12_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[HuggingFace](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export HF_ENDPOINT=\"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[Transformer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. <a id='toc3_1_1_'></a>[train a new tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "# ÂàùÂßãÂåñ‰∏Ä‰∏™Á©∫ÁöÑ WordPiece Ê®°Âûã\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30000,        # ËØçÊ±áË°®Â§ßÂ∞è\n",
    "    min_frequency=2,         # ÊúÄÂ∞èËØçÈ¢ë\n",
    "    show_progress=True,      # ÊòæÁ§∫ËøõÂ∫¶\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ËÆ≠ÁªÉ\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/huggingface/dna_1g.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ‰øùÂ≠ò\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/huggingface/dna_wordpiece_dict.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "# ËÆ≠ÁªÉ\n",
    "tokenizer.train(files=[\"data/huggingface/dna_1g.txt\"], trainer=trainer)\n",
    "\n",
    "# ‰øùÂ≠ò\n",
    "tokenizer.save(\"data/huggingface/dna_wordpiece_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. <a id='toc3_1_2_'></a>[make tokenizer to be used in transformers with AutoTokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/huggingface/dna_wordpiece_dict/tokenizer_config.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/special_tokens_map.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer = Tokenizer.from_file(\"data/huggingface/dna_wordpiece_dict.json\")\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=new_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# ‰øùÂ≠ò\n",
    "wrapped_tokenizer.save_pretrained(\"data/huggingface/dna_wordpiece_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Âä†ËΩΩ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/huggingface/dna_wordpiece_dict\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6, 766, 22, 10], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÁºñÁ†Å\n",
    "tokenizer(\"ATCGGATCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='data/huggingface/dna_wordpiece_dict', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM \n",
    "\n",
    "\n",
    "# ÈÖçÁΩÆ\n",
    "max_len = 1024 \n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size = len(tokenizer),\n",
    "    max_position_embeddings=max_len, \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ") \n",
    "\n",
    "# Ê®°Âûã\n",
    "model = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[datas](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1079595\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "\n",
    "raw_dataset = load_dataset('text', data_files='data/huggingface/dna_1g.txt')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 971635\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 107960\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6371b499b174ff89c4e7bb54315a9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/971635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065019d3ba1c46c598849b00771e8a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/107960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer._tokenizer.model.max_input_chars_per_word = 10000\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_len)\n",
    "\n",
    "\n",
    "# ÂØπÊï∞ÊçÆÈõÜÂ∫îÁî®ÂàÜËØçÂáΩÊï∞\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=False, remove_columns=['text'], num_proc=50)  # ËÆæÁΩÆ‰∏∫‰Ω†ÁöÑ CPU Ê†∏ÂøÉÊï∞ÊàñÊ†πÊçÆÈúÄË¶ÅË∞ÉÊï¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "# ÂàõÂª∫‰∏Ä‰∏™Êï∞ÊçÆÊî∂ÈõÜÂô®ÔºåÁî®‰∫éÂä®ÊÄÅÂ°´ÂÖÖÂíåÈÅÆËîΩ,Ê≥®ÊÑèmlm=true\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GAATATTTGTCTATTCTTCTTAACTTTCTCCACTGTAAATTAAATTGCTCCTCAGGGTGCTATATGGCATCCCTTGCTATTTTTGGAGCAAATCTTAAATTCTTCAACAATTTTATCAAGACAAACACAACTTTCAGTAAATTCATTGTTTAAATTTGGTGAAAAGTCAGATTTCTTTACACATAGTAAAGCAAATGTAAAATAATATATCAATGTGATTCTTTTAATAAAATACCATTATTGCCAATGGTTTTTAATAGTTCACTGTTTGAAAGAGACCACAAAATTCATGTGCAAAAATCACAAGCATTCTTATACAACAGTGACAGACAAACAGAGAGCCAAATCAGGAATGAACTTCCATTCACAATTGCTTCAAAGAGAATCAAATACCTAGGAATCCAACTTACAAGGGATGTAAAGGACCTCTTCAAGGAGAACTACAAACCACTGCTCAGTGAAATAAAAGAGGACACAAACAAATGGAAGAACATACCATGCTCATGGATAGGAAGAATCAATATCGTGAAAATGGCCATACTGCCCAAGGTAATTTATAGATTCAATGCCATCCCCATCAAGCTACCAATGAGTTTCTTCACAGAATTGGAAAAAACTGTTTTAAAGTTCATATGGAACCAAAAAAGAACCCACATTGCCAAGACAATCCTAAGTCAAATGAACAAAGCTGGAGGGATCATGCTACCTGACTTCAAACTATACTACAAGGCTACAGTAACCAAAATAGCATGGTACTGGTACCAAAACAGAAATATAGACCAATGGAACAGCATAGAGTCCTCAGAAATAATACCACACATCTACATCTTTGATAAATCTGACAAAAACAAGAAATGGGGAAAGGATTCTCTATATAATAAATGGTGCTGGGAAAATTGGCTAGCCATAAGTAGAAAGCTGAAACTGGATCCTTTCCTTACTCTTTATACGAAAATTAATTCAAGATGGAGTAGAGACTTAAATGTTAGACCTAATACCA'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GAA',\n",
       " '##TATTTG',\n",
       " '##TCTATT',\n",
       " '##CTTCTTAA',\n",
       " '##CTTTCTCC',\n",
       " '##A',\n",
       " '##CTGTAAATT',\n",
       " '##AAATT',\n",
       " '##GCTCC',\n",
       " '##TCAGG',\n",
       " '##GTGCTA',\n",
       " '##TATGGCA',\n",
       " '##TCCCTT',\n",
       " '##GCTATTTT',\n",
       " '##TGGAGCAA',\n",
       " '##A',\n",
       " '##TCTTAAA',\n",
       " '##T']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(dataset[\"train\"][0][\"text\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "run_path = \"cache/bert_run\"\n",
    "train_epoches = 5\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=run_path,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=train_epoches,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        save_steps=2000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        fp16=True, #v100Ê≤°Ê≥ïÁî®\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"cache/dna_bert_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[GPT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[T5](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[BART](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[LLaMa](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. <a id='toc7_1_'></a>[Tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"data/huggingface/dna_1g.txt,data/huggingface/protein_1g.txt\", \n",
    "    model_prefix=\"dna_llama\", \n",
    "    vocab_size=60000, \n",
    "    model_type=\"bpe\", \n",
    "    # max_sentence_length=1000000,\n",
    "    num_threads=50, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file=\"dna_llama.model\")\n",
    "\n",
    "tokenizer.encode(\"ATCGGATCG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[DeepSeek](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. <a id='toc8_1_'></a>[R1](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model deepseek-ai/DeepSeek-R1 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3589, in from_pretrained\n    if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/utils/hub.py\", line 655, in has_file\n    response = get_session().head(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 624, in head\n    return self.request(\"HEAD\", url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 93, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 682, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 787d5459-e5a7-4279-8e49-e516f6e6aa22)')\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      5\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m pipe(messages)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    295\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model deepseek-ai/DeepSeek-R1 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n               ^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 455, in wrap_socket\n    return self.sslsocket_class._create(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1042, in _create\n    self.do_handshake()\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py\", line 1320, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3589, in from_pretrained\n    if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/utils/hub.py\", line 655, in has_file\n    response = get_session().head(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 624, in head\n    return self.request(\"HEAD\", url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 93, in send\n    return super().send(request, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py\", line 682, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 787d5459-e5a7-4279-8e49-e516f6e6aa22)')\n\n\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)\n",
    "\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. <a id='toc9_'></a>[‰ªÄ‰πàÊòØRAGÔºü](#toc0_)\n",
    "\n",
    "RAGÁöÑÂàÜÁ±ªÔºö\n",
    "\n",
    "|Model | Ê£ÄÁ¥¢Âô®ÂæÆË∞É | Â§ßÈ¢ÑË®ÄÊ®°ÂûãÂæÆË∞É| ‰æãÂ¶Ç |\n",
    "|---|---|---| --- |\n",
    "| ÈªëÁõí | - | - | e.g. In-context ralm |\n",
    "| ÈªëÁõí | ÊòØ | - | e.g. Rplug |\n",
    "| ÁôΩÁõí | - | ÊòØ | e.g. realm, self-rag |\n",
    "| ÁôΩÁõí | ÊòØ | ÊòØ | e.g. altas |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. <a id='toc9_1_'></a>[ÊñáÊú¨Áü•ËØÜÊ£ÄÁ¥¢](#toc0_)\n",
    "Â¶Ç‰ΩïÊ£ÄÁ¥¢Âá∫Áõ∏ÂÖ≥‰ø°ÊÅØÊù•ËæÖÂä©ÊîπÂñÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêË¥®ÈáèÁöÑÁ≥ªÁªü„ÄÇÁü•ËØÜÊ£ÄÁ¥¢ÈÄöÂ∏∏ÂåÖÊã¨Áü•ËØÜÂ∫ìÊûÑÂª∫„ÄÅÊü•ËØ¢ÊûÑÂª∫„ÄÅÊñáÊú¨Ê£ÄÁ¥¢ÂíåÊ£ÄÁ¥¢ÁªìÊûúÈáçÊéíÂõõÈÉ®ÂàÜ„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1. <a id='toc9_1_1_'></a>[Áü•ËØÜÂ∫ìÊûÑÂª∫](#toc0_)\n",
    "ÊñáÊú¨ÂùóÁöÑÁü•ËØÜÂ∫ìÊûÑÂª∫ÔºåÂ¶ÇÁª¥Âü∫ÁôæÁßë„ÄÅÊñ∞Èóª„ÄÅËÆ∫ÊñáÁ≠â„ÄÇ\n",
    "\n",
    "ÊñáÊú¨ÂàÜÂùóÔºöÂ∞ÜÊñáÊú¨ÂàÜÊàêÂ§ö‰∏™ÂùóÔºåÊØè‰∏™ÂùóÂåÖÂê´‰∏Ä‰∏™ÊàñÂ§ö‰∏™Âè•Â≠ê„ÄÇ\n",
    "- Âõ∫ÂÆöÂ§ßÂ∞èÂùóÔºöÂ∞ÜÊñáÊú¨ÂàÜÊàêÂõ∫ÂÆöÂ§ßÂ∞èÁöÑÂùóÔºåÂ¶ÇÊØè‰∏™ÂùóÂåÖÂê´512‰∏™Â≠óÁ¨¶„ÄÇ\n",
    "- Âü∫‰∫éÂÜÖÂÆπÂùóÔºöÂ∞ÜÊñáÊú¨ÂàÜÊàêÂü∫‰∫éÂÜÖÂÆπÁöÑÂùóÔºåÂ¶ÇÊØè‰∏™ÂùóÂåÖÂê´‰∏Ä‰∏™Âè•Â≠ê„ÄÇ\n",
    "  - ÈÄöËøáÂè•Â≠êÂàÜÂâ≤Á¨¶ÂàÜÂâ≤Âè•Â≠ê„ÄÇ\n",
    "  - Áî®LLMËøõË°åÂàÜÂâ≤\n",
    "\n",
    "Áü•ËØÜÂ∫ìÂ¢ûÂº∫ÔºöÁü•ËØÜÂ∫ìÂ¢ûÂº∫ÊòØÈÄöËøáÊîπËøõÂíå‰∏∞ÂØåÁü•ËØÜÂ∫ìÁöÑÂÜÖÂÆπÂíåÁªìÊûÑÔºå‰∏∫Êü•ËØ¢Êèê‰æõ\"ÊäìÊâã‚ÄùÔºåÂåÖÊã¨Êü•ËØ¢ÁîüÊàê‰∏éÊ†áÈ¢òÁîüÊàê‰∏§ÁßçÊñπÊ≥ï„ÄÇ\n",
    "- ‰º™Êü•ËØ¢ÁîüÊàê\n",
    "- Ê†áÈ¢òÁîüÊàê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2. <a id='toc9_1_2_'></a>[Êü•ËØ¢ÊûÑÂª∫](#toc0_)\n",
    "Êü•ËØ¢ÊûÑÂª∫ÔºöÊó®Âú®ÈÄöËøáÊü•ËØ¢Â¢ûÂº∫ÁöÑÊñπÂºèÔºåÊâ©Â±ïÂíå‰∏∞ÂØåÁî®Êà∑Êü•ËØ¢ÁöÑËØ≠‰πâÂíåÂÜÖÂÆπÔºåÊèêÈ´òÊ£ÄÁ¥¢ÁªìÊûúÁöÑÂáÜÁ°ÆÊÄßÂíåÂÖ®Èù¢ÊÄßÔºå‚ÄúÈí©\"Âá∫Áõ∏Â∫îÂÜÖÂÆπ„ÄÇÂ¢ûÂº∫ÊñπÂºèÂèØÂàÜ‰∏∫ËØ≠‰πâÂ¢ûÂº∫‰∏éÂÜÖÂÆπÂ¢ûÂº∫„ÄÇ\n",
    "- ËØ≠‰πâÂ¢ûÂº∫ÔºöÂêå‰∏ÄÂè•ËØùÂ§öÁßçË°®ËææÊñπÂºè\n",
    "- ÂÜÖÂÆπÂ¢ûÂº∫ÔºöÂ¢ûÂä†ËÉåÊôØÁü•ËØÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3. <a id='toc9_1_3_'></a>[Â¶Ç‰ΩïÊ£ÄÁ¥¢Ôºü-ÊñáÊú¨Ê£ÄÁ¥¢](#toc0_)\n",
    "`Ê£ÄÁ¥¢Âô®`ÔºöÁªôÂÆöÁü•ËØÜÂ∫ìÂíåÁî®Êà∑Êü•ËØ¢ÔºåÊñáÊú¨Ê£ÄÁ¥¢Êó®Âú®ÊâæÂà∞Áü•ËØÜÂ∫ì‰∏≠‰∏éÁî®Êà∑Êü•ËØ¢Áõ∏ÂÖ≥ÁöÑÁü•ËØÜÊñáÊú¨;Ê£ÄÁ¥¢ÊïàÁéáÂ¢ûÂº∫Êó®Âú®Ëß£ÂÜ≥Ê£ÄÁ¥¢Êó∂ÁöÑÊÄßËÉΩÁì∂È¢àÈóÆÈ¢ò„ÄÇÊâÄ‰ª•Ê£ÄÁ¥¢Ë¥®Èáè„ÄÅÊ£ÄÁ¥¢ÊïàÁéáÂæàÈáçË¶Å„ÄÇÂ∏∏ËßÅÊ£ÄÁ¥¢Âô®Êúâ‰∏âÁ±ªÔºö\n",
    "- Âà§Âà´ÂºèÊ£ÄÁ¥¢Âô®Ôºö\n",
    "  - Á®ÄÁñèÊ£ÄÁ¥¢Âô®Ôºåe.g. TF-IDF\n",
    "  - ÂèåÂêëÁºñÁ†ÅÊ£ÄÁ¥¢Âô®Ôºåe.g. Áî®bertÈ¢ÑÂÖàÂ∞ÜÊñáÊú¨ÂùóËøõË°åÁºñÁ†ÅÊàêÂêëÈáè\n",
    "  - ‰∫§ÂèâÁºñÁ†ÅÊ£ÄÁ¥¢Âô®Ôºåe.g. \n",
    "- ÁîüÊàêÂºèÊ£ÄÁ¥¢Âô®ÔºöÂô®Áõ¥Êé•Â∞ÜÁü•ËØÜÂ∫ì‰∏≠ÁöÑÊñáÊ°£‰ø°ÊÅØËÆ∞ÂøÜÂú®Ê®°ÂûãÂèÇÊï∞‰∏≠„ÄÇÁÑ∂ÂêéÔºåÂú®Êé•Êî∂Âà∞Êü•ËØ¢ËØ∑Ê±ÇÊó∂ÔºåËÉΩÂ§üÁõ¥Êé•ÁîüÊàêÁõ∏ÂÖ≥ÊñáÊ°£ÁöÑÊ†áËØÜÁ¨¶Â§∫ÔºàÂç≥Doc IDÔºâÔºå‰ª•ÂÆåÊàêÊ£ÄÁ¥¢„ÄÇ\n",
    "- ÂõæÊ£ÄÁ¥¢Âô®ÔºöÂõæÊ£ÄÁ¥¢Âô®ÁöÑÁü•ËØÜÂ∫ì‰∏∫ÂõæÊï∞ÊçÆÂ∫ìÔºåÂåÖÊã¨ÂºÄÊîæÁü•ËØÜÂõæË∞±ÂíåËá™Âª∫Âõæ‰∏§ÁßçÔºåÂÆÉ‰ª¨‰∏ÄËà¨Áî±<‰∏ª‰Ωì„ÄÅË∞ìËØçÂíåÂÆ¢‰Ωì>‰∏âÂÖÉÁªÑÊûÑÊàê„ÄÇËøôÊ†∑ÂÅö‰∏ç‰ªÖÂèØ‰ª•ÊçïÊçâÊ¶ÇÂøµÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ªÔºåËøòÂÖÅËÆ∏‰∫∫Á±ªÂíåÊú∫Âô®ÂèØ‰ª•ÂÖ±ÂêåÂØπÁü•ËØÜËøõË°åÁêÜËß£‰∏éÊé®ÁêÜ„ÄÇ\n",
    "\n",
    "`ÈáçÊéíÂô®`ÔºöÊ£ÄÁ¥¢Èò∂ÊÆµ‰∏∫‰∫Ü‰øùËØÅÊ£ÄÁ¥¢ÈÄüÂ∫¶ÈÄöÂ∏∏‰ºöÊçüÂ§±‰∏ÄÂÆöÁöÑÊÄßËÉΩÔºåÂèØËÉΩÊ£ÄÁ¥¢Âà∞Ë¥®ÈáèËæÉ‰ΩéÁöÑÊñáÊ°£„ÄÇÈáçÊéíÁöÑÁõÆÁöÑÊòØÂØπÊ£ÄÁ¥¢Âà∞ÁöÑÊÆµËêΩËøõË°åËøõ‰∏ÄÊ≠•ÁöÑÊéíÂ∫èÁ≤æÈÄâ„ÄÇÈáçÊéíÂèØ‰ª•ÂàÜ‰∏∫Âü∫‰∫é‰∫§ÂèâÁºñÁ†ÅÁöÑÊñπÊ≥ïÂíåÂü∫‰∫é‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁöÑÊñπÊ≥ï„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.4. <a id='toc9_1_4_'></a>[Â¶Ç‰ΩïÂñÇÁªôÂ§ßÊ®°ÂûãÔºü-ÁîüÊàêÂ¢ûÂº∫](#toc0_)\n",
    "RAGÂ¢ûÂº∫ÊØîËæÉÔºö\n",
    "\n",
    "|Êû∂ÊûÑÂàÜÁ±ª|‰ºòÁÇπ|Áº∫ÁÇπ|\n",
    "|-|-|-|\n",
    "|ËæìÂÖ•Á´Øprompt|ÁÆÄÂçï|tokensÂ§™Â§ö|\n",
    "|‰∏≠Èó¥Â±Ç|È´òÊïà|ËÄóGPUËµÑÊ∫ê|\n",
    "|ËæìÂá∫Á´Ø|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. <a id='toc9_2_'></a>[Â§öÊ®°ÊÄÅÁü•ËØÜÊ£ÄÁ¥¢](#toc0_)\n",
    "## 9.3. <a id='toc9_3_'></a>[Â∫îÁî®](#toc0_)\n",
    "ÂØπËØùÊú∫Âô®‰∫∫„ÄÅÁü•ËØÜÂ∫ìÊñáÁ≠î..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. <a id='toc10_'></a>[ÈÉ®ÁΩ≤Â§ßÊ®°Âûã](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. <a id='toc10_1_'></a>[‰∏ãËΩΩÊ®°Âûã](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
    "hfd.sh deepseek-ai/DeepSeek-R1 -x 10 -j 10 \n",
    "\n",
    "# https://huggingface.co/unsloth/DeepSeek-R1-GGUF\n",
    "hfd.sh unsloth/DeepSeek-R1-GGUF -x 10 -j 10 --include DeepSeek-R1-Q8_0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. <a id='toc10_2_'></a>[ollama](#toc0_)\n",
    "### 10.2.1. <a id='toc10_2_1_'></a>[Install and run model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the serve\n",
    "ollama serve\n",
    "\n",
    "# list all model images\n",
    "ollama list \n",
    "\n",
    "# run model from image\n",
    "ollama run model_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2. <a id='toc10_2_2_'></a>[API on web port](#toc0_)\n",
    "communicatation with local model via web port.\n",
    "\n",
    "`generate` and `chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   456  100   315  100   141    142     64  0:00:02  0:00:02 --:--:--   206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"deepseek-r1:7b\",\"created_at\":\"2025-02-18T02:49:32.744934023Z\",\"response\":\"{\\\"}\\u003cthink\\u003e{\\\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n:\\n\\n{\\n\\n}\\n\\n}\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\",\"done\":false}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"deepseek-r1:7b\",\n",
    "  \"prompt\": \"Who are you?\",\n",
    "  \"stream\": false,\n",
    "  \"options\": {\n",
    "    \"temperature\": 0.6\n",
    "  },\n",
    "  \"format\": \"json\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5034    0  4905  100   129    326      8  0:00:16  0:00:15  0:00:01   719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"deepseek-r1:7b\",\"created_at\":\"2025-02-18T02:49:26.56791501Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\u003cthink\\u003e\\nOkay, so I just read that \\\"Why is the sky blue?\\\" and now I'm trying to figure it out myself. Let me think through this step by step.\\n\\nFirst off, when you look at the sky on a clear day, it's usually blue, especially during the day when the sun is out. But sometimes I've seen it turn other colors too, like red in the evening or during sunrise. So why is it mostly blue?\\n\\nI know that light travels through the atmosphere, but how does it get colored? I remember learning about something called Rayleigh scattering from my science class. Let me try to recall what that was about. Rayleigh scattering involves light interacting with particles much smaller than the wavelength of light itself. When sunlight enters the Earth's atmosphere, it reaches tiny molecules in the air, like nitrogen and oxygen.\\n\\nWait, so these small particles scatter the sunlight in all directions. But why does this result in a blue sky? I think it has something to do with the wavelengths of light. Visible light ranges from violet to red, right? And I remember that violet light has a shorter wavelength than blue. So maybe the shorter wavelengths are scattered more.\\n\\nBut if blue is scattered more, wouldn't it be easier to see at night when there's less atmosphere overhead? Hmm, but then why does the sky turn red in the evening or during sunrise?\\n\\nOh right! During sunrise and sunset, the light has to pass through a much thicker layer of atmosphere. That makes sense because we're looking at the light after it's been scattered through more particles. The longer path means that all the shorter wavelengths (like violet) are scattered out, leaving red to dominate because it has a longer wavelength.\\n\\nSo during the day, when the sun is directly overhead, blue and green wavelengths get scattered away by Rayleigh scattering, making the sky appear blue. But in the early morning or late afternoon, as the sun is near the horizon, the light has to pass through more atmosphere, so red comes through because it's not scattered as much.\\n\\nWait, but isn't there also something called Mie scattering? I think that happens when particles are larger than the wavelength of light. Does that affect the color of the sky too?\\n\\nI believe Mie scattering is more significant for larger particles, like dust or droplets in clouds. So it might cause some effects we see during sunrise and sunset, but not as much as Rayleigh does for small molecules.\\n\\nSo to summarize: The sky appears blue on a clear day because blue light scatters more in the atmosphere due to Rayleigh scattering by tiny gas molecules. During sunrise and sunset, the longer path allows red light to dominate, giving the sky its reddish hues.\\n\\nBut wait, what about when we see other colors? I mean, sometimes during the day it's not just blue; I've seen green or yellow in some places. Is that because of Rayleigh scattering changing as the atmosphere gets thicker?\\n\\nOr maybe it's due to other factors like pollution or particles in the air affecting light differently. That might complicate things.\\n\\nAlso, does humidity play a role? Sometimes when it's humid, does the sky appear clearer instead of blue? I think higher humidity can affect how light scatters because more water vapor means more molecules to scatter off.\\n\\nSo maybe on days with high humidity, the Rayleigh scattering is less effective, making the sky look different. But that's probably a secondary effect compared to the basic reason for the color being blue.\\n\\nIn any case, I think the primary reason is Rayleigh scattering by nitrogen and oxygen in the atmosphere causing shorter wavelengths like blue to scatter more, resulting in a blue sky during clear days.\\n\\u003c/think\\u003e\\n\\nThe sky appears blue primarily due to a phenomenon known as Rayleigh scattering. When sunlight enters Earth's atmosphere, it interacts with tiny molecules of nitrogen and oxygen. These particles scatter shorter wavelengths of light, such as blue and violet, which have shorter wavelengths than red or orange. This scattering is more effective for shorter wavelengths, causing the sky to appear blue during the day.\\n\\nDuring sunrise and sunset, the light passes through a thicker layer of atmosphere, where all shorter wavelengths are scattered away, leaving longer wavelengths like red to dominate, resulting in the reddish hues observed at these times.\\n\\nOther factors such as humidity, pollution, or atmospheric particles can influence how light scatters, but the primary reason for the sky's blue color remains Rayleigh scattering by nitrogen and oxygen molecules.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":15024265818,\"load_duration\":22057058,\"prompt_eval_count\":9,\"prompt_eval_duration\":7000000,\"eval_count\":901,\"eval_duration\":14993000000}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"deepseek-r1:7b\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ],\n",
    "  \"stream\": false\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3. <a id='toc10_2_3_'></a>[Python ollama module](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "texts = '''\n",
    "ËØ¶ÁªÜÊØîËæÉdeepseekÊØçÂÖ¨Âè∏ÂíåopenAIÂÖ¨Âè∏ÁöÑÂå∫Âà´\n",
    "'''\n",
    "\n",
    "# model_card = \"deepseek-r1:7b\"\n",
    "model_card = \"modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF\"\n",
    "\n",
    "# ÊñπÂºè‰∏ÄÔºàÈùûÊµÅÂºèËæìÂá∫ÔºâÔºö\n",
    "# outputs = ollama.generate(model_card, inputs)\n",
    "# print(f'{outputs['response']}')\n",
    "\n",
    "# ÊñπÂºè‰∫åÔºàÊµÅÂºèËæìÂá∫ÔºâÔºö\n",
    "outputs = ollama.generate(\n",
    "    stream=True,\n",
    "    model=model_card,\n",
    "    prompt=texts,\n",
    ")\n",
    "for chunk in outputs:\n",
    "    if not chunk['done']:\n",
    "        print(f'{chunk['response']}', end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.3.1. <a id='toc10_2_3_1_'></a>[demoÔºöÁøªËØë‰∏≠Êñá‰∏∫Ëã±Êñá](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "ÂóØÔºåÈ¶ñÂÖàÊàëË¶ÅÁêÜËß£Ëøô‰∏™È¢òÁõÆÁöÑÊÑèÊÄù„ÄÇ‚ÄúÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†‚ÄùÊåáÁöÑÊòØ‰ΩøÁî®Ê∑±Â∫¶Â≠¶‰π†ÊäÄÊúØÊù•ËøõË°åÁ†îÁ©∂„ÄÇËÄå‚ÄúÂØπÊûØËçâËäΩËÉûÊùÜËèåËäΩËÉûÂΩ¢ÊàêÁõ∏ÂÖ≥Âü∫Âõ†ÁöÑÁ†îÁ©∂‚ÄùÂàôÊòØÂÖ∑‰ΩìÁöÑÁ†îÁ©∂ÂÜÖÂÆπÔºåÊ∂âÂèäÂà∞ÊûØËçâËäΩËÉûÊùÜËèåÂú®ÂΩ¢ÊàêËäΩËÉûËøáÁ®ã‰∏≠Áõ∏ÂÖ≥ÁöÑÂü∫Âõ†„ÄÇ\n",
      "\n",
      "ÊàëÈúÄË¶ÅÊääËøôÊï¥‰∏™Âè•Â≠êÂáÜÁ°ÆÂú∞ÁøªËØëÊàêËã±Êñá„ÄÇÈ¶ñÂÖàÔºå‚ÄúÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†‚ÄùÂèØ‰ª•Áõ¥Êé•ÁøªËØë‰∏∫‚ÄúBased on deep learning‚Äù„ÄÇÊé•‰∏ãÊù•ÊòØ‚ÄúÁ†îÁ©∂‚ÄùÔºåÂØπÂ∫îÁöÑËã±ÊñáÊòØ‚Äústudy‚Äù„ÄÇÁÑ∂ÂêéÊòØ‚ÄúÊûØËçâËäΩËÉûÊùÜËèå‚ÄùÔºåËøô‰∏™Â∫îËØ•ÊòØ‰∏Ä‰∏™‰∏ìÊúâÂêçËØçÔºåÂèØËÉΩÈúÄË¶ÅÊü•‰∏Ä‰∏ãÊ≠£Á°ÆÁöÑËã±ËØëÂêçÁß∞ÔºåÊØîÂ¶Ç‚ÄúBacillus subtilis‚Äù„ÄÇ\n",
      "\n",
      "Êé•ÁùÄÊòØ‚ÄúËäΩËÉûÂΩ¢ÊàêÁõ∏ÂÖ≥Âü∫Âõ†‚ÄùÔºåËøôÈÉ®ÂàÜÂèØ‰ª•ÁøªËØë‰∏∫‚Äúgenes related to spore formation‚Äù„ÄÇÊúÄÂêéÔºåÊääÊï¥‰∏™Âè•Â≠êËøûË¥ØËµ∑Êù•ÔºåÂ∞±ÊòØ‚ÄúBased on deep learning study of genes related to spore formation in Bacillus subtilis.‚Äù\n",
      "\n",
      "ËøôÊ†∑ÁªÑÂêàËµ∑Êù•ÔºåÊó¢ÂáÜÁ°Æ‰º†Ëææ‰∫ÜÂéüÊÑèÔºåÂèàÁ¨¶ÂêàËã±ÊñáÁöÑË°®Ëææ‰π†ÊÉØ„ÄÇÊàëËßâÂæóËøô‰∏™ÁøªËØëÂ∫îËØ•ÊòØÊØîËæÉ‰∏ì‰∏öÂíåÂáÜÁ°ÆÁöÑ„ÄÇ\n",
      "</think>\n",
      "\n",
      "Study of Genes Related to Spore Formation in *Bacillus subtilis* Based on Deep Learning‚ö°\n",
      "<think>\n",
      "Â•ΩÁöÑÔºåÈ¶ñÂÖàÊàëË¶ÅÁêÜËß£Áî®Êà∑ÁöÑÈúÄÊ±Ç„ÄÇ‰ªñÁªô‰∫Ü‰∏Ä‰∏™‰∏≠Ëã±ÂØπÁÖßÁöÑÂè•Â≠êÔºåË¶ÅÊ±Ç‰∏ì‰∏öÁøªËØëÔºåÂπ∂‰∏îÈúÄË¶ÅÂ∞Ü‰∏≠ÊñáÂè•Â≠ê‚ÄúÈÄöËøáÂÆèÂü∫Âõ†ÁªÑÁ†îÁ©∂ÂæÆÁîüÁâ©‰∏éÊ§çÁâ©Áõ∏‰∫í‰ΩúÁî®ÁöÑÊú∫Âà∂„ÄÇ‚ÄùÂáÜÁ°ÆÂú∞ÁøªËØëÊàêËã±Êñá„ÄÇ\n",
      "\n",
      "Êé•‰∏ãÊù•ÔºåÊàëÈúÄË¶ÅÂàÜÊûêÂéüÊñáÁöÑÊÑèÊÄù„ÄÇÂè•Â≠êÁöÑ‰∏ªÂπ≤ÊòØ‚ÄúÈÄöËøáÂÆèÂü∫Âõ†ÁªÑÁ†îÁ©∂‚Ä¶‚ÄùÔºåËøôÈáåÁöÑÂÖ≥ÈîÆËØçÊúâ‚ÄúÂÆèÂü∫Âõ†ÁªÑ‚Äù„ÄÅ‚ÄúÂæÆÁîüÁâ©‚Äù„ÄÅ‚ÄúÊ§çÁâ©‚Äù‰ª•Âèä‚ÄúÁõ∏‰∫í‰ΩúÁî®ÁöÑÊú∫Âà∂‚Äù„ÄÇÊâÄ‰ª•ÔºåÈ¶ñÂÖàË¶ÅÁ°ÆÂÆöËøô‰∫õÊúØËØ≠Âú®Ëã±Êñá‰∏≠ÁöÑÂáÜÁ°ÆÂØπÂ∫îËØç„ÄÇ\n",
      "\n",
      "‚ÄúÂÆèÂü∫Âõ†ÁªÑ‚ÄùÈÄöÂ∏∏ÁøªËØë‰∏∫‚Äúmetagenome‚ÄùÊàñËÄÖ‚Äúmeta-genomics‚ÄùÔºå‰ΩÜÊõ¥Â∏∏ËßÅÁöÑÊòØ‰ΩøÁî®‚Äúmetagenomics‚ÄùÊù•Ë°®Á§∫Ëøô‰∏ÄÁ†îÁ©∂È¢ÜÂüü„ÄÇÂõ†Ê≠§ÔºåËøôÈáåÈÄâÊã©‚Äúmetagenomics‚Äù‰Ωú‰∏∫ÁøªËØë„ÄÇ\n",
      "\n",
      "ÁÑ∂ÂêéÔºå‚ÄúÈÄöËøá‚Ä¶Á†îÁ©∂‚Ä¶‚ÄùÁöÑÁªìÊûÑÂú®Ëã±Êñá‰∏≠ÂèØ‰ª•Áî®‚Äúthrough‚ÄùÊàñËÄÖ‚Äúby means of‚ÄùÊù•Ë°®ËææÔºå‰ΩÜ‰∏∫‰∫ÜÁÆÄÊ¥ÅÂíå‰∏ì‰∏öÔºåÁõ¥Êé•‰ΩøÁî®‚ÄúThrough‚ÄùÊØîËæÉÂêàÈÄÇ„ÄÇ\n",
      "\n",
      "Êé•‰∏ãÊù•ÊòØ‚ÄúÂæÆÁîüÁâ©‰∏éÊ§çÁâ©Áõ∏‰∫í‰ΩúÁî®ÁöÑÊú∫Âà∂‚Äù„ÄÇËøôÈáåÈúÄË¶ÅÊ≥®ÊÑèËØ≠Â∫èÂíåÁî®ËØç„ÄÇÊï¥‰ΩìÁªìÊûÑÂ∫îËØ•ÊòØ‚Äúthe mechanisms underlying the interactions between microorganisms and plants.‚Äù ËøôÊ†∑‰∏ç‰ªÖÊ∏ÖÊô∞ÔºåËÄå‰∏îÁ¨¶ÂêàÂ≠¶ÊúØÂÜô‰ΩúÁöÑËßÑËåÉ„ÄÇ\n",
      "\n",
      "ÊúÄÂêéÔºåÊääËøô‰∫õÈÉ®ÂàÜÁªÑÂêàËµ∑Êù•ÔºåÁ°Æ‰øùÂè•Â≠êÈÄöÈ°∫‰∏îÂáÜÁ°Æ„ÄÇÊâÄ‰ª•ÔºåÊúÄÁªàÁøªËØë‰∏∫Ôºö‚ÄúThrough metagenomics research on the mechanisms of interaction between microorganisms and plants.‚Äù\n",
      "\n",
      "Âú®Êï¥‰∏™ÊÄùËÄÉËøáÁ®ã‰∏≠ÔºåÊàëÊ≥®ÊÑèÂà∞Áî®Êà∑ÂèØËÉΩÊòØÂú®Êí∞ÂÜôÂ≠¶ÊúØËÆ∫ÊñáÊàñËÄÖÂáÜÂ§áÁ†îÁ©∂Êä•ÂëäÔºåÂõ†Ê≠§ÂáÜÁ°ÆÊÄßÂíå‰∏ì‰∏öÊÄßÊòØÂÖ≥ÈîÆ„ÄÇÊ≠§Â§ñÔºå‰øùÊåÅÂè•Â≠êÁöÑÁÆÄÊ¥Å‰πüÊòØÂøÖË¶ÅÁöÑÔºå‰ª•‰æøËØªËÄÖËÉΩÂ§üÂø´ÈÄüÁêÜËß£ÂÜÖÂÆπ„ÄÇ\n",
      "\n",
      "ÊúÄÂêéÔºåÂÜçÊ£ÄÊü•‰∏ÄÈÅçÁøªËØëÊòØÂê¶Âø†ÂÆû‰∫éÂéüÊñáÔºåÂπ∂‰∏îÁ¨¶ÂêàËã±ËØ≠Ë°®Ëææ‰π†ÊÉØ„ÄÇÁ°ÆËÆ§Êó†ËØØÂêéÔºåÂ∞±ÂèØ‰ª•Â∞ÜËøô‰∏™ÁøªËØëÁªìÊûúÊèê‰æõÁªôÁî®Êà∑‰∫Ü„ÄÇ\n",
      "</think>\n",
      "\n",
      "Through metagenomics research on the mechanisms of interaction between microorganisms and plants.‚ö°\n"
     ]
    }
   ],
   "source": [
    "import ollama \n",
    "\n",
    "\n",
    "class zh2en():\n",
    "    def __init__(self, model_card):\n",
    "        self.model_card = model_card\n",
    "        \n",
    "    def build_prompt(self, texts):\n",
    "        # with open(prompt_template_path, 'r') as f:\n",
    "        #     prompt_template = f.read()\n",
    "        #     # str with replace function\n",
    "        #     prompt = prompt_template.replace(var, texts)\n",
    "        prompt_template = \"\"\"\n",
    "        ‰∏ì‰∏öÁøªËØëÔºö\\n\n",
    "        ---\\n\n",
    "        {Chinese_words} \\n\n",
    "        --- \\n\n",
    "        ‰Ωú‰∏∫ÁøªËØë‰∏ìÂÆ∂ÔºåÂ∞Ü‰∏äËø∞‰∏≠ÊñáÂáÜÁ°ÆÁøªËØë‰∏∫Ëã±Êñá„ÄÇ \\n\n",
    "        \"\"\"\n",
    "        prompt = prompt_template.replace(\"{Chinese_words}\", texts)\n",
    "        return prompt\n",
    "\n",
    "    def translate(self, texts):\n",
    "        prompt = self.build_prompt(texts = texts)\n",
    "        # key step\n",
    "        outputs = ollama.generate(\n",
    "            stream=True,\n",
    "            model=self.model_card,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "        for chunk in outputs:\n",
    "            if not chunk['done']:\n",
    "                print(f'{chunk['response']}', end='', flush=True)\n",
    "            else:\n",
    "                print('‚ö°')\n",
    "\n",
    "\n",
    "translater = zh2en(model_card='modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF')\n",
    "\n",
    "translater.translate('Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂØπÊûØËçâËäΩËÉûÊùÜËèåËäΩËÉûÂΩ¢ÊàêÁõ∏ÂÖ≥Âü∫Âõ†ÁöÑÁ†îÁ©∂„ÄÇ')\n",
    "translater.translate('ÈÄöËøáÂÆèÂü∫Âõ†ÁªÑÁ†îÁ©∂ÂæÆÁîüÁâ©‰∏éÊ§çÁâ©Áõ∏‰∫í‰ΩúÁî®ÁöÑÊú∫Âà∂„ÄÇ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. <a id='toc10_3_'></a>[ktransformers](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1. <a id='toc10_3_1_'></a>[DockerÂÆâË£Ö](#toc0_)\n",
    "\n",
    "[https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md](https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the image from docker hub \n",
    "# about 19 GB\n",
    "# docker pull approachingai/ktransformers:0.1.1\n",
    "docker pull approachingai/ktransformers:0.2.1\n",
    "\n",
    "# docker run \\\n",
    "#     --gpus all \\\n",
    "#     -v /path/to/models:/models \\\n",
    "#     -p 10002:10002 \\\n",
    "#     approachingai/ktransformers:v0.1.1 \\\n",
    "#     --port 10002 \\\n",
    "#     --gguf_path /models/path/to/gguf_path \\\n",
    "#     --model_path /models/path/to/model_path \\\n",
    "#     --web True\n",
    "\n",
    "# Directly run\n",
    "docker run  \\\n",
    "    -v /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai:/models \\\n",
    "    -p 10002:10002 \\\n",
    "    approachingai/ktransformers:0.1.1 \\\n",
    "    --port 10002 \\\n",
    "    --model_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1 \\\n",
    "    --gguf_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1-Q4_K_M_GGUF \\\n",
    "    --web True\n",
    "\n",
    "# or\n",
    "docker run \\\n",
    "    # -d \\\n",
    "    --gpus all \\\n",
    "    -it \\\n",
    "    -p 10002:10002 \\\n",
    "    approachingai/ktransformers:0.1.1 \\\n",
    "    /bin/bash \n",
    "\n",
    "## and then\n",
    "docker exec -it container_ID /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA:\n",
    "\n",
    "- Q: DockerËøêË°åÂêéÂá∫Áé∞ Illegal instruction (core dumped)Êä•Èîô\n",
    "  - [https://github.com/kvcache-ai/ktransformers/issues/356](https://github.com/kvcache-ai/ktransformers/issues/356)\n",
    "  - ÈáçÊñ∞ÁºñËØë‰ª•‰∏ã:\n",
    "    ```bash\n",
    "    USE_NUMA=1\n",
    "    bash install.sh\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2. <a id='toc10_3_2_'></a>[ÁºñËØëÂÆâË£Ö](#toc0_)\n",
    "\n",
    "[https://kvcache-ai.github.io/ktransformers/en/install.html](https://kvcache-ai.github.io/ktransformers/en/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3.2.1. <a id='toc10_3_2_1_'></a>[prepare](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"ktransformers\"\n",
    "\n",
    "conda create -n $name python=3.11 -y \n",
    "conda activate $name\n",
    "\n",
    "\n",
    "# Install CudaToolkit and nvcc ...\n",
    "conda install nvidia/label/cuda-12.4.0::cuda -y --channel nvidia/label/cuda-12.4.0\n",
    "\n",
    "# Anaconda provides a package called `libstdcxx-ng` that includes a newer version of `libstdc++`, which can be installed via `conda-forge`.\n",
    "conda install -c conda-forge libstdcxx-ng -y \n",
    "\n",
    "strings ~/miniconda3/envs/${name}/lib/libstdc++.so.6 | grep GLIBCXX\n",
    "\n",
    "\n",
    "# Install PyTorch via pip ...\n",
    "# pip3 install torch torchvision torchaudio\n",
    "conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -c pytorch -y\n",
    "\n",
    "# pip3 install packaging ninja cpufeature numpy\n",
    "conda install conda-forge::ninja conda-forge::packaging anaconda::numpy -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3.2.2. <a id='toc10_3_2_2_'></a>[ÊñπÂºè‰∏ÄÔºöpip3 install whl](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ktransformers\n",
    "pip3 install ktransformers-0.2.1.post1+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n",
    "\n",
    "# flash_attn\n",
    "# pip3 install flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp311-cp311-linux_x86_64.whl\n",
    "pip3 install flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
    "\n",
    "# flashinfer\n",
    "pip3 install flashinfer_python-0.2.2+cu124torch2.4-cp38-abi3-linux_x86_64.whl\n",
    "python -c \"import torch; print(torch.cuda.get_device_capability())\"\n",
    "export TORCH_CUDA_ARCH_LIST=\"8.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3.2.3. <a id='toc10_3_2_3_'></a>[ÊñπÂºè‰∫åÔºöÁºñËØë](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your system has dual sockets and double size RAM than the model's size (e.g. 1T RAM for 512G model)\n",
    "export USE_NUMA=1\n",
    "bash install.sh # or `make dev_install`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3. <a id='toc10_3_3_'></a>[ËôöÊãüÊú∫‰∏≠ÁºñËØëÂÆâË£Ö](#toc0_)\n",
    "\n",
    "Áî±‰∫é`GLIBCxxx`Êä•ÈîôÔºåÊîπÁî®ËôöÊãüÊú∫‰∏≠ubuntuÂÆâË£Öktransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊãâÂèñÂåÖÂê´GLIBC 2.29+ÁöÑÈïúÂÉèÔºàÂ¶ÇUbuntu 20.04Ôºâ\n",
    "docker pull ubuntu:20.04\n",
    "\n",
    "# ÂêØÂä®ÂÆπÂô®Âπ∂ÊåÇËΩΩÈ°πÁõÆÁõÆÂΩï \n",
    "# docker run -it -v /path/to/your/code:/app ubuntu:20.04 /bin/bash \n",
    "docker run \\\n",
    "    --gpus all \\\n",
    "    -p 10002:10002\\\n",
    "    -it \\\n",
    "    ubuntu:20.04 /bin/bash \n",
    "\n",
    "# Install wget and then \n",
    "apt-get update \n",
    "apt-get install wget \n",
    "\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh\n",
    "\n",
    "docker exec --gpus all -it ubuntu:20.04 /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.4. <a id='toc10_3_4_'></a>[‰ΩøÁî®](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â∞Ü‰∏ªÊú∫ÂíåÂÆπÂô®ÁöÑ10002Á´ØÂè£ÂÅöÊò†Â∞Ñ\n",
    "docker run --gpus all -p 10002:10002 -it \\\n",
    "    ubuntu_2004:ktransformers /bin/bash\n",
    "\n",
    "\n",
    "# Ê£ÄÊü•Êò†Â∞Ñ\n",
    "docker port [ÂÆπÂô®ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we use dual socket, we set cpu_infer to 65\n",
    "python -m ktransformers.local_chat \\\n",
    "    --model_path DeepSeek-R1 \\\n",
    "    --gguf_path DeepSeek-R1-Q4_K_M_GGUF \\\n",
    "    --cpu_infer 60 \\\n",
    "    --max_new_tokens 10000 \\\n",
    "    --cache_lens 50000 \\\n",
    "    --total_context 50000 \\\n",
    "    --cache_q4 true \\\n",
    "    --temperature 0.6 \\\n",
    "    --top_p 0.95 \\\n",
    "    --force_think \\\n",
    "    --use_cuda_graph \\\n",
    "    --port 10002\n",
    "    # --optimize_config_path ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml \\\n",
    "    # --host 127.0.0.1 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://localhost:10002/web/index.html#/chat\n",
    "ktransformers \\\n",
    "    --model_path DeepSeek-R1/ \\\n",
    "    --gguf_path DeepSeek-R1-Q4_K_M_GGUF/ \\\n",
    "    --port 10002 \\\n",
    "    --web True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   213  100    81  100   132  40500  66000 --:--:-- --:--:-- --:--:--  104k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"models\":[{\"name\":\"DeepSeek-Coder-V2-Instruct\",\"modified_at\":\"123\",\"size\":123}]}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X 'GET' \\\n",
    "  'http://localhost:10002/api/tags' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": \"tell a joke\",\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"DeepSeek-R1\",\n",
    "  \"stream\": true\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:10002/v1/chat/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": \"tell a joke\",\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"DeepSeek-R1\",\n",
    "  \"stream\": true\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:10002/api/generate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{\n",
    "  \"model\": \"DeepSeek-R1\",\n",
    "  \"prompt\": \"tell me a joke\",\n",
    "  \"stream\": true\n",
    "}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. <a id='toc11_'></a>[ÂêØÂä®Â≠êÈ¢ÑÊµã](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. <a id='toc12_'></a>[ËΩ¨Ê†ºÂºè](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Making directory ./Format/LLMs\n",
      "[NbConvertApp] Converting notebook LLMs.ipynb to html\n",
      "[NbConvertApp] Writing 405847 bytes to Format/LLMs/LLMs.html\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# ipynb to html\n",
    "jupyter nbconvert \\\n",
    "    --to html LLMs.ipynb \\\n",
    "    --output-dir=./Format/LLMs \\\n",
    "    # --NbConvertApp.log_level=ERROR\n",
    "\n",
    "cp -rf Pytorch_Pictures ./Format/LLMs/\n",
    "# browse translate html to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LLMs.ipynb to markdown\n",
      "[NbConvertApp] Writing 40263 bytes to Format/LLMs/LLMs.md\n"
     ]
    }
   ],
   "source": [
    "# ipynb to markdown\n",
    "!jupyter nbconvert --to markdown LLMs.ipynb --output-dir=./Format/LLMs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
