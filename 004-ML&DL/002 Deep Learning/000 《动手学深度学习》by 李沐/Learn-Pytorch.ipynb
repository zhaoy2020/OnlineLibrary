{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [概述](#toc1_)    \n",
    "- 2. [环境配置](#toc2_)    \n",
    "- 3. [utils](#toc3_)    \n",
    "  - 3.1. [实验可重复性](#toc3_1_)    \n",
    "  - 3.2. [Metrics](#toc3_2_)    \n",
    "  - 3.3. [计时器](#toc3_3_)    \n",
    "    - 3.3.1. [cpu计时器](#toc3_3_1_)    \n",
    "    - 3.3.2. [gpu计时器](#toc3_3_2_)    \n",
    "  - 3.4. [统计参数量和内存占用](#toc3_4_)    \n",
    "  - 3.5. [numpy和pytorch计算速度比较](#toc3_5_)    \n",
    "- 4. [安装GPU驱动](#toc4_)    \n",
    "  - 4.1. [安装策略](#toc4_1_)    \n",
    "  - 4.2. [首先确认内核版本和发行版本，再确认显卡型号](#toc4_2_)    \n",
    "  - 4.3. [安装驱动-CUDA Driver](#toc4_3_)    \n",
    "    - 4.3.1. [下载CUDA Driver](#toc4_3_1_)    \n",
    "    - 4.3.2. [禁用nouveau](#toc4_3_2_)    \n",
    "    - 4.3.3. [安装CUDA Driver](#toc4_3_3_)    \n",
    "    - 4.3.4. [查看显卡是否安装成功](#toc4_3_4_)    \n",
    "    - 4.3.5. [查看nvcc](#toc4_3_5_)    \n",
    "  - 4.4. [全局驱动和全局CUDA Toolkit和CuDNN](#toc4_4_)    \n",
    "    - 4.4.1. [下载对应的CUDA Toolkit版本](#toc4_4_1_)    \n",
    "    - 4.4.2. [安装CUDA Toolkit](#toc4_4_2_)    \n",
    "    - 4.4.3. [下载对应的CuDNN](#toc4_4_3_)    \n",
    "    - 4.4.4. [安装CuDNN](#toc4_4_4_)    \n",
    "  - 4.5. [安装对应版本的Pytorch](#toc4_5_)    \n",
    "  - 4.6. [全局驱动个人CUDA Toolkit](#toc4_6_)    \n",
    "  - 4.7. [GPU测试程序](#toc4_7_)    \n",
    "    - 4.7.1. [单机单卡](#toc4_7_1_)    \n",
    "    - 4.7.2. [单机多卡](#toc4_7_2_)    \n",
    "    - 4.7.3. [GPU burn压力测试](#toc4_7_3_)    \n",
    "- 5. [Pytorch模块介绍](#toc5_)    \n",
    "  - 5.1. [导入模块](#toc5_1_)    \n",
    "- 6. [数据封装和加载](#toc6_)    \n",
    "  - 6.1. [torchvison.datasets获得Dataset](#toc6_1_)    \n",
    "  - 6.2. [自定义数据集获得Dataset](#toc6_2_)    \n",
    "    - 6.2.1. [TensorDataset()](#toc6_2_1_)    \n",
    "    - 6.2.2. [重载Dataset类](#toc6_2_2_)    \n",
    "    - 6.2.3. [Pytoch.utils.data.Dataset类分析和总结](#toc6_2_3_)    \n",
    "    - 6.2.4. [Subset](#toc6_2_4_)    \n",
    "    - 6.2.5. [random_split](#toc6_2_5_)    \n",
    "    - 6.2.6. [ConcateDataset](#toc6_2_6_)    \n",
    "    - 6.2.7. [IterableDataset](#toc6_2_7_)    \n",
    "      - 6.2.7.1. [流式数据加载](#toc6_2_7_1_)    \n",
    "      - 6.2.7.2. [动态生成数据](#toc6_2_7_2_)    \n",
    "      - 6.2.7.3. [无限数据流](#toc6_2_7_3_)    \n",
    "      - 6.2.7.4. [多线程数据加载与分布式支持](#toc6_2_7_4_)    \n",
    "      - 6.2.7.5. [使用注意事项](#toc6_2_7_5_)    \n",
    "  - 6.3. [数据加载-DataLoader()](#toc6_3_)    \n",
    "    - 6.3.1. [估计数据加载时间](#toc6_3_1_)    \n",
    "    - 6.3.2. [collate_fn处理不等长tensor](#toc6_3_2_)    \n",
    "    - 6.3.3. [重载DataLoader](#toc6_3_3_)    \n",
    "- 7. [张量(Tensors)](#toc7_)    \n",
    "  - 7.1. [Tensors定义](#toc7_1_)    \n",
    "  - 7.2. [Tensors属性](#toc7_2_)    \n",
    "    - 7.2.1. [数据类型(dtype)](#toc7_2_1_)    \n",
    "      - 7.2.1.1. [转化格式](#toc7_2_1_1_)    \n",
    "    - 7.2.2. [设备(device)](#toc7_2_2_)    \n",
    "    - 7.2.3. [维度(size/shape)](#toc7_2_3_)    \n",
    "      - 7.2.3.1. [标量](#toc7_2_3_1_)    \n",
    "      - 7.2.3.2. [一维张量](#toc7_2_3_2_)    \n",
    "      - 7.2.3.3. [多维张量](#toc7_2_3_3_)    \n",
    "    - 7.2.4. [特殊的一维张量](#toc7_2_4_)    \n",
    "      - 7.2.4.1. [一维张量的例子](#toc7_2_4_1_)    \n",
    "      - 7.2.4.2. [区分行向量和列向量](#toc7_2_4_2_)    \n",
    "        - 7.2.4.2.1. [示例](#toc7_2_4_2_1_)    \n",
    "      - 7.2.4.3. [小结](#toc7_2_4_3_)    \n",
    "  - 7.3. [Tensors操作](#toc7_3_)    \n",
    "    - 7.3.1. [索引和切片](#toc7_3_1_)    \n",
    "    - 7.3.2. [修改维度](#toc7_3_2_)    \n",
    "      - 7.3.2.1. [[: None], [None, :]  ](#toc7_3_2_1_)    \n",
    "      - 7.3.2.2. [reshape函数](#toc7_3_2_2_)    \n",
    "      - 7.3.2.3. [view函数](#toc7_3_2_3_)    \n",
    "      - 7.3.2.4. [transpose函数](#toc7_3_2_4_)    \n",
    "        - 7.3.2.4.1. [二维](#toc7_3_2_4_1_)    \n",
    "        - 7.3.2.4.2. [三维](#toc7_3_2_4_2_)    \n",
    "      - 7.3.2.5. [permute函数](#toc7_3_2_5_)    \n",
    "        - 7.3.2.5.1. [二维](#toc7_3_2_5_1_)    \n",
    "        - 7.3.2.5.2. [三维](#toc7_3_2_5_2_)    \n",
    "      - 7.3.2.6. [unsqueeze函数增加维度](#toc7_3_2_6_)    \n",
    "        - 7.3.2.6.1. [1维](#toc7_3_2_6_1_)    \n",
    "        - 7.3.2.6.2. [多维度](#toc7_3_2_6_2_)    \n",
    "      - 7.3.2.7. [squeeze函数减少维度](#toc7_3_2_7_)    \n",
    "      - 7.3.2.8. [拼接 (concat)](#toc7_3_2_8_)    \n",
    "      - 7.3.2.9. [拆分 (split)](#toc7_3_2_9_)    \n",
    "      - 7.3.2.10. [分块 (chunk)](#toc7_3_2_10_)    \n",
    "      - 7.3.2.11. [拼接 (stack)](#toc7_3_2_11_)    \n",
    "        - 7.3.2.11.1. [cat和stack的比较](#toc7_3_2_11_1_)    \n",
    "      - 7.3.2.12. [广播 (expand)](#toc7_3_2_12_)    \n",
    "      - 7.3.2.13. [repeat](#toc7_3_2_13_)    \n",
    "      - 7.3.2.14. [repeat_interleave](#toc7_3_2_14_)    \n",
    "        - 7.3.2.14.1. [expand和repeat对比](#toc7_3_2_14_1_)    \n",
    "      - 7.3.2.15. [填充padding和打包packing](#toc7_3_2_15_)    \n",
    "  - 7.4. [线性代数运算](#toc7_4_)    \n",
    "    - 7.4.1. [数值运算](#toc7_4_1_)    \n",
    "    - 7.4.2. [数值运算-乘法](#toc7_4_2_)    \n",
    "      - 7.4.2.1. [哈达玛积](#toc7_4_2_1_)    \n",
    "      - 7.4.2.2. [点积（Dot Product）](#toc7_4_2_2_)    \n",
    "      - 7.4.2.3. [矩阵-向量积](#toc7_4_2_3_)    \n",
    "      - 7.4.2.4. [矩阵-矩阵积](#toc7_4_2_4_)    \n",
    "      - 7.4.2.5. [批量矩阵乘法](#toc7_4_2_5_)    \n",
    "      - 7.4.2.6. [乘总结](#toc7_4_2_6_)    \n",
    "    - 7.4.3. [统计运算](#toc7_4_3_)    \n",
    "  - 7.5. [广播机制 (Broadcasting)](#toc7_5_)    \n",
    "    - 7.5.1. [广播规则](#toc7_5_1_)    \n",
    "  - 7.6. [Pytorch的计算图 和 自动微分 (autograd)](#toc7_6_)    \n",
    "    - 7.6.1. [反向传播 (backward)-批量求梯度，但未进行参数更新](#toc7_6_1_)    \n",
    "    - 7.6.2. [仅计算梯度 (求导计算)](#toc7_6_2_)    \n",
    "  - 7.7. [自动微积-autograd](#toc7_7_)    \n",
    "    - 7.7.1. [自己探索](#toc7_7_1_)    \n",
    "      - 7.7.1.1. [标量-一阶导数（得标量）](#toc7_7_1_1_)    \n",
    "      - 7.7.1.2. [标量/向量-一阶导数（得向量）](#toc7_7_1_2_)    \n",
    "      - 7.7.1.3. [向量/向量-一阶导数（得矩阵）](#toc7_7_1_3_)    \n",
    "      - 7.7.1.4. [求高阶导数](#toc7_7_1_4_)    \n",
    "    - 7.7.2. [一个简单的例子](#toc7_7_2_)    \n",
    "    - 7.7.3. [计算另一个](#toc7_7_3_)    \n",
    "    - 7.7.4. [非标量变量的反向传播](#toc7_7_4_)    \n",
    "    - 7.7.5. [分离计算](#toc7_7_5_)    \n",
    "    - 7.7.6. [Python控制流的梯度计算](#toc7_7_6_)    \n",
    "  - 7.8. [概率论](#toc7_8_)    \n",
    "- 8. [神经网络-训练八股](#toc8_)    \n",
    "  - 8.1. [现线性回归模型于训练过程-从零开始](#toc8_1_)    \n",
    "    - 8.1.1. [虚拟出数据](#toc8_1_1_)    \n",
    "    - 8.1.2. [读取数据](#toc8_1_2_)    \n",
    "    - 8.1.3. [初始化模型参数](#toc8_1_3_)    \n",
    "    - 8.1.4. [定义模型](#toc8_1_4_)    \n",
    "    - 8.1.5. [定义损失函数](#toc8_1_5_)    \n",
    "    - 8.1.6. [定义优化算法](#toc8_1_6_)    \n",
    "    - 8.1.7. [训练](#toc8_1_7_)    \n",
    "  - 8.2. [现线性回归模型于训练过程-简洁实现](#toc8_2_)    \n",
    "    - 8.2.1. [虚拟数据](#toc8_2_1_)    \n",
    "    - 8.2.2. [读取数据](#toc8_2_2_)    \n",
    "    - 8.2.3. [定义模型](#toc8_2_3_)    \n",
    "    - 8.2.4. [初始化模型参数](#toc8_2_4_)    \n",
    "    - 8.2.5. [定义损失函数](#toc8_2_5_)    \n",
    "    - 8.2.6. [定义优化算法](#toc8_2_6_)    \n",
    "    - 8.2.7. [训练](#toc8_2_7_)    \n",
    "    - 8.2.8. [参数保存](#toc8_2_8_)    \n",
    "    - 8.2.9. [重载](#toc8_2_9_)    \n",
    "  - 8.3. [分类-softmax](#toc8_3_)    \n",
    "    - 8.3.1. [快速实现](#toc8_3_1_)    \n",
    "    - 8.3.2. [从头实现](#toc8_3_2_)    \n",
    "    - 8.3.3. [交叉熵损失](#toc8_3_3_)    \n",
    "  - 8.4. [专题-模型定义（计算预测值y_hat）](#toc8_4_)    \n",
    "    - 8.4.1. [块：torch.nn模块](#toc8_4_1_)    \n",
    "      - 8.4.1.1. [Sequential、ModuleList、ModuleDict](#toc8_4_1_1_)    \n",
    "      - 8.4.1.2. [比较](#toc8_4_1_2_)    \n",
    "    - 8.4.2. [块：自定义](#toc8_4_2_)    \n",
    "      - 8.4.2.1. [自定义块](#toc8_4_2_1_)    \n",
    "      - 8.4.2.2. [顺序块](#toc8_4_2_2_)    \n",
    "      - 8.4.2.3. [效率](#toc8_4_2_3_)    \n",
    "    - 8.4.3. [模型结构/组成](#toc8_4_3_)    \n",
    "      - 8.4.3.1. [.children()](#toc8_4_3_1_)    \n",
    "      - 8.4.3.2. [.named_children()](#toc8_4_3_2_)    \n",
    "      - 8.4.3.3. [.modules()](#toc8_4_3_3_)    \n",
    "      - 8.4.3.4. [.named_modules()](#toc8_4_3_4_)    \n",
    "      - 8.4.3.5. [删除和添加](#toc8_4_3_5_)    \n",
    "      - 8.4.3.6. [替换](#toc8_4_3_6_)    \n",
    "      - 8.4.3.7. [add_module()](#toc8_4_3_7_)    \n",
    "    - 8.4.4. [模型：参数管理](#toc8_4_4_)    \n",
    "      - 8.4.4.1. [参数访问](#toc8_4_4_1_)    \n",
    "        - 8.4.4.1.1. [state_dict](#toc8_4_4_1_1_)    \n",
    "        - 8.4.4.1.2. [parameters](#toc8_4_4_1_2_)    \n",
    "        - 8.4.4.1.3. [named_parameters](#toc8_4_4_1_3_)    \n",
    "      - 8.4.4.2. [参数初始化](#toc8_4_4_2_)    \n",
    "        - 8.4.4.2.1. [内置初始化](#toc8_4_4_2_1_)    \n",
    "        - 8.4.4.2.2. [自定义初始化](#toc8_4_4_2_2_)    \n",
    "        - 8.4.4.2.3. [参数绑定](#toc8_4_4_2_3_)    \n",
    "    - 8.4.5. [层：自定义](#toc8_4_5_)    \n",
    "      - 8.4.5.1. [不带参数的层](#toc8_4_5_1_)    \n",
    "      - 8.4.5.2. [带参数的层](#toc8_4_5_2_)    \n",
    "  - 8.5. [专题-损失函数 (loss_fn)](#toc8_5_)    \n",
    "    - 8.5.1. [均方误差](#toc8_5_1_)    \n",
    "    - 8.5.2. [交叉熵](#toc8_5_2_)    \n",
    "      - 8.5.2.1. [快速实现](#toc8_5_2_1_)    \n",
    "      - 8.5.2.2. [从头实现](#toc8_5_2_2_)    \n",
    "    - 8.5.3. [自定义](#toc8_5_3_)    \n",
    "  - 8.6. [专题-反向传播（求梯度）](#toc8_6_)    \n",
    "  - 8.7. [专题-更新权重（优化算法）](#toc8_7_)    \n",
    "    - 8.7.1. [小批量随机梯度下降（SGD）](#toc8_7_1_)    \n",
    "    - 8.7.2. [adam](#toc8_7_2_)    \n",
    "    - 8.7.3. [RMSprop](#toc8_7_3_)    \n",
    "    - 8.7.4. [学习率调度器](#toc8_7_4_)    \n",
    "      - 8.7.4.1. [StepLR： 按照固定的步长调整学习率](#toc8_7_4_1_)    \n",
    "      - 8.7.4.2. [MultiStepLR： 在指定的里程碑（milestones）上调整学习率](#toc8_7_4_2_)    \n",
    "      - 8.7.4.3. [ExponentialLR： 以指数衰减的方式调整学习率](#toc8_7_4_3_)    \n",
    "      - 8.7.4.4. [CosineAnnealingLR： 余弦退火调整学习率](#toc8_7_4_4_)    \n",
    "      - 8.7.4.5. [ReduceLROnPlateau： 当指标停止改善时，降低学习率](#toc8_7_4_5_)    \n",
    "      - 8.7.4.6. [LambdaLR： 使用自定义的函数来调整学习率](#toc8_7_4_6_)    \n",
    "      - 8.7.4.7. [自定义](#toc8_7_4_7_)    \n",
    "  - 8.8. [专题-训练](#toc8_8_)    \n",
    "    - 8.8.1. [开始训练](#toc8_8_1_)    \n",
    "    - 8.8.2. [自己探索](#toc8_8_2_)    \n",
    "      - 8.8.2.1. [lr的影响](#toc8_8_2_1_)    \n",
    "      - 8.8.2.2. [不同模型的效率](#toc8_8_2_2_)    \n",
    "    - 8.8.3. [K折交叉验证](#toc8_8_3_)    \n",
    "  - 8.9. [可视化训练过程](#toc8_9_)    \n",
    "- 9. [在 GPU 上训练](#toc9_)    \n",
    "  - 9.1. [查看GPU配置](#toc9_1_)    \n",
    "  - 9.2. [单机单卡（GPU）](#toc9_2_)    \n",
    "  - 9.3. [单机多卡（GPU）](#toc9_3_)    \n",
    "    - 9.3.1. [DP](#toc9_3_1_)    \n",
    "    - 9.3.2. [DDP](#toc9_3_2_)    \n",
    "      - 9.3.2.1. [在colab上测试可用](#toc9_3_2_1_)    \n",
    "  - 9.4. [多机多卡（GPU）- 分布式训练](#toc9_4_)    \n",
    "- 10. [模型和参数的保存与加载](#toc10_)    \n",
    "  - 10.1. [加载和保存-张量](#toc10_1_)    \n",
    "  - 10.2. [加载和保存-模型参数](#toc10_2_)    \n",
    "  - 10.3. [safetensor](#toc10_3_)    \n",
    "- 11. [神经网络类型](#toc11_)    \n",
    "  - 11.1. [CNN](#toc11_1_)    \n",
    "    - 11.1.1. [概述](#toc11_1_1_)    \n",
    "    - 11.1.2. [简单CNN](#toc11_1_2_)    \n",
    "      - 11.1.2.1. [从头实现](#toc11_1_2_1_)    \n",
    "        - 11.1.2.1.1. [卷积计算过程](#toc11_1_2_1_1_)    \n",
    "        - 11.1.2.1.2. [从头卷积层](#toc11_1_2_1_2_)    \n",
    "      - 11.1.2.2. [简洁实现](#toc11_1_2_2_)    \n",
    "      - 11.1.2.3. [填充和步幅](#toc11_1_2_3_)    \n",
    "      - 11.1.2.4. [多输入和多输出通道](#toc11_1_2_4_)    \n",
    "      - 11.1.2.5. [Pooling (汇聚层)](#toc11_1_2_5_)    \n",
    "        - 11.1.2.5.1. [平均Pooling](#toc11_1_2_5_1_)    \n",
    "        - 11.1.2.5.2. [最大Pooling](#toc11_1_2_5_2_)    \n",
    "    - 11.1.3. [LeNet](#toc11_1_3_)    \n",
    "    - 11.1.4. [AlexNet](#toc11_1_4_)    \n",
    "    - 11.1.5. [VGG](#toc11_1_5_)    \n",
    "    - 11.1.6. [NiN](#toc11_1_6_)    \n",
    "    - 11.1.7. [GoogLeNet](#toc11_1_7_)    \n",
    "    - 11.1.8. [批量规范化](#toc11_1_8_)    \n",
    "    - 11.1.9. [ResNet](#toc11_1_9_)    \n",
    "      - 11.1.9.1. [从头实现](#toc11_1_9_1_)    \n",
    "  - 11.2. [序列数据](#toc11_2_)    \n",
    "    - 11.2.1. [什么是序列](#toc11_2_1_)    \n",
    "    - 11.2.2. [语言模型](#toc11_2_2_)    \n",
    "    - 11.2.3. [文本预处理](#toc11_2_3_)    \n",
    "      - 11.2.3.1. [下载《Time machine》并读取数据](#toc11_2_3_1_)    \n",
    "      - 11.2.3.2. [词元化（Tokenization）](#toc11_2_3_2_)    \n",
    "      - 11.2.3.3. [词表（Vocabulary）](#toc11_2_3_3_)    \n",
    "      - 11.2.3.4. [整合所有功能](#toc11_2_3_4_)    \n",
    "      - 11.2.3.5. [文本编码与向量化](#toc11_2_3_5_)    \n",
    "        - 11.2.3.5.1. [word2vec](#toc11_2_3_5_1_)    \n",
    "    - 11.2.4. [语言模型数据集](#toc11_2_4_)    \n",
    "      - 11.2.4.1. [顺序采样 (Sequential Sampling)](#toc11_2_4_1_)    \n",
    "      - 11.2.4.2. [随机采样 (Random Sampling)](#toc11_2_4_2_)    \n",
    "      - 11.2.4.3. [PyTorch分装的顺序或随机采样](#toc11_2_4_3_)    \n",
    "      - 11.2.4.4. [总结](#toc11_2_4_4_)    \n",
    "      - 11.2.4.5. [包装](#toc11_2_4_5_)    \n",
    "  - 11.3. [RNN](#toc11_3_)    \n",
    "    - 11.3.1. [RNN-循环神经网络原理](#toc11_3_1_)    \n",
    "      - 11.3.1.1. [从头实现网络](#toc11_3_1_1_)    \n",
    "      - 11.3.1.2. [简洁实现](#toc11_3_1_2_)    \n",
    "      - 11.3.1.3. [训练和预测](#toc11_3_1_3_)    \n",
    "      - 11.3.1.4. [warm-up 预热期](#toc11_3_1_4_)    \n",
    "      - 11.3.1.5. [深层RNN](#toc11_3_1_5_)    \n",
    "      - 11.3.1.6. [双向RNN](#toc11_3_1_6_)    \n",
    "    - 11.3.2. [GRU](#toc11_3_2_)    \n",
    "      - 11.3.2.1. [从头实现](#toc11_3_2_1_)    \n",
    "      - 11.3.2.2. [简洁实现](#toc11_3_2_2_)    \n",
    "    - 11.3.3. [LSTM](#toc11_3_3_)    \n",
    "      - 11.3.3.1. [从头实现](#toc11_3_3_1_)    \n",
    "      - 11.3.3.2. [简洁实现](#toc11_3_3_2_)    \n",
    "    - 11.3.4. [Encoder-Decoder框架](#toc11_3_4_)    \n",
    "      - 11.3.4.1. [Encoder部分](#toc11_3_4_1_)    \n",
    "      - 11.3.4.2. [Decoder部分](#toc11_3_4_2_)    \n",
    "      - 11.3.4.3. [Encoder-Decoder（合并编码器和解码器）](#toc11_3_4_3_)    \n",
    "  - 11.4. [seq2seq (Sequence to sequence learning)](#toc11_4_)    \n",
    "    - 11.4.1. [机器翻译与数据集](#toc11_4_1_)    \n",
    "      - 11.4.1.1. [下载和预处理数据集](#toc11_4_1_1_)    \n",
    "      - 11.4.1.2. [词元化](#toc11_4_1_2_)    \n",
    "      - 11.4.1.3. [词表](#toc11_4_1_3_)    \n",
    "      - 11.4.1.4. [截断和填充](#toc11_4_1_4_)    \n",
    "      - 11.4.1.5. [集合](#toc11_4_1_5_)    \n",
    "    - 11.4.2. [编码器-解码器架构](#toc11_4_2_)    \n",
    "    - 11.4.3. [序列到序列学习](#toc11_4_3_)    \n",
    "    - 11.4.4. [损失函数](#toc11_4_4_)    \n",
    "      - 11.4.4.1. [掩码](#toc11_4_4_1_)    \n",
    "      - 11.4.4.2. [带掩码的softmax交叉熵损失](#toc11_4_4_2_)    \n",
    "    - 11.4.5. [训练](#toc11_4_5_)    \n",
    "    - 11.4.6. [预测](#toc11_4_6_)    \n",
    "  - 11.5. [Attention](#toc11_5_)    \n",
    "    - 11.5.1. [实例数据](#toc11_5_1_)    \n",
    "    - 11.5.2. [无注意力的方式-如平均汇聚](#toc11_5_2_)    \n",
    "    - 11.5.3. [非参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc11_5_3_)    \n",
    "    - 11.5.4. [参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc11_5_4_)    \n",
    "    - 11.5.5. [注意力分数函数-计算q和k相似度](#toc11_5_5_)    \n",
    "      - 11.5.5.1. [加性注意力 (Additive Attention)-计算q、k相似度](#toc11_5_5_1_)    \n",
    "      - 11.5.5.2. [缩放点积注意力 (Scaled Dot-Product Attention)-计算q、k相似度](#toc11_5_5_2_)    \n",
    "    - 11.5.6. [自注意力机制-q、k和v相同](#toc11_5_6_)    \n",
    "    - 11.5.7. [多头注意力机制-h个q、k和v对](#toc11_5_7_)    \n",
    "    - 11.5.8. [attention-seq2seq](#toc11_5_8_)    \n",
    "  - 11.6. [Transformer](#toc11_6_)    \n",
    "    - 11.6.1. [简洁实现](#toc11_6_1_)    \n",
    "    - 11.6.2. [位置编码](#toc11_6_2_)    \n",
    "      - 11.6.2.1. [绝对位置编码](#toc11_6_2_1_)    \n",
    "      - 11.6.2.2. [相对位置编码](#toc11_6_2_2_)    \n",
    "      - 11.6.2.3. [可学习的位置编码](#toc11_6_2_3_)    \n",
    "    - 11.6.3. [基于位置的前馈网络](#toc11_6_3_)    \n",
    "    - 11.6.4. [残差连接和层规范化](#toc11_6_4_)    \n",
    "    - 11.6.5. [编码器](#toc11_6_5_)    \n",
    "    - 11.6.6. [解码器](#toc11_6_6_)    \n",
    "    - 11.6.7. [基于Transformer的Seq2Seq网络](#toc11_6_7_)    \n",
    "  - 11.7. [BERT](#toc11_7_)    \n",
    "    - 11.7.1. [BERT encode block](#toc11_7_1_)    \n",
    "    - 11.7.2. [Masked Language Modeling](#toc11_7_2_)    \n",
    "    - 11.7.3. [Next Sentence Prediction](#toc11_7_3_)    \n",
    "    - 11.7.4. [BERT模型](#toc11_7_4_)    \n",
    "    - 11.7.5. [Datasets for Pre-training](#toc11_7_5_)    \n",
    "      - 11.7.5.1. [生成下一句预测任务的数据](#toc11_7_5_1_)    \n",
    "      - 11.7.5.2. [生成遮蔽语言模型任务的数据](#toc11_7_5_2_)    \n",
    "      - 11.7.5.3. [将文本转换为预训练数据集](#toc11_7_5_3_)    \n",
    "      - 11.7.5.4. [创建数据集](#toc11_7_5_4_)    \n",
    "    - 11.7.6. [预训练BERT](#toc11_7_6_)    \n",
    "    - 11.7.7. [用BERT表示文本](#toc11_7_7_)    \n",
    "  - 11.8. [用BERT做微调](#toc11_8_)    \n",
    "    - 11.8.1. [情感分析](#toc11_8_1_)    \n",
    "      - 11.8.1.1. [使用RNN](#toc11_8_1_1_)    \n",
    "      - 11.8.1.2. [使用CNN](#toc11_8_1_2_)    \n",
    "    - 11.8.2. [自然语言推断](#toc11_8_2_)    \n",
    "      - 11.8.2.1. [使用Attention](#toc11_8_2_1_)    \n",
    "      - 11.8.2.2. [微调BERT](#toc11_8_2_2_)    \n",
    "  - 11.9. [GPT](#toc11_9_)    \n",
    "  - 11.10. [Mamba](#toc11_10_)    \n",
    "- 12. [==============](#toc12_)    \n",
    "- 13. [炼丹心得](#toc13_)    \n",
    "  - 13.1. [关于调参](#toc13_1_)    \n",
    "  - 13.2. [模型选择](#toc13_2_)    \n",
    "  - 13.3. [离散数据](#toc13_3_)    \n",
    "    - 13.3.1. [one-hot](#toc13_3_1_)    \n",
    "    - 13.3.2. [embedding](#toc13_3_2_)    \n",
    "      - 13.3.2.1. [使用 torch.nn.Embedding](#toc13_3_2_1_)    \n",
    "      - 13.3.2.2. [初始化 Embedding 层](#toc13_3_2_2_)    \n",
    "  - 13.4. [BN和LN](#toc13_4_)    \n",
    "  - 13.5. [掩码 (mask)](#toc13_5_)    \n",
    "    - 13.5.1. [简单演示](#toc13_5_1_)    \n",
    "      - 13.5.1.1. [忽略填充](#toc13_5_1_1_)    \n",
    "      - 13.5.1.2. [加权忽略](#toc13_5_1_2_)    \n",
    "    - 13.5.2. [注意力机制中的掩码](#toc13_5_2_)    \n",
    "      - 13.5.2.1. [Padding Mask](#toc13_5_2_1_)    \n",
    "      - 13.5.2.2. [Causal Mask](#toc13_5_2_2_)    \n",
    "    - 13.5.3. [掩码注意力计算](#toc13_5_3_)    \n",
    "  - 13.6. [MLP、FC、FNN、CNN、RNN](#toc13_6_)    \n",
    "  - 13.7. [优化显存使用](#toc13_7_)    \n",
    "    - 13.7.1. [删除中间暂时不用的变量](#toc13_7_1_)    \n",
    "    - 13.7.2. [混合精度训练(Mixed Precision Training)](#toc13_7_2_)    \n",
    "    - 13.7.3. [梯度检查点（Gradient Checkpointing）](#toc13_7_3_)    \n",
    "    - 13.7.4. [分块计算 (Chunking)](#toc13_7_4_)    \n",
    "      - 13.7.4.1. [简单演示](#toc13_7_4_1_)    \n",
    "      - 13.7.4.2. [重要演示](#toc13_7_4_2_)    \n",
    "  - 13.8. [模型参数量](#toc13_8_)    \n",
    "  - 13.9. [大模型微调](#toc13_9_)    \n",
    "  - 13.10. [加速器](#toc13_10_)    \n",
    "    - 13.10.1. [deepspeed](#toc13_10_1_)    \n",
    "      - 13.10.1.1. [数据并行](#toc13_10_1_1_)    \n",
    "      - 13.10.1.2. [模型并行](#toc13_10_1_2_)    \n",
    "      - 13.10.1.3. [混合并行](#toc13_10_1_3_)    \n",
    "    - 13.10.2. [huggingface trainer](#toc13_10_2_)    \n",
    "      - 13.10.2.1. [数据并行](#toc13_10_2_1_)    \n",
    "      - 13.10.2.2. [模型并行](#toc13_10_2_2_)    \n",
    "      - 13.10.2.3. [混合并行](#toc13_10_2_3_)    \n",
    "- 14. [PyTorch做迁移学习](#toc14_)    \n",
    "  - 14.1. [Fine-tuning](#toc14_1_)    \n",
    "    - 14.1.1. [小的lr](#toc14_1_1_)    \n",
    "    - 14.1.2. [停止计算梯度](#toc14_1_2_)    \n",
    "  - 14.2. [torchvision的应用案例](#toc14_2_)    \n",
    "  - 14.3. [迁移学习案例](#toc14_3_)    \n",
    "- 15. [Metrics](#toc15_)    \n",
    "  - 15.1. [TorchMetrics](#toc15_1_)    \n",
    "    - 15.1.1. [准确率、精确率、召回率和F1分数](#toc15_1_1_)    \n",
    "    - 15.1.2. [自定义计算指标](#toc15_1_2_)    \n",
    "    - 15.1.3. [于PyTorch Lightning联合使用](#toc15_1_3_)    \n",
    "  - 15.2. [分类问题的评估指标](#toc15_2_)    \n",
    "    - 15.2.1. [混淆矩阵](#toc15_2_1_)    \n",
    "      - 15.2.1.1. [二分类混淆矩阵](#toc15_2_1_1_)    \n",
    "      - 15.2.1.2. [多分类混淆矩阵](#toc15_2_1_2_)    \n",
    "      - 15.2.1.3. [可视化混淆矩阵](#toc15_2_1_3_)    \n",
    "      - 15.2.1.4. [混淆矩阵的优点与局限性](#toc15_2_1_4_)    \n",
    "    - 15.2.2. [准确率 (Accuracy)](#toc15_2_2_)    \n",
    "    - 15.2.3. [精确率 (Precision)](#toc15_2_3_)    \n",
    "    - 15.2.4. [召回率 (Recall)](#toc15_2_4_)    \n",
    "    - 15.2.5. [F1-Score](#toc15_2_5_)    \n",
    "    - 15.2.6. [ROC 曲线和 AUC (Area Under Curve)](#toc15_2_6_)    \n",
    "    - 15.2.7. [多分类问题指标](#toc15_2_7_)    \n",
    "  - 15.3. [回归问题的评估指标](#toc15_3_)    \n",
    "    - 15.3.1. [平均绝对误差 (MAE)](#toc15_3_1_)    \n",
    "    - 15.3.2. [均方误差 (MSE)](#toc15_3_2_)    \n",
    "    - 15.3.3. [均方根误差 (RMSE)](#toc15_3_3_)    \n",
    "    - 15.3.4. [R² (决定系数)](#toc15_3_4_)    \n",
    "- 16. [Benchmark](#toc16_)    \n",
    "  - 16.1. [确定 Benchmark 目标](#toc16_1_)    \n",
    "  - 16.2. [Benchmark模板](#toc16_2_)    \n",
    "- 17. [PyTorch lightning](#toc17_)    \n",
    "  - 17.1. [训练逻辑](#toc17_1_)    \n",
    "  - 17.2. [Data.py](#toc17_2_)    \n",
    "  - 17.3. [Model.py](#toc17_3_)    \n",
    "  - 17.4. [ModelWrapper.py](#toc17_4_)    \n",
    "    - 17.4.1. [Training and vlidation](#toc17_4_1_)    \n",
    "    - 17.4.2. [Validation](#toc17_4_2_)    \n",
    "    - 17.4.3. [Test](#toc17_4_3_)    \n",
    "    - 17.4.4. [Prediction](#toc17_4_4_)    \n",
    "      - 17.4.4.1. [PyTorch lightning自身Trainer直接predict](#toc17_4_4_1_)    \n",
    "      - 17.4.4.2. [PyTorch lightning加载权重后预测](#toc17_4_4_2_)    \n",
    "      - 17.4.4.3. [提取权重后加载至纯PyTorch模型](#toc17_4_4_3_)    \n",
    "- 18. [Torchvision](#toc18_)    \n",
    "  - 18.1. [Models](#toc18_1_)    \n",
    "    - 18.1.1. [可用模型](#toc18_1_1_)    \n",
    "    - 18.1.2. [下载模型和权重](#toc18_1_2_)    \n",
    "    - 18.1.3. [模型加载权重](#toc18_1_3_)    \n",
    "    - 18.1.4. [总结](#toc18_1_4_)    \n",
    "  - 18.2. [Dataset](#toc18_2_)    \n",
    "- 19. [Hugging face](#toc19_)    \n",
    "- 20. [PyTorch hub](#toc20_)    \n",
    "- 21. [监督学习 (Supervised learning)](#toc21_)    \n",
    "- 22. [半监督学习 (Semi-supervised learning)](#toc22_)    \n",
    "- 23. [无监督学习 (Unsupervised learning)](#toc23_)    \n",
    "- 24. [深度强化学习 (DRL, Deep Reforcement Learning)](#toc24_)    \n",
    "  - 24.1. [强化学习基础概念](#toc24_1_)    \n",
    "  - 24.2. [深度强化学习的特点](#toc24_2_)    \n",
    "  - 24.3. [马尔可夫](#toc24_3_)    \n",
    "  - 24.4. [深度强化学习的主要方法](#toc24_4_)    \n",
    "    - 24.4.1. [深度 Q 网络（Deep Q-Network, DQN）](#toc24_4_1_)    \n",
    "    - 24.4.2. [策略梯度方法（Policy Gradient Methods）](#toc24_4_2_)    \n",
    "    - 24.4.3. [演员-评论家方法（Actor-Critic Methods）](#toc24_4_3_)    \n",
    "    - 24.4.4. [深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）](#toc24_4_4_)    \n",
    "  - 24.5. [深度强化学习的应用](#toc24_5_)    \n",
    "- 25. [生成对抗网络 (GAN, Generative Adversarial Networks)](#toc25_)    \n",
    "- 26. [扩散模型 (DM, Diffusion Models)](#toc26_)    \n",
    "- 27. [图神经网络 (GNN, Graph Neural Networks)](#toc27_)    \n",
    "- 28. [多模态 (ML, MultiModal Learning)](#toc28_)    \n",
    "  - 28.1. [特征融合](#toc28_1_)    \n",
    "    - 28.1.1. [concatenate融合](#toc28_1_1_)    \n",
    "    - 28.1.2. [加权融合](#toc28_1_2_)    \n",
    "    - 28.1.3. [元素级融合](#toc28_1_3_)    \n",
    "    - 28.1.4. [张量融合](#toc28_1_4_)    \n",
    "    - 28.1.5. [注意力机制融合](#toc28_1_5_)    \n",
    "    - 28.1.6. [高阶融合](#toc28_1_6_)    \n",
    "  - 28.2. [简单示例](#toc28_2_)    \n",
    "- 29. [argparse](#toc29_)    \n",
    "- 30. [ml_collections](#toc30_)    \n",
    "- 31. [functools](#toc31_)    \n",
    "  - 31.1. [partial](#toc31_1_)    \n",
    "- 32. [copy](#toc32_)    \n",
    "  - 32.1. [列表类型的拷贝](#toc32_1_)    \n",
    "  - 32.2. [字典类型的拷贝](#toc32_2_)    \n",
    "- 33. [转格式](#toc33_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[概述](#toc0_)\n",
    "写这个笔记主要是为了记录学习过程中知识的总结、归纳和反思。作为一个非科班出生的生物人，仅凭着对人工智能的热爱开始了自学这条路。前路漫漫不敢想，也不曾觉得以后能端这碗饭。只是，羡慕网上像智慧君、李沐这样的人，能够从事如此炫酷的工作，能把自己的热爱开发成一生从事的职业。仔细想想如果自己不做点什么或是不为此努力点什么，就觉得坐立不安、难以入眠。同时深知，这个过程会是无比艰辛，在百无聊赖之际，记录学习的过程或许会是一种苦中作乐的方式。\n",
    "\n",
    "- d2l EN (及时更新): [https://d2l.ai/index.html](https://d2l.ai/index.html)\n",
    "\n",
    "- d2l ZH: [https://zh-v2.d2l.ai/](https://zh-v2.d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[环境配置](#toc0_)\n",
    "\n",
    "- PyTorch官方教程 [https://pytorch.org/](https://pytorch.org/)\n",
    "\n",
    "- PyTorch lightning官方教程 [https://lightning.ai/docs/pytorch/stable/](https://lightning.ai/docs/pytorch/stable/)\n",
    "\n",
    "- 尽量用conda配置环境，不要conda和pip混搭。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environmental name \n",
    "name=\"pytorch1\"\n",
    "\n",
    "# Create environment \n",
    "# and entry the environment\n",
    "conda create -n $name -y && conda activate $name\n",
    "\n",
    "# Install ipykernel and related packages via conda\n",
    "conda install ipykernel matplotlib pandas seaborn -y\n",
    "\n",
    "# Download and install CUDATOOLkit containing CUDA and nvcc and etc. via conda on NVIDIA channel\n",
    "## 方法一：\n",
    "# conda install nvidia::cuda-toolkit -y\n",
    "## 方法二（推荐）, with ncvv and etc.\n",
    "conda install nvidia/label/cuda-12.4.0::cuda -y\n",
    "\n",
    "# Install PyTorch\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y\n",
    "\n",
    "# Instll packages \n",
    "conda install esri::torch-geometric lightning deepspeed torchmetrics huggingface_hub -c conda-forge -y \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"pytorch1\"\n",
    "\n",
    "conda create -n $name -y && conda activate $name \n",
    "\n",
    "conda install \\\n",
    "    nvidia/label/cuda-12.4.0::cuda \\\n",
    "    pytorch::pytorch \\\n",
    "    pytorch::torchvision \\\n",
    "    pytorch::torchaudio \\\n",
    "    conda-forge::torchmetrics \\\n",
    "    conda-forge::deepspeed \\\n",
    "    conda-forge::mpi4py \\\n",
    "    conda-forge::pytorch-lightning \\\n",
    "    esri::torch-geometric \\\n",
    "    conda-forge::huggingface_hub \\\n",
    "    anaconda::ipykernel \\\n",
    "    conda-forge::matplotlib \\\n",
    "    anaconda::pandas \\\n",
    "    anaconda::seaborn \\\n",
    "    -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[utils](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[实验可重复性](#toc0_)\n",
    "整个代码框架中很多地方使用到随机数的，为了实验的可重复性需要固定随机种子；  \n",
    "另外，有研究表明GPU中的CUDA变成也有很多地方对实验结果的稳定性很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import time \n",
    "from IPython import display \n",
    "\n",
    "\n",
    "def use_svg_display():\n",
    "    '''设置matplotlib的输出格式为svg'''\n",
    "    display.set_matplotlib_formats('svg')\n",
    "    return None\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"如果GPU可用，则返回GPU设备，否则返回CPU设备\"\"\"\n",
    "    if torch.cuda.is_avaliable():\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的GPU设备\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 如果idx是slice，则返回self.data[idx]\n",
    "        if isinstance(idx, slice):\n",
    "            return self.data[idx]\n",
    "        else: \n",
    "            return self.data[idx]  \n",
    "    \n",
    "\n",
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes,]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: d2l.set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"\n",
    "    计算正确预测的数量\n",
    "    Args:\n",
    "        y_hat: 预测的标签\n",
    "        y: 真实的标签\n",
    "    Returns:\n",
    "        float: 正确预测的数量\n",
    "    \"\"\"\n",
    "    # 如果 y_hat 的维度大于1（即多分类情况），并且第二维的大小大于1，表示每个样本有多个类别的预测分数。\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        # 这将 y_hat 转换为形状为 (batch_size,) 的张量，其中每个元素表示预测的类别标签。\n",
    "        y_hat = d2l.argmax(y_hat, axis=1)\n",
    "    # 将 y_hat 转换为与 y 相同的数据类型，以确保类型匹配。\n",
    "    # 使用 == 运算符逐元素比较预测值与真实标签，生成一个布尔张量 cmp，其中每个元素为 True 表示预测正确，False 表示预测错误。\n",
    "    cmp = d2l.astype(y_hat, y.dtype) == y\n",
    "    # 将布尔张量 cmp 转换为与 y 相同的数据类型（通常是整数类型），其中 True 转换为 1，False 转换为 0。\n",
    "    # 使用 d2l.reduce_sum 对转换后的张量进行求和，得到正确预测的总数。\n",
    "    # 将结果转换为浮点数并返回。\n",
    "    return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))\n",
    "\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    \"\"\"\n",
    "    计算模型在数据集上的准确率\n",
    "    Args:\n",
    "        net: 模型\n",
    "        data_iter: 数据集\n",
    "    Returns:\n",
    "        float: 准确率\n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), d2l.size(y))\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[计时器](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. <a id='toc3_3_1_'></a>[cpu计时器](#toc0_)\n",
    "\n",
    "* 自定义的一些使用的脚本。\n",
    "```sehll\n",
    "    __init__(self) # 初始化实例时就会执行\n",
    "    __call__(self) # 再次调用时，自动执行\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== \n",
      " Total：\n",
      " 0.0 d \n",
      " 0.0 h \n",
      " 0.0 m \n",
      " 0.030922651290893555 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class cpuTimer():\n",
    "    '''一个计时器'''\n",
    "    def __init__(self):\n",
    "        '''初始化时候自动执行'''\n",
    "        self.start = time.time()\n",
    "\n",
    "    def __call__(self):\n",
    "        '''再次调用该对象时，会自动执行'''\n",
    "        self.stop = time.time()\n",
    "        seconds = self.stop - self.start\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        print('='*20, '\\n', f\"Total：\\n {days} d \\n {hours} h \\n {minutes} m \\n {remaining_seconds} s\")\n",
    "        \n",
    "# Tiemr使用\n",
    "timer_on_cpu = cpuTimer()\n",
    "for i in range(3):\n",
    "    time.sleep(0.01)\n",
    "timer_on_cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. <a id='toc3_3_2_'></a>[gpu计时器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "GPU time: 0.12635s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "计算GPU的时间是不同于计算CPU的的时间的\n",
    "'''\n",
    "\n",
    "import torch \n",
    "\n",
    "\n",
    "class gpuTimer():\n",
    "    def __init__(self):\n",
    "        # CUDA is asynchronous, so we need to use different timing functions\n",
    "        self.start = torch.cuda.Event(enable_timing=True)\n",
    "        self.end = torch.cuda.Event(enable_timing=True)\n",
    "        self.start.record()\n",
    "\n",
    "    def __call__(self):\n",
    "        self.end.record()\n",
    "        torch.cuda.synchronize()  # Waits for everything to finish running on the GPU\n",
    "        print(\"⚡\"*20, f\"\\nGPU time: {0.001 * self.start.elapsed_time(self.end):6.5f}s\")  # Milliseconds to seconds\n",
    "\n",
    "\n",
    "# Demo for Timer on GPU devices\n",
    "timer_on_gpu = gpuTimer()\n",
    "a = torch.arange(45).reshape(3, 3, 5)\n",
    "b = torch.arange(45).reshape(3, 5, 3)\n",
    "c = torch.bmm(a, b)\n",
    "timer_on_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[统计参数量和内存占用](#toc0_)\n",
    "PyTorch 在进行深度学习训练的时候，有 4 大部分的显存开销：\n",
    "  - `模型参数(parameters)` ；\n",
    "  - `模型参数的梯度(gradients)` ；\n",
    "  - `中间激活值(intermediate activations) 或者叫中间结果(intermediate results)`；\n",
    "  - `优化器状态(optimizer states)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量: 1880.686592M\n",
      "可训练参数数量: 1880.686592M\n",
      "模型大小: 7174.25 MB\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x) # 编码\n",
    "        x = self.decoder(x) # 解码\n",
    "        return x\n",
    "    \n",
    "\n",
    "value = 32\n",
    "model = Model(d_model=value*64, nhead=value, dim_feedforward=1024, dropout=0.1, batch_first=True, num_layers=value)\n",
    "\n",
    "# 计算模型的总参数数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "# 计算可训练的参数数量\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"总参数数量: {total_params / 1000000}M\")\n",
    "print(f\"可训练参数数量: {trainable_params / 1000000}M\")\n",
    "\n",
    "# 计算模型大小（以MB为单位）\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "\n",
    "print(f\"模型大小: {size_all_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def parameter_size(model, dtype=torch.float32):\n",
    "    bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "    total_params = count_parameters(model)\n",
    "    total_size = total_params * bytes_per_param\n",
    "    print(f'{total_params/1000000} M parameters')\n",
    "    print(f'{total_size/(1024*1024):.2f} MB')\n",
    "    # return total_params, total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. <a id='toc3_5_'></a>[numpy和pytorch计算速度比较](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Datas on cpu with arrary\n",
    "a = np.random.rand(1000, 1000)\n",
    "b = np.random.rand(1000, 1000)\n",
    "\n",
    "# Datas on cpu with tensor\n",
    "at = torch.Tensor(a).to('cpu')\n",
    "bt = torch.Tensor(b).to('cpu')\n",
    "\n",
    "# Datas on gpu with tensor\n",
    "at_gpu = torch.Tensor(a).to('cuda:0')\n",
    "bt_gpu = torch.Tensor(b).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 ms ± 15.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a + b   # On cpu via numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.8 μs ± 332 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit at + bt # On cpu via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.08263510131835938 s\n"
     ]
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "stop = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "at_gpu + bt_gpu\n",
    "stop.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "print(f'Time: {0.001 * start.elapsed_time(stop)} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[安装GPU驱动](#toc0_)\n",
    "\n",
    "以CentOS8安装NVIDIA Tesla A100为例，下载CUDA Toolkit和CuDNN，需要注意cudnn的版本必须与cuda的版本相匹配：\n",
    "\n",
    "  1. NVIDIA Driver：NVIDIA驱动是NVIDIA显卡的`驱动程序`，它是CUDA和CuDNN的前提条件。显卡驱动下载地址：https://www.nvidia.com/Download/index.aspx。\n",
    "\n",
    "  2. CUDA Toolkit：CUDA Toolkit是一个`开发工具包`，其中包含了CUDA编译器、IDE、调试器等工具，以及CUDA程序所需的各种库文件和头文件，每个版本的CUDA Toolkit 都对应一个最低版本的显卡驱动版本（CUDA Driver）。\n",
    "\n",
    "  3. NVCC：其实就是`CUDA的编译器`,可以从CUDA Toolkit的/bin目录中获取,类似于gcc就是c语言的编译器。\n",
    "\n",
    "  4. CUDA Deep Neural Network (cuDNN)：CuDNN是NVIDIA提供的一个`深度神经网络加速库`，它包含了一系列高性能的基本函数和算法，用于加速深度学习任务的计算；CuDNN需要与CUDA Toolkit一起使用，以优化深度学习任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. <a id='toc4_1_'></a>[安装策略](#toc0_)\n",
    "\n",
    "- 方式一 `全局驱动，各自cuda`：\n",
    "    - `只安装NVIDIA Tesla A100的driver，每个用户自己利用conda安装CUDA Toolkit、cuDNN和对应的Pytorch版本（推荐），但是得注意选择兼容型号。（推荐）`\n",
    "\n",
    "- 方式二 `全局驱动，全局cuda`：\n",
    "    - `安装Driver、CUDA Toolkit (全局安装)`\n",
    "    \n",
    "- 方式三 `docker`：\n",
    "    - `安装Driver、NVIDIA docker (docker虚拟容器)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. <a id='toc4_2_'></a>[首先确认内核版本和发行版本，再确认显卡型号](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看linux内核版本、架构\n",
      "Linux 135.91.205.202.cau.edu.cn 4.18.0-348.7.1.el8_5.x86_64 #1 SMP Wed Dec 22 13:25:12 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\n",
      "发行版本\n",
      "CentOS Linux release 8.1.1911 (Core) \n",
      "显卡型号 （硬件层面）\n",
      "2f:00.0 3D controller: NVIDIA Corporation Device 20b0 (rev a1)\n",
      "86:00.0 3D controller: NVIDIA Corporation Device 20b0 (rev a1)\n",
      "验证系统是否安装gcc编译器\n",
      "gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4)\n",
      "Copyright (C) 2018 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "echo 查看linux内核版本、架构\n",
    "uname -a\n",
    "# Linux 135.91.205.202.cau.edu.cn 4.18.0-147.el8.x86_64 #1 SMP Wed Dec 4 21:51:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\n",
    "# x86_64\n",
    "\n",
    "echo 发行版本\n",
    "cat /etc/redhat-release\n",
    "# CentOS Linux release 8.1.1911 (Core)\n",
    "# CentOS\n",
    "\n",
    "echo 显卡型号 （硬件层面）\n",
    "lspci | grep -i nvidia\n",
    "# 04:00.0 3D controller: NVIDIA Corporation GK208M [GeForce GT 730M] (rev a1)\n",
    "\n",
    "echo 验证系统是否安装gcc编译器\n",
    "gcc --version\n",
    "\n",
    "# sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. <a id='toc4_3_'></a>[安装驱动-CUDA Driver](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. <a id='toc4_3_1_'></a>[下载CUDA Driver](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 从NVIDIA官网下辖\n",
    "# https://www.nvidia.cn/Download/index.aspx?lang=cn\n",
    "\n",
    "# 2. 通过dnf search nvidia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. <a id='toc4_3_2_'></a>[禁用nouveau](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 貌似在centos8上默认就禁用了，我没改，直接查看了lsmod | grep nouveau命令，发现没有输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. <a id='toc4_3_3_'></a>[安装CUDA Driver](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'sudo' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x *.run\n",
    "# !sudo ./*.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4. <a id='toc4_3_4_'></a>[查看显卡是否安装成功](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 29 18:03:07 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:2F:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             36W /  400W |   31166MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          Off |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             37W /  400W |     425MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   3496130      C   .../miniconda3/envs/pytorch/bin/python        414MiB |\n",
      "|    0   N/A  N/A   3784021      C   python                                      30738MiB |\n",
      "|    1   N/A  N/A   3784021      C   python                                        416MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5. <a id='toc4_3_5_'></a>[查看nvcc](#toc0_)\n",
    "```shell\n",
    "nvcc只是CUDA Toolkit中的一个软件。此时，只是安装了驱动程序，没有安装CUDA Toolkit，所以无法查看nvcc。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /bmp/backup/zhaosy/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate pytorch \n",
    "nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. <a id='toc4_4_'></a>[全局驱动和全局CUDA Toolkit和CuDNN](#toc0_)\n",
    "```shell\n",
    "不推荐一开始作为root为Linux全局配置CUDA Toolkit，每个用户和软件使用的CUDA Toolkit版本可能不一样。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. <a id='toc4_4_1_'></a>[下载对应的CUDA Toolkit版本](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc -V # 查看是否安装好CUDA Toolkit\n",
    "\n",
    "wget https://us.download.nvidia.cn/tesla/535.129.03/NVIDIA-Linux-x86_64-535.129.03.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. <a id='toc4_4_2_'></a>[安装CUDA Toolkit](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 卸载之前安装的cuda\n",
    "sudo dnf remove nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "chmod +x NVIDIA-Linux-x86_64-535.129.03.run\n",
    "sudo sh NVIDIA-Linux-x86_64-535.129.03.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. <a id='toc4_4_3_'></a>[下载对应的CuDNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://link.zhihu.com/?target=https%3A//developer.nvidia.com/rdp/cudnn-download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4. <a id='toc4_4_4_'></a>[安装CuDNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. <a id='toc4_5_'></a>[安装对应版本的Pytorch](#toc0_)\n",
    "```shell\n",
    "在Pytorch的官网进行查询，按照条件检索符合要求的软件版本，最主要的是对应的cuda版本号。\n",
    "```\n",
    "[https://pytorch.org/](https://pytorch.org/)\n",
    "\n",
    "![PyTorch](./Pytorch_Pictures/Install_PyTorch/PyTorch_website.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# https://pytorch.org/\n",
    "# CUDA 12.1\n",
    "conda create -n pytorch-gpu -y && conda activate pytorch-gpu \n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia # CUDA 12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. <a id='toc4_6_'></a>[全局驱动个人CUDA Toolkit](#toc0_)\n",
    "\n",
    "- 全局A100驱动\n",
    "\n",
    "- conda下cuda toolkit、pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviroment and actiate environment\n",
    "conda create -n pytorch-gpu && conda activate pytorch-gpu \n",
    "\n",
    "# Install cudatoolkit via conda containing nvcc etc.\n",
    "# Instead of :\n",
    "# conda install cudatoolkit\n",
    "# or \n",
    "# conda install cuda-nvcc\n",
    "conda install nvidia/label/cuda-12.6.0::cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. <a id='toc4_7_'></a>[GPU测试程序](#toc0_)\n",
    "### 4.7.1. <a id='toc4_7_1_'></a>[单机单卡](#toc0_)\n",
    "```shell\n",
    "net.to('cuda:0')\n",
    "x_gpu = x.to('cuda:0')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Runing on cuda:0\n",
      "====================================================================================================\n",
      "epoch 1/10: train_loss=1.6274998188018799, train_acc=84.3983383178711, test_acc=84.86000061035156\n",
      "epoch 2/10: train_loss=1.5564780235290527, train_acc=91.80333709716797, test_acc=92.20999145507812\n",
      "epoch 3/10: train_loss=1.5359078645706177, train_acc=93.42666625976562, test_acc=93.30999755859375\n",
      "epoch 4/10: train_loss=1.5250604152679443, train_acc=94.48833465576172, test_acc=94.14999389648438\n",
      "epoch 5/10: train_loss=1.5173759460449219, train_acc=95.15666961669922, test_acc=94.68999481201172\n",
      "epoch 6/10: train_loss=1.5116100311279297, train_acc=95.62833404541016, test_acc=94.97000122070312\n",
      "epoch 7/10: train_loss=1.5059237480163574, train_acc=96.125, test_acc=95.5199966430664\n",
      "epoch 8/10: train_loss=1.5021032094955444, train_acc=96.46500396728516, test_acc=95.77999877929688\n",
      "epoch 9/10: train_loss=1.4987181425094604, train_acc=96.77667236328125, test_acc=96.22000122070312\n",
      "epoch 10/10: train_loss=1.4955240488052368, train_acc=97.086669921875, test_acc=96.41000366210938\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 0.0 m/ 55.98274064064026 s\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import time \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据准备\n",
    "dbs = './Pytorch_datasets/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(), \n",
    "        #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "        #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True\n",
    ")\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练\n",
    "\n",
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "\n",
    "    print('='*100)\n",
    "    print(f\"Runing on {device}\")\n",
    "    print('='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "    net.to(device)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            opt.zero_grad()                         # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)                          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)                # 计算loss\n",
    "            loss.backward()                         # 计算梯度\n",
    "            opt.step()                              # 更新网络参数\n",
    "\n",
    "        net.eval()  # 切换至评估模式\n",
    "                    # 模型默认是net.train()\n",
    "                    # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                    # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None\n",
    "\n",
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "net = Net()  \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2. <a id='toc4_7_2_'></a>[单机多卡](#toc0_)\n",
    "```shell\n",
    "torch.nn.DataParallel()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Runing on cuda:0\n",
      "====================================================================================================\n",
      "epoch 1/10: train_loss=1.575600504875183, train_acc=90.44833374023438, test_acc=90.77999877929688\n",
      "epoch 2/10: train_loss=1.547834038734436, train_acc=92.36333465576172, test_acc=92.48999786376953\n",
      "epoch 3/10: train_loss=1.5322562456130981, train_acc=93.78500366210938, test_acc=93.61000061035156\n",
      "epoch 4/10: train_loss=1.5213903188705444, train_acc=94.74333190917969, test_acc=94.43999481201172\n",
      "epoch 5/10: train_loss=1.5164926052093506, train_acc=95.19166564941406, test_acc=94.81999969482422\n",
      "epoch 6/10: train_loss=1.5106992721557617, train_acc=95.68499755859375, test_acc=95.27999877929688\n",
      "epoch 7/10: train_loss=1.5050891637802124, train_acc=96.26000213623047, test_acc=95.63999938964844\n",
      "epoch 8/10: train_loss=1.5017056465148926, train_acc=96.55667114257812, test_acc=95.9000015258789\n",
      "epoch 9/10: train_loss=1.4975892305374146, train_acc=96.89666748046875, test_acc=96.29000091552734\n",
      "epoch 10/10: train_loss=1.4955655336380005, train_acc=97.086669921875, test_acc=96.52999877929688\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 7.511963844299316 s\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import time \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据准备\n",
    "dbs = './Pytorch_datasets/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True\n",
    ")\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练\n",
    "\n",
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "\n",
    "    print('='*100)\n",
    "    print(f\"Runing on {device}\")\n",
    "    print('='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "    net = nn.DataParallel(module=net)\n",
    "    net = net.to(device)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)# 计算loss\n",
    "            loss.backward()         # 计算梯度\n",
    "            opt.step()              # 更新网络参数\n",
    "\n",
    "        net.eval()  # 切换至评估模式\n",
    "                    # 模型默认是net.train()\n",
    "                    # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                    # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None\n",
    "\n",
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "net = Net()  \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = [ 'cpu' if not torch.cuda.is_available() else ]\n",
    "device = [f'cuda:{i}' for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else ['cpu']\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3. <a id='toc4_7_3_'></a>[GPU burn压力测试](#toc0_)\n",
    "```shell\n",
    "李沐在装机配置后，进行GPU压力测试所用的程序为GPU_burn（可从github上下载）\n",
    "```\n",
    "\n",
    "- gpu_burn: \n",
    "  - github地址：`git clone https://github.com/wilicc/gpu-burn.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# git clone:\n",
    "git clone https://github.com/wilicc/gpu-burn.git\n",
    "\n",
    "cd gpu-burn\n",
    "\n",
    "# make \n",
    "make\n",
    "\n",
    "# 或\n",
    "# make CUDAPATH=~/minicnoda3/pytorch-gpu/\n",
    "\n",
    "# help\n",
    "gpu_burn --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# 2h * 60min * 60s = 7200s with tensor core (avaliable)\n",
    "gpu_burn -tc $(( 3 * 24 * 60 * 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[Pytorch模块介绍](#toc0_)\n",
    "## 5.1. <a id='toc5_1_'></a>[导入模块](#toc0_)\n",
    "```python\n",
    "torchvision\n",
    "  models\n",
    "  datasets\n",
    "  transforms\n",
    "  utils\n",
    "torch\n",
    "  utils\n",
    "    data            # 数据加载相关\n",
    "      TensorDataset\n",
    "      Dataset\n",
    "      DataLoader\n",
    "  nn\n",
    "    functional\n",
    "    Sequential\n",
    "    DataParallel\n",
    "    Linear\n",
    "    Softmax\n",
    "  optim\n",
    "    SGD\n",
    "    Adam\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version:  2.4.0\n",
      "torchvision version: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "# 现成的数据库\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "\n",
    "# 数据加载\n",
    "from torch.utils import data                                             # from torch.utils import data\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader     # data.Dataset, data.TensorDataset, data.DataLoader\n",
    "\n",
    "# 神经网络结构\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "# import torch.nn.DataParallel\n",
    "from torch.nn import DataParallel\n",
    "from torch import distributed as dist\n",
    "\n",
    "# 优化器\n",
    "from torch import optim \n",
    "\n",
    "print('pytorch version: ', torch.__version__)\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[数据封装和加载](#toc0_)\n",
    "\n",
    "PyTorch为我们提供的`Dataset`和`DataLoader`类分别负责可被Pytorhc使用的数据集的`创建`以及向训练`传递数据`的任务。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. <a id='toc6_1_'></a>[torchvison.datasets获得Dataset](#toc0_)\n",
    "\n",
    "* `tochvision`主要处理图像数据，包含一些常用的数据集、模型、转换函数等。  torchvision独立于PyTorch，需要专门安装。\n",
    "\n",
    "  * torchvision.`models`: 提供深度学习中各种经典的网络结构、预训练好的模型，如：Alex-Net、VGG、ResNet、Inception等。\n",
    "\n",
    "  * torchvision.`datasets`：提供常用的数据集，设计上继承 torch.utils.data.Dataset，主要包括：MNIST、CIFAR10/100、ImageNet、COCO等。\n",
    "\n",
    "  * torchvision.`transforms`：提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。\n",
    "  \n",
    "  * torchvision.`utils`：工具类，如保存张量作为图像到磁盘，给一个小批量创建一个图像网格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.FashionMNIST,\n",
       " torchvision.datasets.mnist.FashionMNIST)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "# 数据集下载路径\n",
    "dbs = './Pytorch_datasets/'\n",
    "\n",
    "trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),                  # PIL转换为tensor格式\n",
    "    torchvision.transforms.Normalize((0.5,), (1.0,))    # 标准化\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=trans, \n",
    "#   target_transform=False\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=trans, \n",
    "#   target_transform=False\n",
    ")\n",
    "\n",
    "type(train_dataset), type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset FashionMNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ./Pytorch_datasets/\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(1.0,))\n",
       "            ),\n",
       " Dataset FashionMNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./Pytorch_datasets/\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(1.0,))\n",
       "            ))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 封装成torch使用的dataset格式数据\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. <a id='toc6_2_'></a>[自定义数据集获得Dataset](#toc0_)\n",
    "### 6.2.1. <a id='toc6_2_1_'></a>[TensorDataset()](#toc0_)\n",
    "\n",
    "- `TensorDataset`是一个现成的类，用于将数据表示为`张量列表`。\n",
    "\n",
    "- 如果你只是想创建一个包含输入特征和标签的数据集，可以直接使用 TensorDataset：\n",
    "\n",
    "  - `dataset = torch.utils.data.TensorDataset( input_features, labels )` # 按照下标顺序将input_features和labels值对应起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataset.TensorDataset,\n",
       " <torch.utils.data.dataset.TensorDataset at 0x7f802704b260>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# 1. 自建数据集 (Tensor格式的数据)\n",
    "features = torch.tensor([i for i in range(1000)])   # 必须是tensor格式的额数据\n",
    "labels = features * 2                               # labels = torch.mul(features, 2)\n",
    "\n",
    "# 2. 构建dataset数据集\n",
    "datasets = TensorDataset(features, labels) \n",
    "type(datasets), datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0), (tensor(0), tensor(0)))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features0, labels0 = datasets[0] # 取第一个数据对\n",
    "\n",
    "features0, labels0, datasets.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(1), tensor(2)), (tensor(1), tensor(2)))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[1], datasets.__getitem__(1) # 取第二个数据对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.__len__()  # 数据对的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. <a id='toc6_2_2_'></a>[重载Dataset类](#toc0_)\n",
    "\n",
    "- `torch.utils.data.Dataset`是一个抽象类，用于定义新类型的自定义数据集。如果你想创建自己的数据集，可以继承这个类并实现以下方法：\n",
    "\n",
    "  - 重载`__init__(self, *args, **kwargs)`: 初始化方法，可以在其中加载你的数据；\n",
    "\n",
    "  - 重载`__len(self)__`: 返回数据集的长度；\n",
    "\n",
    "  - 重载`__getitem__(self, index)`: 根据索引返回数据集中的一个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.MyData at 0x7f8026f60e30>,\n",
       " (tensor(0), tensor(0)),\n",
       " (tensor(1), tensor(1)),\n",
       " (tensor(1), tensor(1)),\n",
       " (tensor(2), tensor(2)),\n",
       " (tensor(2), tensor(2)),\n",
       " 15)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# 1. 重载Dataset类\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, nums:int=15):\n",
    "        '''初始化参数，耗时的操作初始化时候就完成。'''\n",
    "\n",
    "        self.nums = nums\n",
    "        self.features = torch.arange(self.nums)\n",
    "        self.labels = torch.arange(self.nums)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''返回数据集的总数目。'''\n",
    "\n",
    "        return self.nums\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''耗时的工作初始化时就一步完成，此处依据index或idx查找并返回对应的数据即可。'''\n",
    "        \n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "\n",
    "# 2. 利用重载的Dataset创建数据集\n",
    "datasets = MyData()\n",
    "datasets, datasets[0], datasets[1], datasets.__getitem__(1), datasets[2], datasets.__getitem__(2), datasets.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. <a id='toc6_2_3_'></a>[Pytoch.utils.data.Dataset类分析和总结](#toc0_)\n",
    "\n",
    "- 在PyTorch中数据的封装格式为torch.utils.data.Dataset类；\n",
    "\n",
    "- 第一种方式：直接加载`torchvision.datasets`中对应的数据库生成Dataset格式\n",
    "\n",
    "- 第二种方式：自定义\n",
    "  - 利用`Tensordataset(features, labels)`函数将features和labels配对并生成Dataset格式 (推荐，我觉得更加方便)\n",
    "  \n",
    "  - 重载`Dataset`类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4. <a id='toc6_2_4_'></a>[Subset](#toc0_)\n",
    "用于从数据集中抽取子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子集大小: 3\n",
      "子集中的第一个样本: (tensor(1), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset \n",
    "\n",
    "\n",
    "subset = Subset(dataset=datasets, indices=[1, 2, 3])    # 从datasets中抽取indices=[1, 2, 3]的子集\n",
    "\n",
    "print(\"子集大小:\", len(subset))  # 输出: 3\n",
    "print(\"子集中的第一个样本:\", subset[0])  # 输出: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5. <a id='toc6_2_5_'></a>[random_split](#toc0_)\n",
    "按比例随机划分数据集，常用于划分训练集、验证集和测试集。\n",
    "\n",
    "为什么固定 random_split：\n",
    "- 可重复实验：固定随机数种子后，实验结果可复现。\n",
    "- 调试方便：划分后的数据集一致性便于调试和对比结果。\n",
    "\n",
    "固定 random_split 的方法：\n",
    "- 方法 1：使用 torch.manual_seed，设置全局随机数种子，让分割结果可复现。\n",
    "- 方法 2：使用 Generator 显式指定种子，通过 torch.Generator 显式控制随机数生成器的种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 10\n",
      "验证集大小: 3\n",
      "测试集大小: 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# 使用 Generator 设置随机数种子\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset=datasets, lengths=[10, 3, 2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"验证集大小: {len(validation_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6. <a id='toc6_2_6_'></a>[ConcateDataset](#toc0_)\n",
    "\n",
    "将多个数据集拼接成一个数据集。\n",
    "\n",
    "使用场景：多个数据源时方便整合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x7f8026f614f0>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "combined_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.7. <a id='toc6_2_7_'></a>[IterableDataset](#toc0_)\n",
    "对于特别大的数据集（不能一次性加载到内存中），可以使用 IterableDataset 实现流式加载。\n",
    "\n",
    "- 流式数据：当数据无法一次性加载到内存中时，例如从文件、网络或数据库流式读取的数据。\n",
    "- 动态数据生成：当数据是实时生成的，而不是固定的，比如从传感器读取数据或模拟生成数据。\n",
    "- 超大数据集：处理非常大的数据集，避免内存爆炸。\n",
    "- 与普通的 Dataset 不同，IterableDataset 不需要实现 __len__ 和 __getitem__ 方法，而是通过实现 __iter__ 方法来定义数据生成逻辑。\n",
    "\n",
    "IterableDataset 的设计与传统的 Dataset 有所不同：\n",
    "\n",
    "- 不需要实现 `__getitem__` 方法，因为数据是通过迭代生成的。\n",
    "- 必须实现 `__iter__` 方法，返回一个迭代器，用于逐条生成数据。\n",
    "- 无需实现 `__len__` 方法，但可以实现 `__len__` 来支持数据集大小统计。\n",
    "\n",
    "与普通 Dataset 的对比\n",
    "|特性|Dataset|IterableDataset|\n",
    "|:-|:-|:-|\n",
    "|访问方式|随机访问（通过索引 `__getitem__`）|顺序访问（通过 `__iter__` 迭代）|\n",
    "|适用场景|静态、小型数据集|流式、动态生成或超大数据集|\n",
    "|内存管理|可加载到内存中|流式加载，减少内存占用|\n",
    "|是否支持索引|支持|不支持|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.1. <a id='toc6_2_7_1_'></a>[流式数据加载](#toc0_)\n",
    "如果数据存储在一个非常大的文件中，可以使用 IterableDataset 来流式读取数据，而不是一次性将数据加载到内存中。\n",
    "\n",
    "关键点：\n",
    "\n",
    "- 数据是按行流式读取的，每次只加载一部分到内存。\n",
    "- 使用 DataLoader 对数据按批次进行处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次数据: tensor([0, 1, 2])\n",
      "批次数据: tensor([3, 4, 5])\n",
      "批次数据: tensor([6, 7, 8])\n",
      "批次数据: tensor([9])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class FileDataset(IterableDataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                yield line.strip()  # 每次返回一行数据\n",
    "\n",
    "# 创建数据集和 DataLoader\n",
    "file_path = 'example.txt'  # 假设文件内容非常大\n",
    "dataset = FileDataset(file_path)\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"批次数据:\", batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.2. <a id='toc6_2_7_2_'></a>[动态生成数据](#toc0_)\n",
    "如果数据是动态生成的，比如需要实时生成斐波那契数列或伪随机数序列，可以使用 IterableDataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斐波那契批次: tensor([0, 1, 1])\n",
      "斐波那契批次: tensor([2, 3, 5])\n",
      "斐波那契批次: tensor([ 8, 13, 21])\n",
      "斐波那契批次: tensor([34])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class FibonacciDataset(IterableDataset):\n",
    "    def __init__(self, max_length):\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        a, b = 0, 1\n",
    "        for _ in range(self.max_length):\n",
    "            yield a\n",
    "            a, b = b, a + b\n",
    "\n",
    "# 创建数据集和 DataLoader\n",
    "dataset = FibonacciDataset(max_length=10)\n",
    "dataloader = DataLoader(dataset, batch_size=3)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"斐波那契批次:\", batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.3. <a id='toc6_2_7_3_'></a>[无限数据流](#toc0_)\n",
    "有时我们需要一个无限的数据流，例如训练生成器模型时使用的随机数据流。\n",
    "\n",
    "关键点：\n",
    "\n",
    "- 数据集是无限的，可以动态生成。\n",
    "- 控制数据生成的批次或次数由外部逻辑实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机数批次: tensor([0.2025, 0.9842, 0.9479, 0.4222, 0.9127], dtype=torch.float64)\n",
      "随机数批次: tensor([0.4001, 0.0342, 0.3488, 0.5999, 0.5207], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class RandomDataset(IterableDataset):\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield random.random()  # 无限生成随机数\n",
    "\n",
    "# 创建数据集和 DataLoader\n",
    "dataset = RandomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=5)\n",
    "\n",
    "# 仅读取两批数据\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(\"随机数批次:\", batch)\n",
    "    if i == 1:  # 控制只读取两批\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.4. <a id='toc6_2_7_4_'></a>[多线程数据加载与分布式支持](#toc0_)\n",
    "如果需要在分布式或多线程环境中使用 IterableDataset，可以通过 torch.utils.data.get_worker_info 获取工作线程的信息，并实现分片逻辑。\n",
    "\n",
    "1. 顺序保障：如果数据需要特定顺序（如时序数据），要在 `__iter__` 方法中维护顺序。\n",
    "2. 分布式支持：\n",
    "    - 对于多进程或分布式训练，需要实现 `__iter__` 方法中的分片逻辑。\n",
    "    - 可以使用 `torch.utils.data.get_worker_info` 获取当前进程的 ID 和数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次数据: tensor([ 0,  4,  8, 12, 16])\n",
      "批次数据: tensor([ 1,  5,  9, 13, 17])\n",
      "批次数据: tensor([ 2,  6, 10, 14, 18])\n",
      "批次数据: tensor([ 3,  7, 11, 15, 19])\n",
      "批次数据: tensor([20, 24, 28, 32, 36])\n",
      "批次数据: tensor([21, 25, 29, 33, 37])\n",
      "批次数据: tensor([22, 26, 30, 34, 38])\n",
      "批次数据: tensor([23, 27, 31, 35, 39])\n",
      "批次数据: tensor([40, 44, 48, 52, 56])\n",
      "批次数据: tensor([41, 45, 49, 53, 57])\n",
      "批次数据: tensor([42, 46, 50, 54, 58])\n",
      "批次数据: tensor([43, 47, 51, 55, 59])\n",
      "批次数据: tensor([60, 64, 68, 72, 76])\n",
      "批次数据: tensor([61, 65, 69, 73, 77])\n",
      "批次数据: tensor([62, 66, 70, 74, 78])\n",
      "批次数据: tensor([63, 67, 71, 75, 79])\n",
      "批次数据: tensor([80, 84, 88, 92, 96])\n",
      "批次数据: tensor([81, 85, 89, 93, 97])\n",
      "批次数据: tensor([82, 86, 90, 94, 98])\n",
      "批次数据: tensor([83, 87, 91, 95, 99])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import get_worker_info, IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class DistributedDataset(IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        \n",
    "        if worker_info is None:\n",
    "            # 单线程，返回全部数据\n",
    "            return iter(self.data)\n",
    "        else:\n",
    "            # 多线程，按线程数分片\n",
    "            worker_id = worker_info.id\n",
    "            num_workers = worker_info.num_workers\n",
    "            return iter(self.data[worker_id::num_workers])\n",
    "\n",
    "\n",
    "dataset = DistributedDataset(range(100))\n",
    "dataloader = DataLoader(dataset, num_workers=4, batch_size=5)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"批次数据:\", batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.7.5. <a id='toc6_2_7_5_'></a>[使用注意事项](#toc0_)\n",
    "1. 不可随机访问：\n",
    "\n",
    "   - IterableDataset 不支持通过索引访问数据（没有 `__getitem__` 方法）。\n",
    "   - 只能顺序生成数据。\n",
    "\n",
    "2. 分布式与多线程支持：\n",
    "\n",
    "   - 如果需要并行加载数据，需在 `__iter__` 方法中处理数据分片。\n",
    "\n",
    "3. 效率问题：\n",
    "\n",
    "   - 适合处理内存不足的场景，但如果数据可以一次性加载到内存中，Dataset 会更高效。\n",
    "\n",
    "4. 批次大小：\n",
    "   - 使用 DataLoader 的 batch_size 参数，IterableDataset 的数据流可以按批次返回。\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. <a id='toc6_3_'></a>[数据加载-DataLoader()](#toc0_)\n",
    "1. 先将自制的数据集利用data.TensorDataset生成`dataset`；\n",
    "\n",
    "2. 再用data.DataLoader加载到dataset成最终可用的带有batch_size的格式`DataLoader`，方便后续的训练\n",
    "\n",
    "3. 先测试以下数据加载的速度，必须比训练计算所耗的时间小，否则将降低训练效率；\n",
    "\n",
    "4. 当数据加载时间很长时可以预加载，缩短时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# 加载torchvison数据集（格式化好的torch.utils.data.Dataset）\n",
    "train_iter = DataLoader(\n",
    "    dataset = datasets,          # Dataset\n",
    "    batch_size = 5,                 # batch size\n",
    "    shuffle = True,                 # 打乱顺序\n",
    "    num_workers = 3,                 # 线程数\n",
    "    drop_last = False,              # 是否删除最后一个不是整数的batch\n",
    "    # collate_fn=collate_function     # 处理函数，可以处理不等长数据等等\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataloader.DataLoader,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f4f30665b20>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_iter), train_iter        # 直接打印看不到内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽取: 1 [tensor([ 6, 12,  0,  9, 11]), tensor([ 6, 12,  0,  9, 11])]\n",
      "随机抽取: 2 [tensor([ 2, 14,  8,  5,  1]), tensor([ 2, 14,  8,  5,  1])]\n",
      "随机抽取: 3 [tensor([ 4,  7, 10,  3, 13]), tensor([ 4,  7, 10,  3, 13])]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_iter):  # 小批量的batch_size数据\n",
    "    if batch_idx == 10:\n",
    "        break\n",
    "    print('随机抽取:', batch_idx+1, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1. <a id='toc6_3_1_'></a>[估计数据加载时间](#toc0_)\n",
    "\n",
    "估计加载数据所需时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== \n",
      " Total：\n",
      " 0.0 d \n",
      " 0.0 h \n",
      " 0.0 m \n",
      " 1.068415880203247 s\n"
     ]
    }
   ],
   "source": [
    "# 读完一个epoch的一个batch，耗时\n",
    "timer = cpuTimer()\n",
    "for X, y in train_iter:\n",
    "    break \n",
    "timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== \n",
      " Total：\n",
      " 0.0 d \n",
      " 0.0 h \n",
      " 0.0 m \n",
      " 1.0625123977661133 s\n"
     ]
    }
   ],
   "source": [
    "# 读完一个epoch的所有batch，耗时\n",
    "timer = cpuTimer()\n",
    "for X, y in train_iter:\n",
    "    continue \n",
    "timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. <a id='toc6_3_2_'></a>[collate_fn处理不等长tensor](#toc0_)\n",
    "\n",
    "1. 样本的来源\n",
    "DataLoader 会调用 `Dataset.__getitem__` 获取 `batch_size` 个样本。这些样本是 collate_fn 的输入，形式是一个 Python `列表`，其中每个元素是` __getitem__ `方法返回的结果。\n",
    "\n",
    "2. 默认行为  \n",
    "如果不指定 collate_fn，DataLoader 会尝试自动将样本堆叠成张量：\n",
    "  - 如果样本是 torch.Tensor，会沿第 0 维堆叠（使用 torch.stack）。\n",
    "  - 如果样本是其他可组合的类型（如 dict 或 list），会递归地组合它们的内容。\n",
    "  - 如果样本形状不一致，默认行为会失败。\n",
    "\n",
    "  ```python\n",
    "  # Dataset 提供样本： DataLoader 根据 batch_size 从 Dataset 调用 __getitem__，返回一个列表 batch。\n",
    "  batch = [dataset[i] for i in range(batch_size)]\n",
    "\n",
    "  # 调用 collate_fn： 将这个 batch 传入 collate_fn，进行处理：\n",
    "  processed_batch = collate_fn(batch)\n",
    "  ```\n",
    "\n",
    "3. 自定义 collate_fn 的作用   \n",
    "自定义 collate_fn 可以覆盖默认行为，定义自己的逻辑来处理复杂的数据结构或变长数据。例如：  \n",
    "    - 对变长序列进行填充。\n",
    "    - 按需调整数据的结构或类型。\n",
    "    - 返回额外的辅助信息（如序列长度）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 collate_fn 来处理字典和标签\n",
    "def collate_function(batch):\n",
    "    # 分别提取 batch 中的 msa, pair 和 labels\n",
    "    msa_batch = [item[0]['msa'] for item in batch]\n",
    "    pair_batch = [item[0]['pair'] for item in batch]\n",
    "    labels_batch = [item[1] for item in batch]\n",
    "\n",
    "    # 找到 batch 中最长的 num_residues\n",
    "    max_residues = max([msa.shape[1] for msa in msa_batch])\n",
    "\n",
    "    # 对 MSA 特征填充 num_residues 维度，使其维度一致\n",
    "    padded_msa_batch = []\n",
    "    for msa in msa_batch:\n",
    "        pad_size = max_residues - msa.shape[1]\n",
    "        padded_msa = torch.nn.functional.pad(msa, (0, 0, 0, pad_size))  # 填充第二维度\n",
    "        padded_msa_batch.append(padded_msa)\n",
    "\n",
    "    # 对 Pair 特征填充 num_residues 维度，使其维度一致\n",
    "    padded_pair_batch = []\n",
    "    for pair in pair_batch:\n",
    "        pad_size = max_residues - pair.shape[0]\n",
    "        padded_pair = torch.nn.functional.pad(pair, (0, 0, 0, pad_size, 0, pad_size))  # 填充第一和第二维度\n",
    "        padded_pair_batch.append(padded_pair)\n",
    "\n",
    "    # 将 MSA 和 Pair 特征堆叠为批量数据\n",
    "    padded_msa_batch = torch.stack(padded_msa_batch)\n",
    "    padded_pair_batch = torch.stack(padded_pair_batch)\n",
    "\n",
    "    # 将标签堆叠为批量数据\n",
    "    labels_batch = torch.stack(labels_batch)\n",
    "\n",
    "    # 返回批量化后的字典和标签\n",
    "    return {'msa': padded_msa_batch, 'pair': padded_pair_batch}, labels_batch\n",
    "\n",
    "train_iter = data.DataLoader(\n",
    "    dataset=train_datasets, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    num_workers=20, \n",
    "    collate_fn=collate_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3. <a id='toc6_3_3_'></a>[重载DataLoader](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data \n",
    "\n",
    "\n",
    "class RebuildDataLoader(data.DataLoader):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        return \n",
    "    \n",
    "    # 重载 __iter__ 方法来控制数据加载方式\n",
    "    def __iter__(self):\n",
    "        # 你可以在这里实现自定义的加载逻辑，比如控制每个批次的顺序\n",
    "        iterator = super().__iter__()\n",
    "        for batch in iterator:\n",
    "            # 可以在这里处理或过滤批次数据\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[张量(Tensors)](#toc0_)\n",
    "## 7.1. <a id='toc7_1_'></a>[Tensors定义](#toc0_)\n",
    "\n",
    "PyTorch 的一大作用就是可以代替 Numpy 库，所以首先介绍 Tensors ，也就是张量，它相当于 Numpy 的多维数组(ndarrays)。\n",
    "\n",
    "* 两者的区别就是：\n",
    "    * `数学或物理`概念：张量 (`Tensors`)\n",
    "    \n",
    "    * `编程`概念：数组 (`Array`)\n",
    "    \n",
    "* 总结\n",
    "\n",
    "|函数名称|注释|\n",
    "|:-|:-|\n",
    "|torch.tensor()|tensor|\n",
    "|torch.asarray()||\n",
    "|torch.from_numpy()|numpy2tensor|\n",
    "|torch.empty(size)|垃圾数|\n",
    "|torch.zeros(size)|0|\n",
    "|torch.ones(size)|1|\n",
    "|`torch.full(size,fill_value)`|fill_value|\n",
    "|torch.rand(size)|随机数|\n",
    "|torch.randn(size)|标准正态分布|\n",
    "|torch.normal(mean,std,size)|正态分布|\n",
    "|torch.arange(start,end,step,size)|数组|\n",
    "|.reshape(size)|重塑|\n",
    "|.numpy()|转为numpy的ndarray|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor()\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asarray()\n",
    "\n",
    "x = torch.asarray([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.from_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]]),\n",
       " tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy转tensor, from_numpy()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(0, 15).reshape(5, 3)\n",
    "\n",
    "x, torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7262e-44, 0.0000e+00, 6.7262e-44],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.4013e-45],\n",
       "        [0.0000e+00,        nan,        nan],\n",
       "        [1.3452e-43, 0.0000e+00, 6.7262e-44]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty()\n",
    "\n",
    "torch.empty(size=(5, 3), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros()\n",
    "\n",
    "torch.zeros(size=(5, 3)) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ones()\n",
    "\n",
    "torch.ones(size=(5, 3), dtype=torch.float32) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.full(size, fill_value)\n",
    "\n",
    "可以用来做mask的填充，填充任意数值fill_value，而不只是0或1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "torch.full(size=(2, 3), fill_value=torch.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.rand()，产生随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9477, 0.2008, 0.9838],\n",
       "        [0.3268, 0.5974, 0.7255],\n",
       "        [0.3057, 0.7997, 0.4263],\n",
       "        [0.0774, 0.2863, 0.5388],\n",
       "        [0.0482, 0.8790, 0.8986]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand()\n",
    "\n",
    "torch.rand(size=(5, 3), dtype=torch.float32) # 随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.randn()，标准正态分布随机数，产生正态分布随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3921, -0.4696,  1.1432, -0.6153, -1.3162],\n",
       "        [-1.3564, -0.4595, -1.1473, -0.8091, -0.4368],\n",
       "        [ 0.4175, -0.4860, -0.0468, -1.0885,  1.0387]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randn()\n",
    "\n",
    "torch.randn(size=(3, 5), dtype=torch.float32) # 标准正态分布随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.normal()，正态分布随机数，产生mean和std的size个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4510,  0.2798, -0.8032, -0.5851,  0.7675],\n",
       "        [ 2.0413,  1.1163, -0.1891,  0.9543,  0.6753],\n",
       "        [ 1.4002,  0.8864,  0.6356, -1.2399,  1.1891]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=0, std=1, size=(3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.arange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arange()\n",
    "\n",
    "torch.arange(3) # 0, 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape()\n",
    "\n",
    "torch.arange(start=0, end=15, step=1).reshape(5, 3) # reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .numpy()，将tensor转化为numpy的ndarray格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]),\n",
       " array([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]], dtype=int64))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor转化为numpy\n",
    "\n",
    "x = torch.arange(start=0, end=15, step=1).reshape(5, 3)\n",
    "\n",
    "x, x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. <a id='toc7_2_'></a>[Tensors属性](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. <a id='toc7_2_1_'></a>[数据类型(dtype)](#toc0_)\n",
    "```python\n",
    "torch.float16       # \n",
    "torch.float32       # torch.FloatTensor()\n",
    "torch.float64       # torch.DoubleTensor()\n",
    "torch.int8\n",
    "torch.int16         # \n",
    "torch.int32         # torch.IntTensor()\n",
    "torch.int64         # torch.LongTensor()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12, 5]), device(type='cpu'), torch.float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.normal(\n",
    "    mean=0, \n",
    "    std=1, \n",
    "    size=(128, 12, 5), \n",
    "    dtype=torch.float32, \n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "x.shape, x.device, x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1.1. <a id='toc7_2_1_1_'></a>[转化格式](#toc0_)\n",
    "\n",
    "方法一：\n",
    "\n",
    "|函数|备注|\n",
    "|-|-|\n",
    "|tensor.double()：|把一个张量tensor转为torch.float64 数据类型|\n",
    "|tensor.float()：|把一个张量tensor转为torch.float32 数据类型|\n",
    "|tensor.int()：|把一个张量tensor转为torch.int32 数据类型|\n",
    "|tensor.long(): |把一个张量tensor转为torch.int64 数据类型|\n",
    "\n",
    "方法二： \n",
    "\n",
    "|类型|函数|备注|\n",
    "|-|-|-|\n",
    "|float to int|||\n",
    "||x.int()|float to int32|\n",
    "||x.long()|float to int64|\n",
    "|int to float|||\n",
    "||x.float()|int32 to float|\n",
    "||x.double()|int64 to float|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.float64, torch.int32, torch.int64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, x.float().dtype, x.double().dtype, x.int().dtype, x.long().dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. <a id='toc7_2_2_'></a>[设备(device)](#toc0_)\n",
    "PyTorch识别的设备类型: cpu, cuda:0, cuda:1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12, 5]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def try_gpu():\n",
    "    '''列出cpu或所有的gpu [cuda:0, cuda:1]'''\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        device = [f'cuda:{i}' for i in range(num_gpu)]\n",
    "    else:\n",
    "        device = ['cpu']\n",
    "    return device\n",
    "\n",
    "\n",
    "x = torch.normal(\n",
    "    mean=0, \n",
    "    std=1, \n",
    "    size=(128, 12, 5), \n",
    "    dtype=torch.float32, \n",
    "    device=try_gpu()[0]\n",
    ")\n",
    "\n",
    "x.shape, x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. <a id='toc7_2_3_'></a>[维度(size/shape)](#toc0_)\n",
    "\n",
    "- 查看张量维度：\n",
    "\n",
    "    |函数名称|注释|\n",
    "    |:-|:-|\n",
    "    |x.size()||\n",
    "    |x.shape||\n",
    "\n",
    "- tensor([[[]]]) `直接`表示:\n",
    "  - 几个`[[[`，表示几个维度\n",
    "  - 没有`[]`，表示0维，即标量\n",
    "\n",
    "- Size[] `属性`：\n",
    "  - `[2, 3]`: 几个数字即几个维度\n",
    "  - `[2, 3]`: 每个数字表示对应维度的元素数量\n",
    "  - `[]`: 0维表示标量\n",
    "  - `[2]`: 一维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3.1. <a id='toc7_2_3_1_'></a>[标量](#toc0_)\n",
    "dim=0 表示 `标量`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(1))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1)\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3.2. <a id='toc7_2_3_2_'></a>[一维张量](#toc0_)\n",
    "dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1]), tensor([1]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1])   # 相比较标量，只是多了一个[]\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), tensor([1, 2, 3, 4, 5]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3.3. <a id='toc7_2_3_3_'></a>[多维张量](#toc0_)\n",
    "dim 大于等于 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[ 0,  1,  2,  3,  4,  5],\n",
    "                  [ 6,  7,  8,  9, 10, 11]], dtype=torch.float32)\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 2]),\n",
       " tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5]],\n",
       " \n",
       "         [[ 6,  7],\n",
       "          [ 8,  9],\n",
       "          [10, 11]]]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[ 0,  1],\n",
    "                   [ 2,  3],\n",
    "                   [ 4,  5]],\n",
    "                  [[ 6,  7],\n",
    "                   [ 8,  9],\n",
    "                   [10, 11]]])\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 3, 2]),\n",
       " tensor([[[[ 0,  1],\n",
       "           [ 2,  3],\n",
       "           [ 4,  5]]],\n",
       " \n",
       " \n",
       "         [[[ 6,  7],\n",
       "           [ 8,  9],\n",
       "           [10, 11]]]]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(2, 1, 3, 2)\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 2]),\n",
       " tensor([[[[ 0,  1],\n",
       "           [ 2,  3],\n",
       "           [ 4,  5]],\n",
       " \n",
       "          [[ 6,  7],\n",
       "           [ 8,  9],\n",
       "           [10, 11]]],\n",
       " \n",
       " \n",
       "         [[[12, 13],\n",
       "           [14, 15],\n",
       "           [16, 17]],\n",
       " \n",
       "          [[18, 19],\n",
       "           [20, 21],\n",
       "           [22, 23]]]]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24).reshape(2, 2, 3, 2)\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4. <a id='toc7_2_4_'></a>[特殊的一维张量](#toc0_)\n",
    "`在PyTorch中，一维张量（Tensor）通常表示一个向量，它可以被视为一行或一列的数值。然而，在大多数情况下，一维张量并不明确区分是行向量还是列向量。这是因为一维张量在数学运算中通常是按照向量的规则来处理的，而不是像矩阵那样区分行和列。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4.1. <a id='toc7_2_4_1_'></a>[一维张量的例子](#toc0_)\n",
    "\n",
    "假设我们有一个一维张量 `tensor`，它的形状为 `(n,)`，其中 `n` 表示张量中的元素数量。这样的张量可以表示为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 创建一个一维张量\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(tensor)  # 输出: tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4.2. <a id='toc7_2_4_2_'></a>[区分行向量和列向量](#toc0_)\n",
    "\n",
    "尽管一维张量本身没有明确的行向量或列向量的概念，但在某些情况下，我们可能需要将其视为行向量或列向量来进行`矩阵运算`。这可以通过增加一个额外的维度来实现：\n",
    "\n",
    "- **行向量**：可以通过 `.unsqueeze(-1)` 方法增加一个维度来表示行向量，形状变为 `(n, 1)`。\n",
    "- **列向量**：可以通过 `.unsqueeze(0)` 方法增加一个维度来表示列向量，形状变为 `(1, n)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.4.2.1. <a id='toc7_2_4_2_1_'></a>[示例](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个一维张量\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Vector:\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "# 转换为行向量\n",
    "row_vector = vector.unsqueeze(1)\n",
    "\n",
    "print(\"Row Vector:\", row_vector, sep='\\n')  # 输出: tensor([[1], [2], [3], [4], [5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Vector:\n",
      "tensor([[1, 2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 转换为列向量\n",
    "column_vector = vector.unsqueeze(0)\n",
    "\n",
    "print(\"Column Vector:\", column_vector, sep='\\n')  # 输出: tensor([[1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4.3. <a id='toc7_2_4_3_'></a>[小结](#toc0_)\n",
    "\n",
    "在PyTorch中，一维张量通常表示向量，没有明确的行向量或列向量之分。如果需要明确表示行向量或列向量，可以通过增加维度的方式来进行转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. <a id='toc7_3_'></a>[Tensors操作](#toc0_)\n",
    "### 7.3.1. <a id='toc7_3_1_'></a>[索引和切片](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]),\n",
       " torch.Size([5, 3]))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15).reshape(5, 3)\n",
    "\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # 1行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1] # 2行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3] # 1-3行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  3,  6,  9, 12])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0] # 1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4,  7, 10, 13])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1] # 2列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 6])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3, 0] # 1-3行，1列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. <a id='toc7_3_2_'></a>[修改维度](#toc0_)\n",
    "\n",
    "* 形状/维度：其实整个张量运算就是线性代数中的矩阵运算，所以最重要是明白`矩阵运算前后`的`形状/维度`。  \n",
    "\n",
    "* 高阶张量由若干低阶张量构成，如\n",
    "    * 结构为 (n, c, h, w)的 4 阶张量由 n 个结构为 (c, h, w) 的 3 阶张量构成，\n",
    "    * 结构为 (c, h, w)的 3 阶张量由 c 个结构为 (h, w) 的 2 阶张量构成，\n",
    "    * 结构为 (h, w)的 2 阶张量又由 h 个长度为 w 的 1 阶张量构成，h 为行数，w 为列数。\n",
    "\n",
    "* 修改形状/维度：reshape和view都是用来重塑tensor的shape的。view只适合对满足连续性条件（contiguous）的tensor进行操作，而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。view能干的reshape都能干，如果view不能干就可以用reshape来处理。\n",
    "\n",
    "- 维度`依次重排`：\n",
    "\n",
    "    - `.reshape()`\n",
    "\n",
    "    - `.view()`\n",
    "\n",
    "- 维度重组或转换或`挪动`：\n",
    "\n",
    "    - `permute()`\n",
    "\n",
    "    - `transpose()`\n",
    "\n",
    "- 参考：[https://blog.csdn.net/weixin_44115575/article/details/140742574](https://blog.csdn.net/weixin_44115575/article/details/140742574)\n",
    "\n",
    "  - 技术层面上的实现：一个张量是由`头部信息`部分和`数据存储`部分组成，头部信息部分存储了张量的`形状 (shape)`、`步长 (stride)`、`数据类型 (dtype)`等信息，数据存储部分存储了张量的实际数据。reshape、view、transpose和permute操作都是基于头部信息部分进行操作的，不会改变数据存储部分的数据。只是view处理前可能需要contiguous()一下；transpose和permute操作在交换维度的时候，需要考虑步长stride的重新计算；交换维度后，对对应长stride进行对应的交换，只是视图变了，数据存储部分的数据没有变。\n",
    "\n",
    "  - 应用层面上的理解：\n",
    "\n",
    "    - 图像数据处理：经常需要将图像的维度进行重排，如将 (H, W, C) 转换为 (C, H, W)，或者将 (H, W, C) 转换为 (C, H, W)。\n",
    "        ```python \n",
    "        # 假设图像数据为 (Batch, Channels, Height, Width)\n",
    "        image_tensor = torch.randn(32, 3, 64, 64)\n",
    "\n",
    "        # 转换为 (Batch, Height, Width, Channels)\n",
    "        image_tensor_permuted = image_tensor.permute(0, 2, 3, 1)\n",
    "        ```\n",
    "\n",
    "    - 自然语言处理:在自然语言处理任务中，RNN 或 Transformer 模型可能需要特定的输入维度顺序。例如，输入数据可能需要以 (sequence_length, batch_size, features) 的格式提供。\n",
    "        ```python\n",
    "        # 假设输入数据为 (Batch, Sequence Length, Features)\n",
    "        input_tensor = torch.randn(32, 10, 128)\n",
    "\n",
    "        # 转换为 (Sequence Length, Batch, Features)\n",
    "        input_tensor_permuted = input_tensor.permute(1, 0, 2)\n",
    "        ``` \n",
    "\n",
    "    - 多维数据分析:在处理多维数据时，某些操作可能需要特定的维度顺序。例如，计算某个维度上的均值或标准差时，可能需要先调整维度顺序。\n",
    "        ```python\n",
    "        # 假设有一个 4D 张量\n",
    "        data_tensor = torch.randn(5, 10, 15, 20)\n",
    "\n",
    "        # 需要在第一个维度上进行某种操作，可以先调整维度顺序\n",
    "        data_tensor_permuted = data_tensor.permute(1, 0, 2, 3)\n",
    "        ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.1. <a id='toc7_3_2_1_'></a>[[: None], [None, :]](#toc0_)   [&#8593;](#toc0_)\n",
    "含义：[None, :] 是利用 Python 的切片语法为张量增加一个新维度。\n",
    "- None：相当于在第 0 维增加一个新维度。\n",
    "- :：表示保留张量原本的所有元素。\n",
    "\n",
    "特点:\n",
    "- 只能增加维度（例如将 1D 张量变为 2D 张量）。\n",
    "- 增加的维度的大小为 1，方便用于广播操作。\n",
    "\n",
    "\n",
    "含义：reshape(-1, 1) 是用于调整张量形状的通用操作\n",
    " - -1：表示自动推导该维度的大小（根据张量总元素个数计算）。\n",
    " - 1：将张量变形为具有 1 列的 2D 张量。\n",
    "\n",
    "特点：\n",
    "- 更灵活，可以同时调整多个维度的大小。\n",
    "- 可以改变张量的形状，不局限于增加维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12]),\n",
       " torch.Size([1, 12]),\n",
       " torch.Size([1, 12]),\n",
       " torch.Size([12, 1]),\n",
       " torch.Size([12, 1]))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(12)\n",
    "\n",
    "x1 = x[None, :]\n",
    "x2 = x.reshape(1, -1)\n",
    "\n",
    "x3 = x[:, None]\n",
    "x4 = x.reshape(-1, 1)\n",
    "\n",
    "x.shape, x1.shape, x2.shape, x3.shape, x4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.2. <a id='toc7_3_2_2_'></a>[reshape函数](#toc0_)\n",
    "从左往右拉直，然后依次排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       " torch.Size([15]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15)\n",
    "\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape\n",
    "\n",
    "X.reshape(3, 5), X.reshape(3, 5).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.3. <a id='toc7_3_2_3_'></a>[view函数](#toc0_)\n",
    "从左往右拉直，然后依次排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view\n",
    "\n",
    "X.view(3, 5), X.view(3, 5).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.4. <a id='toc7_3_2_4_'></a>[transpose函数](#toc0_)\n",
    "\n",
    "`transpose()`函数`一次进行两个维度`的交换，参数是 0, 1, 2, 3, … ，随着待转换张量的阶数上升参数越来越多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.4.1. <a id='toc7_3_2_4_1_'></a>[二维](#toc0_)\n",
    "二维下transpose和T效果相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 阶张量，结构为 (h, w)，\n",
    "# 对应 transpose() 函数中的参数是 (0, 1) 两个索引，\n",
    "# 进行 transpose(0, 1) 操作就是在交换 h, w 两个维度，\n",
    "# 得到的结果与常见的矩阵转置相同。\n",
    "\n",
    "X = torch.arange(15).reshape(3, 5)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10],\n",
       "        [ 1,  6, 11],\n",
       "        [ 2,  7, 12],\n",
       "        [ 3,  8, 13],\n",
       "        [ 4,  9, 14]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10],\n",
       "        [ 1,  6, 11],\n",
       "        [ 2,  7, 12],\n",
       "        [ 3,  8, 13],\n",
       "        [ 4,  9, 14]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.4.2. <a id='toc7_3_2_4_2_'></a>[三维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [12, 13, 14, 15]],\n",
       " \n",
       "         [[ 4,  5,  6,  7],\n",
       "          [16, 17, 18, 19]],\n",
       " \n",
       "         [[ 8,  9, 10, 11],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(0, 1).shape, X.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]),\n",
       " tensor([[[ 0, 12],\n",
       "          [ 4, 16],\n",
       "          [ 8, 20]],\n",
       " \n",
       "         [[ 1, 13],\n",
       "          [ 5, 17],\n",
       "          [ 9, 21]],\n",
       " \n",
       "         [[ 2, 14],\n",
       "          [ 6, 18],\n",
       "          [10, 22]],\n",
       " \n",
       "         [[ 3, 15],\n",
       "          [ 7, 19],\n",
       "          [11, 23]]]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(0, 2).shape, X.transpose(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3]),\n",
       " tensor([[[ 0,  4,  8],\n",
       "          [ 1,  5,  9],\n",
       "          [ 2,  6, 10],\n",
       "          [ 3,  7, 11]],\n",
       " \n",
       "         [[12, 16, 20],\n",
       "          [13, 17, 21],\n",
       "          [14, 18, 22],\n",
       "          [15, 19, 23]]]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose(1, 2).shape, X.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]),\n",
       " tensor([[[ 0, 12],\n",
       "          [ 4, 16],\n",
       "          [ 8, 20]],\n",
       " \n",
       "         [[ 1, 13],\n",
       "          [ 5, 17],\n",
       "          [ 9, 21]],\n",
       " \n",
       "         [[ 2, 14],\n",
       "          [ 6, 18],\n",
       "          [10, 22]],\n",
       " \n",
       "         [[ 3, 15],\n",
       "          [ 7, 19],\n",
       "          [11, 23]]]))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape, X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.5. <a id='toc7_3_2_5_'></a>[permute函数](#toc0_)\n",
    "\n",
    "`permute()`函数`一次可以进行多个维度`的交换或者可以成为维度重新排列，参数是 0, 1, 2, 3, … ，随着待转换张量的阶数上升参数越来越多，本质上可以理解为多个 transpose() 操作的叠加，因此理解 permute() 函数的关键在于理解 transpose() 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.5.1. <a id='toc7_3_2_5_1_'></a>[二维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]),\n",
       " tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15).reshape(3, 5)\n",
    "\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]),\n",
       " tensor([[ 0,  5, 10],\n",
       "         [ 1,  6, 11],\n",
       "         [ 2,  7, 12],\n",
       "         [ 3,  8, 13],\n",
       "         [ 4,  9, 14]]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(1, 0).shape, X.permute(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.5.2. <a id='toc7_3_2_5_2_'></a>[三维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 4]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [12, 13, 14, 15]],\n",
       " \n",
       "         [[ 4,  5,  6,  7],\n",
       "          [16, 17, 18, 19]],\n",
       " \n",
       "         [[ 8,  9, 10, 11],\n",
       "          [20, 21, 22, 23]]]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(1, 0, 2).shape, X.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3]),\n",
       " tensor([[[ 0,  4,  8],\n",
       "          [ 1,  5,  9],\n",
       "          [ 2,  6, 10],\n",
       "          [ 3,  7, 11]],\n",
       " \n",
       "         [[12, 16, 20],\n",
       "          [13, 17, 21],\n",
       "          [14, 18, 22],\n",
       "          [15, 19, 23]]]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(0, 2, 1).shape, X.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 2]),\n",
       " tensor([[[ 0, 12],\n",
       "          [ 4, 16],\n",
       "          [ 8, 20]],\n",
       " \n",
       "         [[ 1, 13],\n",
       "          [ 5, 17],\n",
       "          [ 9, 21]],\n",
       " \n",
       "         [[ 2, 14],\n",
       "          [ 6, 18],\n",
       "          [10, 22]],\n",
       " \n",
       "         [[ 3, 15],\n",
       "          [ 7, 19],\n",
       "          [11, 23]]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.permute(2, 1, 0).shape, X.permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.6. <a id='toc7_3_2_6_'></a>[unsqueeze函数增加维度](#toc0_)\n",
    "unsqueeze 用于在指定位置插入一个大小为 1 的新维度。它不会改变数据本身，只是改变张量的形状。  \n",
    "增加/插入大小为`1`的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.6.1. <a id='toc7_3_2_6_1_'></a>[1维](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size: torch.Size([4])\n",
      "tensor([1, 2, 3, 4])\n",
      "x1 size: torch.Size([4])\n",
      "tensor([[1, 2, 3, 4]])\n",
      "x1 size: torch.Size([4])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(f'x size: {x.shape}', x, sep='\\n')\n",
    "\n",
    "x1 = torch.unsqueeze(input=x, dim=0)\n",
    "print(f'x1 size: {x.shape}', x1, sep='\\n')          # 1 x 4\n",
    "\n",
    "x1 = torch.unsqueeze(input=x, dim=1)\n",
    "print(f'x1 size: {x.shape}', x1, sep='\\n')          # 4 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.6.2. <a id='toc7_3_2_6_2_'></a>[多维度](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]),\n",
       " tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).reshape(3, 3)\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 3]),\n",
       " tensor([[[0, 1, 2],\n",
       "          [3, 4, 5],\n",
       "          [6, 7, 8]]]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.unsqueeze(input=x, dim=0)\n",
    "\n",
    "x1.shape, x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 3]),\n",
       " tensor([[[0, 1, 2]],\n",
       " \n",
       "         [[3, 4, 5]],\n",
       " \n",
       "         [[6, 7, 8]]]))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.unsqueeze(input=x, dim=1)\n",
    "\n",
    "x2.shape, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 1]),\n",
       " tensor([[[0],\n",
       "          [1],\n",
       "          [2]],\n",
       " \n",
       "         [[3],\n",
       "          [4],\n",
       "          [5]],\n",
       " \n",
       "         [[6],\n",
       "          [7],\n",
       "          [8]]]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = torch.unsqueeze(input=x, dim=2)\n",
    "\n",
    "x3.shape, x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.7. <a id='toc7_3_2_7_'></a>[squeeze函数减少维度](#toc0_)\n",
    "squeeze 用于移除大小为 1 的维度。它不会改变数据本身，只是改变张量的形状。  \n",
    "移除大小为`1`的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 3]), torch.Size([3, 3]), torch.Size([3, 1, 3]))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).reshape(3, 1, 3)\n",
    "\n",
    "x.shape, torch.squeeze(x, 1).shape, torch.squeeze(x, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.8. <a id='toc7_3_2_8_'></a>[拼接 (concat)](#toc0_)\n",
    "- 作用：torch.cat 用于将一组张量在`已有的维度`上拼接，而`不会创建新的维度`。\n",
    "- 拼接维度：你可以指定沿哪个维度进行拼接。\n",
    "- 输入要求：输入的张量在被拼接的维度以外的维度上`形状必须相同`。\n",
    "- 不增加新维度：拼接后张量的总维度与输入张量`相同`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 两个形状相同的张量\n",
    "a = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "\n",
    "b = torch.tensor([[5, 6], \n",
    "                  [7, 8]])\n",
    "\n",
    "# 沿dim=0拼接 (纵向)\n",
    "cat_result_0 = torch.cat((a, b), dim=0)\n",
    "print(cat_result_0)\n",
    "# 输出:\n",
    "# tensor([[1, 2],\n",
    "#         [3, 4],\n",
    "#         [5, 6],\n",
    "#         [7, 8]])\n",
    "\n",
    "# 沿dim=1拼接 (横向)\n",
    "cat_result_1 = torch.cat((a, b), dim=1)\n",
    "print(cat_result_1)\n",
    "# 输出:\n",
    "# tensor([[1, 2, 5, 6],\n",
    "#         [3, 4, 7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2, 3]), torch.Size([2, 4, 3]), torch.Size([2, 2, 6]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(shape=(2, 2, 3))\n",
    "\n",
    "dim1 = torch.cat([x, x], dim=0)\n",
    "dim2 = torch.cat([x, x], dim=1)\n",
    "dim3 = torch.cat([x, x], dim=2)\n",
    "\n",
    "dim1.shape, dim2.shape, dim3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.9. <a id='toc7_3_2_9_'></a>[拆分 (split)](#toc0_)\n",
    "- 功能: 将张量分成指定大小的块。\n",
    "- 参数:\n",
    "    - input: 要分割的张量。 \n",
    "    - split_size_or_sections: 每个块的大小，或者一个列表，指定每个块的大小。\n",
    "    - dim: 沿着哪个维度进行分割。\n",
    "- 特点: 可以指定每个块的大小，或者通过列表指定每个块的具体大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 0: tensor([0, 1, 2])\n",
      "块 1: tensor([3, 4, 5])\n",
      "块 2: tensor([6, 7, 8])\n",
      "块 3: tensor([9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "tensor = torch.arange(10)\n",
    "splits = torch.split(tensor, 3)  # 每个块大小为3\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"块 {i}: {split}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.10. <a id='toc7_3_2_10_'></a>[分块 (chunk)](#toc0_)\n",
    "- 作用：torch.chunk 将张量沿着指定维度分割成多个较小的张量。\n",
    "- 分割维度：指定沿着哪个维度进行分割。\n",
    "- 分割大小：指定每个小张量的大小。\n",
    "- 输入要求：除了分割维度外，其他维度的大小必须相同。\n",
    "x = torch.arange(12).reshape(2, 6)\n",
    "\n",
    "x, torch.chunk(x, chunks=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 0: tensor([0, 1, 2, 3])\n",
      "块 1: tensor([4, 5, 6, 7])\n",
      "块 2: tensor([8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "tensor = torch.arange(10)\n",
    "chunks = torch.chunk(tensor, 3)  # 分成3块\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"块 {i}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.11. <a id='toc7_3_2_11_'></a>[拼接 (stack)](#toc0_)\n",
    "- 作用：torch.stack 将一组张量沿一个`新维度`拼接，这个新维度是会在指定的位置创建出来的。\n",
    "- 增加新维度：输出的张量的总维度会比输入张量`多一维`。\n",
    "- 输入要求：所有输入张量必须形状`完全相同`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# 使用相同的张量a和b\n",
    "# 两个形状相同的张量\n",
    "a = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "\n",
    "b = torch.tensor([[5, 6], \n",
    "                  [7, 8]])\n",
    "\n",
    "stack_result = torch.stack((a, b), dim=0)\n",
    "print(stack_result)\n",
    "# 输出:\n",
    "# tensor([[[1, 2],\n",
    "#          [3, 4]],\n",
    "# \n",
    "#         [[5, 6],\n",
    "#          [7, 8]]])\n",
    "\n",
    "# 增加了新的第0维度，结果形状为 (2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.11.1. <a id='toc7_3_2_11_1_'></a>[cat和stack的比较](#toc0_)\n",
    "|操作\t|torch.cat\t|torch.stack|\n",
    "|-|-|-|\n",
    "|作用\t|沿`现有`维度拼接张量\t|在`新`维度上拼接张量|\n",
    "|维度\t|`不增加新维度`，输出张量与输入张量维度相同\t|`增加新维度`，输出张量比输入张量多一维|\n",
    "|输入要求\t|`除了拼接维度外，其他维度大小必须相同`\t|`所有输入张量的形状必须完全相同`|\n",
    "|用例\t|沿着现有维度连接多组数据\t|将多组相同形状的数据堆叠为一个`新张量`|\n",
    "\n",
    "torch.cat 更适合在相同维度上拼接数据，而 torch.stack 则用于将数据沿新维度进行组织。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.12. <a id='toc7_3_2_12_'></a>[广播 (expand)](#toc0_)\n",
    "- 功能：通过改变张量的视图（view）来扩展张量的维度。\n",
    "- 特点：\n",
    "    - 不会复制数据。\n",
    "    - 只改变张量的视图，使其在需要的维度上 \"看起来\" 是扩展的。\n",
    "    - 扩展的维度必须是 1 或者是可以广播的。\n",
    "- 用途：适用于需要广播操作的情况，可以减少内存开销。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 广播机制：在进行广播操作时，expand 可以用于将一个较小的张量扩展为与另一个张量相同的形状。\n",
    "- 不会复制数据，只改变张量的视图，使其在需要的维度上 \"看起来\" 是扩展的，扩展的维度必须是 1 或者是可以广播的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 假设有一个形状为 (5, 1) 的张量\n",
    "a = torch.tensor([[1], \n",
    "                  [2], \n",
    "                  [3], \n",
    "                  [4], \n",
    "                  [5]])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3],\n",
       "        [4, 4, 4],\n",
       "        [5, 5, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 扩展为形状 (5, 3)，以便与形状为 (5, 3) 的张量进行运算\n",
    "a_expanded = a.expand(5, 3)\n",
    "\n",
    "a_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 总结：\n",
    "    - 不复制数据：expand 不会实际复制数据，而是通过调整步长来实现扩展。这意味着扩展后的张量与原始张量共享相同的数据。\n",
    "    - 只能扩展大小为 1 的维度：expand 只能扩展那些原始大小为 1 的维度。如果尝试扩展其他维度，会引发错误。\n",
    "    - expand 是一个高效的操作，用于在不复制数据的情况下扩展张量的维度。它在需要进行广播操作或匹配特定形状要求时非常有用。通过理解 expand 的工作原理和应用场景，可以更好地利用它来优化张量操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.13. <a id='toc7_3_2_13_'></a>[repeat](#toc0_)\n",
    "\n",
    "- 功能：通过真正复制数据来重复张量的内容。\n",
    "- 特点：\n",
    "    - 会实际分配新的内存来存储重复的数据。\n",
    "    - 数据被实际复制，因此内存占用会增加。\n",
    "- 用途：适用于需要明确复制张量内容的情况。\n",
    "- 注意：repeat 会创建一个新的张量，与原始张量无关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 假设有一个形状为 (5, 1) 的张量\n",
    "a = torch.tensor([[1], \n",
    "                  [2], \n",
    "                  [3], \n",
    "                  [4], \n",
    "                  [5]])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5]]),\n",
       " tensor([[1, 1],\n",
       "         [2, 2],\n",
       "         [3, 3],\n",
       "         [4, 4],\n",
       "         [5, 5]]))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 表示每个维度的重复次数。可以是多个整型参数，也可以是一个整型元组\n",
    "# 每个维度指定一个整数，表示在该维度上重复多少次。\n",
    "# 重复后的维度大小为原始大小乘以该维度的重复次数。\n",
    "## dim0重复2次，dim1重复1次\n",
    "y = a.repeat((2, 1))\n",
    "## dim0重复1次，dim1重复2次\n",
    "y1 = a.repeat(1, 2)\n",
    "\n",
    "y, y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.14. <a id='toc7_3_2_14_'></a>[repeat_interleave](#toc0_)\n",
    "- 功能：将张量的元素重复指定次数，并返回一个新的张量。\n",
    "- 参数：\n",
    "    - input: 要重复的张量。\n",
    "    - repeats: 每个元素重复的次数。\n",
    "    - dim: 沿着哪个维度进行重复。\n",
    "- 返回：一个新的张量，形状与输入张量相同，但元素被重复。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,\n",
       "          9,  9, 10, 10, 11, 11]),\n",
       " tensor([[ 0,  1,  2,  3],\n",
       "         [ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[ 0,  0,  1,  1,  2,  2,  3,  3],\n",
       "         [ 4,  4,  5,  5,  6,  6,  7,  7],\n",
       "         [ 8,  8,  9,  9, 10, 10, 11, 11]]))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(12).reshape(3, 4)\n",
    "\n",
    "x, torch.repeat_interleave(x, repeats=2), torch.repeat_interleave(x, repeats=2, dim=0), torch.repeat_interleave(x, repeats=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.2.14.1. <a id='toc7_3_2_14_1_'></a>[expand和repeat对比](#toc0_)\n",
    "- 总结\n",
    "  - 如果只是为了广播操作，建议使用 expand，效率更高。\n",
    "  - 如果需要实际的数据重复，使用 repeat。\n",
    "\n",
    "- 比较\n",
    "\n",
    "  |特性|expand|repeat|\n",
    "  |-|-|-|\n",
    "  |内存占用|低，只是改变视图|高，数据实际复制|\n",
    "  |数据共享|是，返回共享内存的视图|否，返回一个新的张量|\n",
    "  |使用场景|适用于广播操作，不需要实际复制数据|适用于需要显式复制数据的情况|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.15. <a id='toc7_3_2_15_'></a>[填充padding和打包packing](#toc0_)\n",
    "\n",
    "在 PyTorch 中，处理变长序列时，常用以下函数来进行填充（padding）和打包（packing）操作:\n",
    "  - pad_sequence 用于将序列填充到相同长度，\n",
    "  - pack_padded_sequence 将填充后的序列压紧以去除填充部分，经过 RNN 处理后，使用 pad_packed_sequence 将压紧的序列解包恢复，\n",
    "  - torch.nn.functional.pad 则用于对张量进行一般性的填充操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "torch.nn.functional.pad(input, pad, mode='constant', value=0)   # 用于在张量的指定维度前后(上下左右)添加数值(padding几个单位距离)，以改变其形状。\n",
    "# input：需要填充的 N 维张量。\n",
    "# pad：一个包含偶数个元素的元组，表示各维度的填充长度。\n",
    "# mode：填充模式，可选 'constant'（常数填充）、'reflect'（反射填充）等。\n",
    "# value：仅在 mode='constant' 时有效，表示填充值。\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0],\n",
       "         [0, 1, 2, 0],\n",
       "         [0, 3, 4, 0],\n",
       "         [0, 0, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 2, 0, 0],\n",
       "         [0, 0, 3, 4, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "x = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "\n",
    "F.pad(input=x, pad=(1, 1, 1, 1), mode='constant', value=0), F.pad(input=x, pad=(2, 2, 2, 2), mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)     # 用于将一系列可变长度的张量填充为相同长度，以便进行批处理。\n",
    "# sequences：变长序列的列表。\n",
    "# batch_first：如果为 True，输出形状为 (batch_size, max_length, *)，否则为 (max_length, batch_size, *)。\n",
    "# padding_value：填充值，默认值为 0。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 0],\n",
       "        [6, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils import rnn \n",
    "\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5])\n",
    "c = torch.tensor([6])\n",
    "\n",
    "padded = rnn.pad_sequence(sequences=[a, b, c], batch_first=True, padding_value=0)\n",
    "\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True, enforce_sorted=False)   # 用于将填充后的序列压紧，去除填充部分，以提高 RNN 的计算效率。\n",
    "# input：填充后的序列张量。\n",
    "# lengths：每个序列的实际长度列表。\n",
    "# batch_first：如果为 True，输入形状应为 (batch_size, max_length, *)。\n",
    "# enforce_sorted：如果为 True，序列应按长度递减排序。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [4, 5, 0],\n",
       "         [6, 0, 0]]),\n",
       " PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([0, 1, 2]), unsorted_indices=tensor([0, 1, 2])))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils import rnn \n",
    "\n",
    "\n",
    "lengths = torch.tensor([3, 2, 1])\n",
    "\n",
    "padded_packed = rnn.pack_padded_sequence(input=padded, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "padded, padded_packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=True, padding_value=0, total_length=None)    # 用于将压紧的序列解包，恢复为填充后的形式，便于后续处理。\n",
    "# sequence：PackedSequence 对象。\n",
    "# batch_first：如果为 True，输出形状为 (batch_size, max_length, *)。\n",
    "# padding_value：填充值，默认值为 0。\n",
    "# total_length：如果不是 None，输出将被填充到该长度。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([0, 1, 2]), unsorted_indices=tensor([0, 1, 2])),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 0],\n",
       "         [6, 0, 0]]),\n",
       " tensor([3, 2, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils import rnn  \n",
    "\n",
    "unpacked, unpacked_lengths = rnn.pad_packed_sequence(sequence=padded_packed, batch_first=True, padding_value=0, total_length=None)\n",
    "padded_packed, unpacked, unpacked_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. <a id='toc7_4_'></a>[线性代数运算](#toc0_)\n",
    "PyTorch的运算很大一块是`线性代数运算-矩阵运算`，所以需要搞清楚每一步计算前后矩阵的`形状/维度`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. <a id='toc7_4_1_'></a>[数值运算](#toc0_)\n",
    "\n",
    "- 自动做广播：\n",
    "    - x, y的size维度对应的维度数值必须为`无 (不是0)`或`1`，才能被广播。\n",
    "\n",
    "|操作|函数|\n",
    "|:-|:-|\n",
    "|+|torch.add(X, Y)|\n",
    "|-|torch.sub(X, Y)|\n",
    "|*|torch.mul(X, Y|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.],\n",
       "         [12., 13., 14.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13., 14.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5, 3, dtype=torch.float32)\n",
    "y = torch.arange(0, 15, 1, dtype=torch.float32).reshape(5, 3)\n",
    "z = torch.arange(0, 15, 1, dtype=torch.float32).reshape(3, 5)\n",
    "\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.],\n",
       "         [ 7.,  8.,  9.],\n",
       "         [10., 11., 12.],\n",
       "         [13., 14., 15.]]),\n",
       " tensor([[ 1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.],\n",
       "         [ 7.,  8.,  9.],\n",
       "         [10., 11., 12.],\n",
       "         [13., 14., 15.]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y, torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2, 1, 1, 9, 9]),\n",
       " torch.Size([10, 1, 9, 9, 1, 1]),\n",
       " torch.Size([10, 2, 9, 9, 9, 9]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动做广播 (很重要)\n",
    "# 两个tensor的维度数完全相同，但对应为维度数值不同时：\n",
    "# x或y中的一个必须是1，才能被自动做广播。\n",
    "\n",
    "x = torch.randn(size=(10, 2, 1, 1, 9, 9))   # 被自动广播成 (10, 2, 9, 9, 9, 9)\n",
    "y = torch.randn(size=(10, 1, 9, 9, 1, 1))   # 被自动广播成 (10, 2, 9, 9, 9, 9)\n",
    "\n",
    "x.size(), y.size(), (x+y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2, 1, 1, 9, 9]),\n",
       " torch.Size([9, 1, 1]),\n",
       " torch.Size([10, 2, 1, 9, 9, 9]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动做广播 (很重要)\n",
    "# 两个tensor的维度数不同，且对应为维度数值不同时：\n",
    "# x或y中的一个必须是1，才能被自动做广播，且\n",
    "# 短的维度会被自动广播成长的一样。\n",
    "\n",
    "x = torch.randn(size=(10, 2, 1, 1, 9, 9))   # 被自动广播成 (10, 2, 1, 9, 9, 9)\n",
    "y = torch.randn(size=(9, 1, 1))             # 被自动广播成 (10, 2, 1, 9, 9, 9)\n",
    "\n",
    "x.size(), y.size(), (x+y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1.,   0.,  -1.],\n",
       "         [ -2.,  -3.,  -4.],\n",
       "         [ -5.,  -6.,  -7.],\n",
       "         [ -8.,  -9., -10.],\n",
       "         [-11., -12., -13.]]),\n",
       " tensor([[  1.,   0.,  -1.],\n",
       "         [ -2.,  -3.,  -4.],\n",
       "         [ -5.,  -6.,  -7.],\n",
       "         [ -8.,  -9., -10.],\n",
       "         [-11., -12., -13.]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y, torch.sub(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * 3 # 数乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(x, 3) # 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.],\n",
       "        [15., 18., 21., 24., 27.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, z) # 矩阵相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. <a id='toc7_4_2_'></a>[数值运算-乘法](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.1. <a id='toc7_4_2_1_'></a>[哈达玛积](#toc0_)\n",
    "* 按照`元素`进行乘法\n",
    "* 乘前形状必须相同，乘后不改变形状\n",
    "* x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 0,  1,  4],\n",
       "         [ 9, 16, 25]]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(6).reshape(2, 3)\n",
    "\n",
    "x, x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.2. <a id='toc7_4_2_2_'></a>[点积（Dot Product）](#toc0_)\n",
    "* 按照元素进行乘法后相加\n",
    "* 乘前形状一样，乘后`标量`\n",
    "* `torch.dot(x, x)` # dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([0, 1, 4]), tensor(5))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.arange(3)\n",
    "\n",
    "x, x * x, torch.dot(x, x) # 打印， 哈德玛积， 点积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.3. <a id='toc7_4_2_3_'></a>[矩阵-向量积](#toc0_)\n",
    "* 矩阵乘法的特殊\n",
    "* 乘后`向量`\n",
    "* `torch.mv(A, x)` # matrix-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([4]), torch.Size([3]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "x = torch.ones(4, dtype=torch.float32)\n",
    "\n",
    "A.shape, x.shape, torch.mv(A, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]),)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A * x).shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.4. <a id='toc7_4_2_4_'></a>[矩阵-矩阵积](#toc0_)\n",
    "* 乘后**矩阵**\n",
    "* `torch.matmul(X, Y)`  # 矩阵乘法，`支持广播`\n",
    "* `X @ Y`               # 同上\n",
    "* `torch.mm(X, Y)`      # 矩阵乘法，`不支持广播`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]),\n",
       " torch.Size([5, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15).reshape(3, 5)\n",
    "Y = torch.arange(15).reshape(5, 3)\n",
    "\n",
    "X.shape, Y.shape, (X @ Y).shape, torch.mm(X, Y).shape, torch.matmul(X, Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.5. <a id='toc7_4_2_5_'></a>[批量矩阵乘法](#toc0_)\n",
    "\n",
    "* A: (b x n x m) \n",
    "* B: (b x m x p)\n",
    "* `torch.bmm(A, B)`   # b x n x p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 5]), torch.Size([3, 5, 3]), torch.Size([3, 3, 3]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(45).reshape(3, 3, 5)\n",
    "Y = torch.arange(45).reshape(3, 5, 3)\n",
    "\n",
    "X.shape, Y.shape, torch.bmm(X, Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.2.6. <a id='toc7_4_2_6_'></a>[乘总结](#toc0_)\n",
    "\n",
    "参考PyTorch lightning 总结：[https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/01-introduction-to-pytorch.html](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/01-introduction-to-pytorch.html)\n",
    "\n",
    "|乘法|函数|\n",
    "|:-|:-|\n",
    "|哈德玛积|A * B|\n",
    "|点积|dot(A, B)|\n",
    "|矩阵-向量|mv(A, x)|\n",
    "|矩阵-矩阵|matmul(A, B) 或  A @ B，同时mm(A, B)不支持广播|\n",
    "|批量矩阵乘法|`bmm(A, B)`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3. <a id='toc7_4_3_'></a>[统计运算](#toc0_)\n",
    "\n",
    "|操作|注释|\n",
    "|:-|:-|\n",
    "|torch.mean()|取平均|\n",
    "|torch.median()||\n",
    "|torch.mode()||\n",
    "|torch.min()||\n",
    "|torch.max()||\n",
    "|torch.std()||\n",
    "|torch.var()||\n",
    "|torch.squar()||\n",
    "|torch.`argmax()`||\n",
    "|torch.`argmin()`||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14.])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15, dtype=torch.float32)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.), tensor(7.))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x), x.mean() # 平均数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.), tensor(7.))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.median(x), x.median() # 中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(x), x.min()   # 最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14.), tensor(14.))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x), x.max()   # 最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.mode(\n",
       " values=tensor(0.),\n",
       " indices=tensor(0)),\n",
       " torch.return_types.mode(\n",
       " values=tensor(0.),\n",
       " indices=tensor(0)))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mode(x), x.mode() # 众数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.4721), tensor(4.4721))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(x), x.std()   # 标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(20.), tensor(20.))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(x), x.var()   # 方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]),\n",
       " tensor([[2, 2, 2, 2, 2]]),\n",
       " tensor([[4],\n",
       "         [4],\n",
       "         [4]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15).reshape(3, 5)\n",
    "\n",
    "# dim = 0, 表示从上往下\n",
    "# dim = 1, 表示从左往右\n",
    "# keepdim = True, 表示保持原始维度信息\n",
    "\n",
    "x, torch.argmax(x, dim=0, keepdim=True), torch.argmax(x, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 2, 2, 2]), tensor([4, 4, 4]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim = False， 表示丢弃原始维度信息\n",
    "\n",
    "torch.argmax(x, dim=0, keepdim=False), torch.argmax(x, dim=1, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == torch.argmax(x, dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[torch.argmax(x, dim=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[torch.argmax(x, dim=0)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. <a id='toc7_5_'></a>[广播机制 (Broadcasting)](#toc0_)\n",
    "\n",
    "PyTorch中的广播机制是指在进行张量运算时，如果两个张量的形状不完全相同但可以通过扩展其中一个张量的尺寸来使它们能够兼容地进行操作，则这个过程就被称为广播（Broadcasting）。这种机制使得不同形状的张量可以一起进行数学运算，而不需要显式地调整张量的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. <a id='toc7_5_1_'></a>[广播规则](#toc0_)\n",
    "\n",
    "1. `从右向左`比较两个张量的维度大小。\n",
    "2. 如果两个维度大小`相等`，或者某一方的维度大小为`1`，则可以进行广播。\n",
    "3. 如果遇到维度大小不一致的情况，并且不符合上述条件，则无法进行广播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **标量与张量的运算**\n",
    "\n",
    "   如果一个标量与一个张量进行运算，那么该标量会被广播到张量的各个元素上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size: torch.Size([3])\n",
      "tensor([2., 4., 6.]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "scalar = 2\n",
    "tensor = torch.tensor([1., 2., 3.])\n",
    "print(f'tensor size: {tensor.shape}')\n",
    "\n",
    "result = scalar * tensor\n",
    "\n",
    "print(result, result.shape)  # 输出 tensor([2., 4., 6.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **维度大小为1的张量**\n",
    "\n",
    "   如果一个张量的一个维度大小为1，那么这个维度可以被扩展到匹配另一个张量的相应维度大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1 size: torch.Size([3, 1])\n",
      "tensor2 size: torch.Size([3])\n",
      "tensor([[2., 3., 4.],\n",
      "        [3., 4., 5.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1.], [2.], [3.]])  # 形状为 (3, 1) 表示： 3 x 1\n",
    "print(f'tensor1 size: {tensor1.shape}')\n",
    "tensor2 = torch.tensor([1., 2., 3.])        # 形状为 (3,)   表示： 1 x 3\n",
    "print(f'tensor2 size: {tensor2.shape}')\n",
    "result = tensor1 + tensor2\n",
    "\n",
    "print(result)  # 输出 tensor([[2., 3., 4.],\n",
    "#                     [3., 4., 5.],\n",
    "#                     [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **不同维度的张量**\n",
    "\n",
    "   当两个张量的维度不同时，较小的张量会在前面添加维度大小为1的维度，然后进行广播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1 size: torch.Size([128, 10, 20, 100])\n",
      "tensor2 size: torch.Size([1, 100])\n",
      "torch.Size([128, 10, 20, 100])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randn(size=(128, 10, 20, 100))    # 形状为 (128, 10, 20, 100)        \n",
    "print(f'tensor1 size: {tensor1.shape}')\n",
    "\n",
    "# tensor2 = torch.randn(size=(100,))                # 形状为 (1, 100)  表示: 1 x 100，广播为：(128, 10, 20, 100) \n",
    "tensor2 = torch.randn(size=(1,100))                # 形状为 (1, 100)  表示: 1 x 100，广播为：(128, 10, 20, 100) \n",
    "print(f'tensor2 size: {tensor2.shape}')\n",
    "\n",
    "result = tensor1 + tensor2\n",
    "\n",
    "print(result.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **不兼容的维度**\n",
    "\n",
    "   如果两个张量的对应维度大小不一致，并且不能通过扩展为1来解决，那么就不能进行广播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1., 2.], [3., 4.]])  # 形状为 (2, 2), 2 x 2\n",
    "tensor2 = torch.tensor([1., 2., 3.])          # 形状为 (3,),   1 x 3\n",
    "# 下面的代码会抛出错误\n",
    "try:\n",
    "    result = tensor1 + tensor2\n",
    "except RuntimeError as e:\n",
    "    print(e)  # 张量形状不匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 小结\n",
    "\n",
    "PyTorch中的广播机制允许开发人员使用更简洁的代码来处理不同形状的张量之间的运算。这种机制在实现复杂的神经网络架构时非常有用，因为它减少了手动调整张量形状的需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. <a id='toc7_6_'></a>[Pytorch的计算图 和 自动微分 (autograd)](#toc0_)\n",
    "\n",
    "- PyTorch是动态图，即`计算图 (有向无环图（DAG），每个节点表示一个张量或运算)`的搭建和运算是同时的，随时可以输出结果；而TensorFlow是静态图。\n",
    "\n",
    "- 在pytorch的计算图里只有两种元素：`数据（tensor）`和 `运算（operation）`\n",
    "\n",
    "  - 运算包括了：加减乘除、开方、幂指对、三角函数等可求导运算\n",
    "\n",
    "  - 数据可分为：`叶子节点（leaf node`）和`非叶子节点`；\n",
    "    - 叶子节点：计算图的起点，是直接由用户创建的张量，通常具有 requires_grad=True 属性，用于存储梯度信息；可以通过 `is_leaf` 属性判断某个张量是否为叶子节点\n",
    "    - 非叶子节点：由叶子节点`通过运算生成的中间张量`，不直接存储梯度，这些张量的 grad 属性默认是 None，但可以通过 retain_grad() 方法显式保存它们的梯度。\n",
    "    - 叶子节点是用户创建的节点，不依赖其它节点；它们表现出来的区别在于`用y.backward()进行反向传播`结束之后，`非叶子节点的梯度会被释放掉`，只保留叶子节点的梯度，这样就节省了内存。如果想要保留非叶子节点的梯度，可以使用`retain_grad()`方法。\n",
    "\n",
    "- torch.tensor节点 具有如下属性：\n",
    "  - 查看 是否为叶子节点 `is_leaf`\n",
    "  - 查看 是否可以求导 `requires_grad`\n",
    "  - 查看 运算名称 `grad_fn`\n",
    "  - 查看 导数值 `grad`\n",
    "\n",
    "- 针对requires_grad属性，自己定义的叶子节点默认为False，而非叶子节点默认为True，神经网络中的权重默认为True。判断哪些节点是True/False的一个原则就是从你需要求导的叶子节点到loss节点之间是一条可求导的通路。\n",
    "\n",
    "---\n",
    "\n",
    "- PyTorch提供两种求梯度的方法：`backward()` 和 `torch.autograd.grad()` ，他们的区别在于前者是给叶子节点填充.grad字段，而后者是直接返回梯度给你，我会在后面举例说明。还需要知道y.backward()其实等同于`torch.autograd.backward(y)`。\n",
    "\n",
    "\n",
    "![PyTorch的计算图](./Pytorch_Pictures/PyTorch_graphacial_demo/graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1. <a id='toc7_6_1_'></a>[反向传播 (backward)-批量求梯度，但未进行参数更新](#toc0_)\n",
    "\n",
    "计算`所有节点 (Tensor)` 的梯度并存储在节点的`grad属性中`，但未进行节点参数更新 (是优化函数干的事)。\n",
    "\n",
    "- `y.backward()` 或 `torch.autograd.backward(y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]], requires_grad=True),\n",
       " tensor([[-0.7109,  0.6400,  0.1948, -0.3200,  1.3037],\n",
       "         [-1.3823,  1.3554,  0.3006,  1.3434,  1.2845],\n",
       "         [-0.1937, -0.7454, -1.6233, -0.3062, -0.8822]], requires_grad=True),\n",
       " tensor(16.0991, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.ones(size=(3, 5), dtype=torch.float32, requires_grad=True)       # 自定义需要存储梯度\n",
    "x2 = torch.randn(size=(3, 5), dtype=torch.float32, requires_grad=True)      # 默认是不存储梯度\n",
    "\n",
    "y = torch.add(x1**2, x2**3).sum()   # 应变量必须是标量\n",
    "\n",
    "x1, x2, y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 查看is_leaf属性\n",
    "x1.is_leaf, x2.is_leaf, y.is_leaf   \n",
    "# x1, x2是叶子节点，y不是叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 查看requires_grad属性\n",
    "x1.requires_grad, x2.requires_grad, y.requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, <SumBackward0 at 0x7fce06bff0d0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 查看grad_fn属性\n",
    "x1.grad_fn, x2.grad_fn, y.grad_fn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32820/1085795512.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  x1.grad, x2.grad, y.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 查看grad属性\n",
    "x1.grad, x2.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "# torch.autograd.backward(y)  # 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32820/1085795512.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  x1.grad, x2.grad, y.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]]),\n",
       " tensor([[4.0998e+00, 3.6832e+00, 5.1059e+00, 5.9216e-05, 6.2451e+00],\n",
       "         [2.4456e+00, 3.4693e+00, 9.5398e-02, 7.5255e-01, 9.7835e-03],\n",
       "         [7.6321e+00, 7.4899e-01, 8.9373e-01, 2.3219e+00, 2.8837e+00]]),\n",
       " None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 查看grad属性\n",
    "x1.grad, x2.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2. <a id='toc7_6_2_'></a>[仅计算梯度 (求导计算)](#toc0_)\n",
    "\n",
    "和backward不同，torch.autograd.grad只是计算`应变量 (output)` 对`自变量 (input)`的`导数 (梯度)`；`应变量必须是标量`。\n",
    "\n",
    "- `torch.autograd.grad(output=y, input=x, retain_grad=False/True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.]],\n",
       " \n",
       "         [[3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.]],\n",
       " \n",
       "         [[3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3., 3.]]]),)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(size=(3, 3, 5), dtype=torch.float32, requires_grad=True) # 必须是float类型\n",
    "y = (x**3).sum()\n",
    "\n",
    "torch.autograd.grad(outputs=y, inputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7. <a id='toc7_7_'></a>[自动微积-autograd](#toc0_)\n",
    "```shell\n",
    "深度学习框架可以自动计算导数：\n",
    "```\n",
    "|操作|函数|\n",
    "|:-|:-|\n",
    "|1. 我们首先将梯度附加到想要对其计算偏导数的变量上，|x.requires_grad_(True)|\n",
    "|2. 然后记录目标值的计算，|y = x * x (grad_fn)|\n",
    "|3. 执行它的反向传播函数(求梯度)，|y.backward()|\n",
    "|4. 并访得到的梯度。|x.grad|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1. <a id='toc7_7_1_'></a>[自己探索](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.1. <a id='toc7_7_1_1_'></a>[标量-一阶导数（得标量）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2., requires_grad=True), tensor(4., grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, dtype=torch.float32, requires_grad=True)  # 标量\n",
    "y = x**2    \n",
    "                                                    # 标量\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为None\n",
    "x.grad, x.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y对x进行求导\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为2 * 2 = 4\n",
    "x.grad                                                          # 标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 2*x # y关于x的一阶导函数就是2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造新的关于x的函数：z = x**3\n",
    "z = x**2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为：\n",
    "x.grad.zero_()\n",
    "x.grad # 应该为0才对，需要手动清零# x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z关于x求导\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时x的导数为：\n",
    "x.grad # 应该为4，但是残留的4 + 本次的4 = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.2. <a id='toc7_7_1_2_'></a>[标量/向量-一阶导数（得向量）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0, dtype=torch.float32, requires_grad=True)  # 向量\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14., grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.dot(x, x)                                              # 标量\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "\n",
    "x.grad                                                          # 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.3. <a id='toc7_7_1_3_'></a>[向量/向量-一阶导数（得矩阵）](#toc0_)\n",
    "\n",
    "- pytorch只能对标量/标量，标量/向量求导，`即x可以为标量也可以为向量，但是y必须为标量`\n",
    "\n",
    "- `只需要先将y转变为标量，对分别求导没影响的就是求和`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.arange(4, dtype=torch.float32, requires_grad=True)    # 向量\n",
    "\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = i ** 2                                                      # 向量\n",
    "\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h.backward()      # 报错\n",
    "h.sum().backward()  # 正常\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7.1.4. <a id='toc7_7_1_4_'></a>[求高阶导数](#toc0_)\n",
    "\n",
    "- 利用`torch.autograd.grad(outputs=y, inputs=x, create_grad=True)`\n",
    "\n",
    "- 保留计算图 (链表指针), `create_grad=True `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12., grad_fn=<MulBackward0>),)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2, dtype=torch.float32, requires_grad=True)\n",
    "y = x**3\n",
    "grad1 = torch.autograd.grad(outputs=y, inputs=x, create_graph=True) # create_graph=True, 必须保留计算图才能进行后续的高阶导数计算\n",
    "\n",
    "grad1 # 3 * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12., grad_fn=<MulBackward0>),)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad2 = torch.autograd.grad(outputs=grad1, inputs=x, create_graph=True)\n",
    "\n",
    "grad2 # 6 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.),)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad3 = torch.autograd.grad(outputs=grad2, inputs=x)\n",
    "\n",
    "grad3 # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2. <a id='toc7_7_2_'></a>[一个简单的例子](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 定义变量\n",
    "x = torch.arange(4.0)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在我们计算关于的梯度之前，需要一个地方来存储梯度。\n",
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "\n",
    "x.grad                  # 默认值是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在计算。\n",
    "y = 2 * torch.dot(x, x)\n",
    "\n",
    "y                       # x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。\n",
    "y.backward()            # [4x, 4x, 4x, 4x] 导函数\n",
    "\n",
    "x.grad                  # [4*0, 4*1, 4*2, 4*3] 导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x         # [4x, 4x, 4x, 4x] 导函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.3. <a id='toc7_7_3_'></a>[计算另一个](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor(6., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.4. <a id='toc7_7_4_'></a>[非标量变量的反向传播](#toc0_)\n",
    "\n",
    "- 当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 \n",
    "\n",
    "- 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。\n",
    "\n",
    "- 然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的**偏导数之和**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x * x \n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0.]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, tensor([0., 2., 4., 6.]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum().backward(), x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.5. <a id='toc7_7_5_'></a>[分离计算](#toc0_)\n",
    "\n",
    "- 有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "\n",
    "- 这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.6. <a id='toc7_7_6_'></a>[Python控制流的梯度计算](#toc0_)\n",
    "\n",
    "- 使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们计算梯度。\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a，因此可以用d/a验证梯度是否正确。\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8. <a id='toc7_8_'></a>[概率论](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正太函数分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7607, -0.3525, -0.6538,  0.3404, -1.0323, -0.4394,  0.0676,  0.7667,\n",
      "         1.7211, -0.0475, -1.1181,  0.7353, -1.7605,  1.4901, -0.9803,  0.8902,\n",
      "        -0.0931, -1.7484,  2.3997,  0.6524,  0.6782, -1.1440, -1.3923,  0.1212,\n",
      "        -1.0076, -1.7668, -1.4322, -0.6901,  0.2830,  0.5470,  1.4634,  0.6256,\n",
      "         1.2161, -1.3545, -1.2281, -0.6693,  0.2557,  0.2750, -0.1981,  0.4620,\n",
      "         0.5137, -0.3635,  0.7580,  0.6187, -2.0609, -1.9659, -0.0752,  0.7554,\n",
      "        -0.6792, -1.2573, -0.1298, -0.4564,  0.3095,  1.2856, -0.7012,  0.5607,\n",
      "        -1.0115,  1.1368,  1.0839,  1.5874, -1.1725,  0.8335, -1.8986,  1.1627,\n",
      "         1.5963, -1.7788,  1.4887,  2.6236,  0.0521, -0.5584, -1.2956, -1.0912,\n",
      "         1.0101,  0.6228,  0.3619,  1.4112,  0.1833,  0.4523, -0.5056,  0.2020,\n",
      "         1.4686,  1.8315, -0.8283,  0.6796, -0.1077,  0.0794, -0.2321,  1.2689,\n",
      "        -0.5188,  0.6315,  1.2953, -0.0427,  0.0622, -0.6244, -0.6351, -1.3894,\n",
      "         0.1629, -0.7895, -0.0437,  1.6747])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f350d9c5df0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "data = torch.normal(mean=0, std=1, size=(100,), dtype=torch.float32, device=device)\n",
    "print(data)\n",
    "plt.figure()\n",
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[神经网络-训练八股](#toc0_)\n",
    "\n",
    "|步骤|计算|操作|\n",
    "|:-|:-|:-|\n",
    "|1|定义网络模型|->计算出`y_hat`|\n",
    "|2|选择损失函数|->计算`loss值`、求梯度|\n",
    "|3|选择优化器|->`更新`网络权重参数|\n",
    "|4|训练|->实施1、2、3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. <a id='toc8_1_'></a>[现线性回归模型于训练过程-从零开始](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. <a id='toc8_1_1_'></a>[虚拟出数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([1.6710, 0.3170])\n",
      "label: tensor([6.4774])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "\n",
    "\n",
    "def synthetic_data(w, b, num_examples):  \n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(mean=0, std=1, size=(num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(mean=0, std=0.01, size=y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "print('features:', features[0])\n",
    "print('label:', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"231.442187pt\" height=\"169.678125pt\" viewBox=\"0 0 231.442187 169.678125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-05-18T15:08:53.316260</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 169.678125 \n",
       "L 231.442187 169.678125 \n",
       "L 231.442187 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "L 28.942188 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m18bb6ce550\" d=\"M 0 0.5 \n",
       "C 0.132602 0.5 0.25979 0.447317 0.353553 0.353553 \n",
       "C 0.447317 0.25979 0.5 0.132602 0.5 0 \n",
       "C 0.5 -0.132602 0.447317 -0.25979 0.353553 -0.353553 \n",
       "C 0.25979 -0.447317 0.132602 -0.5 0 -0.5 \n",
       "C -0.132602 -0.5 -0.25979 -0.447317 -0.353553 -0.353553 \n",
       "C -0.447317 -0.25979 -0.5 -0.132602 -0.5 0 \n",
       "C -0.5 0.132602 -0.447317 0.25979 -0.353553 0.353553 \n",
       "C -0.25979 0.447317 -0.132602 0.5 0 0.5 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p149bb96678)\">\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.095479\" y=\"47.161377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.851713\" y=\"83.62544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.208597\" y=\"87.238868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.729918\" y=\"40.397538\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.573273\" y=\"74.594754\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.717888\" y=\"60.945456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.324851\" y=\"78.7769\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.745046\" y=\"79.445989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.425785\" y=\"38.908429\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.929806\" y=\"116.402206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.433764\" y=\"88.486938\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.993977\" y=\"81.085971\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.88151\" y=\"87.997522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.448528\" y=\"88.393808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.731191\" y=\"76.554431\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.365738\" y=\"73.519191\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.023584\" y=\"74.321666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.343054\" y=\"93.603985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.480389\" y=\"88.551763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"82.783874\" y=\"35.366294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.40635\" y=\"58.043827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.163633\" y=\"103.28369\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.402431\" y=\"57.861658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.67707\" y=\"81.847122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.810681\" y=\"77.415171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.459828\" y=\"90.031881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"74.291277\" y=\"35.628451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.353451\" y=\"92.726373\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.567219\" y=\"65.921645\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.252503\" y=\"67.827082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.357592\" y=\"73.868138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.556005\" y=\"81.497491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.588391\" y=\"57.255578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.873451\" y=\"56.665809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.53247\" y=\"72.549742\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.363962\" y=\"85.38564\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.858176\" y=\"58.074153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.627228\" y=\"51.205587\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.274568\" y=\"46.708364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.191399\" y=\"111.673949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.057851\" y=\"75.989104\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.467418\" y=\"67.739194\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.631192\" y=\"59.798297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.163912\" y=\"74.989388\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.397586\" y=\"59.375105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.25971\" y=\"59.267597\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.847122\" y=\"93.854999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.1543\" y=\"71.690158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.433433\" y=\"87.617628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.147698\" y=\"63.623364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.640488\" y=\"73.829216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.738102\" y=\"89.36582\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.302647\" y=\"94.878713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.085924\" y=\"98.631002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.452017\" y=\"69.606532\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.770517\" y=\"77.322051\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.949251\" y=\"72.458828\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"75.617311\" y=\"44.551745\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.242985\" y=\"71.231531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.456331\" y=\"87.507944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.942623\" y=\"89.154946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.84967\" y=\"115.023299\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.293778\" y=\"57.941606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.996861\" y=\"64.990028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.079155\" y=\"124.951583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.600268\" y=\"47.266292\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.097375\" y=\"82.798065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"171.793196\" y=\"84.905216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.647189\" y=\"63.342156\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.612985\" y=\"71.311502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.608414\" y=\"71.440978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.656532\" y=\"109.509315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.409678\" y=\"76.930463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.748237\" y=\"81.979596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.86734\" y=\"45.388764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.971797\" y=\"96.679841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.111144\" y=\"39.170635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.522205\" y=\"104.843924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"215.364915\" y=\"136.669454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.039242\" y=\"64.485384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.026192\" y=\"92.242205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.829908\" y=\"58.495525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.715044\" y=\"43.485036\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.536737\" y=\"66.785002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.411851\" y=\"75.943248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.084039\" y=\"105.039855\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.865947\" y=\"90.586554\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.345475\" y=\"70.239975\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.715025\" y=\"86.234712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.761775\" y=\"72.272405\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"90.937862\" y=\"42.120415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.620502\" y=\"52.733002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.71258\" y=\"91.534395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.038561\" y=\"78.277075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.303632\" y=\"80.007046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.680851\" y=\"123.495815\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.102381\" y=\"87.647679\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.983381\" y=\"68.147087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.287017\" y=\"77.973753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.728341\" y=\"102.953634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.255003\" y=\"48.117778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.176357\" y=\"79.685957\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.072006\" y=\"105.065753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.081318\" y=\"94.163776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.789688\" y=\"39.249632\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.093641\" y=\"94.996263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.020821\" y=\"110.657126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.413335\" y=\"56.029453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.839972\" y=\"95.454916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.064537\" y=\"95.256836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.861671\" y=\"79.859375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.424467\" y=\"86.897922\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.523298\" y=\"94.19315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"174.220273\" y=\"100.574648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.411392\" y=\"95.633883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.245321\" y=\"77.128793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.974341\" y=\"42.823713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.346943\" y=\"71.69238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.730968\" y=\"43.229075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.599003\" y=\"84.788153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.921167\" y=\"72.66032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.318025\" y=\"58.450722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"194.960884\" y=\"131.832798\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.851446\" y=\"54.265354\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.738185\" y=\"79.788226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.232769\" y=\"39.581108\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.609002\" y=\"74.271061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.604959\" y=\"96.641912\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.290707\" y=\"95.322537\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.920401\" y=\"37.140992\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.569027\" y=\"88.381482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.631163\" y=\"82.87251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.473124\" y=\"114.644034\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.089957\" y=\"81.887664\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"171.2269\" y=\"110.831563\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.389193\" y=\"79.688158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.145673\" y=\"72.500684\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.439791\" y=\"64.647256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.448923\" y=\"83.772278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.538471\" y=\"126.892836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.310948\" y=\"52.077508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.07802\" y=\"71.031701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.269235\" y=\"77.309642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.764291\" y=\"95.714536\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.771941\" y=\"78.662995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.05873\" y=\"52.853989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.825865\" y=\"85.024525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.04984\" y=\"65.790707\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.985694\" y=\"73.972357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.372596\" y=\"73.366803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.97065\" y=\"103.786181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.344855\" y=\"70.019699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"93.447802\" y=\"46.796222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.990588\" y=\"71.131398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.143886\" y=\"67.727624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.66859\" y=\"87.823199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.899883\" y=\"74.458643\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.576442\" y=\"119.338468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.434056\" y=\"65.114343\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.373461\" y=\"90.545777\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"74.95746\" y=\"32.072086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.379211\" y=\"89.646188\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.659058\" y=\"131.976622\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.916788\" y=\"37.519916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.311539\" y=\"68.471924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.407896\" y=\"58.213502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.98083\" y=\"67.209455\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.578836\" y=\"83.648026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.530611\" y=\"81.247013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.676106\" y=\"78.386436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"188.590076\" y=\"110.099002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.701229\" y=\"79.870857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.85927\" y=\"86.82456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.10514\" y=\"55.800128\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.751938\" y=\"95.543365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.73323\" y=\"80.716385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.229499\" y=\"49.869361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.169676\" y=\"78.313592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"181.657673\" y=\"108.965791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.523107\" y=\"89.407648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.105095\" y=\"102.732158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.184204\" y=\"130.289923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.883837\" y=\"77.297087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.828564\" y=\"77.985982\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.511402\" y=\"86.539024\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.205736\" y=\"88.497173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.420465\" y=\"82.445164\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.183605\" y=\"100.566278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.508009\" y=\"78.2606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.071365\" y=\"104.95213\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.33869\" y=\"90.006435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.497756\" y=\"66.676858\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.538572\" y=\"44.376892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.319541\" y=\"50.800963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.013391\" y=\"73.130755\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.436765\" y=\"90.054365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.851902\" y=\"88.832337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.298429\" y=\"109.265351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.81924\" y=\"105.520126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.125532\" y=\"45.042519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.520817\" y=\"71.352702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.616857\" y=\"87.712444\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.361615\" y=\"93.431903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.955365\" y=\"115.084394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.302701\" y=\"65.664837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.00782\" y=\"41.543072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.22642\" y=\"65.483171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.02213\" y=\"73.541159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.117686\" y=\"107.280884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.369294\" y=\"81.808232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.787771\" y=\"92.773059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.140729\" y=\"110.615348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.002826\" y=\"87.620506\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.425689\" y=\"95.841866\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.446106\" y=\"67.122825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.509686\" y=\"77.513579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.088532\" y=\"65.123268\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.731574\" y=\"108.43805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.832592\" y=\"59.570065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.975699\" y=\"71.318317\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.907336\" y=\"103.523987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.159527\" y=\"43.963427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.55038\" y=\"71.800816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.639005\" y=\"50.486663\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.806314\" y=\"80.491925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.015768\" y=\"101.020893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.847569\" y=\"103.960201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.777105\" y=\"64.961187\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"178.333764\" y=\"108.101154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.172839\" y=\"71.272436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.066135\" y=\"54.533553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.714613\" y=\"77.715356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.050538\" y=\"85.805685\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.294464\" y=\"84.457284\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.123238\" y=\"110.462091\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.150669\" y=\"40.894705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.184716\" y=\"69.718393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.640153\" y=\"99.022385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.863809\" y=\"47.724193\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.626844\" y=\"94.62609\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.241278\" y=\"69.007238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"178.5462\" y=\"115.545579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.599468\" y=\"50.701739\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.598236\" y=\"93.231596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.028281\" y=\"48.494835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.972378\" y=\"60.52896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"175.093025\" y=\"115.579612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.674163\" y=\"57.422106\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"81.816997\" y=\"48.033332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.268201\" y=\"92.460497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.982868\" y=\"94.537197\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.366972\" y=\"105.885217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.981166\" y=\"89.998881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.909952\" y=\"57.369795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.800072\" y=\"109.391435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.3025\" y=\"68.146646\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.112621\" y=\"77.863288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.019004\" y=\"82.309716\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.437598\" y=\"64.998791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.85551\" y=\"62.189196\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.337286\" y=\"90.072432\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.260642\" y=\"75.777629\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.736644\" y=\"88.600963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.817745\" y=\"62.371521\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.558975\" y=\"81.635209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.054067\" y=\"54.280767\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.664876\" y=\"73.743823\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.419599\" y=\"70.005569\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.615409\" y=\"67.434763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.973077\" y=\"76.827061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.851912\" y=\"83.246127\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.465775\" y=\"62.814925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.69414\" y=\"82.432944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.5567\" y=\"66.571063\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.865298\" y=\"61.460029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.895165\" y=\"84.160395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.69307\" y=\"68.818434\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.775156\" y=\"52.552853\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.131616\" y=\"95.612473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.314575\" y=\"60.307027\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.398024\" y=\"65.64585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.260952\" y=\"90.950681\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.303587\" y=\"67.75297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.26854\" y=\"53.050234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"184.810641\" y=\"106.433803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.187906\" y=\"49.907413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.298809\" y=\"81.185502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.691714\" y=\"64.86224\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.551724\" y=\"63.739985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"190.442405\" y=\"101.59375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.636939\" y=\"110.318736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"189.434988\" y=\"122.525807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"83.010608\" y=\"38.326326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.135154\" y=\"91.321178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.262778\" y=\"77.393835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.275451\" y=\"49.56985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.832572\" y=\"85.533263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.746231\" y=\"72.671207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.820777\" y=\"99.386596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.683545\" y=\"67.775845\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.196624\" y=\"74.056585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.203098\" y=\"75.661508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.907138\" y=\"61.146314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.719176\" y=\"105.44496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"90.349765\" y=\"37.429393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.196461\" y=\"78.800497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.30426\" y=\"76.243697\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.995264\" y=\"65.434945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.206289\" y=\"95.744662\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.555366\" y=\"60.535636\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.926729\" y=\"87.970989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.454368\" y=\"86.192893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.242903\" y=\"57.788411\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.740043\" y=\"75.358756\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.248791\" y=\"35.551553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.69951\" y=\"78.30334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.329754\" y=\"64.711556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.247098\" y=\"101.88973\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.406229\" y=\"97.819789\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.031335\" y=\"88.734081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.843379\" y=\"45.330227\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.434758\" y=\"44.209348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.13027\" y=\"76.632356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.58724\" y=\"108.769978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.484005\" y=\"100.102314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.946035\" y=\"88.620482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.437023\" y=\"96.788656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.015895\" y=\"60.901465\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.169374\" y=\"93.607883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.803278\" y=\"74.933473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.571926\" y=\"82.22746\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.47718\" y=\"95.62631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.262069\" y=\"60.61157\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.718515\" y=\"81.496522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.181732\" y=\"41.516842\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.331169\" y=\"88.36295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.890814\" y=\"70.030588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.969004\" y=\"78.772412\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.483839\" y=\"69.940603\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.305641\" y=\"72.17007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.064401\" y=\"108.453568\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.168083\" y=\"66.977319\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"186.070028\" y=\"121.685772\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.440592\" y=\"96.364357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.615906\" y=\"83.36262\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.334251\" y=\"77.685671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.832708\" y=\"77.021255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.584402\" y=\"81.933449\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.030805\" y=\"56.89179\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.2568\" y=\"109.930138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"81.284677\" y=\"46.477058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.833589\" y=\"107.725896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.628025\" y=\"95.836344\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.51822\" y=\"25.80849\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.05078\" y=\"83.93642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.24022\" y=\"41.798879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"176.812922\" y=\"100.699816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"206.112711\" y=\"132.059178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.459017\" y=\"61.119264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.340598\" y=\"102.003628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.568646\" y=\"90.938355\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"178.466042\" y=\"135.076353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.576191\" y=\"89.697253\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.493513\" y=\"84.102082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.957632\" y=\"70.643879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.199816\" y=\"77.271077\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"175.249178\" y=\"118.26173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.033036\" y=\"100.42625\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.798255\" y=\"85.449113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.32953\" y=\"91.51232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.504324\" y=\"98.445219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.286187\" y=\"69.827219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.735623\" y=\"96.551888\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.230718\" y=\"69.991721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.659505\" y=\"77.144345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.011388\" y=\"53.943991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.674035\" y=\"42.491325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.566322\" y=\"71.860096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.753018\" y=\"71.452624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.905282\" y=\"87.64505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.361549\" y=\"59.370096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.572454\" y=\"71.761607\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.975877\" y=\"59.617879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.789529\" y=\"76.742222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.474806\" y=\"104.341687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.264875\" y=\"68.026442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.512114\" y=\"87.193492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.937993\" y=\"69.71948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.383548\" y=\"45.774082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.136088\" y=\"64.811021\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.571522\" y=\"54.048753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.810063\" y=\"47.273691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.294015\" y=\"79.306596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.120782\" y=\"90.852565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.06859\" y=\"66.468719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.146701\" y=\"63.262394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.221559\" y=\"65.665216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.708983\" y=\"89.563288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.273493\" y=\"66.439287\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.837173\" y=\"83.170556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.803884\" y=\"65.386806\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.632525\" y=\"94.60237\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.695338\" y=\"77.253336\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.70951\" y=\"40.351655\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.687648\" y=\"79.915883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.793452\" y=\"52.793148\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.863376\" y=\"74.635042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.501331\" y=\"91.09296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.99569\" y=\"73.305529\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.416351\" y=\"88.123721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.367629\" y=\"93.357726\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.029471\" y=\"96.797956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.194088\" y=\"101.376634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.180676\" y=\"59.779791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.57167\" y=\"83.17462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.986711\" y=\"58.26251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.046832\" y=\"93.280006\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.214217\" y=\"89.448882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.969972\" y=\"92.86561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.20047\" y=\"76.563584\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.767122\" y=\"73.075454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.681444\" y=\"59.21978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.292355\" y=\"66.063946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.960186\" y=\"93.067459\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.53253\" y=\"74.32969\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"177.84401\" y=\"120.319759\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.647964\" y=\"89.083226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"75.729887\" y=\"21.716144\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.927387\" y=\"67.650078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.175365\" y=\"65.223208\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.556223\" y=\"97.346334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.029483\" y=\"110.278113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.402947\" y=\"55.04702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.987071\" y=\"36.320377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.230393\" y=\"66.727421\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.536374\" y=\"54.699249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.740495\" y=\"68.524397\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"68.20164\" y=\"18.152251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.864201\" y=\"96.415787\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.393444\" y=\"53.098791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.272119\" y=\"55.945706\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.119748\" y=\"67.417656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.167812\" y=\"50.437323\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.387767\" y=\"53.971836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.477571\" y=\"60.758656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.02508\" y=\"108.205526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.285307\" y=\"67.258656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.331823\" y=\"82.418834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.486232\" y=\"77.876968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.91161\" y=\"82.143424\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.717006\" y=\"57.152768\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.891315\" y=\"94.68598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.848712\" y=\"112.619282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.33896\" y=\"84.338058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.909578\" y=\"105.269701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.05359\" y=\"90.123526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.726965\" y=\"103.768415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.333197\" y=\"90.380013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.915242\" y=\"55.457805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.672469\" y=\"80.84138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.840902\" y=\"70.947599\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.958121\" y=\"23.162991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.110344\" y=\"77.873558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.218008\" y=\"105.443857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.779349\" y=\"91.711486\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.503636\" y=\"44.255994\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.471323\" y=\"95.849579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.37937\" y=\"83.283928\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.675286\" y=\"47.93496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"76.540277\" y=\"41.384245\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.953678\" y=\"78.467352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.887518\" y=\"50.069211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.066197\" y=\"94.835505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.852849\" y=\"108.472892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.112367\" y=\"104.72846\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.869502\" y=\"106.281367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.342459\" y=\"51.793366\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.511957\" y=\"70.717656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.192634\" y=\"98.065695\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.357245\" y=\"93.409271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.565301\" y=\"91.783085\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.027944\" y=\"60.077816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.546238\" y=\"51.08775\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.902369\" y=\"70.340151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.13645\" y=\"64.663011\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.119843\" y=\"55.93901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.90781\" y=\"54.780247\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.844246\" y=\"88.241005\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.684121\" y=\"81.15877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.593471\" y=\"81.571363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.760849\" y=\"62.661631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.407119\" y=\"69.693332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.228326\" y=\"74.384589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.196136\" y=\"98.598832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.523503\" y=\"66.33793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"206.761358\" y=\"125.79795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.095197\" y=\"98.4751\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.539013\" y=\"64.56139\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.164185\" y=\"81.606658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.702628\" y=\"98.219326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.258917\" y=\"94.831201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.037722\" y=\"104.699339\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.170226\" y=\"89.41371\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.059483\" y=\"72.784708\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.702482\" y=\"87.411776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.989185\" y=\"52.687565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"87.075942\" y=\"50.725829\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.970726\" y=\"65.130989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.883673\" y=\"63.986565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.96272\" y=\"80.834248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.150811\" y=\"81.052541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.807994\" y=\"68.298331\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.10131\" y=\"63.476082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.210835\" y=\"84.375522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.352392\" y=\"73.802402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.34687\" y=\"91.968214\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.696498\" y=\"35.232469\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.963518\" y=\"78.597462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.385956\" y=\"70.788778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.184783\" y=\"59.685314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.853204\" y=\"90.993283\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.690509\" y=\"129.760231\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.434951\" y=\"95.79107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.306675\" y=\"49.856484\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.074512\" y=\"86.650945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.928025\" y=\"84.532026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.962191\" y=\"63.882353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.203076\" y=\"56.071804\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.794781\" y=\"91.749217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.275888\" y=\"88.082351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.2505\" y=\"105.130473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.045802\" y=\"51.890282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.08613\" y=\"58.570508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.192583\" y=\"59.913878\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.382741\" y=\"77.805222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.026704\" y=\"88.190008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.660371\" y=\"52.29818\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.102869\" y=\"74.407627\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.28041\" y=\"86.544145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.533615\" y=\"97.2812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.039942\" y=\"47.401588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.798571\" y=\"72.467508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.605425\" y=\"117.705185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.117282\" y=\"77.578839\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.863978\" y=\"68.35539\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.916677\" y=\"67.610349\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"70.69861\" y=\"29.829656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.49309\" y=\"101.365519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.800132\" y=\"65.258895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.875062\" y=\"77.986457\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"37.81946\" y=\"22.041043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.9688\" y=\"82.919953\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"184.717724\" y=\"139.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.23157\" y=\"93.679042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"177.22424\" y=\"110.310029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.395139\" y=\"96.225358\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.573684\" y=\"51.327124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.758672\" y=\"58.736199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.570633\" y=\"66.047748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.098949\" y=\"80.562145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"180.18112\" y=\"122.612181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.484323\" y=\"72.588527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.19446\" y=\"68.419977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.194549\" y=\"69.203448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.39048\" y=\"99.604671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.836511\" y=\"95.278895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.276288\" y=\"116.960095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.729036\" y=\"62.502891\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.406236\" y=\"75.34703\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.079402\" y=\"65.876611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.053701\" y=\"59.436446\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.071025\" y=\"94.729298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.838797\" y=\"65.936659\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.557707\" y=\"66.413173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.492099\" y=\"71.617763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.297919\" y=\"65.554642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.249566\" y=\"50.537598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.751482\" y=\"60.66267\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.829179\" y=\"85.850677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.804102\" y=\"69.59409\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.009757\" y=\"33.39075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.889071\" y=\"84.740389\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"92.930904\" y=\"56.896367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.109569\" y=\"80.144903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.085701\" y=\"78.820328\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.607475\" y=\"90.114192\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.197228\" y=\"90.689906\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.888189\" y=\"86.676935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.803833\" y=\"100.273606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.385509\" y=\"71.28935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"168.428015\" y=\"93.395327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.227251\" y=\"75.908799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"83.610668\" y=\"47.334452\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"77.11768\" y=\"42.681032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.21452\" y=\"68.254932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"194.397655\" y=\"121.483361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.216604\" y=\"92.04012\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.076437\" y=\"84.291729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.395224\" y=\"80.507211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"189.387072\" y=\"111.456825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.094041\" y=\"70.558834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.844024\" y=\"99.113743\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.045958\" y=\"79.203896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.793533\" y=\"43.082933\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.747365\" y=\"49.999154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.860075\" y=\"80.905868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.956165\" y=\"53.759361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"106.715826\" y=\"57.926678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.062914\" y=\"30.864672\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.261816\" y=\"84.073393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.33017\" y=\"85.092948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.086697\" y=\"59.609608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.664584\" y=\"85.599525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.832551\" y=\"37.233837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.292719\" y=\"57.501901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"197.885911\" y=\"136.476002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.417242\" y=\"93.124352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.590631\" y=\"79.8113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.826924\" y=\"62.819942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.21641\" y=\"81.136541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.116507\" y=\"106.779282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.198812\" y=\"73.861437\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.610014\" y=\"107.285008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.8329\" y=\"86.318162\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"85.447827\" y=\"41.339427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.662964\" y=\"98.494541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.15241\" y=\"55.854203\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.675945\" y=\"74.743863\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.303793\" y=\"96.890186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.650283\" y=\"68.410737\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.823973\" y=\"86.884353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.11893\" y=\"48.318827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.12326\" y=\"82.182124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"86.71922\" y=\"43.483263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.301609\" y=\"72.569612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.466356\" y=\"72.806161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.890497\" y=\"47.114905\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"69.84394\" y=\"49.194621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.667698\" y=\"66.412408\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.492479\" y=\"105.564466\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.887408\" y=\"100.306264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.067891\" y=\"106.950633\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.148809\" y=\"64.954008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.429482\" y=\"90.1105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.664646\" y=\"86.995488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.867215\" y=\"62.85089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.549793\" y=\"93.940841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.053745\" y=\"79.150901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.650576\" y=\"68.84641\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.230696\" y=\"101.662611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.701619\" y=\"49.77235\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.691186\" y=\"69.579363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.18138\" y=\"111.892028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.551708\" y=\"60.302929\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.733292\" y=\"103.65314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"58.010948\" y=\"15.765491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.198177\" y=\"94.586921\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.120971\" y=\"54.787315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.041368\" y=\"83.67882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.275511\" y=\"68.62897\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.086569\" y=\"82.079448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.750245\" y=\"67.164617\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.120777\" y=\"97.354337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.347228\" y=\"90.121634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.348334\" y=\"79.292481\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.444262\" y=\"61.17923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.839766\" y=\"64.295246\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.445458\" y=\"116.605219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.649238\" y=\"82.715272\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.715663\" y=\"92.86394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.159696\" y=\"70.888666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"65.476058\" y=\"13.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.099375\" y=\"69.149559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.263962\" y=\"42.032186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.166695\" y=\"57.837107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.732869\" y=\"75.377153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.679882\" y=\"131.895592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.939425\" y=\"59.141719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.714627\" y=\"111.353278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.763694\" y=\"96.236795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.906559\" y=\"64.276207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.604701\" y=\"60.290799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.973405\" y=\"106.724808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.629961\" y=\"85.050771\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.981322\" y=\"107.254549\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"180.513474\" y=\"111.184544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.836516\" y=\"95.462359\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.914484\" y=\"67.259161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.479348\" y=\"70.232714\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"96.272523\" y=\"49.957704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.135323\" y=\"66.102736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.628174\" y=\"83.945404\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.519095\" y=\"77.177288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.061573\" y=\"76.923153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.333124\" y=\"90.314048\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.929194\" y=\"62.515236\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.347144\" y=\"59.281458\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.947892\" y=\"101.318345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.997123\" y=\"64.226513\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.902053\" y=\"102.041242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.913735\" y=\"77.644053\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.177522\" y=\"91.387142\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.664466\" y=\"27.810463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.855771\" y=\"81.917821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.229796\" y=\"89.348764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"79.634661\" y=\"35.092983\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.891549\" y=\"65.048166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"93.968327\" y=\"43.871166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.907416\" y=\"75.364399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.486247\" y=\"76.088841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.810386\" y=\"72.693093\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.978057\" y=\"83.175531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.110827\" y=\"79.505546\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.016312\" y=\"100.975342\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.702803\" y=\"124.807206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.412038\" y=\"90.983812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.235486\" y=\"83.077619\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.534762\" y=\"83.761282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.610104\" y=\"87.596558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.090768\" y=\"84.604151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"187.20176\" y=\"116.228875\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.439972\" y=\"77.64884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.274607\" y=\"113.955505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.646856\" y=\"68.287306\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.591017\" y=\"71.528892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.994477\" y=\"74.031821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.041914\" y=\"98.097999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.900527\" y=\"48.35415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.638862\" y=\"86.280172\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.804202\" y=\"67.597294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.845581\" y=\"77.180758\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.671126\" y=\"64.412474\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"174.310451\" y=\"112.626968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.847771\" y=\"95.91145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.881409\" y=\"79.49606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.306849\" y=\"84.052712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.250617\" y=\"83.414741\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.994616\" y=\"75.428867\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.067052\" y=\"78.310834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.191582\" y=\"70.326748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.147501\" y=\"73.301386\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.13053\" y=\"71.147066\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"167.426643\" y=\"99.081935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.168215\" y=\"78.237334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.189183\" y=\"89.874683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.419576\" y=\"74.284852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.419249\" y=\"72.372303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.053958\" y=\"75.555399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.359279\" y=\"121.223819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.815822\" y=\"80.050575\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.911547\" y=\"78.240407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.923255\" y=\"75.147613\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.133718\" y=\"102.542952\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.329025\" y=\"87.033987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"93.422504\" y=\"45.301327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.517571\" y=\"91.822422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.024305\" y=\"76.642438\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.614931\" y=\"77.949601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"64.970981\" y=\"30.519615\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.081606\" y=\"73.579209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.838706\" y=\"82.110649\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.292935\" y=\"79.188468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.982502\" y=\"104.574014\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.096378\" y=\"61.68454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.129246\" y=\"62.090174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.758126\" y=\"64.382054\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.906803\" y=\"114.576675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.911888\" y=\"93.266282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.247932\" y=\"45.160321\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.519383\" y=\"104.21218\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.846885\" y=\"71.754109\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.148961\" y=\"68.087322\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.615576\" y=\"81.411749\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"90.316396\" y=\"50.130527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"95.132399\" y=\"50.705691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.188074\" y=\"63.967413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.547703\" y=\"57.232677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.194564\" y=\"84.365181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.661788\" y=\"77.832046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.034306\" y=\"113.094996\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.93483\" y=\"91.140991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.505665\" y=\"105.741195\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"105.334975\" y=\"68.392803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.794785\" y=\"75.12353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.469321\" y=\"90.649089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.66032\" y=\"104.070657\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.906548\" y=\"63.468873\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.338784\" y=\"82.86794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.549296\" y=\"85.608542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.546588\" y=\"97.206753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.718311\" y=\"68.260558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.147706\" y=\"56.226981\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"97.842273\" y=\"60.232185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.668063\" y=\"51.873542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"84.617334\" y=\"18.103516\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.451103\" y=\"93.246023\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.265351\" y=\"73.027207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.902407\" y=\"43.075637\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.954757\" y=\"67.577574\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.89097\" y=\"79.029583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"187.621362\" y=\"119.915368\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.612109\" y=\"83.74487\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.686826\" y=\"76.370095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.799684\" y=\"92.48442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.290279\" y=\"49.338792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.838858\" y=\"100.184448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.269882\" y=\"97.096052\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.038635\" y=\"67.031251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.041575\" y=\"87.363325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"84.014338\" y=\"21.562226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.515553\" y=\"69.816669\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.565861\" y=\"78.500167\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.95154\" y=\"89.430395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"145.14065\" y=\"91.456843\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"103.202022\" y=\"56.366234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.22455\" y=\"64.606874\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"131.958635\" y=\"78.651995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.252548\" y=\"72.41565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.756206\" y=\"107.662652\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.515326\" y=\"99.172621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"162.706676\" y=\"96.370217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.105019\" y=\"79.02333\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.779544\" y=\"89.016402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.200818\" y=\"104.754022\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.632423\" y=\"96.801986\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"172.282279\" y=\"111.927308\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.18855\" y=\"82.793202\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.458103\" y=\"41.859056\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.123526\" y=\"93.851658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.584503\" y=\"60.980031\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.909372\" y=\"74.145029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.550977\" y=\"94.163492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.472081\" y=\"68.159095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.929361\" y=\"102.06078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.826549\" y=\"57.863059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.134436\" y=\"85.641259\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.168939\" y=\"94.75956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"169.293359\" y=\"112.293427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"144.506265\" y=\"90.257794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.356788\" y=\"65.223327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.398334\" y=\"83.79076\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.570078\" y=\"75.850764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.328916\" y=\"55.738297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.610495\" y=\"72.706857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"115.378193\" y=\"78.429072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.583122\" y=\"73.384083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.425629\" y=\"88.150926\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.353432\" y=\"84.380879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.209797\" y=\"89.189101\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.685341\" y=\"62.18428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.618398\" y=\"71.018209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.333136\" y=\"70.038488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.322245\" y=\"72.531255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.823646\" y=\"74.393081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"98.608225\" y=\"53.162821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"202.783655\" y=\"121.830153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.99309\" y=\"80.320939\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"104.391942\" y=\"64.345249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.876304\" y=\"80.548704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.58977\" y=\"63.643561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.906441\" y=\"81.658512\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.266019\" y=\"101.152384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.281286\" y=\"66.060699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.366461\" y=\"92.592949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.516261\" y=\"67.56122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.92194\" y=\"90.036556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.144481\" y=\"57.539528\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.135178\" y=\"79.329016\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.908034\" y=\"78.861112\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"137.491266\" y=\"83.921422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"113.807697\" y=\"75.450786\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.051651\" y=\"66.182709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"166.019487\" y=\"98.983666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.111516\" y=\"65.131705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.162638\" y=\"69.319809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.660903\" y=\"78.456473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"91.734064\" y=\"51.867579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"109.351045\" y=\"82.685242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.6349\" y=\"73.055611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.068739\" y=\"84.355827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.63787\" y=\"87.599534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.409568\" y=\"93.319693\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.704529\" y=\"67.56309\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"101.300494\" y=\"63.42464\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"120.499743\" y=\"48.451007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.69255\" y=\"55.90116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.128581\" y=\"62.556877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.883643\" y=\"67.885119\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.241836\" y=\"77.585043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.797919\" y=\"87.156956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.0616\" y=\"58.183763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.015626\" y=\"82.54097\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.302346\" y=\"63.575948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.302998\" y=\"76.863967\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"155.911611\" y=\"99.133675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.123813\" y=\"66.921095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.865968\" y=\"52.645248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.78829\" y=\"79.424332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"110.709863\" y=\"63.33256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.644056\" y=\"39.476263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"124.256625\" y=\"86.515271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.738507\" y=\"99.654722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.612702\" y=\"91.443661\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.821672\" y=\"93.766817\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"119.388459\" y=\"72.543665\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.919281\" y=\"96.980838\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.278235\" y=\"93.142453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"179.94484\" y=\"130.32799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"89.820298\" y=\"37.928601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.650908\" y=\"62.978298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.414425\" y=\"73.937807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.594095\" y=\"88.439678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"170.995955\" y=\"109.732608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.47018\" y=\"97.713903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"91.416199\" y=\"51.397677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.637216\" y=\"69.78173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"135.319177\" y=\"71.655387\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.374822\" y=\"74.834836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.427259\" y=\"52.43887\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.449895\" y=\"78.713174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.719218\" y=\"70.531854\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.072781\" y=\"61.076881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.88235\" y=\"74.547729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"88.217137\" y=\"52.674687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.762105\" y=\"59.810942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"117.558407\" y=\"61.533266\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"165.053974\" y=\"105.538605\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"134.286644\" y=\"83.827882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.266477\" y=\"73.022932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.574237\" y=\"72.99407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"102.381512\" y=\"64.55689\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.693437\" y=\"83.376626\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"154.460302\" y=\"91.934398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.00611\" y=\"85.236683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.702211\" y=\"80.325857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"94.346003\" y=\"53.38592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"125.269631\" y=\"86.51086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.152682\" y=\"68.541043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.952226\" y=\"100.24009\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"171.111968\" y=\"118.103977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.53369\" y=\"90.561111\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.717881\" y=\"69.697515\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.815171\" y=\"62.456778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.308034\" y=\"94.957518\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"157.427407\" y=\"92.102869\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"116.582537\" y=\"66.853116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.54365\" y=\"90.435422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"123.207181\" y=\"71.333254\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"151.161298\" y=\"103.614709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"158.907013\" y=\"98.96007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.818717\" y=\"81.294635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"141.449835\" y=\"81.23083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.090995\" y=\"102.947303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"150.379022\" y=\"79.235428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.079829\" y=\"82.178295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.273181\" y=\"91.059508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.334522\" y=\"86.045153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"100.770433\" y=\"65.405451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"108.552052\" y=\"69.00258\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.765937\" y=\"82.574475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"161.607644\" y=\"115.920234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.087753\" y=\"89.551559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.319823\" y=\"77.133567\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"132.731657\" y=\"68.029976\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.131994\" y=\"84.591819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"160.73471\" y=\"95.674044\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"128.18093\" y=\"83.951852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"118.339812\" y=\"48.107719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"126.868383\" y=\"91.144692\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"111.915862\" y=\"77.966158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"159.038497\" y=\"101.867718\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"121.953976\" y=\"69.104578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"147.381854\" y=\"95.437832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"163.145448\" y=\"91.398395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"140.161531\" y=\"82.985289\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.390359\" y=\"113.470475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"139.705958\" y=\"87.582783\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"122.643029\" y=\"80.594545\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.61515\" y=\"75.882131\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"129.371491\" y=\"83.48542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"136.091775\" y=\"75.831255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"164.084409\" y=\"84.442152\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"107.313126\" y=\"66.885205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"138.356645\" y=\"87.367559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"112.569709\" y=\"75.389946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"99.88968\" y=\"62.102534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"130.395297\" y=\"89.780347\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.960033\" y=\"73.903159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"156.651549\" y=\"115.759169\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"149.600207\" y=\"99.445677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"173.972016\" y=\"100.660773\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"146.195861\" y=\"95.951702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"143.305644\" y=\"91.992748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"142.934713\" y=\"93.511792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"148.517035\" y=\"93.644631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"114.15317\" y=\"39.124589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"152.259368\" y=\"89.347345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"127.762384\" y=\"69.862951\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"153.40995\" y=\"101.115803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m18bb6ce550\" x=\"133.643811\" y=\"68.037402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m2cbfe66e94\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"37.108605\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −4 -->\n",
       "      <g transform=\"translate(29.737511 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"84.839648\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(77.468554 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"132.57069\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(129.38944 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cbfe66e94\" x=\"180.301733\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(177.120483 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <defs>\n",
       "       <path id=\"m67ee9f9fc6\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"125.71812\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- −5 -->\n",
       "      <g transform=\"translate(7.2 129.517339) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"99.751174\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(15.579688 103.550393) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"73.784228\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(15.579688 77.583446) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"47.817281\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(9.217188 51.6165) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m67ee9f9fc6\" x=\"28.942188\" y=\"21.850335\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(9.217188 25.649554) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 28.942188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 28.942188 7.2 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p149bb96678\">\n",
       "   <rect x=\"28.942188\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘图，查看分布\n",
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:, (1)].detach().numpy(), \n",
    "                labels.detach().numpy(), 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"231.442187pt\" height=\"169.678125pt\" viewBox=\"0 0 231.442187 169.678125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-05-18T15:08:56.251352</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 169.678125 \n",
       "L 231.442187 169.678125 \n",
       "L 231.442187 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "L 28.942188 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m823589f946\" d=\"M 0 0.5 \n",
       "C 0.132602 0.5 0.25979 0.447317 0.353553 0.353553 \n",
       "C 0.447317 0.25979 0.5 0.132602 0.5 0 \n",
       "C 0.5 -0.132602 0.447317 -0.25979 0.353553 -0.353553 \n",
       "C 0.25979 -0.447317 0.132602 -0.5 0 -0.5 \n",
       "C -0.132602 -0.5 -0.25979 -0.447317 -0.353553 -0.353553 \n",
       "C -0.447317 -0.25979 -0.5 -0.132602 -0.5 0 \n",
       "C -0.5 0.132602 -0.447317 0.25979 -0.353553 0.353553 \n",
       "C -0.25979 0.447317 -0.132602 0.5 0 0.5 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p4610e2cf3f)\">\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.049598\" y=\"47.161377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.963107\" y=\"83.62544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.927232\" y=\"87.238868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.499187\" y=\"40.397538\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.738829\" y=\"74.594754\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"168.177498\" y=\"60.945456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.120588\" y=\"78.7769\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.100157\" y=\"79.445989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"167.203315\" y=\"38.908429\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"66.24924\" y=\"116.402206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"96.90644\" y=\"88.486938\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.287288\" y=\"81.085971\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.367315\" y=\"87.997522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.325299\" y=\"88.393808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.647682\" y=\"76.554431\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.127695\" y=\"73.519191\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.422087\" y=\"74.321666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.875464\" y=\"93.603985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.86115\" y=\"88.551763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.341229\" y=\"35.366294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.2019\" y=\"58.043827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.565749\" y=\"103.28369\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.72149\" y=\"57.861658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.517474\" y=\"81.847122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.731428\" y=\"77.415171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.879391\" y=\"90.031881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.688161\" y=\"35.628451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.386178\" y=\"92.726373\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.192063\" y=\"65.921645\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.646121\" y=\"67.827082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.295666\" y=\"73.868138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.981588\" y=\"81.497491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.834866\" y=\"57.255578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.896673\" y=\"56.665809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.227189\" y=\"72.549742\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.329422\" y=\"85.38564\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.058958\" y=\"58.074153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.342288\" y=\"51.205587\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.373088\" y=\"46.708364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"77.803974\" y=\"111.673949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.240795\" y=\"75.989104\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.312012\" y=\"67.739194\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.926694\" y=\"59.798297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.913672\" y=\"74.989388\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.635024\" y=\"59.375105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.512233\" y=\"59.267597\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.543894\" y=\"93.854999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.832725\" y=\"71.690158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.390046\" y=\"87.617628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.727296\" y=\"63.623364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.260721\" y=\"73.829216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.220665\" y=\"89.36582\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.38085\" y=\"94.878713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.725452\" y=\"98.631002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.94471\" y=\"69.606532\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.676253\" y=\"77.322051\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.217505\" y=\"72.458828\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.958828\" y=\"44.551745\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.123908\" y=\"71.231531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.629155\" y=\"87.507944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.86245\" y=\"89.154946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.219201\" y=\"115.023299\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"189.643748\" y=\"57.941606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.691932\" y=\"64.990028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.878438\" y=\"124.951583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.820831\" y=\"47.266292\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.795653\" y=\"82.798065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"185.46487\" y=\"84.905216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.289184\" y=\"63.342156\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.790243\" y=\"71.311502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.555683\" y=\"71.440978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"96.680579\" y=\"109.509315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.142542\" y=\"76.930463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.204389\" y=\"81.979596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.270455\" y=\"45.388764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"37.81946\" y=\"96.679841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"187.724045\" y=\"39.170635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.818021\" y=\"104.843924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.548633\" y=\"136.669454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.098167\" y=\"64.485384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"69.900495\" y=\"92.242205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.078741\" y=\"58.495525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.047477\" y=\"43.485036\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.94966\" y=\"66.785002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"170.966781\" y=\"75.943248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.77618\" y=\"105.039855\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.366653\" y=\"90.586554\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.319769\" y=\"70.239975\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.740655\" y=\"86.234712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.564353\" y=\"72.272405\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.289525\" y=\"42.120415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.977459\" y=\"52.733002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.326608\" y=\"91.534395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.793803\" y=\"78.277075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.637716\" y=\"80.007046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.535262\" y=\"123.495815\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.666409\" y=\"87.647679\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.243464\" y=\"68.147087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.233118\" y=\"77.973753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.615171\" y=\"102.953634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.83466\" y=\"48.117778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.245982\" y=\"79.685957\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.339264\" y=\"105.065753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.043811\" y=\"94.163776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.807544\" y=\"39.249632\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.697482\" y=\"94.996263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.091272\" y=\"110.657126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.549356\" y=\"56.029453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.928383\" y=\"95.454916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.897108\" y=\"95.256836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.102496\" y=\"79.859375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"51.276685\" y=\"86.897922\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.651787\" y=\"94.19315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.784394\" y=\"100.574648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.524805\" y=\"95.633883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"61.405454\" y=\"77.128793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"165.479099\" y=\"42.823713\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.107527\" y=\"71.69238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.024353\" y=\"43.229075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.383267\" y=\"84.788153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.958005\" y=\"72.66032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.9598\" y=\"58.450722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.732142\" y=\"131.832798\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"204.588149\" y=\"54.265354\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.193145\" y=\"79.788226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.838601\" y=\"39.581108\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.481894\" y=\"74.271061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.181891\" y=\"96.641912\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.217305\" y=\"95.322537\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.981023\" y=\"37.140992\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.457846\" y=\"88.381482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.444136\" y=\"82.87251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.320171\" y=\"114.644034\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.182927\" y=\"81.887664\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.052671\" y=\"110.831563\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.614091\" y=\"79.688158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.600058\" y=\"72.500684\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.438867\" y=\"64.647256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.804829\" y=\"83.772278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"63.421576\" y=\"126.892836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.066561\" y=\"52.077508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.440322\" y=\"71.031701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.984559\" y=\"77.309642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.146232\" y=\"95.714536\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.036364\" y=\"78.662995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.580275\" y=\"52.853989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.086729\" y=\"85.024525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.583032\" y=\"65.790707\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.297572\" y=\"73.972357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.872947\" y=\"73.366803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.728672\" y=\"103.786181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.589504\" y=\"70.019699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.80647\" y=\"46.796222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.874851\" y=\"71.131398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"170.34891\" y=\"67.727624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.207703\" y=\"87.823199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.000766\" y=\"74.458643\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"60.052055\" y=\"119.338468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.077031\" y=\"65.114343\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.30747\" y=\"90.545777\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.660782\" y=\"32.072086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.63782\" y=\"89.646188\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"74.008351\" y=\"131.976622\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.802539\" y=\"37.519916\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.031993\" y=\"68.471924\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"160.842431\" y=\"58.213502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.530854\" y=\"67.209455\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.319874\" y=\"83.648026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.749716\" y=\"81.247013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.943817\" y=\"78.386436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.952825\" y=\"110.099002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.80105\" y=\"79.870857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.17755\" y=\"86.82456\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.775787\" y=\"55.800128\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.787785\" y=\"95.543365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.680858\" y=\"80.716385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.31618\" y=\"49.869361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.986793\" y=\"78.313592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.027008\" y=\"108.965791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.632561\" y=\"89.407648\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.120077\" y=\"102.732158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"55.340667\" y=\"130.289923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.168331\" y=\"77.297087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.052556\" y=\"77.985982\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.276859\" y=\"86.539024\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.447145\" y=\"88.497173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.160475\" y=\"82.445164\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"77.770008\" y=\"100.566278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.059892\" y=\"78.2606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.837459\" y=\"104.95213\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.805963\" y=\"90.006435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.199906\" y=\"66.676858\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.479051\" y=\"44.376892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.643194\" y=\"50.800963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.651571\" y=\"73.130755\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.739921\" y=\"90.054365\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.782681\" y=\"88.832337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.722082\" y=\"109.265351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.765036\" y=\"105.520126\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.92917\" y=\"45.042519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.462902\" y=\"71.352702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.305053\" y=\"87.712444\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.404515\" y=\"93.431903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.110943\" y=\"115.084394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.583832\" y=\"65.664837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.929312\" y=\"41.543072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.699533\" y=\"65.483171\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.61974\" y=\"73.541159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.611058\" y=\"107.280884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.971819\" y=\"81.808232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.09323\" y=\"92.773059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"80.631611\" y=\"110.615348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.394381\" y=\"87.620506\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.171816\" y=\"95.841866\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.954207\" y=\"67.122825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.024508\" y=\"77.513579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.642053\" y=\"65.123268\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.571848\" y=\"108.43805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"187.924902\" y=\"59.570065\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.405573\" y=\"71.318317\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.291421\" y=\"103.523987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.894751\" y=\"43.963427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.396273\" y=\"71.800816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.090681\" y=\"50.486663\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.573461\" y=\"80.491925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.203491\" y=\"101.020893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.981394\" y=\"103.960201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.084978\" y=\"64.961187\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.747365\" y=\"108.101154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.659146\" y=\"71.272436\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.896406\" y=\"54.533553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.622954\" y=\"77.715356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.506828\" y=\"85.805685\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.689788\" y=\"84.457284\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.744492\" y=\"110.462091\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.914233\" y=\"40.894705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.029819\" y=\"69.718393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.924367\" y=\"99.022385\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.206793\" y=\"47.724193\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.371572\" y=\"94.62609\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.283811\" y=\"69.007238\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.098857\" y=\"115.545579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"187.707201\" y=\"50.701739\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.111841\" y=\"93.231596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.278095\" y=\"48.494835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.721283\" y=\"60.52896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.206497\" y=\"115.579612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.264923\" y=\"57.422106\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.301552\" y=\"48.033332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.042311\" y=\"92.460497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.554385\" y=\"94.537197\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.620078\" y=\"105.885217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.920391\" y=\"89.998881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.870621\" y=\"57.369795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.159061\" y=\"109.391435\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.698184\" y=\"68.146646\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.944293\" y=\"77.863288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.880527\" y=\"82.309716\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.515502\" y=\"64.998791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.94588\" y=\"62.189196\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"82.297836\" y=\"90.072432\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.01675\" y=\"75.777629\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.402728\" y=\"88.600963\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.38606\" y=\"62.371521\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.955738\" y=\"81.635209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.671034\" y=\"54.280767\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.351852\" y=\"73.743823\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.858871\" y=\"70.005569\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.11235\" y=\"67.434763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.694098\" y=\"76.827061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.895866\" y=\"83.246127\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.46799\" y=\"62.814925\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.1394\" y=\"82.432944\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.65086\" y=\"66.571063\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.977175\" y=\"61.460029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.67534\" y=\"84.160395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.576162\" y=\"68.818434\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.764923\" y=\"52.552853\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.268663\" y=\"95.612473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.919849\" y=\"60.307027\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.769421\" y=\"65.64585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.102148\" y=\"90.950681\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.715173\" y=\"67.75297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.335523\" y=\"53.050234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.15796\" y=\"106.433803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.824974\" y=\"49.907413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.53108\" y=\"81.185502\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.397239\" y=\"64.86224\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.153422\" y=\"63.739985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"177.608589\" y=\"101.59375\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.390429\" y=\"110.318736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.027687\" y=\"122.525807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.667361\" y=\"38.326326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.64613\" y=\"91.321178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"96.587818\" y=\"77.393835\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.031757\" y=\"49.56985\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.687921\" y=\"85.533263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.260207\" y=\"72.671207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.050953\" y=\"99.386596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.476967\" y=\"67.775845\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.306363\" y=\"74.056585\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.989877\" y=\"75.661508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.810489\" y=\"61.146314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.631908\" y=\"105.44496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.028653\" y=\"37.429393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.080218\" y=\"78.800497\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.75805\" y=\"76.243697\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.333868\" y=\"65.434945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.884273\" y=\"95.744662\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.75585\" y=\"60.535636\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.694347\" y=\"87.970989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.355425\" y=\"86.192893\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.322212\" y=\"57.788411\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"169.078373\" y=\"75.358756\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"203.677167\" y=\"35.551553\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.900537\" y=\"78.30334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.003074\" y=\"64.711556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.375884\" y=\"101.88973\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.776485\" y=\"97.819789\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.548848\" y=\"88.734081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.623095\" y=\"45.330227\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"166.504097\" y=\"44.209348\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.490532\" y=\"76.632356\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"42.223604\" y=\"108.769978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"79.483809\" y=\"100.102314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.557705\" y=\"88.620482\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.493029\" y=\"96.788656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.602751\" y=\"60.901465\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.314864\" y=\"93.607883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.405215\" y=\"74.933473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.2544\" y=\"82.22746\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.82136\" y=\"95.62631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.820357\" y=\"60.61157\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.426303\" y=\"81.496522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.750114\" y=\"41.516842\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.995442\" y=\"88.36295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.801026\" y=\"70.030588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.461677\" y=\"78.772412\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.195137\" y=\"69.940603\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.935805\" y=\"72.17007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"76.383706\" y=\"108.453568\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.861479\" y=\"66.977319\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.519279\" y=\"121.685772\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.620947\" y=\"96.364357\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.402655\" y=\"83.36262\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.037137\" y=\"77.685671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.506589\" y=\"77.021255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"177.185268\" y=\"81.933449\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.590134\" y=\"56.89179\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.6819\" y=\"109.930138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.241482\" y=\"46.477058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.572872\" y=\"107.725896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.637728\" y=\"95.836344\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"212.651217\" y=\"25.80849\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.587829\" y=\"83.93642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"180.754959\" y=\"41.798879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.633346\" y=\"100.699816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.661397\" y=\"132.059178\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.076599\" y=\"61.119264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.355169\" y=\"102.003628\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.63579\" y=\"90.938355\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"62.976754\" y=\"135.076353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.202629\" y=\"89.697253\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.238987\" y=\"84.102082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"168.109165\" y=\"70.643879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.793745\" y=\"77.271077\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.972281\" y=\"118.26173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.883808\" y=\"100.42625\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.039785\" y=\"85.449113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.66428\" y=\"91.51232\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.241492\" y=\"98.445219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.03768\" y=\"69.827219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"75.822165\" y=\"96.551888\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.904047\" y=\"69.991721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.273474\" y=\"77.144345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.75058\" y=\"53.943991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.731207\" y=\"42.491325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.123343\" y=\"71.860096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.473095\" y=\"71.452624\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"76.20589\" y=\"87.64505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.625086\" y=\"59.370096\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.623662\" y=\"71.761607\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.872865\" y=\"59.617879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.328513\" y=\"76.742222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.067496\" y=\"104.341687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.101\" y=\"68.026442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.540093\" y=\"87.193492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.715822\" y=\"69.71948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.45679\" y=\"45.774082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.390465\" y=\"64.811021\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.321209\" y=\"54.048753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.308722\" y=\"47.273691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.437076\" y=\"79.306596\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.410571\" y=\"90.852565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.786144\" y=\"66.468719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.533711\" y=\"63.262394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.389089\" y=\"65.665216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.792984\" y=\"89.563288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.134847\" y=\"66.439287\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.283384\" y=\"83.170556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.321473\" y=\"65.386806\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.674786\" y=\"94.60237\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.015272\" y=\"77.253336\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.597825\" y=\"40.351655\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.524061\" y=\"79.915883\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.986621\" y=\"52.793148\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.451529\" y=\"74.635042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.319855\" y=\"91.09296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.287857\" y=\"73.305529\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.69111\" y=\"88.123721\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.736298\" y=\"93.357726\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.638866\" y=\"96.797956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"63.749778\" y=\"101.376634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.916784\" y=\"59.779791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.478063\" y=\"83.17462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.579643\" y=\"58.26251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.260889\" y=\"93.280006\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.856746\" y=\"89.448882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.132326\" y=\"92.86561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.64914\" y=\"76.563584\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.98032\" y=\"73.075454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.448037\" y=\"59.21978\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.196027\" y=\"66.063946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.623498\" y=\"93.067459\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"167.48427\" y=\"74.32969\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.613633\" y=\"120.319759\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.849133\" y=\"89.083226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.136872\" y=\"21.716144\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.194044\" y=\"67.650078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.50902\" y=\"65.223208\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.345405\" y=\"97.346334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.290961\" y=\"110.278113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.149373\" y=\"55.04702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.172144\" y=\"36.320377\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.482255\" y=\"66.727421\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.525141\" y=\"54.699249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.40025\" y=\"68.524397\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.578988\" y=\"18.152251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.351455\" y=\"96.415787\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.619062\" y=\"53.098791\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.703068\" y=\"55.945706\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.422522\" y=\"67.417656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"179.246541\" y=\"50.437323\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.299476\" y=\"53.971836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.247584\" y=\"60.758656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.829639\" y=\"108.205526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.127188\" y=\"67.258656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.219419\" y=\"82.418834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.733263\" y=\"77.876968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.098952\" y=\"82.143424\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.306709\" y=\"57.152768\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.29035\" y=\"94.68598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"56.441672\" y=\"112.619282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.18383\" y=\"84.338058\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.610361\" y=\"105.269701\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.990516\" y=\"90.123526\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.297022\" y=\"103.768415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.657461\" y=\"90.380013\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.048586\" y=\"55.457805\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.176953\" y=\"80.84138\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.15855\" y=\"70.947599\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"188.818438\" y=\"23.162991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.81954\" y=\"77.873558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"52.788866\" y=\"105.443857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.073555\" y=\"91.711486\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.648184\" y=\"44.255994\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.24535\" y=\"95.849579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.719859\" y=\"83.283928\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.82831\" y=\"47.93496\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.600949\" y=\"41.384245\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"170.925208\" y=\"78.467352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.491491\" y=\"50.069211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.285578\" y=\"94.835505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.612509\" y=\"108.472892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.57265\" y=\"104.72846\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.8394\" y=\"106.281367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.025283\" y=\"51.793366\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.112622\" y=\"70.717656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.392245\" y=\"98.065695\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.588362\" y=\"93.409271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.28\" y=\"91.783085\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.005108\" y=\"60.077816\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.487075\" y=\"51.08775\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.931134\" y=\"70.340151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.782326\" y=\"64.663011\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.487126\" y=\"55.93901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.874491\" y=\"54.780247\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.387538\" y=\"88.241005\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.230036\" y=\"81.15877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.805315\" y=\"81.571363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.518051\" y=\"62.661631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.912117\" y=\"69.693332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.50853\" y=\"74.384589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.022979\" y=\"98.598832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.308852\" y=\"66.33793\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.66247\" y=\"125.79795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.230846\" y=\"98.4751\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.927235\" y=\"64.56139\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.136891\" y=\"81.606658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.011061\" y=\"98.219326\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.433355\" y=\"94.831201\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.01999\" y=\"104.699339\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.124803\" y=\"89.41371\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.731162\" y=\"72.784708\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.449115\" y=\"87.411776\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.901989\" y=\"52.687565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.230845\" y=\"50.725829\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.264013\" y=\"65.130989\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.363999\" y=\"63.986565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.755118\" y=\"80.834248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.336992\" y=\"81.052541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.442112\" y=\"68.298331\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.985402\" y=\"63.476082\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.662602\" y=\"84.375522\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.727325\" y=\"73.802402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"91.581023\" y=\"91.968214\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"183.538607\" y=\"35.232469\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.842907\" y=\"78.597462\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.752634\" y=\"70.788778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.254133\" y=\"59.685314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.991266\" y=\"90.993283\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"51.962785\" y=\"129.760231\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.49515\" y=\"95.79107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.077792\" y=\"49.856484\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.310112\" y=\"86.650945\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.556629\" y=\"84.532026\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.689037\" y=\"63.882353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.347151\" y=\"56.071804\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.809807\" y=\"91.749217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.976101\" y=\"88.082351\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.36503\" y=\"105.130473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.369996\" y=\"51.890282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.0653\" y=\"58.570508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.694646\" y=\"59.913878\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.833458\" y=\"77.805222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.038152\" y=\"88.190008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.299748\" y=\"52.29818\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.303556\" y=\"74.407627\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.91774\" y=\"86.544145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.32546\" y=\"97.2812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"209.389793\" y=\"47.401588\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"181.015184\" y=\"72.467508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.360497\" y=\"117.705185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.733952\" y=\"77.578839\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.36438\" y=\"68.35539\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.414222\" y=\"67.610349\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.119107\" y=\"29.829656\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"80.299656\" y=\"101.365519\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.623621\" y=\"65.258895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.323999\" y=\"77.986457\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.074237\" y=\"22.041043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.23561\" y=\"82.919953\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"63.623552\" y=\"139.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.357694\" y=\"93.679042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.611059\" y=\"110.310029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.936631\" y=\"96.225358\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.522297\" y=\"51.327124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.057859\" y=\"58.736199\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.800353\" y=\"66.047748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.564327\" y=\"80.562145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.434737\" y=\"122.612181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.082975\" y=\"72.588527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.615517\" y=\"68.419977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.593645\" y=\"69.203448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.359412\" y=\"99.604671\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.507215\" y=\"95.278895\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"91.779967\" y=\"116.960095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.826354\" y=\"62.502891\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.243687\" y=\"75.34703\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.302767\" y=\"65.876611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.021664\" y=\"59.436446\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.267938\" y=\"94.729298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.014277\" y=\"65.936659\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.896302\" y=\"66.413173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.794339\" y=\"71.617763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.880567\" y=\"65.554642\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"169.210858\" y=\"50.537598\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.656678\" y=\"60.66267\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.911634\" y=\"85.850677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.673002\" y=\"69.59409\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"189.161769\" y=\"33.39075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.753796\" y=\"84.740389\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.063714\" y=\"56.896367\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.889163\" y=\"80.144903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.337622\" y=\"78.820328\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.048405\" y=\"90.114192\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.60175\" y=\"90.689906\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.546441\" y=\"86.676935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.150959\" y=\"100.273606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.452988\" y=\"71.28935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.797849\" y=\"93.395327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.605712\" y=\"75.908799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.511792\" y=\"47.334452\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.202954\" y=\"42.681032\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.34902\" y=\"68.254932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.52931\" y=\"121.483361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.118569\" y=\"92.04012\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.717182\" y=\"84.291729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.564481\" y=\"80.507211\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.997267\" y=\"111.456825\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.925205\" y=\"70.558834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.319636\" y=\"99.113743\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.278791\" y=\"79.203896\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.434161\" y=\"43.082933\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"167.558883\" y=\"49.999154\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.350653\" y=\"80.905868\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.014254\" y=\"53.759361\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.995502\" y=\"57.926678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"160.34066\" y=\"30.864672\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.452077\" y=\"84.073393\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.964594\" y=\"85.092948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.420073\" y=\"59.609608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.2422\" y=\"85.599525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.279531\" y=\"37.233837\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.299046\" y=\"57.501901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.090431\" y=\"136.476002\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.285742\" y=\"93.124352\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.883758\" y=\"79.8113\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.341734\" y=\"62.819942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.614272\" y=\"81.136541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.947694\" y=\"106.779282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.874459\" y=\"73.861437\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.434917\" y=\"107.285008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.587661\" y=\"86.318162\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.568218\" y=\"41.339427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.334003\" y=\"98.494541\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.792341\" y=\"55.854203\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.765284\" y=\"74.743863\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"65.950443\" y=\"96.890186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.953128\" y=\"68.410737\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.076785\" y=\"86.884353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"165.077753\" y=\"48.318827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.557638\" y=\"82.182124\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.465336\" y=\"43.483263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.999096\" y=\"72.569612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.658194\" y=\"72.806161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.614113\" y=\"47.114905\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.097028\" y=\"49.194621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.168313\" y=\"66.412408\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.900855\" y=\"105.564466\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.904687\" y=\"100.306264\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.283924\" y=\"106.950633\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.055145\" y=\"64.954008\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.513609\" y=\"90.1105\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.437905\" y=\"86.995488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.146056\" y=\"62.85089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.417844\" y=\"93.940841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"161.497704\" y=\"79.150901\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.479509\" y=\"68.84641\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.86093\" y=\"101.662611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.037075\" y=\"49.77235\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.592107\" y=\"69.579363\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.298391\" y=\"111.892028\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.540151\" y=\"60.302929\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.701447\" y=\"103.65314\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.675451\" y=\"15.765491\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.119625\" y=\"94.586921\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.41555\" y=\"54.787315\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.166349\" y=\"83.67882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.124819\" y=\"68.62897\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.697529\" y=\"82.079448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.465559\" y=\"67.164617\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.395364\" y=\"97.354337\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.4787\" y=\"90.121634\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.71934\" y=\"79.292481\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.647301\" y=\"61.17923\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.39114\" y=\"64.295246\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"68.965886\" y=\"116.605219\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.765768\" y=\"82.715272\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.540192\" y=\"92.86394\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.89068\" y=\"70.888666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"165.89304\" y=\"13.5\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"176.577749\" y=\"69.149559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"172.099286\" y=\"42.032186\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.240875\" y=\"57.837107\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.595161\" y=\"75.377153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"54.132348\" y=\"131.895592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.467119\" y=\"59.141719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.744611\" y=\"111.353278\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.646477\" y=\"96.236795\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.454028\" y=\"64.276207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.72858\" y=\"60.290799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.81705\" y=\"106.724808\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.66873\" y=\"85.050771\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.426944\" y=\"107.254549\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.781012\" y=\"111.184544\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.073136\" y=\"95.462359\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"175.5756\" y=\"67.259161\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.399948\" y=\"70.232714\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.661228\" y=\"49.957704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.02515\" y=\"66.102736\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.587138\" y=\"83.945404\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.828517\" y=\"77.177288\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.246521\" y=\"76.923153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.757621\" y=\"90.314048\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.148795\" y=\"62.515236\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.402578\" y=\"59.281458\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"71.00132\" y=\"101.318345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.57908\" y=\"64.226513\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.297553\" y=\"102.041242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"149.14116\" y=\"77.644053\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.532508\" y=\"91.387142\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"215.364915\" y=\"27.810463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.557131\" y=\"81.917821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.399975\" y=\"89.348764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.696899\" y=\"35.092983\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.216523\" y=\"65.048166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.750923\" y=\"43.871166\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.567312\" y=\"75.364399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.841989\" y=\"76.088841\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"174.497954\" y=\"72.693093\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"82.079833\" y=\"83.175531\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.359196\" y=\"79.505546\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"86.410509\" y=\"100.975342\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.404638\" y=\"124.807206\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.355983\" y=\"90.983812\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.025712\" y=\"83.077619\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.753408\" y=\"83.761282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.473648\" y=\"87.596558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"76.848333\" y=\"84.604151\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.54825\" y=\"116.228875\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"72.241611\" y=\"77.64884\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.800053\" y=\"113.955505\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.94458\" y=\"68.287306\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"151.352201\" y=\"71.528892\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.151741\" y=\"74.031821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.390124\" y=\"98.097999\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.295013\" y=\"48.35415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.282461\" y=\"86.280172\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.137694\" y=\"67.597294\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.418614\" y=\"77.180758\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"180.585042\" y=\"64.412474\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.505276\" y=\"112.626968\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.83676\" y=\"95.91145\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.953483\" y=\"79.49606\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.450492\" y=\"84.052712\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.20357\" y=\"83.414741\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.473731\" y=\"75.428867\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.974924\" y=\"78.310834\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.555073\" y=\"70.326748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"155.47176\" y=\"73.301386\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.135547\" y=\"71.147066\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.265214\" y=\"99.081935\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.139364\" y=\"78.237334\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.121391\" y=\"89.874683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.026376\" y=\"74.284852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.496048\" y=\"72.372303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.263738\" y=\"75.555399\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"66.330597\" y=\"121.223819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.496239\" y=\"80.050575\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.746725\" y=\"78.240407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.019036\" y=\"75.147613\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.167031\" y=\"102.542952\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.733378\" y=\"87.033987\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.758837\" y=\"45.301327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"110.225777\" y=\"91.822422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.013595\" y=\"76.642438\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.577436\" y=\"77.949601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.717064\" y=\"30.519615\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.419942\" y=\"73.579209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.874708\" y=\"82.110649\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.592761\" y=\"79.188468\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"70.614825\" y=\"104.574014\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.928618\" y=\"61.68454\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"173.787735\" y=\"62.090174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.897787\" y=\"64.382054\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.262547\" y=\"114.576675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.003667\" y=\"93.266282\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.857955\" y=\"45.160321\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.470098\" y=\"104.21218\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.86453\" y=\"71.754109\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.791658\" y=\"68.087322\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.37824\" y=\"81.411749\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.522723\" y=\"50.130527\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.619915\" y=\"50.705691\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"164.740067\" y=\"63.967413\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.503223\" y=\"57.232677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.90262\" y=\"84.365181\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.082842\" y=\"77.832046\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"73.714402\" y=\"113.094996\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.922321\" y=\"91.140991\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.429953\" y=\"105.741195\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"97.251272\" y=\"68.392803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.652541\" y=\"75.12353\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.281016\" y=\"90.649089\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.225874\" y=\"104.070657\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.851862\" y=\"63.468873\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.874801\" y=\"82.86794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.361829\" y=\"85.608542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"133.679553\" y=\"97.206753\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.433652\" y=\"68.260558\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.707459\" y=\"56.226981\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.466984\" y=\"60.232185\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"162.211002\" y=\"51.873542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"191.749207\" y=\"18.103516\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.151989\" y=\"93.246023\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.586849\" y=\"73.027207\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.664094\" y=\"43.075637\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.632218\" y=\"67.577574\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.688456\" y=\"79.029583\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.443717\" y=\"119.915368\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.064883\" y=\"83.74487\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"166.324197\" y=\"76.370095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.687512\" y=\"92.48442\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"184.488892\" y=\"49.338792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"74.158453\" y=\"100.184448\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.142167\" y=\"97.096052\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.452225\" y=\"67.031251\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.464654\" y=\"87.363325\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"180.956995\" y=\"21.562226\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"169.849401\" y=\"69.816669\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.234793\" y=\"78.500167\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.43934\" y=\"89.430395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.511813\" y=\"91.456843\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.525078\" y=\"56.366234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.370516\" y=\"64.606874\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.840956\" y=\"78.651995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.986175\" y=\"72.41565\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"67.812426\" y=\"107.662652\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.24277\" y=\"99.172621\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.122348\" y=\"96.370217\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.984574\" y=\"79.02333\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.10275\" y=\"89.016402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.469622\" y=\"104.754022\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.055326\" y=\"96.801986\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.311371\" y=\"111.927308\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.977943\" y=\"82.793202\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"181.183955\" y=\"41.859056\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.899333\" y=\"93.851658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.641859\" y=\"60.980031\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.884806\" y=\"74.145029\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"82.031302\" y=\"94.163492\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.943946\" y=\"68.159095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.227504\" y=\"102.06078\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.542706\" y=\"57.863059\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.00117\" y=\"85.641259\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.462926\" y=\"94.75956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.415666\" y=\"112.293427\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.501266\" y=\"90.257794\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.875095\" y=\"65.223327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.522966\" y=\"83.79076\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.38404\" y=\"75.850764\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"191.302369\" y=\"55.738297\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.952036\" y=\"72.706857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"90.052766\" y=\"78.429072\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"142.100739\" y=\"73.384083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.68414\" y=\"88.150926\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"83.949254\" y=\"84.380879\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"126.488359\" y=\"89.189101\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.403712\" y=\"62.18428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"88.634546\" y=\"71.018209\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.618662\" y=\"70.038488\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.950225\" y=\"72.531255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"117.796374\" y=\"74.393081\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"124.97434\" y=\"53.162821\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"147.619682\" y=\"121.830153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.26606\" y=\"80.320939\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.049219\" y=\"64.345249\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"107.42358\" y=\"80.548704\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.697011\" y=\"63.643561\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.620693\" y=\"81.658512\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.167502\" y=\"101.152384\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"153.36571\" y=\"66.060699\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.537929\" y=\"92.592949\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.695567\" y=\"67.56122\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.542432\" y=\"90.036556\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"168.065405\" y=\"57.539528\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.202409\" y=\"79.329016\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.17291\" y=\"78.861112\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.472423\" y=\"83.921422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.067353\" y=\"75.450786\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"158.600733\" y=\"66.182709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"135.687881\" y=\"98.983666\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.388839\" y=\"65.131705\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.364471\" y=\"69.319809\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"144.523987\" y=\"78.456473\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.684456\" y=\"51.867579\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"66.500922\" y=\"82.685242\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.931686\" y=\"73.055611\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.580351\" y=\"84.355827\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.128055\" y=\"87.599534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"130.042181\" y=\"93.319693\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.098316\" y=\"67.56309\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"102.452956\" y=\"63.42464\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"181.400577\" y=\"48.451007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.510389\" y=\"55.90116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.469543\" y=\"62.556877\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"141.694968\" y=\"67.885119\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"106.093586\" y=\"77.585043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.199189\" y=\"87.156956\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.044037\" y=\"58.183763\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.598942\" y=\"82.54097\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.157904\" y=\"63.575948\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.28948\" y=\"76.863967\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.132618\" y=\"99.133675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.697074\" y=\"66.921095\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"182.751259\" y=\"52.645248\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"136.206545\" y=\"79.424332\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.563205\" y=\"63.33256\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"143.903969\" y=\"39.476263\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.87899\" y=\"86.515271\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.267502\" y=\"99.654722\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.307719\" y=\"91.443661\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.555387\" y=\"93.766817\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.017868\" y=\"72.543665\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.969671\" y=\"96.980838\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"118.293017\" y=\"93.142453\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.994584\" y=\"130.32799\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.427291\" y=\"37.928601\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"166.354605\" y=\"62.978298\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.483096\" y=\"73.937807\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.54527\" y=\"88.439678\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.541587\" y=\"109.732608\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.425463\" y=\"97.713903\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.188517\" y=\"51.397677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.721828\" y=\"69.78173\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.177918\" y=\"71.655387\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"129.965602\" y=\"74.834836\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"188.57917\" y=\"52.43887\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.583692\" y=\"78.713174\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.018702\" y=\"70.531854\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"122.213885\" y=\"61.076881\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.605197\" y=\"74.547729\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.640975\" y=\"52.674687\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"139.011495\" y=\"59.810942\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"140.194426\" y=\"61.533266\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"116.221769\" y=\"105.538605\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"113.302124\" y=\"83.827882\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.453243\" y=\"73.022932\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.00141\" y=\"72.99407\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.698996\" y=\"64.55689\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.138168\" y=\"83.376626\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.811148\" y=\"91.934398\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"120.818379\" y=\"85.236683\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"111.508163\" y=\"80.325857\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.762443\" y=\"53.38592\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"87.850859\" y=\"86.51086\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"132.383111\" y=\"68.541043\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"68.207159\" y=\"100.24009\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"94.417239\" y=\"118.103977\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"99.672379\" y=\"90.561111\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"156.416054\" y=\"69.697515\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"148.346262\" y=\"62.456778\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"131.383049\" y=\"94.957518\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.253293\" y=\"92.102869\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"123.880967\" y=\"66.853116\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.0285\" y=\"90.435422\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"125.009441\" y=\"71.333254\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.573869\" y=\"103.614709\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"121.573603\" y=\"98.96007\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"145.108641\" y=\"81.294635\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.700306\" y=\"81.23083\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"85.120846\" y=\"102.947303\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"157.863831\" y=\"79.235428\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"91.39912\" y=\"82.178295\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.719783\" y=\"91.059508\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"119.136266\" y=\"86.045153\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"95.852558\" y=\"65.405451\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.931748\" y=\"69.00258\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.923474\" y=\"82.574475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"81.203872\" y=\"115.920234\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.399908\" y=\"89.551559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.549853\" y=\"77.133567\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"152.650045\" y=\"68.029976\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.900625\" y=\"84.591819\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.236454\" y=\"95.674044\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"100.441901\" y=\"83.951852\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"178.453651\" y=\"48.107719\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"78.528815\" y=\"91.144692\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"84.272931\" y=\"77.966158\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.235085\" y=\"101.867718\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"128.639236\" y=\"69.104578\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"108.173343\" y=\"95.437832\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"150.640558\" y=\"91.398395\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"127.366991\" y=\"82.985289\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"93.357063\" y=\"113.470475\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"114.19085\" y=\"87.582783\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"98.92588\" y=\"80.594545\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"163.507434\" y=\"75.882131\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.130443\" y=\"83.48542\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"138.525102\" y=\"75.831255\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"171.416936\" y=\"84.442152\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"105.333151\" y=\"66.885205\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"112.185721\" y=\"87.367559\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"92.657561\" y=\"75.389946\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"103.15855\" y=\"62.102534\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"89.341837\" y=\"89.780347\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"159.317039\" y=\"73.903159\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"71.70301\" y=\"115.759169\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"101.533778\" y=\"99.445677\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"146.90197\" y=\"100.660773\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.346168\" y=\"95.951702\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"109.431077\" y=\"91.992748\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.271051\" y=\"93.511792\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"115.176293\" y=\"93.644631\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"193.891924\" y=\"39.124589\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"134.574687\" y=\"89.347345\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"137.69041\" y=\"69.862951\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"104.956908\" y=\"101.115803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m823589f946\" x=\"154.691024\" y=\"68.037402\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m6ff9f381ac\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6ff9f381ac\" x=\"69.573382\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(62.202288 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6ff9f381ac\" x=\"125.776441\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(122.595191 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6ff9f381ac\" x=\"181.979501\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(178.798251 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <defs>\n",
       "       <path id=\"m28622f4ef6\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"125.71812\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- −5 -->\n",
       "      <g transform=\"translate(7.2 129.517339) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"99.751174\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(15.579688 103.550393) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"73.784228\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(15.579688 77.583446) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"47.817281\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(9.217188 51.6165) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m28622f4ef6\" x=\"28.942188\" y=\"21.850335\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(9.217188 25.649554) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 28.942188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 224.242188 145.8 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 28.942188 145.8 \n",
       "L 224.242188 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 28.942188 7.2 \n",
       "L 224.242188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p4610e2cf3f\">\n",
       "   <rect x=\"28.942188\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘图，查看分布\n",
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:, (0)].detach().numpy(), \n",
    "                labels.detach().numpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. <a id='toc8_1_2_'></a>[读取数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7269, -0.1631],\n",
      "        [ 2.0933, -0.5410],\n",
      "        [ 0.7562, -0.6686],\n",
      "        [-0.4302,  0.3302],\n",
      "        [-0.1591,  1.4465],\n",
      "        [ 0.7235, -0.8781],\n",
      "        [ 0.0123,  0.3597],\n",
      "        [ 1.0409, -2.0936],\n",
      "        [ 0.6744, -0.2588],\n",
      "        [-0.2561, -0.4138]]) \n",
      " tensor([[ 6.2141],\n",
      "        [10.2228],\n",
      "        [ 7.9691],\n",
      "        [ 2.2033],\n",
      "        [-1.0338],\n",
      "        [ 8.6305],\n",
      "        [ 2.9873],\n",
      "        [13.3875],\n",
      "        [ 6.4233],\n",
      "        [ 5.0989]])\n"
     ]
    }
   ],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)                                 # 把原来的indices顺序给打乱了\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. <a id='toc8_1_3_'></a>[初始化模型参数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(mean=0, std=0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(size=(1,), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4. <a id='toc8_1_4_'></a>[定义模型](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b): \n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5. <a id='toc8_1_5_'></a>[定义损失函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  \n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.6. <a id='toc8_1_6_'></a>[定义优化算法](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  \n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.7. <a id='toc8_1_7_'></a>[训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000051\n",
      "epoch 2, loss 0.000056\n",
      "epoch 3, loss 0.000051\n",
      "epoch 4, loss 0.000050\n",
      "epoch 5, loss 0.000048\n",
      "epoch 6, loss 0.000052\n",
      "epoch 7, loss 0.000052\n",
      "epoch 8, loss 0.000049\n",
      "epoch 9, loss 0.000049\n",
      "epoch 10, loss 0.000049\n",
      "w的估计误差: tensor([-0.0010,  0.0004], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([-0.0013], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "num_epochs = 10\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n",
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. <a id='toc8_2_'></a>[现线性回归模型于训练过程-简洁实现](#toc0_)\n",
    "### 8.2.1. <a id='toc8_2_1_'></a>[虚拟数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. <a id='toc8_2_2_'></a>[读取数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3. <a id='toc8_2_3_'></a>[定义模型](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4. <a id='toc8_2_4_'></a>[初始化模型参数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5. <a id='toc8_2_5_'></a>[定义损失函数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.6. <a id='toc8_2_6_'></a>[定义优化算法](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "trainer = optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.7. <a id='toc8_2_7_'></a>[训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000298\n",
      "epoch 2, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 5, loss 0.000100\n",
      "epoch 6, loss 0.000099\n",
      "epoch 7, loss 0.000099\n",
      "epoch 8, loss 0.000099\n",
      "epoch 9, loss 0.000100\n",
      "epoch 10, loss 0.000098\n",
      "w的估计误差： tensor([-0.0003,  0.0004])\n",
      "b的估计误差： tensor([-0.0004])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        y_hat = net(X)                  # 1. 计算y_hat\n",
    "        loss = loss_fn(y_hat ,y)        # 2. 计算loss值\n",
    "        trainer.zero_grad()\n",
    "        loss.backward()                 # 2. 求梯度           \n",
    "        trainer.step()                  # 3. 更新网络权重参数\n",
    "    train_loss = loss_fn(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {train_loss:f}')\n",
    "\n",
    "\n",
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.8. <a id='toc8_2_8_'></a>[参数保存](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": num_epochs, \n",
    "        'mode_state_dict': net.state_dict(), \n",
    "        'opt_state_dict': trainer.state_dict(), \n",
    "        'loss': 'loss'\n",
    "    }, \n",
    "    'Pytorch_params/line_params.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.9. <a id='toc8_2_9_'></a>[重载](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0603])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32820/1977999358.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load('./Pytorch_params/line_params.pt')\n"
     ]
    }
   ],
   "source": [
    "check_point = torch.load('./Pytorch_params/line_params.pt')\n",
    "\n",
    "new_net = net = nn.Sequential(nn.Linear(2, 1))\n",
    "new_net.load_state_dict(check_point['mode_state_dict'])\n",
    "\n",
    "new_opt = optim.SGD(new_net.parameters(), lr=0.03)\n",
    "new_opt.load_state_dict(check_point['opt_state_dict'])\n",
    "\n",
    "# Stop BN、Dropout ...\n",
    "new_net.eval()\n",
    "\n",
    "# 停止计算梯度，节省运算和内存\n",
    "with torch.no_grad():\n",
    "    pre = new_net(torch.Tensor([3.0, 2.1]))\n",
    "    print(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. <a id='toc8_3_'></a>[分类-softmax](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. <a id='toc8_3_1_'></a>[快速实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([0.0900, 0.2447, 0.6652]), tensor(1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x_softmax = torch.nn.functional.softmax(x, dim=0)\n",
    "\n",
    "x, x_softmax, x_softmax.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. <a id='toc8_3_2_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([0.0900, 0.2447, 0.6652]), tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    x_exp = torch.exp(x)\n",
    "    partition = x_exp.sum()\n",
    "    return x_exp / partition \n",
    "\n",
    "\n",
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x_sf = softmax(x)\n",
    "\n",
    "x, x_sf, x_sf.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3. <a id='toc8_3_3_'></a>[交叉熵损失](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1599)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.arange(3, dtype=torch.float32)\n",
    "y = torch.tensor([0, 2, 1], dtype=torch.float32)\n",
    "\n",
    "y_hat = torch.tensor([0.1, 0.3, 0.6])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss = loss_fn(y_hat, y)\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. <a id='toc8_4_'></a>[专题-模型定义（计算预测值y_hat）](#toc0_)\n",
    "PyTorch的`nn`模块，提供了`nn.Module`类，用于定义神经网络模型。`nn.Module`是所有神经网络模型的基类，提供了构建、初始化、前向传播等功能。有很多`nn.Module`的子类，如`nn.Sequential`、`nn.ModuleList`、`nn.ModuleDict`等，用于构建和组织神经网络模型。`nn`中有很多现成的模块可以直接调用，比如`nn.Linear`、`nn.Conv2d`、`nn.LSTM`等。当然，也可以自定义神经网络模型，如通过继承`nn.Module`类，并重写`__init__`和`forward`方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1. <a id='toc8_4_1_'></a>[块：torch.nn模块](#toc0_)\n",
    "#### 8.4.1.1. <a id='toc8_4_1_1_'></a>[Sequential、ModuleList、ModuleDict](#toc0_)\n",
    "1. nn.`Sequential`(module1, module2, module3, ...)\n",
    "    1. .append()\n",
    "    2. .extend()\n",
    "    3. .insert()\n",
    "    4. .pop()\n",
    "    5. `.add_module()`\n",
    "\n",
    "2. nn.`ModuleList`([module1, module2, modeul3, ...])\n",
    "    1. .append()    # 追加\n",
    "    2. .extend()    # 拼接两个ModuleList\n",
    "    3. .insert()    # 指定位置插入\n",
    "    4. `.add_module()`\n",
    "\n",
    "3. nn.`ModuleDict`({'m1': module1, 'm2': module2, 'm3': module3, ...})\n",
    "    1. clear()  # 清空ModuleDict\n",
    "    2. items()  # 返回可迭代key: value\n",
    "    3. keys()   # 返回keys\n",
    "    4. values() # 返回values\n",
    "    5. pop()    # 返回一对key: value，并从字典中删除\n",
    "    6. `add_module()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nn.ModuleDict), help(nn.ModuleList), help(nn.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (4): Tanh()\n",
       "  (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (6): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(786, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 256), \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(256, 10), \n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = nn.ModuleList([net, net, net])\n",
    "\n",
    "net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (m1): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       "  (m2): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       "  (m3): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=786, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (6): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = nn.ModuleDict(\n",
    "    {\n",
    "        'm1': net,\n",
    "        'm2': net, \n",
    "        'm3': net\n",
    "    }\n",
    ")\n",
    "\n",
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "model2:\n",
      "ModelWithModuleList(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "model3:\n",
      "ModelWithModuleDict(\n",
      "  (layers): ModuleDict(\n",
      "    (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "# Sequential 实现\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "# ModuleList 实现\n",
    "class ModelWithModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithModuleList, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model2 = ModelWithModuleList()\n",
    "\n",
    "# ModuleDict 实现\n",
    "class ModelWithModuleDict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithModuleDict, self).__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'fc1': nn.Linear(10, 20),\n",
    "            'activation': nn.ReLU(),\n",
    "            'fc2': nn.Linear(20, 1)\n",
    "        })\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers['fc1'](x)\n",
    "        x = self.layers['activation'](x)\n",
    "        x = self.layers['fc2'](x)\n",
    "        return x\n",
    "\n",
    "model3 = ModelWithModuleDict()\n",
    "\n",
    "print('model1:', model1, sep='\\n')\n",
    "print('model2:', model2, sep='\\n')\n",
    "print('model3:', model3, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1.2. <a id='toc8_4_1_2_'></a>[比较](#toc0_)\n",
    "\n",
    "|特性|Sequential|ModuleList|ModuleDict|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|子模块组织方式|按顺序组织|按列表形式组织|按键值对形式组织|\n",
    "|前向传播|自动实现，按顺序调用|需要手动实现，灵活|需要手动实现，可按键值灵活调用|\n",
    "|灵活性|低|中|高|\n",
    "|适用场景|简单顺序模型|动态模型、循环模型|非顺序复杂模型|\n",
    "|动态添加子模块|不支持|支持|支持|\n",
    "|子模块调用方式|固定顺序调用|按索引调用|按键名调用|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2. <a id='toc8_4_2_'></a>[块：自定义](#toc0_)\n",
    "通过继承`nn.Module`类，并重写`__init__`和`forward`方法，可以自定义神经网络模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2.1. <a id='toc8_4_2_1_'></a>[自定义块](#toc0_)\n",
    "\n",
    "* 从编程的角度看：块就是Class\n",
    "\n",
    "* `nn.Module`会自动调用`forward()`方法，我们也可以重写该方法，从而实现更加灵活的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''定义每个块或层'''\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''正向传播'''\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2.2. <a id='toc8_4_2_2_'></a>[顺序块](#toc0_)\n",
    "```\n",
    "Sequential就是顺序块，这里我们自己从头实现一边Sequential这个方法\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2.3. <a id='toc8_4_2_3_'></a>[效率](#toc0_)\n",
    "\n",
    "1. 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "2. 块可以包含代码。\n",
    "3. 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "4. 层和块的顺序连接由Sequential块处理。\n",
    "\n",
    "读者可能会开始担心操作效率的问题。 毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、 代码执行和许多其他的Python代码。 Python的问题全局解释器锁 是众所周知的。 在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.3. <a id='toc8_4_3_'></a>[模型结构/组成](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Init the Net()\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.1. <a id='toc8_4_3_1_'></a>[.children()](#toc0_)\n",
    "列出`第一级别`的module权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=2, out_features=3, bias=True),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=3, out_features=128, bias=True)\n",
       " ),\n",
       " Linear(in_features=128, out_features=2, bias=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net\n",
    "list(net.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.2. <a id='toc8_4_3_2_'></a>[.named_children()](#toc0_)\n",
    "列出`第一级别`的module权重名称和权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear \t Linear(in_features=2, out_features=3, bias=True)\n",
      "block \t Sequential(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      ")\n",
      "decode \t Linear(in_features=128, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, children in net.named_children():\n",
    "    print(name, '\\t', children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.3. <a id='toc8_4_3_3_'></a>[.modules()](#toc0_)\n",
    "依次列出`所有`的module权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (block): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  )\n",
      "  (decode): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "==========\n",
      "Linear(in_features=2, out_features=3, bias=True)\n",
      "==========\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      ")\n",
      "==========\n",
      "Linear(in_features=3, out_features=128, bias=True)\n",
      "==========\n",
      "Linear(in_features=128, out_features=2, bias=True)\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for module in net.modules():\n",
    "    print( module)\n",
    "    print('='*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.4. <a id='toc8_4_3_4_'></a>[.named_modules()](#toc0_)\n",
    "依次列出`所有`的module权重名和权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> Net(\n",
      "  (linear): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (block): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  )\n",
      "  (decode): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "==========\n",
      "linear >>> Linear(in_features=2, out_features=3, bias=True)\n",
      "==========\n",
      "block >>> Sequential(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      ")\n",
      "==========\n",
      "block.0 >>> Linear(in_features=3, out_features=128, bias=True)\n",
      "==========\n",
      "decode >>> Linear(in_features=128, out_features=2, bias=True)\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for name, module in net.named_modules():\n",
    "    print(name, '>>>', module)\n",
    "    print('='*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.5. <a id='toc8_4_3_5_'></a>[删除和添加](#toc0_)\n",
    "先利用 net`.children()`迭代话模型第一层级，再`列表化 (list())` 并进行`索引提取`，最终实现删除或添加的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "   (1): Sequential(\n",
       "     (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "   )\n",
       "   (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "   (3): Linear(in_features=256, out_features=2, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "class NetDel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.net = Net()                                              ## 会报错\n",
    "        self.netdel_list = list(Net().children())[0:-1]                 # 删除最后一个结构\n",
    "        self.netdel_list += [nn.Linear(128, 256), nn.Linear(256, 2)]    # 添加两个新的结构\n",
    "        self.netdel = nn.Sequential(*self.netdel_list)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.netdel(X)\n",
    "\n",
    "netdel = NetDel()\n",
    "# netdel\n",
    "list(netdel.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.6. <a id='toc8_4_3_6_'></a>[替换](#toc0_)\n",
    "直接访`问模型的具体层级`，`替换`即可。\n",
    "\n",
    "* 当通过 `Sequential类` 定义模型时，我们可以通过 `索引 (下标)` 来访问模型的任意层；\n",
    "\n",
    "* `自定义的重载nn.Module` 的layer1、layer2等等，需要net`.`layer1或net`.`layer2方式进行调用；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetMod(\n",
       "  (model): Net(\n",
       "    (linear): Linear(in_features=2, out_features=3, bias=True)\n",
       "    (block): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    )\n",
       "    (decode): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "class NetMod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Net()\n",
    "        in_features = self.model.decode.in_features\n",
    "        self.model.decode = nn.Linear(in_features=in_features, out_features=10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "\n",
    "netmod = NetMod()\n",
    "netmod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3.7. <a id='toc8_4_3_7_'></a>[add_module()](#toc0_)\n",
    "`add_module()` 方法用于将子模块添加到当前模块中，并为其指定一个名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (block): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  )\n",
       "  (decode): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (add_demo): Linear(in_features=2, out_features=256, bias=True)\n",
       "  (final_demo): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "        self.block = nn.Sequential(nn.Linear(3, 128))\n",
    "        self.decode = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        y = self.bloack(y)\n",
    "        y = self.decode(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.add_module(name='add_demo', module=nn.Linear(2, 256))\n",
    "net.add_module(name='final_demo', module=nn.Sequential(nn.Linear(256, 128), nn.Linear(128, 2)))\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.4. <a id='toc8_4_4_'></a>[模型：参数管理](#toc0_)\n",
    "\n",
    "* 其实可以将`nn.Sequential`视为Python的`list数据结构`，`按顺序`储存神经网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0865],\n",
       "        [-0.0746]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.4.1. <a id='toc8_4_4_1_'></a>[参数访问](#toc0_)\n",
    "\n",
    "* 我们从已有模型中访问参数；\n",
    "\n",
    "* 当通过 `Sequential类` 定义模型时，我们可以通过 `索引 (下标)` 来访问模型的任意层；\n",
    "\n",
    "* `自定义的重载nn.Module` 的layer1、layer2等等，需要net`.`layer1或net`.`layer2方式进行调用；\n",
    "\n",
    "* 这就像模型是一个列表一样，每层的参数都在其属性中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.1.1. <a id='toc8_4_4_1_1_'></a>[state_dict](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net # nn.Sequential类，可以直接用下标进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=8, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=8, out_features=1, bias=True))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0], net[1], net[2] # nn.Sequential类，可以直接用下标进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "                      [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "                      [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "                      [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "                      [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "                      [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "                      [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "                      [ 0.4440, -0.2937, -0.0911, -0.4929]])),\n",
       "             ('bias',\n",
       "              tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880]))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "        [ 0.4440, -0.2937, -0.0911, -0.4929]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data # 访问目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias.data # 访问目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
       "                      [-0.1280, -0.2531, -0.0131, -0.2696],\n",
       "                      [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
       "                      [-0.2477, -0.0914,  0.1431,  0.2419],\n",
       "                      [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
       "                      [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
       "                      [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
       "                      [ 0.4440, -0.2937, -0.0911, -0.4929]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0894, -0.1603, -0.1185,  0.0858, -0.0592, -0.1632,  0.1876, -0.0784]])),\n",
       "             ('2.bias', tensor([-0.1285]))])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以直接输出神经网络的所有层参数信息，net[1]是relu激活函数，没有参数，所以就显示无\n",
    "# 后续，torch.save(net.state_dict(), 'Pytorch_datasets/net_params)\n",
    "\n",
    "net.state_dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.1.2. <a id='toc8_4_4_1_2_'></a>[parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<generator object Module.parameters at 0x7f0dc565da80>,\n",
       " <bound method Module.parameters of Sequential(\n",
       "   (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=8, out_features=1, bias=True)\n",
       " )>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters(), net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
      "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
      "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
      "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
      "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
      "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
      "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
      "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
      "       requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[-0.0894, -0.1603, -0.1185,  0.0858, -0.0592, -0.1632,  0.1876, -0.0784]],\n",
      "       requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.1285], requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(type(param))\n",
    "    print(param)\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "    print(param.is_leaf)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
      "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
      "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
      "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
      "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
      "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
      "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
      "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n",
      "Parameter containing:\n",
      "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
      "       requires_grad=True)\n",
      "True\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param  in net[0].parameters():\n",
    "    print(param)\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "    print(param.is_leaf)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.1.3. <a id='toc8_4_4_1_3_'></a>[named_parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight \t Parameter containing:\n",
      "tensor([[ 0.4129, -0.2663,  0.0648, -0.3372],\n",
      "        [-0.1280, -0.2531, -0.0131, -0.2696],\n",
      "        [ 0.0538,  0.1759, -0.1103, -0.3805],\n",
      "        [-0.2477, -0.0914,  0.1431,  0.2419],\n",
      "        [ 0.1345, -0.0516, -0.0536, -0.4364],\n",
      "        [ 0.1144, -0.3585, -0.2615,  0.1957],\n",
      "        [ 0.2924,  0.0015,  0.4087,  0.3759],\n",
      "        [ 0.4440, -0.2937, -0.0911, -0.4929]], requires_grad=True)\n",
      "0.bias \t Parameter containing:\n",
      "tensor([-0.4462,  0.1640,  0.4165, -0.2921, -0.0450, -0.2606, -0.1634,  0.2880],\n",
      "       requires_grad=True)\n",
      "2.weight \t Parameter containing:\n",
      "tensor([[-0.0894, -0.1603, -0.1185,  0.0858, -0.0592, -0.1632,  0.1876, -0.0784]],\n",
      "       requires_grad=True)\n",
      "2.bias \t Parameter containing:\n",
      "tensor([-0.1285], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# list(net.named_parameters())\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, '\\t', param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.4.2. <a id='toc8_4_4_2_'></a>[参数初始化](#toc0_)\n",
    "\n",
    "* 初始化，主要是为了不要再一开始训练就炸掉了，其实不用太迷信了。\n",
    "\n",
    "* 默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 \n",
    "\n",
    "* PyTorch的nn.init模块提供了多种预置初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.2.1. <a id='toc8_4_4_2_1_'></a>[内置初始化](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = net[0]\n",
    "nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.2.2. <a id='toc8_4_4_2_2_'></a>[自定义初始化](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.4.4.2.3. <a id='toc8_4_4_2_3_'></a>[参数绑定](#toc0_)\n",
    "```\n",
    "有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 8), \n",
    "    nn.ReLU(),\n",
    "    shared, \n",
    "    nn.ReLU(),\n",
    "    shared, \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "net(X)\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "\n",
    "net[2].weight.data[0, 0] = 100\n",
    "\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.5. <a id='toc8_4_5_'></a>[层：自定义](#toc0_)\n",
    "深度学习成功背后的一个因素是神经网络的灵活性：我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。\n",
    "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。\n",
    "\n",
    "```python\n",
    "块和层其实并无本质的区别，因为都是torch.nn.Module的子类\n",
    "e.g. \n",
    "    全连接层（FC）\n",
    "    池化层（Pooling）\n",
    "    BN层\n",
    "    Dropout层\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.5.1. <a id='toc8_4_5_1_'></a>[不带参数的层](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0.,  1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9.3132e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在，我们可以将层作为组件合并到更复杂的模型中。\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(8, 128), \n",
    "    CenteredLayer()\n",
    ")\n",
    "\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.5.2. <a id='toc8_4_5_2_'></a>[带参数的层](#toc0_)\n",
    "\n",
    "用到`nn.Parameter()`可以将参数加入神经网络中，便于自动管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.1466, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units)) \n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "    \n",
    "linear = MyLinear(5, 3)\n",
    "# linear.weight\n",
    "\n",
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们还可以使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层。\n",
    "net = nn.Sequential(\n",
    "    MyLinear(64, 8), \n",
    "    MyLinear(8, 1)\n",
    ")\n",
    "\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. <a id='toc8_5_'></a>[专题-损失函数 (loss_fn)](#toc0_)\n",
    "损失函数的输入是 loss_fn(y_hat, y) ，即网络输出和真实标签对的数据，然后返回一个数值表示网络输出和真实标签的差距。\n",
    "\n",
    "  1. 均方误差\n",
    "\n",
    "  2. 交叉熵\n",
    "  \n",
    "  3. 自定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. <a id='toc8_5_1_'></a>[均方误差](#toc0_)\n",
    "回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. <a id='toc8_5_2_'></a>[交叉熵](#toc0_)\n",
    "分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.2.1. <a id='toc8_5_2_1_'></a>[快速实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 2, 1]),\n",
       " tensor([[0.1000, 0.3000, 0.6000],\n",
       "         [0.3000, 0.2000, 0.5000],\n",
       "         [0.0000, 0.1000, 0.9000]]),\n",
       " tensor(1.2372))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 3个样本，3个类别的索引\n",
    "y = torch.tensor([0, 2, 1])\n",
    "\n",
    "# 3个样本，3个类别，每个样本的概率\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], \n",
    "                      [0.3, 0.2, 0.5], \n",
    "                      [0.0, 0.1, 0.9]])\n",
    "\n",
    "y, y_hat, loss_fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.2.2. <a id='toc8_5_2_2_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 2, 1]),\n",
       " tensor([[0.1000, 0.3000, 0.6000],\n",
       "         [0.3000, 0.2000, 0.5000],\n",
       "         [0.0000, 0.1000, 0.9000]]),\n",
       " tensor([2.3026, 0.6931, 2.3026]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "\n",
    "# 3个样本，3个类别的索引\n",
    "y = torch.tensor([0, 2, 1])\n",
    "\n",
    "# 3个样本，3个类别，每个样本的概率\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], \n",
    "                      [0.3, 0.2, 0.5], \n",
    "                      [0.0, 0.1, 0.9]])\n",
    "\n",
    "y, y_hat, cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. <a id='toc8_5_3_'></a>[自定义](#toc0_)\n",
    "自己定义赏罚分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, y_hat):\n",
    "    '''例如真实值于预测值之差'''\n",
    "    error_values = y - y_hat\n",
    "    return error_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. <a id='toc8_6_'></a>[专题-反向传播（求梯度）](#toc0_)\n",
    "```\n",
    "求梯度（求偏导数）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 见autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7. <a id='toc8_7_'></a>[专题-更新权重（优化算法）](#toc0_)\n",
    "- 优化算法，在深度学习中是非常重要的一环。在对损失函数进行优化的时候，比较关注损失函数的凹凸性的问题。  \n",
    "- 可惜的是，在现有损失函数中，只有线性函数网络结构和softmax结构是凸函数，其它例如MLP、CNN、RNN、注意力等都是非凸函数。  \n",
    "- 并且，在优化过程中通常只是得到了局部最小值，而不是全局最小值；\n",
    "- 小批量随机梯度下降算法是最常用的优化算法；\n",
    "- 冲量对梯度做平滑；\n",
    "- Adam对梯度做平滑，且对梯度各纬度值重新做调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 原函数图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Function')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "x = torch.arange(-1, 2, 0.01, dtype=torch.float32, requires_grad=True)\n",
    "y = x * torch.cos(torch.pi * x)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 导函数图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'grad')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def y(x):\n",
    "    y_hat = x * torch.cos(torch.pi * x)\n",
    "    x_grad = torch.autograd.grad(outputs=y_hat, inputs=x)\n",
    "    return x_grad[0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "x_grads = [y(i) for i in x]\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(x.detach().cpu().numpy(), x_grads)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y`')\n",
    "plt.title('grad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f800fba10>]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADtCAYAAACms3k/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjyklEQVR4nO3deXhU9d338ffMJJnsk40khKwIGmiAhLDIpkFtaqQq4l0rIkKVPmqDZWltBR9RbJX74baIrUJvq4AbFIuxWLDWtOybgIZ9kTUJCZCEZSYbCZmc54+ZCZOVLDNzMsn3dV1zXWbmzMzXmI/nnN/ve35HoyiKghBCFVq1CxCiO5MACqEiCaAQKpIACqEiCaAQKpIACqEiCaAQKpIACqEiCaAQKpIAdnIrVqxAo9E0+fj1r3+tWl0rV65k8eLFTb6m0Wh45ZVXXFqPu/JQuwDROsuXLycxMbHec1FRUSpVYwngoUOHmDlzZqPXdu7cSXR0tOuLckMSQDeRlJTEkCFD1C6jVW6//Xa1S3Abcgjq5po73IuPj2fq1Kl1P9sOZTdu3Mizzz5LWFgYoaGhTJgwgcLCwkbvX7lyJSNGjMDf3x9/f3+Sk5N5//33AUhLS2P9+vXk5ubWOyRuqaZDhw7x4IMPEhwcjLe3N8nJyXzwwQf1ttm0aRMajYZVq1bx4osvEhUVRWBgIPfccw/Hjx9v/y+pE5MAugmz2UxNTU29R3tMmzYNT09PVq5cycKFC9m0aROPP/54vW3mzZvHpEmTiIqKYsWKFXz++edMmTKF3NxcAJYsWcKoUaOIjIxk586ddY/mHD9+nJEjR3L48GH++Mc/kpWVRf/+/Zk6dSoLFy5stP3cuXPJzc3lvffe49133+XEiRPcf//9mM3mdv07d2qK6NSWL1+uAE0+rl+/rgDKyy+/3Oh9cXFxypQpUxp9zi9+8Yt62y1cuFABlPPnzyuKoiinT59WdDqdMmnSpBbrGjdunBIXF9fkaw1revTRRxW9Xq/k5eXV2y4jI0Px9fVVrl69qiiKomzcuFEBlPvuu6/edp9++qkCKDt37myxJncke0A38eGHH7Jnz556Dw+Ptp/CP/DAA/V+HjhwIEDd3i07Oxuz2UxmZmbHi7basGEDd999NzExMfWenzp1KhUVFY32njersSuRQRg30a9fP4cMwoSGhtb7Wa/XA1BZWQlAcXExgENHMS9dukTPnj0bPW8bxb106VKbauxKZA/o5vR6PVVVVY2eb/hH3Vo9evQA4Ny5cx2qy15oaCjnz59v9Lxt8CcsLMxh3+VuJIBuLj4+ngMHDtR7bsOGDZSVlbXr89LT09HpdCxdurTF7fR6fav3SHfffTcbNmxoNNr64Ycf4uvr262nLeQQ1M1NnjyZl156iXnz5nHnnXdy5MgR3n77bQwGQ7s+Lz4+nrlz5/K73/2OyspKJk6ciMFg4MiRI5SUlDB//nwABgwYQFZWFkuXLiU1NRWtVtvsIfLLL7/MunXrGDt2LPPmzSMkJIRPPvmE9evXs3DhwnbX2hVIAN3c888/j8lkYsWKFbzxxhsMGzaMTz/9lAcffLDdn/nqq6/St29f/vSnPzFp0iQ8PDzo27cvv/zlL+u2mTFjBocPH2bu3LkYjUYURUFpZn2v2267jR07djB37lwyMzOprKykX79+LF++vN5cZXekUZr7rQkhnE7OAYVQkQRQCBVJAIVQkQRQCBVJAIVQkQRQCBV1q3nA2tpaCgsLCQgIqHf9mhCOpigKpaWlREVFodU2v5/rVgEsLCxs1JEvhDPl5+e32NjerQIYEBAAWH4pgYGBKlcjujKTyURMTEzd31xzulUAbYedgYGBEkDRfmYzbNpkeQCkpVkeOl2jTW92quM2gzALFixg6NChBAQEEB4ezvjx47vsOiGiE8vKgvBwuOce+P3vLY977oGICMtrbeQ2Ady8eTOZmZns2rWL7OxsampqSE9Pp7y8XO3SRHeRlQUPPwyXLzd+7dIly2ttDKHbNmMXFxcTHh7O5s2bueOOO1r1HpPJhMFgwGg0yiGoaBuzGQwGsP4P34wGs1aHV22DxbGio+HsWUzl5a36W3ObPWBDRqMRgJCQkGa3qaqqwmQy1XsI0S6PPVYXPoBdcQPwqq2hWttgGOXcOdi6tdUf65YBVBSF2bNnM3r0aJKSkprdbsGCBRgMhrqHTEGIdlmzBj79tO7HIz3iGZF7EIBjPeIbb9/E8hvNccsATp8+nQMHDrBq1aoWt5szZw5Go7HukZ+f76IKRZdhNsNTT9X9WKXVYbhWhhaFb6J/wMCLJxu/p4kFqJrjdtMQzz33HF988QVbtmy56cpder2+bkUtIdrltdfA7tQlp1c/bs8/RJFfED+4eKrx9mFhMGZMvcPVlrjNHlBRFKZPn05WVhYbNmwgISFB7ZJEV2c2w1tv1f14TedJYvFZAM4GReF//Vrj9yxZ0uR8YHPcJoCZmZl8/PHHrFy5koCAAC5cuMCFCxe65FqRopPYurXelMPByD4EXSvjXGA4KYXHGm//q1/BT37Spq9wmwAuXboUo9FIWloaPXv2rHusXr1a7dJEV7V2bd0/mtHQs7QEgNygSDyV2vrbzp4Nb7zR5q9wm3NAN52uFO7KbIZPPqn78Uh4AgOKTlPq5UNyYYMOrEcegT/8oV1f4zZ7QCFcautWsC7TD1Cjs+yrDkfcgl+N3UrkgYGwcmW7v0YCKERT7ObyTJ4+9C86A0BQZWn97Z58sk2DLg1JAIVoyokTdf94NKI3evN1zgT15LaSBndo6sACyCABFKIxsxn+8pe6H32sh5wX/UOod3FRdLRlzq8DJIBCNLR1q6WnEzB5+dL/4mkAokzF9bf7+c87dPgJEkAhGrM7//u+RyweSi0nQ6KJNRXV365v3w5/lQRQiIbszv801tmvEr+gxtu1oeezOW4zDyiES2RlwSuvAHBN61HXetajzO4iXI3GIed/IHtAIW4wm2HGDLA2fZwMi8Hv+jVKfA30vmJ3c1FFgcWLO3z+BxJAIW6wG3wBqPT0BuBkaHT90c/582HCBId8pQRQCJsGF9JGll4CQFfboO/TAYMvNhJAIWzsBlUu+gURYyrCrNHStyS/2e06SgIohM2YMRAaCkBuUBRgOfwMqiqzvK7RQEyMQwZfbCSAQtisXWtZXhBL1gCueNutbO3AwRcbCaAQcGMEFFCA3pcsgzFB18pubBMa2uHez4YkgEJAvRHQ3KBIQitNVHjo64IIWPaObVhysDUkgEJAvRHQYj/LWrMnwmLxUszNbucIEkAhoN7Ipod1tetyL58Wt3MECaAQYBnZjI5GAWKvXgAgqNJuJXUnjICCBFAIC50OFi0izxBBaKWJKp0nt1yyzv/ZhkQdPAIKEkAhLLKyYPZszgeEAXAqpBf6Wuv5X3S0ZXl6B7Wf2ZOrIYTIyoL/+i9QFLS9LHcyMun9bry+aJFTwgeyBxTdXYMrIKKtF936V1sXfNZoLGt+ms3NfUKHSABF92Y3/1fiE0hUaQk1Gi1xtsuPFAXy8x0+/2cjARTdm9283jlDBACnQ3oR0PC+Dw6e/7ORAIruzW5e77rOE3De8hNNkQCK7s06/4dGg+GaZdFdrf31f06a/7ORAIruTaeDt96iWqMj/orlMNN2ExZnzv/ZSACFmDCB3Bd/h1dtDZd8Aok1XrQ878T5PxuZBxTCbObKCcu9H/JibiV03kzo1cty2OmkPZ+N7AFF95aVBfHxeGzZDEDltSp44QXLjTmdHD6QAIruzNYBc+4cvYyWCfjAa+VQUGB5PivL6SVIAEX3ZNcBU+wbRET5FWo0WhIuF9R1xTBzptM6YGwkgKJ7suuAybdOwOcG97xx800nd8DYSABF92TX2VLlYZ2A9w1qcTtncKsAbtmyhfvvv5+oqCg0Gg1///vf1S5JuCu7zpbAqgoAtLZDz2a2cwa3CmB5eTmDBg3i7bffVrsU4e6sHTC1aIi7atnLhVZcvfG6kztgbNxqHjAjI4OMjAynfPby7WdIjgkiJTbYKZ8vOhlrB0zBk88SYyyiwkNPrLUTxhUdMDZutQdsq6qqKkwmU71HUz7dk8/8fxzhyRV7OF1c1uQ2oguaMIGihx4F4GxwTzywHoK6oAPGpksHcMGCBRgMhrpHTExMk9uNG9iTgdEGrlRc54llu7lcXu3iSoXLmc2waRPm778HwJiUAitXwsaNcOaMS8IHXTyAc+bMwWg01j3y8/Ob3M5P78GyqUOJC/Xl3JVKnv/bfpSmTshF12DtfmHsWIKPHwZAV1gAej2kpbmkA8amSwdQr9cTGBhY79GcMH89f348FS8PLf85VsRHu3JdWKlwGbvulxq7AZjwi/ku636x16UD2Fb9egYyJyMRgIVfHeei6dpN3iHcSoP1X84ZIvAy13DV259YaxBd0f1iz60CWFZWxr59+9i3bx8AZ86cYd++feTl5TnsO6aMiCc5Joiyqhp+v/6owz5XdAIN7oB7ydcAQG5QT0sQXNT9Ys+tArh3715SUlJISUkBYPbs2aSkpDBv3jyHfYdWq+H345PQauAf+wvZcbLEYZ8tVNawq8U621Cq9215OydyqwCmpaWhKEqjx4oVKxz6PUm9DDx+exwAr//zKLW1MiDTJTToagmxLj3vVXO9xe2cya0C6Eoz77kVPy8dhwpM/PPQBbXLEY5gt/5LtUZHzFXLle9RpcWW113U/WJPAtiMED8vpo3pDcAfso9TY669yTtEp2ftfgHID47EQ6mlxNdAL1OxS7tf7EkAWzBtTALBvp6cLi7n85wCtcsRjjBhAqxZw5UQyyVI+YYIy6mgC7tf7EkAWxDg7cnTd94CwNLNpzDLuWDXMGECml69AKhIGuTy7hd7EsCbmDQ8lkBvD04Xl5N9RM4Fu4rQ44cA8H5gnMu7X+xJAG8iwNuTKSPjAViy6ZS0qLk7s5lr674k5qKl0ykqbaSq5UgAW2HqyHi8PbUcOGdk+8lLapcj2svaA5o79Rl0Si3FvkFEZtzl8vYzexLAVgj11/Po0FgAlm4+qXI1ol3sekCNPgEAnDOEo3HhCmhNkQC20s/v6I1Oq2H7yUscu9D0dYWik2rQA2pbeqLSU+/SFdCaIgFspV5BPtz7g0gAVmw/q24xom0a9ID2KL8CgE+1a1dAa4oEsA2eHB0PQFZOAZfKqtQtRrSeXW9nuYe+rgMmxnYPiCa2cxUJYBsMjg1mYLSB6ppaVu123BUYwsnsejtzg3uiRaHIL5iwSmOz27mKBLANNBoNT45KAODDnblU10h7mluw6wE1evsDUBDY48brKvSA2kgA2+i+AT0JD9BTVFrFlwddf8gi2sGuB9Sz1jLQUuXhZXlNpR5QmzYHcOrUqWzZssUZtbgFLw8tT4ywXKq0bPsZmZh3F9YeUNsAjG91peV5lXpAbdocwNLSUtLT0+nbty+vv/46BQXdr0l54rBYvDwsE/Pf5l5RuxzRSqYxaTcGYF56XtUeUJs2B/Czzz6joKCA6dOn87e//Y34+HgyMjJYs2YN169fv/kHdAGh/noeSrY0876/7YzK1YjWyvv3NrQoXDSEE5z5tKo9oDbtOgcMDQ1lxowZ5OTksHv3bvr06cPkyZOJiopi1qxZnDhxwtF1djo/s05J/OvwBfIvV6hbjLg5sxnT2vUAnI/to8qke1M6NAhz/vx5vv76a77++mt0Oh333Xcfhw8fpn///rz55puOqrFTSowMZHSfMGoV+HDnWbXLES2x9oB6brNMtFddMVrWBVWxB7SO0kbV1dXKmjVrlHHjximenp5KamqqsnTpUsVkMtVts2rVKiUoKKitH+10RqNRARSj0eiQz/vP0QtK3G/XKUnzvlJKr113yGcKB/vsM0XRaBQFlFxDhKKAcig8wfKcRmN53Qla+7fW5puz9OzZk9raWiZOnMju3btJTk5utM2PfvQjgoKCOvw/h84u7dZweof5cbqknDV785lqnSMUnYRdD+gVvT+x1s6XmKsXLe1nGo2lB/TBB93nesA333yTwsJC3nnnnSbDBxAcHMyZM11/cEKr1fCzUfEALN9xVq6Y72zsekDzgix9vBf8Qwmstp6zq9gDatPmAE6ePBlvb29n1OKWHk6NJtDbg9xLFWw4VqR2OcKeXW9nmXXtzwv+IS1u52rSCdNBvl4eTBxuuVbw/W2nVa5G1GPX26k3W6bIqq23o25uO1eTADrAlBHx6LQadp2+zOFC483fIFzDrge0p8my9qd/ld2UkYo9oDYSQAeICvIhI8lyjrFs21l1ixE3WHtAS3wC6VVqucVAzFXrwloq94DaSAAd5KnRlhHQf+wvpEjuqtR5TJhAwdMzAMg3hBNw3frfRuUeUBsJoIOkxAYzODaIanOttKd1MuVXLEuIFPXpr8pdcFsiAXSgzLF9APh4Vy5XK+Q216qz3obaZ9d2AGpGjIKJEztFD6hNmyfiRfPuSgynX89Ajp43sXz7WWb98Fa1S6rnakU1O05d4mCBkasV1/H10nFLD3/u7hdORGAXm1rKyrJMwp87R7RvEACGr9ZB1uBOseez0ShK97mgzWQyYTAYMBqNLd6uuiPWHzhP5srvCPT2YPsLdxHg3cSwt4vlXapg8X++Z92B801exa/RwA/7RfCbexPpE+6vQoUOZluCUFEo9g2iR8VVzBot1TpPfMzVLjn3a+3fmhyCOti9SZH07uGH6VoNH+5U9z7zVTVmFn51jHsWbSbruwKqa2rpE+7PpOGxzLrnVqaNTiA1LhhFga+PXCTjrS2scPeLjBssQWhbeiIvKBKfGutCWiotQdgUOQR1MJ1Ww3N39WHW6v38efMpHhsWS7Cfl8vrOFNSzvSV33G40DIAMbpPGL9Kv5XkmCA0tiF4q5NFpby2/igbjxfzyj+OcOxCKa89NACdVtPUR3duDZYgrPTUA1DsF0TClcL67WdpaSoVeYPsAZ3ggUG9SIwMoPRaDUs2uX4l7T1nL/PA29s4XGgi2NeTPz8+mI+eGkZKbHCj8AH0CQ9g2dShzPtxf7Qa+OuefJ5fs9897wzcoK3Mzzbt0PBfRcX2M3sSQCfQaTW8kJEIwAc7cjl3xXUX7G46XsTk97+h9FoNqXHBfDljDPcm9WwyePY0Gg1Pjk7grUdT0Gk1ZH1XwKLs711UtQPZtZWZNVr+544n+O87pxJSof4ShE1xuwAuWbKEhIQEvL29SU1NZauKnewtufPWHozoHUq1uZY/fO2aP+R1Bwr5+Yd7uXa9lrsSw/lk2nB6Gnza9Bn3D4rivycMAODtjSdZd6DQGaU6j1372cnQGLYmDObDweNIuGrd43WC9jN7bhXA1atXM3PmTF588UVycnIYM2YMGRkZ5OV1vkVyNRoNc+6z7AU/zylg95nLTv2+VbvzeG5VDtfNCg8MiuJ/J6fi7dm+ua6fDInh6Tstt+eem3WQwquVjizVueyWINwXdRsAA8+fQKfUdpr2M3tuFcBFixbx1FNPMW3aNPr168fixYuJiYlh6dKlapfWpIHRQUwcFgPA3M8PUlXjnJG3/918ijlZB1EUyw1F3/xpMp66jv2n/XX6bQyKCcJ0rYZZq/e517WO1iUIc/oOBiCl8Ljl+U7SfmbPbQJYXV3Nt99+S3p6er3n09PT2bFjR5PvqaqqwmQy1Xu42gv39iPM34uTRWX8eZNjL1dSFIWFXx1jwT+PAfBs2i38fnySQ0YvPXVa3vppMr5eOr45c5lPvlF3SqXNJkxg36h7AUieOK5TtZ/Zc5sAlpSUYDabiYiIqPd8REQEFy40fevoBQsWYDAY6h4xMTGuKLUeg68nL/24PwB/2nDCYeuI1tYqvLT2EEs2nQLgt/cm8tt7E2862NIW8WF+zLEOJv3Pv45TXOo+N6Qpq6rheFEZACmP3d+p2s/suU0AbRr+gSmK0uwf3Zw5czAajXWP/Px8V5TYyAODorh/UBQ1tQq/XJXT4T7Rqhozv/xrDh/vykOjgdceSuLZtFscVG19jw2PI6lXIKXXaljwz6NO+Q5nOJB/FUWx3FYuvBO32blNAMPCwtDpdI32dkVFRY32ijZ6vZ7AwMB6DzVoNBpefyiJuFBfCq5WWgdL2ndjF9O160xZtpt1B87jqdPw1qMpTBoe5+CKb9BpNfzuwSQ0Gsj6roCcPPdYCTwn/yoAybFBqtZxM24TQC8vL1JTU8nOzq73fHZ2NiNHjlSpqtYL8PbknccG4+OpY+uJEn6z5kCbBzZOF5fxX0t3sOv0Zfy8dCyfOowHBkU5qeIbUmKDeXhwNAD/76tjbtGqlpN3FYCUmCBV67gZtwkgwOzZs3nvvfdYtmwZR48eZdasWeTl5fHMM8+oXVqrJPUysOTxwei0Gj7PKSDzk++4dv3mI6OKorB2XwEPvL2d7y+W0SNAz+qnRzC6b5gLqraY9cNb8fLQsuv0ZTZ/X+yy720PRVHYZ90Dpsge0HF++tOfsnjxYl599VWSk5PZsmULX375JXFxzjsEc7Sxt4Xzp4kpeOm0fHX4Avf/aVvdH0tTTlws5Yllu5nx132UVdUwLD6E9c+NJqmXwXVFYzmXeuJ2y+954VfHO3Wb2rkrlZSUVeGp0/CDKNf+ntpKLkdSya7Tl3huVU7dyOKYvmGk94+gdw9/ahWFU0Vl/PtoEdtOWtYy8fLQkpnWh1+MvaXDc3ztdaW8mjELN1JWVcO7k1NJ/0GkKnXczOc555i1ej+DYoJYmzlKlRpa+7cmV0Oo5Pbeofxr5h28tv4of99XwNYTJWw9UdJoO40GftQ/kt9mJJIQ5qdCpTcE+3nxxIg4lmw6xdsbT/LD/hEOnfZwlN1nLANFw+KDVa7k5iSAKgrx8+IPjwxixt19+WJ/Ad+cucxF0zXL8HmwD7f3DiUjKZK4UHWDZ++p0Qks236GA+eMbD1Rwh239rj5m1xsz1lL29/Q+CYW4e1kJICdQGyoL9Pv6st0tQtphVB/PY8Ni2PZ9jO8veFkpwvg5fJqTlon4Ie4QQDdahBGdA7/547eeOm07D572elN5m2117r36xPuT4gKF0K3lQRQtFmkwZuHUy3zgn/Z2rmW43enw0+QAIp2mjbGshDxv49e5ExJucrV3LD7rHUAJqHzD8CABFC00y09/LkrMRxFgeXbO8dCxBXVNRwusFz5LntA0eXZluP/295zGCuuq1yNpf2splYhyuBNdLCv2uW0igRQtNvIW0JJjAyg8rqZlbvVX5XANiA0NME99n4gARQdoNFomDbGsnTFBzvOtvsKD0fZccrSyDA8IVTVOtpCAig65P5BPQnz13PBdI0vD6q31F95VU3dFRBjXNik3lESQNEheg8dU0ZYmrTf26reqtq7z1ymplYhJsSHmBD3OP8DCaBwgMeGx6L30HKwwMheBy250VbbrU3ro/u4z94PJIDCAUL99TyU0guAZSrdG9F21cjIWySAohv62SjLlMS/Dl8g/7LrVgIHKCmr4tiFUsAyMutOJIDCIW6LDGBM3zBqFcuIqCvtOHUJgP49Awn117v0uztKAigc5knrXnD1nnzKqmpc9r0bjxUB7jX6aSMBFA5z56096B3mR2lVDWv2umYJSHOtwsbjlgDe3a/p1fE6MwmgcBitVsPPRsUDsHzHWZesG5OTd4WrFdcx+HgyuJMvwNQUCaBwqIdTown09iD3UgX/sR4aOpPtO9Ju64GHSmvldIT7VSw6NV8vDyYOjwVcMyWx4aglgHclhjv9u5xBAigcbsqIeHRaDTtPX+JIofNuiJN/uYLjF0vRaTXc2cmWxmgtCaBwuKggHzKSLEsWLnPitYL/Omy5TUFqXDBBvp1/+YmmSACFUzxpvVbwi32FTrur0roDlubvcQM6x+2m20MCKJxicGwwyTFBVJtrnXJvwfzLFezLv4pGQ93e1h1JAIXT2PaCH+w4S7mDJ+b/eciy9xueENKpbz92MxJA4TT3JUUSF+rLlYrrDt0LKorC5zmFAIwb6Py7QzmTBFA4jYdOS+bYPgC8u+U0ldU3vxNUaxwuNHH0vAkvDy33D3Tf8z+QAAoneyilF9HBPpSUVTts3ZjVeyxtbj/6QaTbjn7aSACFU3na7QX/vPlUq+6H2JLKajNr9xUA8MiQ6A7XpzYJoHC6hwdH0yvIh+LSKt7vYHfMZ9+dw3SthpgQH0a52cW3TZEACqfz8tDy/I9uA2DJxpPtnhesrVXqAvzkqAS02s53a7S2kgAKl3hgUBSDog2UV5tZlP19uz4j27oMfqC3B48MiXFwheqQAAqX0Go1/N8f9wfgr3vyyMlr2+JN5lqFN63BnXR7HH76rnFnPQmgcJmh8SE8lNILRYHfrDlAVU3rB2SyvjvHsQulBHp78PQdvZ1YpWu5TQBfe+01Ro4cia+vL0FBQWqXI9rppR/3J9TPixNFZbzxr+Oteo+x4jpvfG3ZNnNsH7eferDnNgGsrq7mJz/5Cc8++6zapYgOCPHz4rWHBgDwl61n+OrQzVfTfmntIS6aqkgI82PKyHgnV+habhPA+fPnM2vWLAYMGKB2KaKD7k2K5OfW+wvO+Ou+uns6NOXjXbl8sb8QnVbDokcG4e2pc1WZLuE2AWyPqqoqTCZTvYfoHH5zbyJ3J4ZTVVPLUyv2sv5A/T2hoih8vCuXeWsPATDrnr6kxLrHTTfbomsMJTVjwYIFzJ8/X+0yRBM8dVremTSYZz7+lk3Hi8lc+R1/3RNGRpKlt/Mf+wvZedqy3uek4bF13TRdjap7wFdeeQWNRtPiY+/eve3+/Dlz5mA0Guse+fmuWSpPtI63p473nhjCL9JuwUOrYeuJEuZ+fpC5nx9k5+lLeOm0vJCRyO/HJ6HRuP+ke1NU3QNOnz6dRx99tMVt4uPj2/35er0evd69Vkrubjx0Wn5zbyKPDInhs+/OcdB6i+kBvQw8MiTGre501B6qBjAsLIywMPfv5xMdFx/mx6/Sb1O7DJdzm3PAvLw8Ll++TF5eHmazmX379gHQp08f/P391S1OiHZymwDOmzePDz74oO7nlJQUADZu3EhaWppKVQnRMRpFrVuaqsBkMmEwGDAajQQGBqpdjujCWvu31qXnAYXo7NzmENQRbDt7mZAXzmb7G7vZAWa3CmBpqeUuqjExXeNaMtH5lZaWYjAYmn29W50D1tbWUlhYSEBAQJea2DWZTMTExJCfny/nti7U0u9dURRKS0uJiopCq23+TK9b7QG1Wi3R0e6/kE9zAgMDJYAqaO733tKez0YGYYRQkQRQCBVJALsAvV7Pyy+/LH2vLuaI33u3GoQRorORPaAQKpIACqEiCaAQKpIACqEiCaCbW7JkCQkJCXh7e5OamsrWrVvVLqlLW7BgAUOHDiUgIIDw8HDGjx/P8eOtW9+0KRJAN7Z69WpmzpzJiy++SE5ODmPGjCEjI4O8PMfch080tnnzZjIzM9m1axfZ2dnU1NSQnp5OeXl5uz5PpiHc2PDhwxk8eDBLly6te65fv36MHz+eBQsWqFhZ91FcXEx4eDibN2/mjjvuaPP7ZQ/opqqrq/n2229JT0+v93x6ejo7duxQqarux2i0LCIVEhLSrvdLAN1USUkJZrOZiIiIes9HRERw4cIFlarqXhRFYfbs2YwePZqkpKR2fUa3uhqiK2p4WZWiKF3qUqvObPr06Rw4cIBt27a1+zMkgG4qLCwMnU7XaG9XVFTUaK8oHO+5557jiy++YMuWLR26xE0OQd2Ul5cXqampZGdn13s+OzubkSNHqlRV16coCtOnTycrK4sNGzaQkJDQoc+TPaAbmz17NpMnT2bIkCGMGDGCd999l7y8PJ555hm1S+uyMjMzWblyJWvXriUgIKDuCMRgMODj49P2D1SEW3vnnXeUuLg4xcvLSxk8eLCyefNmtUvq0oAmH8uXL2/X58k8oBAqknNAIVQkARRCRRJAIVQkARRCRRJAIVQkARRCRRJAIVQkARRCRRJAIVQkARRCRRJAIVQkARRNKi4uJjIyktdff73uuW+++QYvLy++/vprFSvrWqQZWzTryy+/ZPz48ezYsYPExERSUlIYN24cixcvVru0LkMCKFqUmZnJv//9b4YOHcr+/fvZs2cP3t7eapfVZUgARYsqKytJSkoiPz+fvXv3MnDgQLVL6lLkHFC06PTp0xQWFlJbW0tubq7a5XQ5sgcUzaqurmbYsGEkJyeTmJjIokWLOHjwoCz65EASQNGs559/njVr1rB//378/f0ZO3YsAQEBrFu3Tu3Sugw5BBVN2rRpE4sXL+ajjz4iMDAQrVbLRx99xLZt2+othS86RvaAQqhI9oBCqEgCKISKJIBCqEgCKISKJIBCqEgCKISKJIBCqEgCKISKJIBCqEgCKISKJIBCqOj/A5Q72LxzjpzGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x * torch.cos(torch.pi * x)\n",
    "\n",
    "def gd(x, y=f, eta=0.01, iter:int=5):\n",
    "    x_list = [x]    # 先存第一个数\n",
    "    i = 1\n",
    "    for _ in range(iter):\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "        x_grad = torch.autograd.grad(outputs= f(x_tensor), inputs=x_tensor)\n",
    "        x -= (eta * x_grad[0].item())\n",
    "        x_list.append(x)\n",
    "        i += 1\n",
    "    return x_list\n",
    "\n",
    "# 从x开始，迭代iter次\n",
    "def demo(x, y, eta, iter, c):\n",
    "    xx = gd(x=x, y=f, eta=eta, iter=iter )\n",
    "    yy = f(torch.tensor(xx))\n",
    "    return xx, yy.detach().cpu().numpy(), c\n",
    "\n",
    "\n",
    "x = torch.arange(-1, 2, 0.01, dtype=torch.float32, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function')\n",
    "\n",
    "x, y, c = demo(x=2, y=f, eta=0.01, iter=15, c='red')    # lr很小就接近收敛\n",
    "plt.scatter(x=x, y=y, c=c)\n",
    "plt.plot(x, y, c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f3041ec30>]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.arange(-1, 2, 0.01, dtype=torch.float32, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function')\n",
    "\n",
    "x, y, c = demo(x=2, y=f, eta=0.1, iter=30, c='blue')    # lr很大就很快收敛\n",
    "plt.scatter(x=x, y=y, c=c)\n",
    "plt.plot(x, y, c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f3046f9b0>]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.arange(-1, 2, 0.01, dtype=torch.float32, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function')\n",
    "\n",
    "x, y, c = demo(x=0, y=f, eta=0.1, iter=30, c='blue')    # 陷入0左侧附近的局部最小值点\n",
    "plt.scatter(x=x, y=y, c=c)\n",
    "plt.plot(x, y, c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f304dc860>]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.arange(-1, 2, 0.01, dtype=torch.float32, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function')\n",
    "\n",
    "x, y, c = demo(x=0, y=f, eta=0.5, iter=5, c='blue')   # lr太大就乱跳\n",
    "plt.scatter(x=x, y=y, c=c)\n",
    "plt.plot(x, y, c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.1. <a id='toc8_7_1_'></a>[小批量随机梯度下降（SGD）](#toc0_)\n",
    "- 随机梯度下降法，利用单个样本进行估算所有样本的梯度，然后进行后续的优化。这是计算效率很低的方式，所有改成小批量的随机梯度下降，可以提高计算效率。  \n",
    "- `小批量随机梯度下降法`是最常用的优化算法；\n",
    "- batch_size是所有样本，就是`梯度下降`。\n",
    "- `动量 (momentum)` 可以起到缓冲的作用，使得优化方向不会不停跳动，而是考虑之前几步的方向，具体考虑前多少步依赖于值的大小，一般取值为：0.5, 0.90, 0.99 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0.99\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.optim.SGD(\n",
    "    params=net.parameters(), \n",
    "    lr=0.01, \n",
    "    momentum=0.99, \n",
    "    # weight_decay=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.2. <a id='toc8_7_2_'></a>[adam](#toc0_)\n",
    "- Adam其实就是非常平滑的SGD，只是其对lr不敏感；  \n",
    "- Adam未必比SGD效果更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.optim.Adam(\n",
    "    params=net.parameters(), \n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.3. <a id='toc8_7_3_'></a>[RMSprop](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.99\n",
       "    centered: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.optim.RMSprop(\n",
    "    params=net.parameters(), \n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.4. <a id='toc8_7_4_'></a>[学习率调度器](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.1. <a id='toc8_7_4_1_'></a>[StepLR： 按照固定的步长调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.2. <a id='toc8_7_4_2_'></a>[MultiStepLR： 在指定的里程碑（milestones）上调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.3. <a id='toc8_7_4_3_'></a>[ExponentialLR： 以指数衰减的方式调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.4. <a id='toc8_7_4_4_'></a>[CosineAnnealingLR： 余弦退火调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.5. <a id='toc8_7_4_5_'></a>[ReduceLROnPlateau： 当指标停止改善时，降低学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.6. <a id='toc8_7_4_6_'></a>[LambdaLR： 使用自定义的函数来调整学习率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "lambda1 = lambda epoch: 0.65 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4.7. <a id='toc8_7_4_7_'></a>[自定义](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# torch.optim.lr_scheduler._LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.optim.lr_scheduler._LRScheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. <a id='toc8_8_'></a>[专题-训练](#toc0_)\n",
    "\n",
    "![Train step via pure PyTorch](./Pytorch_Pictures/PyTorch_graphacial_demo/Train_step_via_pure_PyTorch.jpg)\n",
    "\n",
    "```python\n",
    "训练的模板代码\n",
    "```\n",
    "```python\n",
    "net.train():\n",
    "    启用 Batch Normalization 和 Dropout。\n",
    "    如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()\n",
    "    model.train()作用： \n",
    "                        对BN层，保证BN层能够用到每一批数据的均值和方差，并进行计算更新；\n",
    "                        对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。\n",
    "\n",
    "net.eval()\n",
    "    不启用 Batch Normalization 和 Dropout。\n",
    "    如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。\n",
    "    model.eval()是保证BN层直接利用之前训练阶段得到的均值和方差，即测试过程中要保证BN层的均值和方差不变；\n",
    "                        对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。\n",
    "                        \n",
    "with torch.no_grad():\n",
    "    pass\n",
    "\n",
    "    无论是train() 还是eval() 模式，各层的gradient计算和存储都在进行且完全一致，在forward的时候会保存中间结果和创建计算图以为后续的\n",
    "    反向传播做准备。而with torch.no_grad()则主要是用于停止autograd模块的工作，在内存中不储存的forward计算结果和不构建计算图，以起到加速和节省显存的作用。它的作用是将该with语句包裹起来的部分停止梯度的更新，从而节省了GPU算力和显存，但是并不会影响dropout和BN层的行为。\n",
    "    若想节约算力，可在test阶段带上torch.no_grad()，示例代码：\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./Pytorch_datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3659860.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Pytorch_datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./Pytorch_datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./Pytorch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 119645.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Pytorch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./Pytorch_datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./Pytorch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 912416.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Pytorch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./Pytorch_datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./Pytorch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 17224709.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Pytorch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./Pytorch_datasets/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据准备\n",
    "import torch \n",
    "from torch import nn  \n",
    "# import torch.nn.functional as F \n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "dbs = './Pytorch_datasets/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10), \n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.network(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程封装\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import IPython.display as display\n",
    "import os\n",
    "\n",
    "\n",
    "def train_steps(\n",
    "        epochs, \n",
    "        train_dataset, \n",
    "        train_iter, \n",
    "        test_dataset, \n",
    "        net, \n",
    "        loss_fn, \n",
    "        opt, \n",
    "        device, \n",
    "        train_figure = False, \n",
    "        resume = False, \n",
    "        PATH = 'Pytorch_params/weights'\n",
    "    ):\n",
    "    '''\n",
    "    参数记录:\n",
    "            epochs = epochs                         # epoch\n",
    "            train_dataset = train_dataset           # 全部train数据集\n",
    "            train_iter = train_iter                 # batch之后的train数据集\n",
    "            test_dataset = test_dataset             # 全部test数据集\n",
    "            net = net                               # 网络模型\n",
    "            loss_fn = loss_fn                       # 损失函数\n",
    "            opt = opt                               # 优化器\n",
    "            device = device                         # device GPU/CPU\n",
    "            train_figure = False                    # 可视化训练过程\n",
    "            resume = False                          # 断点续训\n",
    "    '''\n",
    "    # 拷贝数据和模型到device上\n",
    "    print('='*100)\n",
    "    print(f\"Runing on {device}\")\n",
    "    print('='*100)\n",
    "    ## 数据\n",
    "    train_all_data_gpu = train_dataset.data.to(device)                                      # .to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)                                # .to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)                                        # .to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)                                  # .to(device)\n",
    "    ## 模型\n",
    "    net.to(device)                                                                          # .to(device)\n",
    "\n",
    "    def dl_plot(epochs:int, epoch_list:list, train_loss_list:list, train_acc_list:list, test_acc_list:list):\n",
    "        '''绘图'''\n",
    "        plt.rcParams['font.sans-serif']=['Times new roman', 'Arial', 'KaiTi']\n",
    "        plt.style.context(['ggplot', 'seaborn'])\n",
    "        \n",
    "        plt.close()\n",
    "        fig = plt.figure(figsize=(3.0, 3.0))\n",
    "\n",
    "        # for y, label in zip([train_loss_list, train_acc_list, test_acc_list], ['train_loss', 'train_acc', 'test_acc']):\n",
    "        for y, label in zip([train_acc_list, test_acc_list], ['train_acc', 'test_acc']):\n",
    "            plt.plot(epoch_list, y, label=label)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((1, epochs))\n",
    "        plt.ylabel('Values')\n",
    "        plt.ylim((0, 1))\n",
    "        plt.yticks(torch.arange(0, 1, 0.05).numpy())\n",
    "        # plt.tight_layout()\n",
    "\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    best_test_acc = 0\n",
    "\n",
    "    # 断点续训\n",
    "    start_epoch = 0\n",
    "    if resume:\n",
    "        if os.path.isfile(PATH+'/last.pt'):\n",
    "            check_point = torch.load(PATH+'/last.pt')\n",
    "            start_epoch = check_point['epoch']\n",
    "            net.load_state_dict(check_point['model_state_dict'])\n",
    "            opt.load_state_dict(check_point['opt_state_dict'])\n",
    "        else:\n",
    "            print(f'没有训练记录。')\n",
    "        \n",
    "    print('start_epoch: ', start_epoch)\n",
    "    for epoch in range(start_epoch, epochs, 1):\n",
    "        net.train()                             # 训练模式\n",
    "        epoch_list.append(epoch+1)\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   ## 复制到device（GPU/CPU）上                    # .to(device)\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            y_hat = net(X)                      # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)            # 计算loss\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            loss.backward()                     # 计算梯度\n",
    "            opt.step()                          # 更新网络参数\n",
    "\n",
    "        net.eval()                              # 切换至评估模式\n",
    "                                                # 模型默认是net.train()\n",
    "                                                # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                                                # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad():                   # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            train_loss_list.append(train_loss.item())\n",
    "            # print(train_loss)\n",
    "\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) \n",
    "            train_acc_list.append(train_acc.item())\n",
    "            # print(train_acc)\n",
    "\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp))\n",
    "            test_acc_list.append(test_acc.item())\n",
    "            # print(test_acc)\n",
    "\n",
    "            if train_figure:\n",
    "                if epoch % 1 == 0:\n",
    "                    dl_plot(epochs, epoch_list, train_loss_list, train_acc_list, test_acc_list)\n",
    "            else:\n",
    "                print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "        # 保存权重参数：last.pt和best.pt\n",
    "        torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/last.pt') \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/best.pt') \n",
    "\n",
    "    stop = time.time()\n",
    "    print('='*100)\n",
    "    print(f\"耗时： {stop - start} seconds.\")\n",
    "    return (train_loss, train_acc, test_acc)\n",
    "    # return (epoch_list, train_loss_list, train_acc_list, test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程封装\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import IPython.display as display\n",
    "import os\n",
    "\n",
    "def training_step(\n",
    "        epochs, \n",
    "        train_dataset, \n",
    "        train_iter, \n",
    "        test_dataset, \n",
    "        net, \n",
    "        loss_fn, \n",
    "        opt, \n",
    "        device, \n",
    "        train_figure = False, \n",
    "        resume = False, \n",
    "        PATH = 'Pytorch_params/weights'):\n",
    "    '''\n",
    "    训练过程\n",
    "    params:\n",
    "            epochs = epochs                         # epoch\n",
    "            train_dataset = train_dataset           # 全部train数据集\n",
    "            train_iter = train_iter                 # batch之后的train数据集\n",
    "            test_dataset = test_dataset             # 全部test数据集\n",
    "            net = net                               # 网络模型\n",
    "            loss_fn = loss_fn                       # 损失函数\n",
    "            opt = opt                               # 优化器\n",
    "            device = device                         # device GPU/CPU\n",
    "            train_figure = False                    # 可视化训练过程\n",
    "            resume = False                          # 断点续训\n",
    "    return:\n",
    "            tra_loss, val_loss, val_acc, test_loss, test_acc\n",
    "    '''\n",
    "    # 拷贝数据和模型到device上\n",
    "    print('='*100)\n",
    "    print(f\"Runing on {device}\")\n",
    "    print('='*100)\n",
    "    ## 数据\n",
    "    train_all_data_gpu = train_dataset.data.to(device)                                      # .to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)                                # .to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)                                        # .to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)                                  # .to(device)\n",
    "    ## 模型\n",
    "    net.to(device)                                                                          # .to(device)\n",
    "\n",
    "    def dl_plot(epochs:int, epoch_list:list, train_loss_list:list, train_acc_list:list, test_acc_list:list):\n",
    "        '''绘图'''\n",
    "        plt.rcParams['font.sans-serif']=['Times new roman', 'Arial', 'KaiTi']\n",
    "        plt.style.context(['ggplot', 'seaborn'])\n",
    "        \n",
    "        plt.close()\n",
    "        fig = plt.figure(figsize=(3.0, 3.0))\n",
    "\n",
    "        # for y, label in zip([train_loss_list, train_acc_list, test_acc_list], ['train_loss', 'train_acc', 'test_acc']):\n",
    "        for y, label in zip([train_acc_list, test_acc_list], ['train_acc', 'test_acc']):\n",
    "            plt.plot(epoch_list, y, label=label)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((1, epochs))\n",
    "        plt.ylabel('Values')\n",
    "        plt.ylim((0, 1))\n",
    "        plt.yticks(torch.arange(0, 1, 0.05).numpy())\n",
    "        # plt.tight_layout()\n",
    "\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    best_test_acc = 0\n",
    "\n",
    "    # 断点续训\n",
    "    start_epoch = 0\n",
    "    if resume:\n",
    "        if os.path.isfile(PATH+'/last.pt'):\n",
    "            check_point = torch.load(PATH+'/last.pt')\n",
    "            start_epoch = check_point['epoch']\n",
    "            net.load_state_dict(check_point['model_state_dict'])\n",
    "            opt.load_state_dict(check_point['opt_state_dict'])\n",
    "        else:\n",
    "            print(f'没有训练记录。')\n",
    "        \n",
    "    print('start_epoch: ', start_epoch)\n",
    "    for epoch in range(start_epoch, epochs, 1):\n",
    "        net.train()                             # 训练模式\n",
    "        epoch_list.append(epoch+1)\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   ## 复制到device（GPU/CPU）上                    # .to(device)\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            y_hat = net(X)                      # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)            # 计算loss\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            loss.backward()                     # 计算梯度\n",
    "            opt.step()                          # 更新网络参数\n",
    "\n",
    "        net.eval()                              # 切换至评估模式\n",
    "                                                # 模型默认是net.train()\n",
    "                                                # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                                                # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad():                   # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            train_loss_list.append(train_loss.item())\n",
    "            # print(train_loss)\n",
    "\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) \n",
    "            train_acc_list.append(train_acc.item())\n",
    "            # print(train_acc)\n",
    "\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp))\n",
    "            test_acc_list.append(test_acc.item())\n",
    "            # print(test_acc)\n",
    "\n",
    "            if train_figure:\n",
    "                if epoch % 1 == 0:\n",
    "                    dl_plot(epochs, epoch_list, train_loss_list, train_acc_list, test_acc_list)\n",
    "            else:\n",
    "                print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "        # 保存权重参数：last.pt和best.pt\n",
    "        torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/last.pt') \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'opt_state_dict':opt.state_dict(), 'loss':test_acc}, PATH+'/best.pt') \n",
    "\n",
    "    stop = time.time()\n",
    "    print('='*100)\n",
    "    print(f\"耗时： {stop - start} seconds.\")\n",
    "    return (train_loss, train_acc, test_acc)\n",
    "    # return (epoch_list, train_loss_list, train_acc_list, test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.1. <a id='toc8_8_1_'></a>[开始训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "耗时： 59.95664095878601 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.7594, device='cuda:0'),\n",
       " tensor(0.7902, device='cuda:0'),\n",
       " tensor(0.8000, device='cuda:0'))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Times new roman, Arial, KaiTi\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.01)\n",
    "\n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device, \n",
    "    train_figure=True, \n",
    "    resume = False, \n",
    "    PATH = './Pytorch_params/weights'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32820/198423743.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best = torch.load('./Pytorch_params/weights/best.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " tensor(0.8000, device='cuda:0'),\n",
       " OrderedDict([('network.1.weight',\n",
       "               tensor([[-0.0241, -0.0069,  0.0088,  ..., -0.0267, -0.0129, -0.0065],\n",
       "                       [ 0.0302, -0.0323,  0.0237,  ..., -0.0168,  0.0240,  0.0341],\n",
       "                       [-0.0151,  0.0176, -0.0027,  ...,  0.0083, -0.0131,  0.0065],\n",
       "                       ...,\n",
       "                       [ 0.0030,  0.0136, -0.0048,  ..., -0.0145, -0.0219, -0.0337],\n",
       "                       [-0.0136, -0.0261,  0.0065,  ..., -0.0004,  0.0057,  0.0013],\n",
       "                       [-0.0356, -0.0046, -0.0026,  ..., -0.0298,  0.0247,  0.0137]],\n",
       "                      device='cuda:0')),\n",
       "              ('network.1.bias',\n",
       "               tensor([ 3.9702e-02,  3.4285e-02,  4.3656e-02, -7.5687e-04,  4.5628e-02,\n",
       "                       -1.5764e-03,  9.1527e-03,  1.2884e-02,  5.1464e-02,  1.3203e-02,\n",
       "                        3.2777e-02,  2.3281e-02, -7.0645e-07, -2.0750e-02,  1.0967e-02,\n",
       "                        1.6538e-02, -3.8702e-02,  1.7775e-02,  4.1511e-02,  1.2678e-02,\n",
       "                        2.1664e-02, -2.0623e-02,  3.2701e-02,  2.3492e-02,  4.9457e-03,\n",
       "                        3.9780e-02, -1.1985e-02,  4.2158e-02,  5.6506e-02,  2.7593e-02,\n",
       "                        9.5884e-03,  2.4593e-02,  5.3445e-03,  1.0407e-02, -2.9220e-02,\n",
       "                       -7.0291e-03,  1.5674e-02, -1.9235e-02,  2.1151e-02,  2.2816e-02,\n",
       "                        7.4965e-03,  5.0568e-02, -8.6301e-03, -1.1696e-02,  1.7590e-02,\n",
       "                        5.1232e-02, -1.1965e-02,  3.1024e-02,  3.3625e-02,  1.2786e-02,\n",
       "                        2.7856e-02,  2.0230e-02,  1.9761e-02,  4.9548e-02,  4.0749e-02,\n",
       "                        1.2994e-02,  5.2624e-02,  2.0533e-02,  2.9313e-02,  6.8340e-03,\n",
       "                       -2.8082e-02,  1.5062e-02, -3.0884e-02, -2.3156e-02, -2.2881e-03,\n",
       "                        2.6252e-03, -3.0055e-02, -1.5311e-02,  3.1937e-02,  3.6528e-02,\n",
       "                        3.3189e-02, -1.0149e-02,  2.0503e-02, -1.4367e-02,  4.2755e-02,\n",
       "                       -1.9800e-02, -1.1765e-02,  3.1096e-02,  2.9567e-02, -2.6079e-02,\n",
       "                        2.4057e-02,  2.8671e-02,  6.5098e-03,  1.6217e-02,  1.6545e-02,\n",
       "                        1.9598e-02,  3.9184e-02, -1.1046e-02,  6.2070e-03,  3.1246e-02,\n",
       "                        9.4902e-03,  3.0827e-02, -1.2420e-02, -1.8113e-02, -7.5191e-03,\n",
       "                       -2.3051e-02,  1.2938e-02, -1.2393e-02, -2.3524e-02, -9.9774e-03,\n",
       "                        8.9223e-03,  3.3172e-02,  3.3347e-02,  1.9719e-02, -5.4457e-03,\n",
       "                        1.0521e-02, -2.5926e-03,  3.6220e-03, -8.9649e-03,  3.5096e-03,\n",
       "                        3.9662e-02,  8.8037e-03,  2.5711e-02, -6.7204e-03, -8.7729e-03,\n",
       "                        1.3268e-02,  2.2215e-02, -7.2406e-03,  1.6775e-03, -1.3178e-02,\n",
       "                        2.2741e-02, -1.6813e-02,  6.5540e-03,  4.7898e-02,  1.7329e-02,\n",
       "                       -1.8237e-02,  2.3492e-02, -2.1715e-02,  3.1797e-02, -2.3865e-03,\n",
       "                        1.9146e-03,  2.0191e-02,  2.1575e-02, -5.3633e-03,  1.5864e-02,\n",
       "                        4.8251e-02, -1.8978e-03, -2.8512e-03,  3.3479e-02,  3.3359e-02,\n",
       "                       -2.1005e-02,  1.0200e-04, -2.5433e-02, -8.3805e-03,  5.2502e-02,\n",
       "                       -2.7321e-02,  6.5998e-02,  8.3883e-03,  2.5566e-02,  8.6262e-03,\n",
       "                       -2.2832e-02,  4.9074e-02, -2.7729e-02, -6.6179e-03,  3.1971e-02,\n",
       "                        4.1276e-02, -2.6954e-02, -9.3661e-03, -6.8694e-03,  1.2666e-02,\n",
       "                       -1.8432e-02, -3.2942e-02,  3.6010e-02,  1.3340e-02,  1.3230e-02,\n",
       "                        1.2177e-03, -4.0268e-03,  2.4358e-02, -1.4252e-02,  1.7725e-02,\n",
       "                        1.4390e-02,  4.5555e-02,  4.3820e-03, -9.9201e-03,  2.5283e-03,\n",
       "                       -1.3692e-02, -3.6250e-02,  2.1885e-03,  3.5891e-02,  2.4769e-02,\n",
       "                        9.7989e-03,  4.4549e-02,  1.1250e-02,  2.9513e-04, -8.4876e-03,\n",
       "                       -1.0764e-02, -5.3896e-03,  3.9276e-02,  3.1027e-02,  1.1406e-02,\n",
       "                        3.0260e-02,  1.2267e-02,  4.6958e-03,  3.7215e-02,  3.9977e-02,\n",
       "                        3.3820e-02, -1.0931e-02,  1.0565e-02,  1.1181e-02, -1.3401e-02,\n",
       "                        3.6825e-02,  4.5035e-02, -2.7077e-02, -2.0625e-02, -1.2974e-02,\n",
       "                       -2.3754e-02,  3.1041e-02,  4.6583e-02,  5.2671e-02, -1.2074e-02,\n",
       "                       -1.2163e-02, -2.2311e-02, -1.4121e-02, -2.3022e-02,  1.5178e-02,\n",
       "                        2.2413e-02,  2.1362e-02, -1.1371e-02, -2.4531e-02,  3.2458e-02,\n",
       "                        2.1671e-02,  3.3461e-02,  6.0620e-03,  2.3813e-02,  4.7138e-02,\n",
       "                       -3.1214e-02, -2.6454e-03, -2.5188e-02,  3.4154e-02,  2.0549e-02,\n",
       "                       -9.4273e-03,  2.9883e-02, -8.0837e-03,  4.4898e-02,  5.3602e-03,\n",
       "                       -2.3646e-02,  2.8702e-02,  2.7271e-02,  5.5416e-02, -2.3502e-02,\n",
       "                        1.4266e-02, -4.1066e-02,  3.4123e-02,  2.6651e-02,  3.1687e-02,\n",
       "                        2.6590e-02, -3.7192e-03,  2.8239e-02,  5.7477e-03, -2.0121e-02,\n",
       "                        5.4461e-03,  3.0526e-02, -9.5012e-03,  2.7479e-02, -2.8464e-02,\n",
       "                        4.6071e-02], device='cuda:0')),\n",
       "              ('network.3.weight',\n",
       "               tensor([[-0.0505, -0.1163, -0.0902,  ..., -0.0762,  0.0453,  0.0470],\n",
       "                       [ 0.0724,  0.1765,  0.0936,  ..., -0.0425,  0.0104, -0.0149],\n",
       "                       [-0.0884, -0.0270,  0.0364,  ..., -0.0410, -0.0254, -0.0206],\n",
       "                       ...,\n",
       "                       [-0.0540,  0.0514, -0.1432,  ..., -0.1346, -0.0148,  0.1571],\n",
       "                       [ 0.0946,  0.0494,  0.0184,  ...,  0.0630,  0.0512, -0.0797],\n",
       "                       [ 0.0776,  0.0689, -0.0066,  ..., -0.0522,  0.0436, -0.0084]],\n",
       "                      device='cuda:0')),\n",
       "              ('network.3.bias',\n",
       "               tensor([-0.0526,  0.1243, -0.0987,  0.0301,  0.0522, -0.0409,  0.0423,  0.0652,\n",
       "                       -0.1231,  0.0462], device='cuda:0'))]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = torch.load('./Pytorch_params/weights/best.pt')\n",
    "best['epoch'], best['loss'], best['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "耗时： 70.18707489967346 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.7867), tensor(0.7317), tensor(0.7402))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAEmCAYAAAAZYee/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABDTUlEQVR4nO3dd1yV9fvH8dc5h71EXIAoYs6cpeZIMxTNQY5cXwffEkeKYo5McWRqKYqaVt9CzY0r+ZVpmaG5ysiVZg7MgYjgADFA1oFz7t8fp3MCGR7mGXyej4cP5Yz7fC7By3Of+37fl0ySJAlBEAQzJTf0AgRBEMqTaHKCIJg10eQEQTBroskJgmDWRJMTBMGsiSYnCIJZE01OEASzJpqcIAhmzcLQCzAktVpNfHw8jo6OyGQyQy9HEIRcJEkiNTUVd3d35PKSvx+r1E0uPj6eOnXqGHoZgiAUITY2Fg8PjxI/v1I3OUdHR0Dzl+jk5FQur5GdnU1ERAQ9e/bE0tKyXF6jIol6jJc51QKQlJSEl5eX7t9pSVXqJqfdRXVycirXJmdnZ4eTk5NZ/OCJeoyXOdUCmnqAUn+UJA48CIJg1kSTEwTBrIkmJwiCWavUn8kJlZMkSeTk5KBSqcjOzsbCwoLMzExUKpWhl1YqplaLQqHAwsKi3E/fMmiTS0tLY+bMmVSpUoW0tDRCQkKwtrbO85jk5GRmzpyJm5sb0dHRTJ8+ndatW+vuP3nyJJ07dwbA0tKSO3fu4OrqWpFlCCZEqVRy79490tPTAU3Dc3V1JTY21uTPlTTFWuzs7HBzc8PKyqrcXsOgTW7ixIkMHDiQgQMHsnXrVoKCgli1alWex0yaNAlfX1/+85//cP/+fTp37szFixexs7MDYM+ePRw6dAjQHCUVDU4ojFqtJjo6GoVCgbu7O1ZWVkiSxJMnT3BwcCjVCafGQK1Wm0wtkiShVCpJSEggOjqahg0blt+aJQOJi4uTbGxspIyMDEmSJOnhw4eSra2tlJKSontMZmampFAopEuXLulu69Kli7Ru3TpJkiQpKipKGjZsmPTgwYMSrSE5OVkCpOTk5FJUUjSlUint3btXUiqV5fYaFcmU68nIyJCuXLkipaWl6W5TqVTS48ePJZVKZcCVlQ1TrCUtLU26cuWKrg/klpiYWCb/Pg32Tu7YsWNUr14dGxsbAGrUqIG1tTWnT5+me/fugGZ3VqVSERcXR7NmzQCoU6cOly5dAiAsLIxvv/2WPXv2MHXqVIKDg4s8PygrK4usrCzd1ykpKYDmswztOTllTbvd8tp+RTPlerKzs5H+GWmiVqsBdF9LkqS7zVSZai2SJJGdnY1Cochze1n9jBmsycXFxeHi4pLnNgcHB+Lj43Vfu7i40KZNG9asWUP37t1JS0sjKiqKTp06AbB48WLmz59PeHg4kydPRi6XExISUuhrLl26lIULF+a7PSIiQrf7W160u9TmwhTrsbCwwNXVlSdPnqBUKvPcl5qaaqBVlT1TqkWpVJKRkcGJEyfIycnJc5/2c9PSMliTk8lkundxWkqlMt87sfDwcN59910GDhxIt27duHLlCv7+/rr7raysGDFiBK6urvj6+hIcHJzvfwStoKAgpk+frvs6JSWFOnXq0LNnz3JNPBw6dIgePXqYzVnoplpPZmYmsbGxODg46H72pH9C4OZwkQZTrCUzMxNbW1teeeWVfP3g0aNHZfIaBmty7u7uJCcn57ntyZMnuLu757mtXr16hIeHA3DgwAFUKhVDhgzJt71u3brh6elJYmIitWrVKvA1ra2t8x29Bc1R2fL+B1sRr1GRTLEelUqFTCZDLpfrPuTW7tZpbzdlBdXi6+vLqFGj+M9//mPIpRVKLpcjk8kK/Hkqq58vg31Xvb29uXv3rm63Qbub+tJLLxX4eLVazeLFiwkKCqJmzZoFPqZu3bqF3icIlVFAQIDu453KymBNzs3NjV69enH8+HFA87lYQEAA1tbWzJkzh3v37uV5/MKFC6lfvz7z58/X3bZhwwbdu8Hw8HDGjh1rMm/TBaE4Ll68qPu3Uhx9+vShbt265bAi02HQ9+ehoaHs3r2bDz/8kIsXL/LRRx+RmZnJzp07iYmJAWD//v188MEH1K5dm7CwMCwsNHvYarWasLAwmjRpgp+fH5aWlgXuxgpCUSRJIkOpIl2ZU6G/tEdC9ZGcnMx///vfYj1H+JdBTwauXr06X375Zb7bo6OjdX9+/fXXef311/M9Ri6Xc/To0XJdn2D+MrJVdFz1W4W/7pVFr2Fnpd8/v6+++oro6GjWrl3L8ePH2bVrFwsXLmTKlCksXryYTp06sWrVKurXr8++fftYt24drVq14ujRo3z00Uf4+fkxYMAAVq9ezf79+1myZAkTJ07Ezs6O48eP5zvL4WmXL19m1apVPPfcc3z//feEhobSokULAPbt28f58+f5888/qVWrFp9++ilyuZyoqCg2b95MZmYmly5dYufOndSoUaPUf28lYdqftApCJTBu3DiqVq3K22+/jZ+fH1FRUcTFxbFp0ybat2/PggUL6Nq1K0FBQbRo0YL169cD0KFDB+Li4pAkCXt7e1q2bMmtW7fIzMzk2rVryOVy9uzZ88zX125/zpw5tG7dmnXr1gFw/vx5tmzZwoIFC1i3bh1r164lMjKStLQ0/Pz8WLBgAatXryYpKUn3HEMw+ezqkiVLSE5OJiEhgQULFuDp6VnBVQimzNZSQeT0Djg6OVbo0VVby4JPc3oWLy8vAAYMGKD789y5c/Hy8uLmzZvcuXNHd4aCra2t7kCchYUFzs7OODk50a9fPwBatGjBgwcPnvmaubd/+/Zt3TbXrl2Lt7c3oDmn9datW3h4ePDVV1/h6emJra0tAD/++GO5n4daFJPOrm7cuJEHDx6wZs0aoqOjGTp0KJGRkSZ/KoBQcWQyGbZWCuysLEzi50Z7YC33AbY6deqwbNky2rVrR6tWrXj48GG+xz/9Z9A0Pn2SEdrtt2/fnhdffJHY2FgAYmJiaNiwoe5x2gMcMTExeZJFhtpN1TLYdzU+Pp49e/bQu3dvAHr37k1oaGies7WzsrLYtWuXbv/f1dUVd3d3tm/fDsDy5cvp378/oPkf7smTJxw5cqSCKxEEw3rjjTfo2bMnAwYMKPRE+PLYvru7OwcPHtR9rVKpOHXqFO7u7vzyyy+kpaXp7jt58mSZr0tfJptdjY+P59q1a3l2Txs1asTx48fx8fEp8DVFdrX0TLkebXZVrVabXHbVysqKR48ecfXqVUBTi3a958+f58GDByQlJXHhwgWcnJy4efMmXl5eSJKESqVCrVaTk5OTp86n/y4Ko93+o0ePOHv2LHZ2dty8eZNhw4bx2muvMXfuXF5//XXCwsKYO3cuzz33HGq1muHDhzNr1iwiIyNp3rx5ga+jVqtFdrWw7GpcXJzuMYU9/2kiu1p2TLEeU86uDho0iClTpvDOO+8AsHLlSubMmYOTkxMBAQGMHz8eX19fevXqxfLly7l9+za3b9/m0qVLfPfdd7z00kvs2LGD+/fvs2fPHurWrcuZM2e4ceMGgwcPLvJcutzb9/Hx0W3/pZdeYsmSJaxevZpdu3axcuVK3edwYWFhzJgxg/79+zNt2jQ6dOige1ORW0VkV2WSgU6+WbFiBXv27OHUqVO622rVqsWaNWvyRFBu377Nu+++i1KppFu3bgQFBbFixQrat29Pu3btSE9P1/3FDhs2jKpVqxIaGlrgaxb0Tq5OnTokJiaK7KqeTLkebXa1Xr16IrtqJDIzM7l9+zZ16tQpMLvq5uZGcnJyqf59mmx2Vdv1k5OTdU0uNTVVt1tbEJFdLTumWE9lzK4au4rIrhqsyXl7ezN+/HiUSiVWVlYlyq42bdqU69ev664GfOPGDYKCgiqmAEEwE5s2bSo0MtazZ09GjBhRwSsqWwZrcrmzqz169MiXXQ0MDMTNzU33+IKyqwEBARw8eJAuXbpw69YtXFxc6NKliyHKEQSTNXr0aEaPHm3oZZQbg54nFxoayuzZszl16hRJSUkEBwfrsqv9+vXDzc2N/fv3c+7cOWrXrs0HH3yQ57OGgIAAZs+ezaJFi3SnpAiCIORmstlV0OzPL1++vNzWJwiC6TP6WFdOTg5z5syhevXqpKWlUbVqVaZOnaq7PyYmhgYNGugORJw7d44XX3yxIssQBMGIGX2sKzQ0lCpVqvDee+8B0L17dzp27Ej79u0B+PLLL9m/fz8WFhZYWlqKBicIQh5GHesCuHr1ap7bbGxsdKeePH78mHPnzvH888/j4+ND165dK64AQRBMglHHukCTm+vXrx+vv/467u7uVK9enR49egCaqwGfOHECT09PRo4cSWhoKA4ODoW+poh1lZ4p12PKsS59mGItlT7WBZrd02XLlvHaa6/Rr18/wsLCdEdYx40bh7+/PxEREUyYMIHRo0cXeYRVxLrKjinWY8qxruIwpVrESMJ/2NnZsXv3bkaPHk1AQECe2JZCoaB3795ERETQokUL4uPj86UmtMRIwtIz5XrESELjI0YSAtu2bSMjI4O+ffty5MgRXn75Zby9vRk2bFiexzVu3Jju3bsTGxtbaJMTsa6yY4r1mHKs6+LFizx+/LjIz50Lq+XTTz8lMDCw3NdYEmIkIbB7924aNGgAQPPmzZk+fTo///xzgdu0t7enSZMm5bhqwexIEmSngzKtYn9V0CCbzZs38/XXXxf7eebE6GNdrVu35vz587z22muAZvdU2wh37NiBt7c3bm5u/Prrr3Tp0oUqVaoYqiTBFGWn4/y/phX/unPiwcper4dqB9msW7eOmJgYatasyZkzZzh58iQNGzbkk08+QS6Xs2rVKrKysjhw4ABdunTh7bffJjw8nJs3bzJ79mwCAwOpXbt2oa/z888/s337dmrWrMmxY8fYuXOn7vEbNmzgwYMHnDhxgs6dOzNv3jwAfv31V77//nvu3btHamoqW7du1V0ww1gY/UjCuXPncv/+fVavXs0XX3yBlZUVfn5+APzwww80b96cYcOGce3aNd21tgTBnGgH2YwfP54uXbqwb98+3n//ffbu3Ut4eDjbt2/n8uXL3Lx5k0mTJnHgwAGcnZ3x8vJi8ODB1K9fn+Dg4CIbHMC0adMYMWIEixYtwtHRkV27dgGaiVx//vknc+bMYfny5cyfP5+4uDji4+OZNWsWH374IevXr+fYsWNG+a7R6GNdtra2rF69usDnb9u2rbyWJlQWlnb8PekqTo4VO8gGy5Idzd+1axdJSUm6fxNdu3YlLS0NOzs7tm3bRt26dZk6dSr+/v7F3vYnn3xCmzZt+OOPP0hMTOTJkycAfP7550yePBmAli1bEh0dTe3atQkODqZ9+/bIZDIUCgV//PEH1atXL1Fd5cmgTU4QDE4m0zQcK3sw4gMPWrGxsbRu3VoXbcwdcdy4cSOBgYGEhobqdjuLw83Njfnz59OrVy+aNm2q+wzw6cE09erV092uHfYOFHrAz9AM+l1NS0sjICCAoKAgpkyZkucvUisnJ4f33nuP5cuX6+Y45rZ+/XpmzJiBv78/Fy5cqJiFC4KBuLm55dslPH36NHFxcQwYMIAzZ87w6quvFvsacJIk4e3tzcSJE+nWrVue+54eWJOSksKlS5dwd3cnIiIizwERQw6sKYxBm9zEiRPp0aMHS5cupW3btgVe8DJ3dnXhwoXs379fd8n0n376ie+//56VK1eyZs0aRo0alWdCkCCYCysrKx4/fkz//v05f/48I0aM4MiRIyxatIicnByuX7/O3r17cXJy4pNPPtE1Hu3zMjMzdZ9zFyQpKYmYmBgSEhKIi4vjypUrZGRkEB0dzfDhw9m0aRNr1qzht99+Y+bMmTRq1IihQ4dy8+ZNxo8fz+nTp5k7d65RHvgz6exqSEiIblCuo6Mjnp6e7Ny5s4IqEISKM3LkSKZMmUJsbCw7duwgMjKS4cOHU61aNTp16gTAmDFjWLBgAcuWLWPjxo0AvPrqq6SkpDBy5EjdFbQLUq1aNd566y169erFypUr8fX1Ze/evahUKvz9/Xn33XdZvHgxEydOZOLEiVhZWdG4cWPCwsKIiIjgjTfeoHnz5jRv3rxC/j6Kw2CDbHbs2MGsWbN0g2oBqlatSnh4eJ7s6k8//US/fv2IiIjA3d2dDz74gM2bN6NWq3FwcOC7777TPX7atGkkJiYWekBCDLIpPVOuRwyyMT5mPcimtNnVpKQkMjMz840kvHjxYqGvKbKrZccU6xHZVeMjsqv/KCi7qv2fKvc2Cnu+lsiulp4p11PZs6srVqzg8uXLBT53+PDh9OzZs7yXmI/IrlJ4dnXo0KFYW1vn2UZqamqRh7FFdrXsmGI9ppxd1cezatFeeNaYiOwqhWdXZTIZ3t7eXL9+XffYGzdu4O3tXUEVCIJgCgzW5HJnV4F82dV79+4B6LKrWrmzq5MmTdKdv5OSkkJcXBxDhgyp4EoEU2OgY21CASrie2H0Iwnnzp1LUFAQq1evxtraOk921dfXl0uXLjFv3jySkpLYuXNnvv16QdDS7v6kp6cbXYi8stIeXCjPjz5MOrsKMHv27PJYmmCGFAoFzs7OPHz4ENAc0JIkCaVSSWZmpll8JmcqtUiSRHp6Og8fPsTZ2Tnfpc/LktGPJNy+fTujRo3Kc9vgwYN1lzkXIwmF4tCeEKttdJIkkZGRga2trVkcXTW1WpydnYs8SbksGP1IwrNnz7Jv3z5q1KgBaE4ibteune5+MZJQKA6ZTIabmxs1a9bUDTA6ceIEr7zyiskdLX6aqdViaWlZru/gdCQDiYuLk2xsbKSMjAxJkiTp4cOHkq2trZSSkpLncbGxsXm+7tOnj5SUlCRJkiQlJSVJvXv3lmJiYkq0huTkZAmQkpOTS/R8fSiVSmnv3r2SUqkst9eoSKIe42VOtUiSJCUmJpbJv0+jH0no4eGh+3NycjKSJFG1alVAjCQ0BFGP8TKnWqASjSTM7fvvv6dv3766r8VIQsMR9Rgvc6mlUsW6tPbt28fKlSvz3CZGElYsUY/xMrlaJAlUSsjOgJyMf37PRJadCTnpJCVmlsnLGH2sS0upVJKYmFjoderFSMKKJeoxXuVSS04WpCXAk4ea39OTNFPOcjI1zemfBkV2OmRn/tu0dPdlaG7P3dCyM4DCTwa2ySqbE4UN1uS8vb0ZP348SqUSKyurQmNdWj/99FOez+oKIkYSCoKeJAmyUjUNK3fzKvDPiZCV/OxtlmY5MjmShS1qhQ0qCxtUchuSs+TAuVJv2+hHEmrt3buXadOm5dmGGEkoCHlZ5qRBQhRkJhXdtNIeat55FYfcErV9dbJtqpNp6YxSbku2zAqlzJpMrMnCigzJkvR/fk9TW5GmsiRVbUGqypLUHEtSchQkZ1vwd44Fj7MtSM2xIBMrslEAec/tU2elA0NL/Xdi9LEu0JzkeP369Xzv0n744QcCAwPx8fGhV69eYiShUHkl3UIR8T59ovbDn8V4nqU9ONRAbVeDTGsX0ixc+FtelUTJifsqR+5mOxKTYcv1dDtupVqQmqAql+XLZGBnqcDWygI7KwV2VgoUORbEPvupz2T0sS7QHKQ4cuRIvseJkYRCpZfxN5wIgVNrkas1p1xItlWR2dcE+xqo7WuQbuVCityZJJx5KDkRn23PnSwHbqXbEvtEzoPkTP6+p8/pGpoGZ2eloIajNXa5GpKtpeZ3O2sL7P75s7Zh2f7zGM3jcj3HSqHbhrWFPF9K49GjR1SfWfq/IjGSUBBMkSobzm6CY0shIwmAZPcuLEnpw32XNiSkKnkYl8mjNCUFX+hDDeQd+mRtIaeWkw21nKyp6WRDLUfNn2s52VDzn99rOdngYG1abcPks6vr168nKiqKx48fM2XKFFq3bl1RyxeEiidJ8NePEDEPHmmupZhVtRFrbUaz6pan5jGJiXmeYqmQUdPxn0blmKuJOf3bxGo52uBka2EymdfiMOnsqnYk4d69e0lNTaVjx46cOnUKe3v7Cq9FEMrd/T/hx7kQrbkGo8q2Gt9WfYtZ0a3JlhTIZdCmupqBLzfHraq9rqFVtbNCLje/5qUvox9JOGPGDF5//XU6dOhAhw4duHnzJr6+voAYSShUEqn34dvJENoFoo8jKaz51c2PdqkhTL/VhmxJQc/na/HdpE6MaqBmSBsPvBvX5Hl3J6o5WFfqBgcGfCdX2uyqSqXi+PHjzJz57yeTjRo14vjx44wdO7bA1xTZ1dIT9VSg7HTkp75A/usnyLI1n59dq96TwIR+/BWtiUS2q1eVmT0a8kJdZ7Kzs7mOkdZSApU+uypGEhqWqKccSWo8HkfyfPwebLM1BxViLBswN3MUv9xtBEBtOwnfumqaOidw71IC9y79+3SjqqUUKn12VYwkNAxRT/mS3fkV+aH5yO//AUCarTshquFsTnkRkOFR1ZZp3Rvg28I1326osdVSWpVmJKHW09nVatWqiZGEBiTqKWOPbsLhBXB1PwA5FvZssxxC8ONXycKK6g5WBHZryPCX6mJlUfRH6QavpYyUVQ0mm13NPZKwQ4cOgGYkoXbIjSCYhIzHcGIFnFoL6mwkmZzDtr2ZneTLI6rgYG3BpFfqM6azF/Ymdn6asTD6kYRae/fuZeDAgXluEyMJBZOlytY0tk9egMjPQJ3NJduX6JkZzLikkaQqqjKmsxcn3vNmSveGosGVgklnV8VIQsHkSBL8dRAi5utO5r1v7cXs1KEcy2yFXAaD23gwrUcjajuLsYllwaSzqyBGEgom5N5FiJgL0ScASLOoSnDWIHYkd0WFgh7P12Lma41pVMvRwAs1L0Yf69J69OgRGzZswMPDg+bNm9OyZUtAjCQUjFjGY0i8Dol/QfTPcHE3IJEjs2Kz1IfVT3x5gh0v1XNhVu/GtPF0eeYmheIz+lgXaN7ZBQYGsmXLFqpVq5bnPjGSUDAotQr+vvNvM0v8Cx7d0PyelpDv4T/KXmZx5lDuSjVo4urIrF5NeLVxDbPMjBoLgzU5baxr3bp1gCbWNWHCBBYuXIij479v17OyshgwYADh4eH5Gtzjx485d+4c48aNo27duhW6fqGSyXqi+QwtdzNLvK459UOVVejTUixrctfCgytKV7ant+e81JA6Lras7tGYfq3cK33kqiIYfaxr7dq12NjYsHv3bo4fP07Pnj159913kclkxR5JKAhFkiRIifunkT3VzFILT+JkyyyJk9fmutqNK9mu3FS7c1NyJ1pyIz3z3wNh1R2sWKjnuW5C2TH6WNfOnTvp2rUrc+fOZfjw4bzwwgs4OjoyYcKEYo8kFNnV0jOretQ5SGc28uLt/Si+DEFKuqXLiBYkiSpcV7vpmpjmlxtxUg3Uuc7GcrSxwMPZlper2lKnqi21//n9pXpVNaeCSCqys8v+Crtm9b3BDLKr+sa6Ll++zNy5c5HJZDz33HMMGTKErVu3MmHCBKB4IwlFdrXsmHo9FjlptL39P2qlXqJOrtuzJQV3pJq5mpj7P03NjRQ0ewlWcgkXa3CxlqhnA22sJVysVVSz0dxuZ5EDZAKPNcOokjTXtTx+s2JqM/XvjZbJZ1f1jXXl5OSgUv37v17Lli355Zdf8m1Pn5GEIrtaemZRT9JN2DUCy9SbpEvWhOa8zlWpLjcld+5INZEpLKntbItHVc2v7s62vPnPnz2cbXCxtzLKAwVm8b3JxeSzq/rGulq2bMn169d1X1tYWNCsWbMCt/mskYQiu1p2TLae6BPk7ByFhTKZeMmFQGZR3bU2r3VoiWcNR+pUtaOmo2lfg81kvzdPKasajD7WNX36dP7v//5P97zIyEjdVK4dO3boHidGEgrPIp3ZiHrrQCyUyZxXN2Ca08csefs/9Kmrpn9rd9rVc8G1io1JNzghP6OPdQ0dOpSYmBhmzJhBjRo1eOWVV+jatSsgRhIKelLloPxhDlZn1yID9qo6caLJAjYNaYulTCLK0OsTypVJxLpyX/03NzGSUHimzGTStvthH6vZY1iZM5RqvYJY+bIXMpnMbI5ECoUTlzYQzFfSLVI3DcYx9SYZkhULLacwZPQkEZ+qZEw+uypGEgoFybn1M8rtI3BUpXBPcmFNjYXMeHMYNRwL/vkSzJdJZ1fFSEKhICknN2B3aCZ2qLigrs/PbT7lQ99OWChEyqAyMvqRhNrs6scff5wvuypGEgp5qFXc/2o6ToemY4GKH6ROJAz+hsD+nUWDq8RMNruqVqvFSEIDMNZ6pMwU7m3ywzPpJABbrEfQ8c0leNVwKHKtxlpPSZhTLWAGsa7SZlcHDRokRhIakDHVo0h/SIu/VuMp3SVDsuJ/tm9Tt1E7rp45wVU9t2FM9ZSWudRi8rGu0mZXBw8eDIiRhBXN2OqJ++MIzt8twpkU7ktVOd3+U6b4vKZ37MrY6ikNc6oFzCDWVdrsqhhJaFjGUM+FfZ/R7Nz7WMpUXJU9R/bQ7fR7vmmJtmUM9ZQVc6nF5GNd3t7e3L17F6VSCVDs7GrukYRaN27cwNvbuwJWLxhSdnY2v34xkda/z8VSpuI3my7UmPITLUvY4ATzZtLZVTGSsPJJSEzkjxV96PRgBwC/1B5L23f3Ur1qVQOvTDBWJp1dFSMJK5fzF//A8etRtOUOmZIl1zoso3PvMYZelmDkTDq7CmIkYWUgSRL793/Dy+feoZoshUeyqmQOCaNVs86GXppgAkq0uxoREUFERASJiYk8ePCAt956izfffJPY2NhibSctLY2AgACCgoKYMmVKnnPYcouJicHS0hKZTIZMJuP333/X6z7B9D3JymFr6FJeOzeOarIU7lo3xG7ScWqLBifoqURNbuzYsdjb21O9enUGDx7M1atXGTJkCKtXry7WdiZOnEiPHj1YunQpbdu2JSgoqMDHaccOHjp0iGPHjuUZO1jUfYJpu34/mf0rx/Hmg2VYy3KIqdmd2tOPYVvd09BLE0xIiXZXAwICePnll/nuu+84e/YsV69epV69ely6dEnvbeg7krCosYNiJKF5kiSJ789dx3b/RIbLzgJwr9VkPPsvBrmIZwnFU6KfmIyMDMLDwwkMDGTWrFnUq1ePuLg4NmzYoPc2iop15ZZ77OCoUaN48uSJXvcJpul2YhrLv1hL430D6C47ixJLUvt+gdvAj0SDE0qkRO/k3nvvPbZs2cLChQvx8/MjJiaG3bt3M2rUKL23oW+sq6ixg2IkYcUrr3oys1XsiThGvd+XMUv+O8ghzdIFxX/CsKn7kvj+6MGcagEDZ1ft7e3p27cv9+7dQyaTkZKSQkBAQLEGO+sb64Kixw6KkYSGUZb13ExMw/XOt/hxCEu5ihwURFXtzh2PAWRfSoRLB8rstQpjTt8fc6nFoNnVTZs28fbbb+Pj48OBAwdo1KgR06dPZ+TIkXTq1Emvbegb68qtqLGDYiRhxSjLeuIfpfDrVyvxf7QZZ5lmqPP9Wl1xGRBM4+oNaVwWC34Gc/r+mFMtYODs6rp16zh16hRHjx4FNJnQoUOHMnbsWK5cuaLXNvQdSfi0osYOipGEFac09SizVRz+ditN/lzOcFk8yOChbX0c+4fg2sSnjFeqH3P6/phLLQbNrvbu3ZsXXngBC4t/e+SJEyeK1Xn1jXUVNXZQjCQ0PRfOnuTPYG/6XJpKfVk8yfIq3H9lKTXfPYOtgRqcYN5K1ORcXFzYvn07CQkJnDp1ipkzZ7Jo0SLGjRtXrO2Ehoaye/duPvzwQy5evMhHH32ki3XFxMQAmrGDzZs3Z9iwYVy7di3P2MGi7hOMS8L9WE6uHkWL/X1po/oDJRZce84fp5kXce0WAAoxU0koHyX6yZo8eTLbt2/n9OnT/N///R9ubm588cUXjBlTvByhPrGuosYOipGExi8nK4Pz4Utp+tc6XpZlgAwuVfGm7rAQGrs3NPTyhEqgxP99jhw5kpEjR+q+VqvV3Lhxg4YNxQ+uAEgSN0/swO74QtqpH4AMblg0gF5Lad62p6FXJ1QiJWpyixYtyndbQkICKSkpbNmyRe/t6DuSMCYmhgYNGpCTkwPAuXPndPEtMZLQ+CTfPMOj/5vBc+l/APCQqtxoOYP2/QNQKBQGXp1Q2ZSoye3atYv27dvnue3PP/+kbdu2xdqOviMJtflUCwsLLC0tdQ3OJEYSZqdTLTUKVD3ADI54FUX9dxwxe2bjFbePKkCGZMUvNUfw4vAFdHIRA50Fwyhxk9MOd9b6/fffOXz4sN7bKIvsakhICEOHDgXyjiQsbFpXhcrJgnObsTixgs5pD1HvOQ3/2Q6WZni9O2U6Dw4up8rvn+OFJlHyk+WrVB+whB7Nmhl4cUJlV6Im93SDA80R1xUrVvDee+/ptQ19RxLmzqeOHDmS0NBQHBwcUKlUxjmSUJ2D7OIuFD+vQJZyF+04FfnNw6h3jUQ1eDNYmG6jyxMdktRk/r4b9U+LqJWdAMB5qRG3286lV4/eWCjkRh8xMqcolDnVAgaOdXl5eeWZhqRSqXjw4AHDhw/Xexulza4mJSUZ10hCSU3tx6docv9rHLIeAJrPotZkD+SOVJN1lquwvXmYh1/05YzXFNRy0951Pfv1/6gfswOP7FsA3JWqs8vmP9R8rh1V1DIifjxo4BUWj7lEocB8ajForKtHjx6MGDFC1+jkcjm1atWiUaNGem+jtNlVKysrwAhGEkoSsr9+QHFiObKHmrRHirwKa7J8CVP1wN7OnmoWWfg/mclGyxBcU/6g75OdqAZvAYv8B1mMXU7iLeJ3TOa5VM3VYp5INuy0Hkzj/u/xTqPCI3nGypyiUOZUCxg41vXRRx9Ro0aNfLffv38fV1dXvbZR2uzqSy+9ZNiRhJIEt47CkQ8h7hwAGXIHPlf2ZmNOL7IV9ozuWo+3O3ty7KdDHE3vjv8l2GgZgu3Nw8i/Hg3Dwkyq0alvHke1YxTPqVJQSzLCJW/SXp7Ff7u3w9rCtI+amksUCsynlrKqQa8mt23bNiRJKvIxmuvw7yc8PFyvFy5tdjX3SMIOHToAmpGEfn5+er1+qdz5DX5aDDG/AJAtt2GzqhefpfchGQd6N3clqHdT6lazIzs7G4UMQga1YK6FBf4X/ml01yNg9yiTaXTZZ7Yg/34aNqi4oK7PXo/3GDO4P3VcyvfqLYJQWno1ubCwMGJjY6lZs2ahk8klSeLy5ct6v3Du7GqPHj3yZVcDAwNxc3Njx44deHt74+bmli+fOmnSJHbu3Imfn1/FjCSMv6B553ZD85mHWm7J/8lfY9mTviRShWbuTqz1fZ4O9avle6pCLiNkcEvmWsjwP5u70fnBsG3G2+jUatJ/mIfdmf8B8J26I2c8xjJvdH+zeLcgmD+9mtz7779Pq1atnnm9uHPnzhXrxfUZSfjDDz8QGBiIj48PvXr1ypNPrbCRhA+j4OhHcHUfAJJMwRHbnsxL6sM9qlHD0ZrlrzVm0IseKOQF/ycAIJfL+GhACz5QyPE/pW10Pxpvo1Om8WTnaByifwTgc4bScuRiXow6ZeCFCYL+9GpyL7/88jMfExMT88xd2qeVNrsK5TySMCkajgXDn1+BpEZCxgVnH6Y96M3tDFesLORM7lKfCa8+h4O1fh9vyuUyFvZrxmK5nDGREhssV2ga3Vf/haFbjafRpcTzZNNgHB5fJkuyZJlNICPHzqCuszUHogy9OEHQX4kOPFy+fJl169bx5MkTXWPLyMjgl19+KfZYQqOUEg/Hl8P5baDWRMluVfdmekJfLtzXHNjwbenG7N5N8Kha/M+kZDIZ832bssxCzpifNe/obP46aDyNLv486VuG4pD1kETJiY+rLWDGmP/iYm9lNudgCZVHiS619Pbbb+vOjfPw8MDT05O0tDQWLFhQrO3oO3dVKzg4mLfeeivPbWU6dzUtEQ7OgTWt4dwmUOeQUKszY6xD6HZ3HBey3GnlUYXwCR35bMSLJWpwWjKZjFm9GtP21QH4Z88kU7IEbaPLKfrvoTypr+xD+WUv7LIeck3twf+eW8v7AaNxsbcy2JoEoTRK9E6ub9++BAUFcevWLS5dukS/fv34+++/mT59erEiVfpmVwEuXrzIunXreOWVV/LcXliutVgy/oYza+C3LyBbcxnuNNeXWK4cwpaY2gC4Otkwq3dj+reqjbyIz92KQyaTMb1nYz5RvIH/T/++o5O++i+yin5HJ0lk/7waxZGFWCFxTNWKy51W836vFwo92CQIpkDvd3K5Pzu7efMm27dvp1q1avz2228cP36cw4cP88033+j9wtrsau/evQFNdjU0NJTU1NR8j1Uqlaxfvz7fNDBtrvX555/Hx8eHrl276v36eXzRCX5eAdlpZNdqxTrPFTSPeYct8bWxsZTzTveGHHm3KwNf8CizBpfblO4NeeW1wbp3dLJ/Gl2FvaPLUZIRPhHLIx8gR2KrqieP+29lUu8XRYMTTJ7e7+QmTZrEpUuXmDBhAtOnT2fBggU0b96cGTNm8MYbb3D+/Pk8aYJn0Te7CrBixQpmzJjB5s2b89xeWK61MIVlV8lKQVW7CQeq+zPriifpSjUA/Vu5MaNHQ9yq2ABSiT6P0jdPOKZTXeQMYsyPmoMRNn8dRL3bD9WgTaAox13F9CSUu/ywv3cKlSRjuWw0r/x3Ni/VcylwzeaajzSHesypFjBAdnXZsmUMGjSI3bt3ExUVRa9evWjQoAH29va6OQ3FoW929ddff8XDw4N69erl20Zx564Wll3dW9WfNQ+78eiuAlBTz0HijXoqPO1iOX8ylvPFri4/ffKEtQA3z+cZE/OuptFd/5EHn/fljFcgkrzsLw9un3mPF69/jEvOfVIlW+bIAmnZpDmJV37jwDPmEZlLPlLLnOoxl1oqPLs6depUAN59910AIiMjmTdvHpIkMWTIEL1OM8lNn+xqWloae/fuZfny5YVupzhzVwvLrs679zJyawXuVWyY2bMhfVu4ltluWnHzhH2A3WdbMG4/rLdcgVvKefqm7UY9uGzf0clu/4xq92Ssc1KIVddgVY2FzP/vAKraFf0a5paPNKd6zKkWMHB2FaBjx4507NiRhw8fMmTIEB48eIC/v7/el1rSJ7v69ddfExoaysaNGwFNZ1er1Vy8eDHfUVR95q4Wll21tZIT2LMRY7vUx8ayfDKYxckTjurohY3VKMb9n6bR2dz4EfnXY/45GFH6Rqc+twXpu2lYSyrOqRuyt3EIwcNeKVb+1FzykVrmVI+51GLQkYQAly5dYvLkyTRu3Jg//vgDHx8f+vbtq/fzvb29uXv3LkqlEqDA7OqgQYO4cuUKFy5c4MKFC0yYMIF+/fpx4EDBE9WfNXe1MN8HdmFyt4bl1uBKYnAbDwYP9WN89rv/HIz4AfVX/4UcZck3qlaRc3Ae8v1TUEgqvlV14tfOm1g04lWTD9gLQmH0bnLffPMNmZmZbNu2jc6dO9OqVSsiIyNZvnw58fHxfPbZZzQrxlVg9Zm7amdnh4eHh+6Xk5MTdnZ2uiudlNXc1ZpOxnkRy/6tazPsP28yIUfT6OR//YD6qzdL1uiUaWRtH4HFb58C8IlqMOqB6wjs2UIcQRXMmt5Nbvjw4VSrVo0JEybQqFEjIiMjdZclL+kFJ/WZu1qUyjB3tW9LN4aPeIuJqnfJkiyR/3Wg+I0uOY7MdT2xvnmQLMmS2bJ36OAfwsAX65TfwgXBSOj9mZyTkxNz5szhrbfewtnZuUxeXJ/sam4ffPBBnq8ry9zV15q5YjlqNBPCIFSxAuu/DqD66k0UQ7c8+zO6+PNkbRuGTcYDEiQnPrCby7tj/PCqbkTDfgShHOnd5L744gsGDRpUpi+u70hCreDgYKKiovKcL1dZRhJ2a1ILi/+OYeI2+EKubXRvoRi6ufBGd2UfOeHjsFZnck3twSe1PuTDt/pSVUS0hEpE793Vsm5woIl19ejRg6VLl9K2bVuCgoIKfaw21pWbdiThypUrWbNmDaNGjSItLa3M12ksXmlUg7FvjiVAPZMsyRLFX9+TU9CuqySh/vlj+MoPC3Umx1St2NRkHave7icanFDplPjoammVRawrJCSEfv36AXlHEpqzTg2q87b/OAIlzWd0Fn8dIOert/5tdDlKcr4JQP7TBwBsynmNi6+sZenwTuIIqlAplf1p9HoqbazLaEcSPqU8ojYveDgy5s0xBG6V8akUgvVf36Pc9SayviGow8diHReJSpKxWP0WLQZMZ1QrN3Jycsrktc01OmQO9ZhTLWDgkYRlobSxLqMbSfgM5RG1ad6oCZOvTuMzxcdY3zhAzieHsJaySZFsmakOpHGj5ljEnedAXFkE0/Iyl+iQljnVYy61GHQkYVkobaxLe26XwUcSPkN5R20ux3dm2iYLPpZCsCabWHUN3refz9w3B+BZrewbt7lFh8ypHnOqBYwg1lVapY11nTt3zrAjCYupvF6jtWc1At+eyMT1drRVnua8+wg+fqs7zs/IoJaWuUSHtMypHnOpxeCxrtIqbawr90hCrRs3buDt7V2xhRiBpm5OLJ4yltqDg/lsfM9yb3CCYEoM1uTKItY1adIkDh48CFAxIwmNWG1nW/q3ri2OoArCUwy2uwr6jSQsSoWNJBQEwWQZtMmVNtYF5TySUBAEk2ew3VVBEISKYNAmp89IwrS0NAYPHoyDgwOdOnXi9u3bee4v05GEgiCYHYM2OX2yq1u2bGHRokVcvXoVpVLJvHnz8tyvHUl46NAhjh07VrKRhIIgmC2jz66OHj2a559/njp16uDv749C8e/RwzIbSSgIgtky+uyqra2t7s/x8fF53smV1UhCU8uuGpKox3iZUy1QibKrAPfu3ePTTz8lPDyc119/XXd7WY0kNNXsqiGJeoyXudRSKbKrWs7OzvTu3ZvIyEh8fX2JiYnRNaWyGEloytnViibqMV7mVAtUkuyqlq2tLV26dGH//v24ublx+fJl2rVrl+cxpRlJaMrZVUMR9Rgvc6mlUmRXn+bg4EDjxo0LbWIlHUkoCIL5MursKsD58+d1++bR0dE0b96c2rVrA2U3klAQBPNl9NnVmTNnEhUVRb9+/XB1deXzzz/XPf+HH34gMDAQHx8fevXqZZYjCQVBKB2jz64ePny40OdXlpGEgiCUnMnHutavX8+MGTPw9/fnwoULFbNwQRBMhknHuirbSEJBEIrPpGNdlXEkoSAIxWOysa7KPJLQkEQ9xsucagER6xIjCQ1M1GO8zKWWSh/rEiMJDUPUY7zMqRYQsS7atm0rRhIakKjHeJlLLZU+1iVGEgqCoA+TjnWJkYSCIDyLSce6xEhCQRCexaRjXSBGEgqCUDQxklAQBLNm9NnVBw8e0KdPHxwdHenSpQvXrl3Lc//Jkyd14witrKy4f/9+RS1fEAQTYPTZ1eDgYMaNG8fhw4fJyclh0KBBee7fs2cPhw4d4tChQ/zyyy+4urpW1PIFQTABRp1dlSSJ/v37M3DgQNq3b8/GjRu5fPkyCQkJAFy7do379+/TsmVLfHx8ijz9RBCEysmos6symYxXX31V95zatWvj4OCAs7MzAGFhYXz77bfs2bOHqVOnEhwcXOQJhCK7WnqiHuNlTrVAJcuuap06dQp/f39dI1u8eDHz588nPDycyZMnI5fLCQkJKfT5IrtadkQ9xstcaqlU2VWtsLAwVq1alec2KysrRowYgaurK76+vgQHB+e5HFNuIrtaeqIe42VOtUAly64C7Nq1i3HjxlGtWrUC7+/WrRuenp4kJiZSq1atAh8jsqtlR9RjvMyllkqVXT116hQKhYLOnTsXuc26detSs2bNsl+sIAgmy+izq3/++Sf79u2jXbt23L59m1OnTrF161YANmzYoHs3GB4eztixY3WXYBIEQQADnycXGhrK7t27+fDDD7l48SIfffSRLrsaExPDzZs36d69O0uWLMHLywsvLy86dOhA48aNUavVhIWF0aRJE/z8/LC0tBThfEEQ8jH67OrDhw8Lff7Ro0fLZV2CIJgPk491LVmyhFmzZuHv709MTExFLV0QBBNh0rGujRs38uDBA5YtW8b8+fMZOnQoarW6IksQBMHImXSsa/ny5fTv3x8ALy8vnjx5wpEjRyq+GEEQjJbBmlxRsS6tomJd8fHxXLt2DU9PT9392pGEgiAIWiYb64qLiwPIN5KwqOeL7GrpiXqMlznVAmaQXS1trKuwkYT29vaFPl9kV8uOqMd4mUstJp9dLW2sS/u45ORkbG1tAc1IwmbNmhX6miK7WnqiHuNlTrWAGWRXvb29GT9+PEqlEisrq2LHutzd3WnatCnXr1/XXSjzxo0bBR6h1RLZ1bIj6jFe5lKLyWdXyyLWFRAQoBtJeOvWLVxcXOjSpYthChIEwSgZ9UjC9PR0unfvTkJCAkuWLNE977fffgM0TW727NksWrRId0qKIAhCbiYd65LL5Sxfvrxc1iYIgnkQIwkFQTBrRp9dBc05dRMnTsyzy6oVExODpaWlbizh77//Xt7LFgTBhBh9dhXg9u3bnD17VneBzdy+/PJL9u/fz6FDhzh27BgvvvhieS9bEAQTYtTZVa2XX36Zpk2b5rv98ePHnDt3jueffx4fHx+6du1a7usWBMG0GPVIwtzk8vz9ODw8nBMnTuDp6cnIkSMJDQ3FwcGh0NcUsa7SE/UYL3OqBcwg1lWS7OrTxo0bh7+/PxEREUyYMIHRo0cXeRqJiHWVHVGP8TKXWkw+1lWS7GpBFAoFvXv3JiIighYtWhAfH19oNEzEukpP1GO8zKkWMINYV3Gzq8/SuHFjunfvTmxsbKHbELGusiPqMV7mUovJx7qKM5JQX/b29jRp0qRM1icIgnkw+uyqliRJSJKU57YdO3boHvfrr7/SpUsXqlSpUjEFCIJgEox6JKHWiRMnOH36NEeOHMlzsu8PP/xA8+bNGTZsGNeuXeOdd94xRBmCIBgxo8+uArzyyitcvXo13+O2bdtWbmsTBME8GLTJpaWlMXPmTKpUqUJaWhohISEFHhiIi4vjww8/pE6dOsyZMyfPfevXrycqKorHjx8zZcoUWrduXUGrFwTBFJh0rOunn37i+++/Z+XKlaxZs4ZRo0aRlpZWEUsXBMFEmHSsKyQkhH79+gHg6OiIp6cnO3fuLN+FC4JgUox6JGFuT8e6VCoVx48fFyMJBUEoksnGupKSksjMzMw3kvDixYuFPkdkV0tP1GO8zKkWMIPsamljXYWNJCzq+SK7WnZEPcbLXGox+exqaWNd1apVw9raOs82UlNTi3y+yK6WnqjHeJlTLWAG2dXijCQsiEwmw9vbm+vXr9OhQwdAM5LQz8+v0OeI7GrZEfUYL3OpxeSzq2UR65o0aZJuJGFKSgpxcXEMGTKkYgoQBMEkGPVIQjc3N+DfWNetW7fo37+/7hLnvr6+XLp0iXnz5pGUlMTOnTvzfc4nCELlZtKxLoDZs2eXy9oEQTAPYiShIAhmzSSyq0XlU0+ePEnnzp0BzQeVd+7cwdXVtaJKEATByBm0yU2cOJGBAwcycOBAtm7dSlBQEKtWrcrzGG0+de/evaSmptKxY0dOnTqFvb09AHv27NGdF+Tk5CQanCAIeRh9drWofOq1a9e4f/8+LVu2xMfHp1RXFRYEwTwZ9UhCbT515syZuudp86ljx44lLCyMb7/9lj179jB16lSCg4OLPLdGxLpKT9RjvMypFjCDWJc+2dVn5VMXL17M/PnzCQ8PZ/LkycjlckJCQgp9TRHrKjuiHuNlLrWYfKxLn+yqPvlUKysrRowYgaurK76+vgQHB6NQKAp8TRHrKj1Rj/Eyp1rADGJd+mRXi5NP7datG56eniQmJlKrVq0CX1PEusqOqMd4mUstJh/r0mckYe58qtaNGzfw9vYucJt169alZs2a5bhqQRBMjdFnV4vKp27YsEH3Li88PJyxY8fqdnEFQRDABEYS+vr60qJFC+bNm8fs2bN1+VS1Wk1YWBhNmjTBz88PS0tLEc4XBCEfk8iuFpRPlcvlHD16tNzWJgiCeRDZVUEQzJrJZ1eXLFlCcnIyCQkJLFiwIM9gG0EQBJPOrm7cuJEHDx6wZs0aoqOjGTp0KJGRkfkmewmCUHmZdHZ1+fLl9O/fHwAvLy+ePHnCkSNHKrAKQRCMnclmV/v06cO1a9cKnLvq4+NT4Gs+nV3Vnn6SlJRUrtnV9PR0Hj16ZBYnaIp6jJc51QKaf5dAvrEHxWWy2dW4uDiAfPcVNbe1sOyql5dXiesQBKF8PXr0iCpVqpT4+SabXS3sPu115grydHZVrVaTlJREtWrVyu0kYm0+NjY2ttzysRVJ1GO8zKkW0Oxp1a1bN9+boeIy2eyq9nHJycnY2trq7mvWrFmhr1lQdtXZ2bm0pejFycnJLH7wtEQ9xsucagFKfSDRZLOr7u7uNG3aVO9cqyAIlZNJZ1cDAgJ09926dQsXFxe6dOligGoEQTBWRj93tajZqgEBAcyePZtFixbpTkkxNtbW1ixYsKDAk5xNkajHeJlTLVB29cik0h6fFQRBMGIiGiAIglkTTU4QBLMmmpwgCGZNNDlBEMyaaHLl6MCBAzRo0AAXFxcCAwPJyckx9JLKhFKppFWrVhw7dszQSykTv/76KytXrmTv3r1lNiHKEK5evcqkSZP4+OOPCQgI4MKFC4ZeUrEdPnyY9u3bc/v2bd1taWlpBAQEEBQUxJQpU/Lkz/UiCeUiISFBGjFihHT69GkpLCxMsre3l0JCQgy9rDLx4YcfSk5OTtLRo0cNvZRSW79+vTRnzhxDL6NMtGnTRrp7964kSZIUExMjNWnSxMArKp6HDx9K33zzjQRI0dHRutv9/Pykr7/+WpIkSdqyZYs0bdq0Ym1XNLlyEhkZKaWnp+u+fu+996Q+ffoYcEVl4+TJk9KGDRskT09Pk29yR48elXx8fCS1Wm3opZQJOzs76erVq5IkaRqGm5ubgVdUfCqVKk+Ti4uLk2xsbKSMjAxJkjR12draSikpKXpvU+yulpMOHTroMrUAtWvXxsPDw4ArKr20tDT27NmDv7+/oZdSJqZPn07Tpk0JDAykd+/eREZGGnpJpTJ48GDGjh1LamoqYWFhfPrpp4ZeUrE9nVMt6pJsem+zTFcoFOrMmTO8/fbbhl5GqSxbtoygoCBDL6NMXLt2jQsXLjBu3Dg+++wzunXrxmuvvUZCQoKhl1Zi//vf/7C0tKRdu3Y4ODgwaNAgQy+p1PS5JNuziCZXAaKjo6latSovvviioZdSYgcPHqRt27ZmM7z78uXLuLi40KJFCwAmT56MWq3mm2++MfDKSi4zM5ORI0cyYsQIpk6dyuHDhw29pFLT55Jsz2LQ7GploFar+eKLL1i+fLmhl1IqK1eu5Pz587qvHz9+TP/+/Zk7dy7vvfeeAVdWMjk5OahUKt3Xtra2NGzY0KSPro4aNYpdu3bh7OyMTCZj+PDh3L59u8hrLBo7fS7J9kzl8eGh8K+VK1dKcXFxhl5GqT18+FCKjY3V/fLw8JC++uorKTk52dBLK5GrV69KgJSQkKC7rW3bttK3335rwFWVXEJCguTq6qr7Wq1WS/Xr15fOnDljwFWVDLkOPMTHx0v29vZSVlaWJEmaAxF2dna6AxH6ELur5WjVqlU0btwYpVLJrVu32LhxIzdu3DD0skqkRo0aeHh46H4pFApq1KhhshdnbNKkCb179yY8PByAv//+m5ycHPr27WvglZWMi4sLNjY2urEAoLnobKNGjQy4quKT/rleiPb3wi7J9vQubFHE7mo5+eSTT5gxY0ae25o2bWo2RybNwdatW3nnnXfIyMggNjaWHTt2oFAoDL2sEpHL5ezdu5dFixbRpk0bHjx4QEhIiEn9J/TkyRO2bdsGwJYtW5g8eTLVq1cv8JJsxSEutSQIglkTu6uCIJg10eQEQTBroskJgmDWRJMTBMGsiSYnCIJZE01OEASzJpqcIAhmTTQ5odLJyclh3bp1eHp6GnopQgUQiQfBKJw9e5b333+fn3/+mTFjxgCaaE9kZKTuqhplRa1W4+Liwp07d8psm4LxEk1OMApt27bljTfe4OLFi6xevVp3e1ZWFl999VWZvpaVlZVJX/ZKKB6xuyoYDQuL/P/nWltbM2TIkDJ/raevQCuYL/FOTjBqmzdvplOnTixduhRra2tq1arFxx9/TPv27dm5cyfVq1dHkiRCQkJIS0vj0qVLeHl5sXz5cuRyOWq1mo8//pisrCwiIiLw8/PT7Q4D/P7777z55ps8efKEo0ePUq9ePcMVK5QL8d+ZYFRSUlKYPXs2s2fPpl+/fvz0008899xz2Nvbc+rUKXx9ffnjjz+Iiopi9uzZAKxdu5bk5GQWLlzInj17iIiIYOXKlQB89tlnKBQK5syZw/Tp05k0aVKei2Xevn2bCxcu0KRJEzZu3GiQmoXyJZqcYFScnJwIDg4mODiYb775hlatWqFQKKhevTqtWrWiXbt2eHl5MXnyZL777jtAM9ugY8eOgGY39K233mLdunUAfP755/j4+ADQr18/oqKi8lxO6Y033kChUNCmTRvu3btXwdUKFUE0OcFoKRQKBgwYUOB9zZo1010W+/r162RnZ+vuq1+/Pnfv3gUgJiYmzzDiwnZHLSwszGb4t5CXaHKCUWvQoAF37twhNTU1z+1KpZKGDRsCULduXaKionT3SZJE48aNAc2MgIMHD+rui46OLvQdm7i0onkSTU4wGmq1Ol+jUavVrF69GkdHxzzN6dixYwQEBAAwYcIEtm3bpnsndvr0aSZOnAjA8OHDWbJkCdu2bePEiROsXLkSNze3AhuaaHLmSRxdFYzCmTNn2LlzJ/fv32fSpEnY2tqiUqmIjIykc+fOAMTHx7N06VIAqlSpwrhx4wCYOnUqd+/eZcCAAbzwwgtUqVKF8ePHAzBv3jzu379PYGAgrVq1YsuWLWRnZ+sOMnz55Zd0796dn3/+mXv37hEVFUWTJk0M8DcglBdx+XPBJHzwwQfcvn2bzZs3G3opgokRu6uCSZAkSexOCiUimpxg9P744w8OHTrEqVOnOHXqlKGXI5gYsbsqCIJZE+/kBEEwa6LJCYJg1kSTEwTBrIkmJwiCWRNNThAEsyaanCAIZk00OUEQzJpocoIgmDXR5ARBMGv/DzpEx/unkbibAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 再试一次，会不会造成net的parameter的累加，结果表明不会\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net()   \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.01)  \n",
    "    \n",
    "train_steps(epochs=10, \n",
    "            train_dataset=train_dataset, \n",
    "            train_iter=train_iter, \n",
    "            test_dataset=test_dataset, \n",
    "            net=net,                        \n",
    "            loss_fn=loss_fn, \n",
    "            opt=opt, \n",
    "            device=device, \n",
    "            train_figure=True, \n",
    "            resume = False\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.2. <a id='toc8_8_2_'></a>[自己探索](#toc0_)\n",
    "#### 8.8.2.1. <a id='toc8_8_2_1_'></a>[lr的影响](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "耗时： 69.80070853233337 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.4991), tensor(0.9673), tensor(0.9614))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net()      \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)   \n",
    "   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device, \n",
    "    train_figure=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.8.2.2. <a id='toc8_8_2_2_'></a>[不同模型的效率](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "耗时： 87.9712917804718 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.4962), tensor(0.9704), tensor(0.9643))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"335.982812pt\" height=\"265.325625pt\" viewBox=\"0 0 335.982812 265.325625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-04-25T11:13:04.145402</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 265.325625 \n",
       "L 335.982812 265.325625 \n",
       "L 335.982812 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 44.782812 228.96 \n",
       "L 323.782812 228.96 \n",
       "L 323.782812 7.2 \n",
       "L 44.782812 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 44.782812 228.96 \n",
       "L 44.782812 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m92953bacde\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"44.782812\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(42.282812 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-31\" d=\"M 750 3822 \n",
       "L 1781 4325 \n",
       "L 1884 4325 \n",
       "L 1884 747 \n",
       "Q 1884 391 1914 303 \n",
       "Q 1944 216 2037 169 \n",
       "Q 2131 122 2419 116 \n",
       "L 2419 0 \n",
       "L 825 0 \n",
       "L 825 116 \n",
       "Q 1125 122 1212 167 \n",
       "Q 1300 213 1334 289 \n",
       "Q 1369 366 1369 747 \n",
       "L 1369 3034 \n",
       "Q 1369 3497 1338 3628 \n",
       "Q 1316 3728 1258 3775 \n",
       "Q 1200 3822 1119 3822 \n",
       "Q 1003 3822 797 3725 \n",
       "L 750 3822 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 75.782813 228.96 \n",
       "L 75.782813 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"75.782813\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(73.282813 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-32\" d=\"M 2934 816 \n",
       "L 2638 0 \n",
       "L 138 0 \n",
       "L 138 116 \n",
       "Q 1241 1122 1691 1759 \n",
       "Q 2141 2397 2141 2925 \n",
       "Q 2141 3328 1894 3587 \n",
       "Q 1647 3847 1303 3847 \n",
       "Q 991 3847 742 3664 \n",
       "Q 494 3481 375 3128 \n",
       "L 259 3128 \n",
       "Q 338 3706 661 4015 \n",
       "Q 984 4325 1469 4325 \n",
       "Q 1984 4325 2329 3994 \n",
       "Q 2675 3663 2675 3213 \n",
       "Q 2675 2891 2525 2569 \n",
       "Q 2294 2063 1775 1497 \n",
       "Q 997 647 803 472 \n",
       "L 1909 472 \n",
       "Q 2247 472 2383 497 \n",
       "Q 2519 522 2628 598 \n",
       "Q 2738 675 2819 816 \n",
       "L 2934 816 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 106.782813 228.96 \n",
       "L 106.782813 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"106.782813\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(104.282813 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-33\" d=\"M 325 3431 \n",
       "Q 506 3859 782 4092 \n",
       "Q 1059 4325 1472 4325 \n",
       "Q 1981 4325 2253 3994 \n",
       "Q 2459 3747 2459 3466 \n",
       "Q 2459 3003 1878 2509 \n",
       "Q 2269 2356 2469 2072 \n",
       "Q 2669 1788 2669 1403 \n",
       "Q 2669 853 2319 450 \n",
       "Q 1863 -75 997 -75 \n",
       "Q 569 -75 414 31 \n",
       "Q 259 138 259 259 \n",
       "Q 259 350 332 419 \n",
       "Q 406 488 509 488 \n",
       "Q 588 488 669 463 \n",
       "Q 722 447 909 348 \n",
       "Q 1097 250 1169 231 \n",
       "Q 1284 197 1416 197 \n",
       "Q 1734 197 1970 444 \n",
       "Q 2206 691 2206 1028 \n",
       "Q 2206 1275 2097 1509 \n",
       "Q 2016 1684 1919 1775 \n",
       "Q 1784 1900 1550 2001 \n",
       "Q 1316 2103 1072 2103 \n",
       "L 972 2103 \n",
       "L 972 2197 \n",
       "Q 1219 2228 1467 2375 \n",
       "Q 1716 2522 1828 2728 \n",
       "Q 1941 2934 1941 3181 \n",
       "Q 1941 3503 1739 3701 \n",
       "Q 1538 3900 1238 3900 \n",
       "Q 753 3900 428 3381 \n",
       "L 325 3431 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 137.782813 228.96 \n",
       "L 137.782813 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"137.782813\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(135.282813 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-34\" d=\"M 2978 1563 \n",
       "L 2978 1119 \n",
       "L 2409 1119 \n",
       "L 2409 0 \n",
       "L 1894 0 \n",
       "L 1894 1119 \n",
       "L 100 1119 \n",
       "L 100 1519 \n",
       "L 2066 4325 \n",
       "L 2409 4325 \n",
       "L 2409 1563 \n",
       "L 2978 1563 \n",
       "z\n",
       "M 1894 1563 \n",
       "L 1894 3666 \n",
       "L 406 1563 \n",
       "L 1894 1563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 168.782813 228.96 \n",
       "L 168.782813 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"168.782813\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(166.282813 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-35\" d=\"M 2778 4238 \n",
       "L 2534 3706 \n",
       "L 1259 3706 \n",
       "L 981 3138 \n",
       "Q 1809 3016 2294 2522 \n",
       "Q 2709 2097 2709 1522 \n",
       "Q 2709 1188 2573 903 \n",
       "Q 2438 619 2231 419 \n",
       "Q 2025 219 1772 97 \n",
       "Q 1413 -75 1034 -75 \n",
       "Q 653 -75 479 54 \n",
       "Q 306 184 306 341 \n",
       "Q 306 428 378 495 \n",
       "Q 450 563 559 563 \n",
       "Q 641 563 702 538 \n",
       "Q 763 513 909 409 \n",
       "Q 1144 247 1384 247 \n",
       "Q 1750 247 2026 523 \n",
       "Q 2303 800 2303 1197 \n",
       "Q 2303 1581 2056 1914 \n",
       "Q 1809 2247 1375 2428 \n",
       "Q 1034 2569 447 2591 \n",
       "L 1259 4238 \n",
       "L 2778 4238 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 199.782813 228.96 \n",
       "L 199.782813 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"199.782813\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(197.282813 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-36\" d=\"M 2869 4325 \n",
       "L 2869 4209 \n",
       "Q 2456 4169 2195 4045 \n",
       "Q 1934 3922 1679 3669 \n",
       "Q 1425 3416 1258 3105 \n",
       "Q 1091 2794 978 2366 \n",
       "Q 1428 2675 1881 2675 \n",
       "Q 2316 2675 2634 2325 \n",
       "Q 2953 1975 2953 1425 \n",
       "Q 2953 894 2631 456 \n",
       "Q 2244 -75 1606 -75 \n",
       "Q 1172 -75 869 213 \n",
       "Q 275 772 275 1663 \n",
       "Q 275 2231 503 2743 \n",
       "Q 731 3256 1154 3653 \n",
       "Q 1578 4050 1965 4187 \n",
       "Q 2353 4325 2688 4325 \n",
       "L 2869 4325 \n",
       "z\n",
       "M 925 2138 \n",
       "Q 869 1716 869 1456 \n",
       "Q 869 1156 980 804 \n",
       "Q 1091 453 1309 247 \n",
       "Q 1469 100 1697 100 \n",
       "Q 1969 100 2183 356 \n",
       "Q 2397 613 2397 1088 \n",
       "Q 2397 1622 2184 2012 \n",
       "Q 1972 2403 1581 2403 \n",
       "Q 1463 2403 1327 2353 \n",
       "Q 1191 2303 925 2138 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 230.782813 228.96 \n",
       "L 230.782813 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"230.782813\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(228.282813 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-37\" d=\"M 644 4238 \n",
       "L 2916 4238 \n",
       "L 2916 4119 \n",
       "L 1503 -88 \n",
       "L 1153 -88 \n",
       "L 2419 3728 \n",
       "L 1253 3728 \n",
       "Q 900 3728 750 3644 \n",
       "Q 488 3500 328 3200 \n",
       "L 238 3234 \n",
       "L 644 4238 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 261.782812 228.96 \n",
       "L 261.782812 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"261.782812\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(259.282812 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-38\" d=\"M 1228 2134 \n",
       "Q 725 2547 579 2797 \n",
       "Q 434 3047 434 3316 \n",
       "Q 434 3728 753 4026 \n",
       "Q 1072 4325 1600 4325 \n",
       "Q 2113 4325 2425 4047 \n",
       "Q 2738 3769 2738 3413 \n",
       "Q 2738 3175 2569 2928 \n",
       "Q 2400 2681 1866 2347 \n",
       "Q 2416 1922 2594 1678 \n",
       "Q 2831 1359 2831 1006 \n",
       "Q 2831 559 2490 242 \n",
       "Q 2150 -75 1597 -75 \n",
       "Q 994 -75 656 303 \n",
       "Q 388 606 388 966 \n",
       "Q 388 1247 577 1523 \n",
       "Q 766 1800 1228 2134 \n",
       "z\n",
       "M 1719 2469 \n",
       "Q 2094 2806 2194 3001 \n",
       "Q 2294 3197 2294 3444 \n",
       "Q 2294 3772 2109 3958 \n",
       "Q 1925 4144 1606 4144 \n",
       "Q 1288 4144 1088 3959 \n",
       "Q 888 3775 888 3528 \n",
       "Q 888 3366 970 3203 \n",
       "Q 1053 3041 1206 2894 \n",
       "L 1719 2469 \n",
       "z\n",
       "M 1375 2016 \n",
       "Q 1116 1797 991 1539 \n",
       "Q 866 1281 866 981 \n",
       "Q 866 578 1086 336 \n",
       "Q 1306 94 1647 94 \n",
       "Q 1984 94 2187 284 \n",
       "Q 2391 475 2391 747 \n",
       "Q 2391 972 2272 1150 \n",
       "Q 2050 1481 1375 2016 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 292.782812 228.96 \n",
       "L 292.782812 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"292.782812\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 9 -->\n",
       "      <g transform=\"translate(290.282812 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-39\" d=\"M 338 -88 \n",
       "L 338 28 \n",
       "Q 744 34 1094 217 \n",
       "Q 1444 400 1770 856 \n",
       "Q 2097 1313 2225 1859 \n",
       "Q 1734 1544 1338 1544 \n",
       "Q 891 1544 572 1889 \n",
       "Q 253 2234 253 2806 \n",
       "Q 253 3363 572 3797 \n",
       "Q 956 4325 1575 4325 \n",
       "Q 2097 4325 2469 3894 \n",
       "Q 2925 3359 2925 2575 \n",
       "Q 2925 1869 2578 1258 \n",
       "Q 2231 647 1613 244 \n",
       "Q 1109 -88 516 -88 \n",
       "L 338 -88 \n",
       "z\n",
       "M 2275 2091 \n",
       "Q 2331 2497 2331 2741 \n",
       "Q 2331 3044 2228 3395 \n",
       "Q 2125 3747 1936 3934 \n",
       "Q 1747 4122 1506 4122 \n",
       "Q 1228 4122 1018 3872 \n",
       "Q 809 3622 809 3128 \n",
       "Q 809 2469 1088 2097 \n",
       "Q 1291 1828 1588 1828 \n",
       "Q 1731 1828 1928 1897 \n",
       "Q 2125 1966 2275 2091 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 323.782812 228.96 \n",
       "L 323.782812 7.2 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92953bacde\" x=\"323.782812\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(318.782812 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-30\" d=\"M 231 2094 \n",
       "Q 231 2819 450 3342 \n",
       "Q 669 3866 1031 4122 \n",
       "Q 1313 4325 1613 4325 \n",
       "Q 2100 4325 2488 3828 \n",
       "Q 2972 3213 2972 2159 \n",
       "Q 2972 1422 2759 906 \n",
       "Q 2547 391 2217 158 \n",
       "Q 1888 -75 1581 -75 \n",
       "Q 975 -75 572 641 \n",
       "Q 231 1244 231 2094 \n",
       "z\n",
       "M 844 2016 \n",
       "Q 844 1141 1059 588 \n",
       "Q 1238 122 1591 122 \n",
       "Q 1759 122 1940 273 \n",
       "Q 2122 425 2216 781 \n",
       "Q 2359 1319 2359 2297 \n",
       "Q 2359 3022 2209 3506 \n",
       "Q 2097 3866 1919 4016 \n",
       "Q 1791 4119 1609 4119 \n",
       "Q 1397 4119 1231 3928 \n",
       "Q 1006 3669 925 3112 \n",
       "Q 844 2556 844 2016 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- Epoch -->\n",
       "     <g transform=\"translate(171.509375 255.986562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"TimesNewRomanPSMT-45\" d=\"M 1338 4006 \n",
       "L 1338 2331 \n",
       "L 2269 2331 \n",
       "Q 2631 2331 2753 2441 \n",
       "Q 2916 2584 2934 2947 \n",
       "L 3050 2947 \n",
       "L 3050 1472 \n",
       "L 2934 1472 \n",
       "Q 2891 1781 2847 1869 \n",
       "Q 2791 1978 2662 2040 \n",
       "Q 2534 2103 2269 2103 \n",
       "L 1338 2103 \n",
       "L 1338 706 \n",
       "Q 1338 425 1363 364 \n",
       "Q 1388 303 1450 267 \n",
       "Q 1513 231 1688 231 \n",
       "L 2406 231 \n",
       "Q 2766 231 2928 281 \n",
       "Q 3091 331 3241 478 \n",
       "Q 3434 672 3638 1063 \n",
       "L 3763 1063 \n",
       "L 3397 0 \n",
       "L 131 0 \n",
       "L 131 116 \n",
       "L 281 116 \n",
       "Q 431 116 566 188 \n",
       "Q 666 238 702 338 \n",
       "Q 738 438 738 747 \n",
       "L 738 3500 \n",
       "Q 738 3903 656 3997 \n",
       "Q 544 4122 281 4122 \n",
       "L 131 4122 \n",
       "L 131 4238 \n",
       "L 3397 4238 \n",
       "L 3444 3309 \n",
       "L 3322 3309 \n",
       "Q 3256 3644 3176 3769 \n",
       "Q 3097 3894 2941 3959 \n",
       "Q 2816 4006 2500 4006 \n",
       "L 1338 4006 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-70\" d=\"M -6 2578 \n",
       "L 875 2934 \n",
       "L 994 2934 \n",
       "L 994 2266 \n",
       "Q 1216 2644 1439 2795 \n",
       "Q 1663 2947 1909 2947 \n",
       "Q 2341 2947 2628 2609 \n",
       "Q 2981 2197 2981 1534 \n",
       "Q 2981 794 2556 309 \n",
       "Q 2206 -88 1675 -88 \n",
       "Q 1444 -88 1275 -22 \n",
       "Q 1150 25 994 166 \n",
       "L 994 -706 \n",
       "Q 994 -1000 1030 -1079 \n",
       "Q 1066 -1159 1155 -1206 \n",
       "Q 1244 -1253 1478 -1253 \n",
       "L 1478 -1369 \n",
       "L -22 -1369 \n",
       "L -22 -1253 \n",
       "L 56 -1253 \n",
       "Q 228 -1256 350 -1188 \n",
       "Q 409 -1153 442 -1076 \n",
       "Q 475 -1000 475 -688 \n",
       "L 475 2019 \n",
       "Q 475 2297 450 2372 \n",
       "Q 425 2447 370 2484 \n",
       "Q 316 2522 222 2522 \n",
       "Q 147 2522 31 2478 \n",
       "L -6 2578 \n",
       "z\n",
       "M 994 2081 \n",
       "L 994 1013 \n",
       "Q 994 666 1022 556 \n",
       "Q 1066 375 1236 237 \n",
       "Q 1406 100 1666 100 \n",
       "Q 1978 100 2172 344 \n",
       "Q 2425 663 2425 1241 \n",
       "Q 2425 1897 2138 2250 \n",
       "Q 1938 2494 1663 2494 \n",
       "Q 1513 2494 1366 2419 \n",
       "Q 1253 2363 994 2081 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-6f\" d=\"M 1600 2947 \n",
       "Q 2250 2947 2644 2453 \n",
       "Q 2978 2031 2978 1484 \n",
       "Q 2978 1100 2793 706 \n",
       "Q 2609 313 2286 112 \n",
       "Q 1963 -88 1566 -88 \n",
       "Q 919 -88 538 428 \n",
       "Q 216 863 216 1403 \n",
       "Q 216 1797 411 2186 \n",
       "Q 606 2575 925 2761 \n",
       "Q 1244 2947 1600 2947 \n",
       "z\n",
       "M 1503 2744 \n",
       "Q 1338 2744 1170 2645 \n",
       "Q 1003 2547 900 2300 \n",
       "Q 797 2053 797 1666 \n",
       "Q 797 1041 1045 587 \n",
       "Q 1294 134 1700 134 \n",
       "Q 2003 134 2200 384 \n",
       "Q 2397 634 2397 1244 \n",
       "Q 2397 2006 2069 2444 \n",
       "Q 1847 2744 1503 2744 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-63\" d=\"M 2631 1088 \n",
       "Q 2516 522 2178 217 \n",
       "Q 1841 -88 1431 -88 \n",
       "Q 944 -88 581 321 \n",
       "Q 219 731 219 1428 \n",
       "Q 219 2103 620 2525 \n",
       "Q 1022 2947 1584 2947 \n",
       "Q 2006 2947 2278 2723 \n",
       "Q 2550 2500 2550 2259 \n",
       "Q 2550 2141 2473 2067 \n",
       "Q 2397 1994 2259 1994 \n",
       "Q 2075 1994 1981 2113 \n",
       "Q 1928 2178 1911 2362 \n",
       "Q 1894 2547 1784 2644 \n",
       "Q 1675 2738 1481 2738 \n",
       "Q 1169 2738 978 2506 \n",
       "Q 725 2200 725 1697 \n",
       "Q 725 1184 976 792 \n",
       "Q 1228 400 1656 400 \n",
       "Q 1963 400 2206 609 \n",
       "Q 2378 753 2541 1131 \n",
       "L 2631 1088 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-68\" d=\"M 1041 4444 \n",
       "L 1041 2350 \n",
       "Q 1388 2731 1591 2839 \n",
       "Q 1794 2947 1997 2947 \n",
       "Q 2241 2947 2416 2812 \n",
       "Q 2591 2678 2675 2391 \n",
       "Q 2734 2191 2734 1659 \n",
       "L 2734 647 \n",
       "Q 2734 375 2778 275 \n",
       "Q 2809 200 2884 156 \n",
       "Q 2959 113 3159 113 \n",
       "L 3159 0 \n",
       "L 1753 0 \n",
       "L 1753 113 \n",
       "L 1819 113 \n",
       "Q 2019 113 2097 173 \n",
       "Q 2175 234 2206 353 \n",
       "Q 2216 403 2216 647 \n",
       "L 2216 1659 \n",
       "Q 2216 2128 2167 2275 \n",
       "Q 2119 2422 2012 2495 \n",
       "Q 1906 2569 1756 2569 \n",
       "Q 1603 2569 1437 2487 \n",
       "Q 1272 2406 1041 2159 \n",
       "L 1041 647 \n",
       "Q 1041 353 1073 281 \n",
       "Q 1106 209 1195 161 \n",
       "Q 1284 113 1503 113 \n",
       "L 1503 0 \n",
       "L 84 0 \n",
       "L 84 113 \n",
       "Q 275 113 384 172 \n",
       "Q 447 203 484 290 \n",
       "Q 522 378 522 647 \n",
       "L 522 3238 \n",
       "Q 522 3728 498 3840 \n",
       "Q 475 3953 426 3993 \n",
       "Q 378 4034 297 4034 \n",
       "Q 231 4034 84 3984 \n",
       "L 41 4094 \n",
       "L 897 4444 \n",
       "L 1041 4444 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-45\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-70\" x=\"61.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-6f\" x=\"111.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"161.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-68\" x=\"205.46875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 44.782812 228.96 \n",
       "L 323.782812 228.96 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <defs>\n",
       "       <path id=\"m2ec95d50d3\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(20.282812 232.431875) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-2e\" d=\"M 800 606 \n",
       "Q 947 606 1047 504 \n",
       "Q 1147 403 1147 259 \n",
       "Q 1147 116 1045 14 \n",
       "Q 944 -88 800 -88 \n",
       "Q 656 -88 554 14 \n",
       "Q 453 116 453 259 \n",
       "Q 453 406 554 506 \n",
       "Q 656 606 800 606 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 44.782812 217.872 \n",
       "L 323.782812 217.872 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"217.872\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.282812 221.343875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 44.782812 206.784 \n",
       "L 323.782812 206.784 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"206.784\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.282812 210.255875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 44.782812 195.695999 \n",
       "L 323.782812 195.695999 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"195.695999\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.282812 199.167874) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <path d=\"M 44.782812 184.607999 \n",
       "L 323.782812 184.607999 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"184.607999\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.282812 188.079874) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-32\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <path d=\"M 44.782812 173.52 \n",
       "L 323.782812 173.52 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"173.52\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(20.282812 176.991875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-32\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <path d=\"M 44.782812 162.431997 \n",
       "L 323.782812 162.431997 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_34\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"162.431997\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 0.30 -->\n",
       "      <g transform=\"translate(20.282812 165.903872) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-33\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_35\">\n",
       "      <path d=\"M 44.782812 151.344001 \n",
       "L 323.782812 151.344001 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_36\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"151.344001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 0.35 -->\n",
       "      <g transform=\"translate(20.282812 154.815876) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-33\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_37\">\n",
       "      <path d=\"M 44.782812 140.255999 \n",
       "L 323.782812 140.255999 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_38\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"140.255999\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 0.40 -->\n",
       "      <g transform=\"translate(20.282812 143.727874) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-34\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <path d=\"M 44.782812 129.167996 \n",
       "L 323.782812 129.167996 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_40\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"129.167996\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 0.45 -->\n",
       "      <g transform=\"translate(20.282812 132.639871) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-34\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <path d=\"M 44.782812 118.08 \n",
       "L 323.782812 118.08 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_42\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(20.282812 121.551875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_43\">\n",
       "      <path d=\"M 44.782812 106.991997 \n",
       "L 323.782812 106.991997 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_44\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"106.991997\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.55 -->\n",
       "      <g transform=\"translate(20.282812 110.463872) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_45\">\n",
       "      <path d=\"M 44.782812 95.903995 \n",
       "L 323.782812 95.903995 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_46\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"95.903995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.60 -->\n",
       "      <g transform=\"translate(20.282812 99.37587) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-36\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_47\">\n",
       "      <path d=\"M 44.782812 84.816005 \n",
       "L 323.782812 84.816005 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_48\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"84.816005\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 0.65 -->\n",
       "      <g transform=\"translate(20.282812 88.28788) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-36\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_49\">\n",
       "      <path d=\"M 44.782812 73.728003 \n",
       "L 323.782812 73.728003 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_50\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"73.728003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 0.70 -->\n",
       "      <g transform=\"translate(20.282812 77.199878) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-37\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_51\">\n",
       "      <path d=\"M 44.782812 62.64 \n",
       "L 323.782812 62.64 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_52\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"62.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(20.282812 66.111875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-37\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_17\">\n",
       "     <g id=\"line2d_53\">\n",
       "      <path d=\"M 44.782812 51.551997 \n",
       "L 323.782812 51.551997 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_54\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"51.551997\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 0.80 -->\n",
       "      <g transform=\"translate(20.282812 55.023872) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-38\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_18\">\n",
       "     <g id=\"line2d_55\">\n",
       "      <path d=\"M 44.782812 40.463995 \n",
       "L 323.782812 40.463995 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_56\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"40.463995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 0.85 -->\n",
       "      <g transform=\"translate(20.282812 43.93587) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-38\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_19\">\n",
       "     <g id=\"line2d_57\">\n",
       "      <path d=\"M 44.782812 29.376005 \n",
       "L 323.782812 29.376005 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_58\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"29.376005\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 0.90 -->\n",
       "      <g transform=\"translate(20.282812 32.84788) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-39\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_20\">\n",
       "     <g id=\"line2d_59\">\n",
       "      <path d=\"M 44.782812 18.288003 \n",
       "L 323.782812 18.288003 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_60\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2ec95d50d3\" x=\"44.782812\" y=\"18.288003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 0.95 -->\n",
       "      <g transform=\"translate(20.282812 21.759878) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-39\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_32\">\n",
       "     <!-- Values -->\n",
       "     <g transform=\"translate(14.14375 131.408906) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"TimesNewRomanPSMT-56\" d=\"M 4544 4238 \n",
       "L 4544 4122 \n",
       "Q 4319 4081 4203 3978 \n",
       "Q 4038 3825 3909 3509 \n",
       "L 2431 -97 \n",
       "L 2316 -97 \n",
       "L 728 3556 \n",
       "Q 606 3838 556 3900 \n",
       "Q 478 3997 364 4051 \n",
       "Q 250 4106 56 4122 \n",
       "L 56 4238 \n",
       "L 1788 4238 \n",
       "L 1788 4122 \n",
       "Q 1494 4094 1406 4022 \n",
       "Q 1319 3950 1319 3838 \n",
       "Q 1319 3681 1463 3350 \n",
       "L 2541 866 \n",
       "L 3541 3319 \n",
       "Q 3688 3681 3688 3822 \n",
       "Q 3688 3913 3597 3995 \n",
       "Q 3506 4078 3291 4113 \n",
       "Q 3275 4116 3238 4122 \n",
       "L 3238 4238 \n",
       "L 4544 4238 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-61\" d=\"M 1822 413 \n",
       "Q 1381 72 1269 19 \n",
       "Q 1100 -59 909 -59 \n",
       "Q 613 -59 420 144 \n",
       "Q 228 347 228 678 \n",
       "Q 228 888 322 1041 \n",
       "Q 450 1253 767 1440 \n",
       "Q 1084 1628 1822 1897 \n",
       "L 1822 2009 \n",
       "Q 1822 2438 1686 2597 \n",
       "Q 1550 2756 1291 2756 \n",
       "Q 1094 2756 978 2650 \n",
       "Q 859 2544 859 2406 \n",
       "L 866 2225 \n",
       "Q 866 2081 792 2003 \n",
       "Q 719 1925 600 1925 \n",
       "Q 484 1925 411 2006 \n",
       "Q 338 2088 338 2228 \n",
       "Q 338 2497 613 2722 \n",
       "Q 888 2947 1384 2947 \n",
       "Q 1766 2947 2009 2819 \n",
       "Q 2194 2722 2281 2516 \n",
       "Q 2338 2381 2338 1966 \n",
       "L 2338 994 \n",
       "Q 2338 584 2353 492 \n",
       "Q 2369 400 2405 369 \n",
       "Q 2441 338 2488 338 \n",
       "Q 2538 338 2575 359 \n",
       "Q 2641 400 2828 588 \n",
       "L 2828 413 \n",
       "Q 2478 -56 2159 -56 \n",
       "Q 2006 -56 1915 50 \n",
       "Q 1825 156 1822 413 \n",
       "z\n",
       "M 1822 616 \n",
       "L 1822 1706 \n",
       "Q 1350 1519 1213 1441 \n",
       "Q 966 1303 859 1153 \n",
       "Q 753 1003 753 825 \n",
       "Q 753 600 887 451 \n",
       "Q 1022 303 1197 303 \n",
       "Q 1434 303 1822 616 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-6c\" d=\"M 1184 4444 \n",
       "L 1184 647 \n",
       "Q 1184 378 1223 290 \n",
       "Q 1263 203 1344 158 \n",
       "Q 1425 113 1647 113 \n",
       "L 1647 0 \n",
       "L 244 0 \n",
       "L 244 113 \n",
       "Q 441 113 512 153 \n",
       "Q 584 194 625 287 \n",
       "Q 666 381 666 647 \n",
       "L 666 3247 \n",
       "Q 666 3731 644 3842 \n",
       "Q 622 3953 573 3993 \n",
       "Q 525 4034 450 4034 \n",
       "Q 369 4034 244 3984 \n",
       "L 191 4094 \n",
       "L 1044 4444 \n",
       "L 1184 4444 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-75\" d=\"M 2709 2863 \n",
       "L 2709 1128 \n",
       "Q 2709 631 2732 520 \n",
       "Q 2756 409 2807 365 \n",
       "Q 2859 322 2928 322 \n",
       "Q 3025 322 3147 375 \n",
       "L 3191 266 \n",
       "L 2334 -88 \n",
       "L 2194 -88 \n",
       "L 2194 519 \n",
       "Q 1825 119 1631 15 \n",
       "Q 1438 -88 1222 -88 \n",
       "Q 981 -88 804 51 \n",
       "Q 628 191 559 409 \n",
       "Q 491 628 491 1028 \n",
       "L 491 2306 \n",
       "Q 491 2509 447 2587 \n",
       "Q 403 2666 317 2708 \n",
       "Q 231 2750 6 2747 \n",
       "L 6 2863 \n",
       "L 1009 2863 \n",
       "L 1009 947 \n",
       "Q 1009 547 1148 422 \n",
       "Q 1288 297 1484 297 \n",
       "Q 1619 297 1789 381 \n",
       "Q 1959 466 2194 703 \n",
       "L 2194 2325 \n",
       "Q 2194 2569 2105 2655 \n",
       "Q 2016 2741 1734 2747 \n",
       "L 1734 2863 \n",
       "L 2709 2863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-65\" d=\"M 681 1784 \n",
       "Q 678 1147 991 784 \n",
       "Q 1303 422 1725 422 \n",
       "Q 2006 422 2214 576 \n",
       "Q 2422 731 2563 1106 \n",
       "L 2659 1044 \n",
       "Q 2594 616 2278 264 \n",
       "Q 1963 -88 1488 -88 \n",
       "Q 972 -88 605 314 \n",
       "Q 238 716 238 1394 \n",
       "Q 238 2128 614 2539 \n",
       "Q 991 2950 1559 2950 \n",
       "Q 2041 2950 2350 2633 \n",
       "Q 2659 2316 2659 1784 \n",
       "L 681 1784 \n",
       "z\n",
       "M 681 1966 \n",
       "L 2006 1966 \n",
       "Q 1991 2241 1941 2353 \n",
       "Q 1863 2528 1708 2628 \n",
       "Q 1553 2728 1384 2728 \n",
       "Q 1125 2728 920 2526 \n",
       "Q 716 2325 681 1966 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-73\" d=\"M 2050 2947 \n",
       "L 2050 1972 \n",
       "L 1947 1972 \n",
       "Q 1828 2431 1642 2597 \n",
       "Q 1456 2763 1169 2763 \n",
       "Q 950 2763 815 2647 \n",
       "Q 681 2531 681 2391 \n",
       "Q 681 2216 781 2091 \n",
       "Q 878 1963 1175 1819 \n",
       "L 1631 1597 \n",
       "Q 2266 1288 2266 781 \n",
       "Q 2266 391 1970 151 \n",
       "Q 1675 -88 1309 -88 \n",
       "Q 1047 -88 709 6 \n",
       "Q 606 38 541 38 \n",
       "Q 469 38 428 -44 \n",
       "L 325 -44 \n",
       "L 325 978 \n",
       "L 428 978 \n",
       "Q 516 541 762 319 \n",
       "Q 1009 97 1316 97 \n",
       "Q 1531 97 1667 223 \n",
       "Q 1803 350 1803 528 \n",
       "Q 1803 744 1651 891 \n",
       "Q 1500 1038 1047 1263 \n",
       "Q 594 1488 453 1669 \n",
       "Q 313 1847 313 2119 \n",
       "Q 313 2472 555 2709 \n",
       "Q 797 2947 1181 2947 \n",
       "Q 1350 2947 1591 2875 \n",
       "Q 1750 2828 1803 2828 \n",
       "Q 1853 2828 1881 2850 \n",
       "Q 1909 2872 1947 2947 \n",
       "L 2050 2947 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-56\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"61.091797\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-6c\" x=\"105.476562\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-75\" x=\"133.259766\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-65\" x=\"183.259766\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-73\" x=\"227.644531\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_61\">\n",
       "    <path d=\"M 44.782812 41.894346 \n",
       "L 75.782813 24.622945 \n",
       "L 106.782813 21.26698 \n",
       "L 137.782813 20.35407 \n",
       "L 168.782813 18.262122 \n",
       "L 199.782813 17.020271 \n",
       "L 230.782813 15.996477 \n",
       "L 261.782812 14.891379 \n",
       "L 292.782812 14.248274 \n",
       "L 323.782812 13.753012 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_62\">\n",
       "    <path d=\"M 44.782812 40.707932 \n",
       "L 75.782813 24.031588 \n",
       "L 106.782813 21.614399 \n",
       "L 137.782813 21.126526 \n",
       "L 168.782813 18.975454 \n",
       "L 199.782813 18.177117 \n",
       "L 230.782813 17.134844 \n",
       "L 261.782812 16.136931 \n",
       "L 292.782812 15.693403 \n",
       "L 323.782812 15.116837 \n",
       "\" clip-path=\"url(#p21d23a3a71)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 44.782812 228.96 \n",
       "L 44.782812 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 323.782812 228.96 \n",
       "L 323.782812 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 44.782812 228.96 \n",
       "L 323.782812 228.96 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 44.782812 7.2 \n",
       "L 323.782812 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 51.782812 223.96 \n",
       "L 120.425 223.96 \n",
       "Q 122.425 223.96 122.425 221.96 \n",
       "L 122.425 194.644375 \n",
       "Q 122.425 192.644375 120.425 192.644375 \n",
       "L 51.782812 192.644375 \n",
       "Q 49.782812 192.644375 49.782812 194.644375 \n",
       "L 49.782812 221.96 \n",
       "Q 49.782812 223.96 51.782812 223.96 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_63\">\n",
       "     <path d=\"M 53.782812 200.144375 \n",
       "L 63.782812 200.144375 \n",
       "L 73.782813 200.144375 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_33\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(81.782813 203.644375) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"TimesNewRomanPSMT-74\" d=\"M 1031 3803 \n",
       "L 1031 2863 \n",
       "L 1700 2863 \n",
       "L 1700 2644 \n",
       "L 1031 2644 \n",
       "L 1031 788 \n",
       "Q 1031 509 1111 412 \n",
       "Q 1191 316 1316 316 \n",
       "Q 1419 316 1516 380 \n",
       "Q 1613 444 1666 569 \n",
       "L 1788 569 \n",
       "Q 1678 263 1478 108 \n",
       "Q 1278 -47 1066 -47 \n",
       "Q 922 -47 784 33 \n",
       "Q 647 113 581 261 \n",
       "Q 516 409 516 719 \n",
       "L 516 2644 \n",
       "L 63 2644 \n",
       "L 63 2747 \n",
       "Q 234 2816 414 2980 \n",
       "Q 594 3144 734 3369 \n",
       "Q 806 3488 934 3803 \n",
       "L 1031 3803 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-72\" d=\"M 1038 2947 \n",
       "L 1038 2303 \n",
       "Q 1397 2947 1775 2947 \n",
       "Q 1947 2947 2059 2842 \n",
       "Q 2172 2738 2172 2600 \n",
       "Q 2172 2478 2090 2393 \n",
       "Q 2009 2309 1897 2309 \n",
       "Q 1788 2309 1652 2417 \n",
       "Q 1516 2525 1450 2525 \n",
       "Q 1394 2525 1328 2463 \n",
       "Q 1188 2334 1038 2041 \n",
       "L 1038 669 \n",
       "Q 1038 431 1097 309 \n",
       "Q 1138 225 1241 169 \n",
       "Q 1344 113 1538 113 \n",
       "L 1538 0 \n",
       "L 72 0 \n",
       "L 72 113 \n",
       "Q 291 113 397 181 \n",
       "Q 475 231 506 341 \n",
       "Q 522 394 522 644 \n",
       "L 522 1753 \n",
       "Q 522 2253 501 2348 \n",
       "Q 481 2444 426 2487 \n",
       "Q 372 2531 291 2531 \n",
       "Q 194 2531 72 2484 \n",
       "L 41 2597 \n",
       "L 906 2947 \n",
       "L 1038 2947 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-69\" d=\"M 928 4444 \n",
       "Q 1059 4444 1151 4351 \n",
       "Q 1244 4259 1244 4128 \n",
       "Q 1244 3997 1151 3903 \n",
       "Q 1059 3809 928 3809 \n",
       "Q 797 3809 703 3903 \n",
       "Q 609 3997 609 4128 \n",
       "Q 609 4259 701 4351 \n",
       "Q 794 4444 928 4444 \n",
       "z\n",
       "M 1188 2947 \n",
       "L 1188 647 \n",
       "Q 1188 378 1227 289 \n",
       "Q 1266 200 1342 156 \n",
       "Q 1419 113 1622 113 \n",
       "L 1622 0 \n",
       "L 231 0 \n",
       "L 231 113 \n",
       "Q 441 113 512 153 \n",
       "Q 584 194 626 287 \n",
       "Q 669 381 669 647 \n",
       "L 669 1750 \n",
       "Q 669 2216 641 2353 \n",
       "Q 619 2453 572 2492 \n",
       "Q 525 2531 444 2531 \n",
       "Q 356 2531 231 2484 \n",
       "L 188 2597 \n",
       "L 1050 2947 \n",
       "L 1188 2947 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-6e\" d=\"M 1034 2341 \n",
       "Q 1538 2947 1994 2947 \n",
       "Q 2228 2947 2397 2830 \n",
       "Q 2566 2713 2666 2444 \n",
       "Q 2734 2256 2734 1869 \n",
       "L 2734 647 \n",
       "Q 2734 375 2778 278 \n",
       "Q 2813 200 2889 156 \n",
       "Q 2966 113 3172 113 \n",
       "L 3172 0 \n",
       "L 1756 0 \n",
       "L 1756 113 \n",
       "L 1816 113 \n",
       "Q 2016 113 2095 173 \n",
       "Q 2175 234 2206 353 \n",
       "Q 2219 400 2219 647 \n",
       "L 2219 1819 \n",
       "Q 2219 2209 2117 2386 \n",
       "Q 2016 2563 1775 2563 \n",
       "Q 1403 2563 1034 2156 \n",
       "L 1034 647 \n",
       "Q 1034 356 1069 288 \n",
       "Q 1113 197 1189 155 \n",
       "Q 1266 113 1500 113 \n",
       "L 1500 0 \n",
       "L 84 0 \n",
       "L 84 113 \n",
       "L 147 113 \n",
       "Q 366 113 442 223 \n",
       "Q 519 334 519 647 \n",
       "L 519 1709 \n",
       "Q 519 2225 495 2337 \n",
       "Q 472 2450 423 2490 \n",
       "Q 375 2531 294 2531 \n",
       "Q 206 2531 84 2484 \n",
       "L 38 2597 \n",
       "L 900 2947 \n",
       "L 1034 2947 \n",
       "L 1034 2341 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-5f\" d=\"M 3256 -1381 \n",
       "L -53 -1381 \n",
       "L -53 -1119 \n",
       "L 3256 -1119 \n",
       "L 3256 -1381 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-74\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-72\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"61.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-69\" x=\"105.46875\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-6e\" x=\"133.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-5f\" x=\"183.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"233.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"277.636719\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"322.021484\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_64\">\n",
       "     <path d=\"M 53.782812 214.302187 \n",
       "L 63.782812 214.302187 \n",
       "L 73.782813 214.302187 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_34\">\n",
       "     <!-- test_acc -->\n",
       "     <g transform=\"translate(81.782813 217.802187) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-74\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-65\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-73\" x=\"72.167969\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-74\" x=\"111.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-5f\" x=\"138.867188\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"188.867188\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"233.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"277.636719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p21d23a3a71\">\n",
       "   <rect x=\"44.782812\" y=\"7.2\" width=\"279\" height=\"221.76\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_acc一直在92%左右，如何才能提高？\n",
    "# 使用CNN会好一点吗？\n",
    "# 我们来试一试：\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net1()  \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)   \n",
    "   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device, \n",
    "    train_figure=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "耗时： 210.56247329711914 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.4977), tensor(0.9650), tensor(0.9571))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"335.982812pt\" height=\"265.325625pt\" viewBox=\"0 0 335.982812 265.325625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-04-25T11:16:47.537576</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 265.325625 \n",
       "L 335.982812 265.325625 \n",
       "L 335.982812 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 44.782812 228.96 \n",
       "L 323.782812 228.96 \n",
       "L 323.782812 7.2 \n",
       "L 44.782812 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 83.265571 228.96 \n",
       "L 83.265571 7.2 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mc04ace902a\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc04ace902a\" x=\"83.265571\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(80.765571 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-35\" d=\"M 2778 4238 \n",
       "L 2534 3706 \n",
       "L 1259 3706 \n",
       "L 981 3138 \n",
       "Q 1809 3016 2294 2522 \n",
       "Q 2709 2097 2709 1522 \n",
       "Q 2709 1188 2573 903 \n",
       "Q 2438 619 2231 419 \n",
       "Q 2025 219 1772 97 \n",
       "Q 1413 -75 1034 -75 \n",
       "Q 653 -75 479 54 \n",
       "Q 306 184 306 341 \n",
       "Q 306 428 378 495 \n",
       "Q 450 563 559 563 \n",
       "Q 641 563 702 538 \n",
       "Q 763 513 909 409 \n",
       "Q 1144 247 1384 247 \n",
       "Q 1750 247 2026 523 \n",
       "Q 2303 800 2303 1197 \n",
       "Q 2303 1581 2056 1914 \n",
       "Q 1809 2247 1375 2428 \n",
       "Q 1034 2569 447 2591 \n",
       "L 1259 4238 \n",
       "L 2778 4238 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 131.369019 228.96 \n",
       "L 131.369019 7.2 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc04ace902a\" x=\"131.369019\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(126.369019 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-31\" d=\"M 750 3822 \n",
       "L 1781 4325 \n",
       "L 1884 4325 \n",
       "L 1884 747 \n",
       "Q 1884 391 1914 303 \n",
       "Q 1944 216 2037 169 \n",
       "Q 2131 122 2419 116 \n",
       "L 2419 0 \n",
       "L 825 0 \n",
       "L 825 116 \n",
       "Q 1125 122 1212 167 \n",
       "Q 1300 213 1334 289 \n",
       "Q 1369 366 1369 747 \n",
       "L 1369 3034 \n",
       "Q 1369 3497 1338 3628 \n",
       "Q 1316 3728 1258 3775 \n",
       "Q 1200 3822 1119 3822 \n",
       "Q 1003 3822 797 3725 \n",
       "L 750 3822 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"TimesNewRomanPSMT-30\" d=\"M 231 2094 \n",
       "Q 231 2819 450 3342 \n",
       "Q 669 3866 1031 4122 \n",
       "Q 1313 4325 1613 4325 \n",
       "Q 2100 4325 2488 3828 \n",
       "Q 2972 3213 2972 2159 \n",
       "Q 2972 1422 2759 906 \n",
       "Q 2547 391 2217 158 \n",
       "Q 1888 -75 1581 -75 \n",
       "Q 975 -75 572 641 \n",
       "Q 231 1244 231 2094 \n",
       "z\n",
       "M 844 2016 \n",
       "Q 844 1141 1059 588 \n",
       "Q 1238 122 1591 122 \n",
       "Q 1759 122 1940 273 \n",
       "Q 2122 425 2216 781 \n",
       "Q 2359 1319 2359 2297 \n",
       "Q 2359 3022 2209 3506 \n",
       "Q 2097 3866 1919 4016 \n",
       "Q 1791 4119 1609 4119 \n",
       "Q 1397 4119 1231 3928 \n",
       "Q 1006 3669 925 3112 \n",
       "Q 844 2556 844 2016 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 179.472468 228.96 \n",
       "L 179.472468 7.2 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc04ace902a\" x=\"179.472468\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(174.472468 242.90375) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 227.575916 228.96 \n",
       "L 227.575916 7.2 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc04ace902a\" x=\"227.575916\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(222.575916 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-32\" d=\"M 2934 816 \n",
       "L 2638 0 \n",
       "L 138 0 \n",
       "L 138 116 \n",
       "Q 1241 1122 1691 1759 \n",
       "Q 2141 2397 2141 2925 \n",
       "Q 2141 3328 1894 3587 \n",
       "Q 1647 3847 1303 3847 \n",
       "Q 991 3847 742 3664 \n",
       "Q 494 3481 375 3128 \n",
       "L 259 3128 \n",
       "Q 338 3706 661 4015 \n",
       "Q 984 4325 1469 4325 \n",
       "Q 1984 4325 2329 3994 \n",
       "Q 2675 3663 2675 3213 \n",
       "Q 2675 2891 2525 2569 \n",
       "Q 2294 2063 1775 1497 \n",
       "Q 997 647 803 472 \n",
       "L 1909 472 \n",
       "Q 2247 472 2383 497 \n",
       "Q 2519 522 2628 598 \n",
       "Q 2738 675 2819 816 \n",
       "L 2934 816 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-32\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 275.679364 228.96 \n",
       "L 275.679364 7.2 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc04ace902a\" x=\"275.679364\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(270.679364 242.90375) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-32\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 323.782812 228.96 \n",
       "L 323.782812 7.2 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc04ace902a\" x=\"323.782812\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(318.782812 242.90375) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-33\" d=\"M 325 3431 \n",
       "Q 506 3859 782 4092 \n",
       "Q 1059 4325 1472 4325 \n",
       "Q 1981 4325 2253 3994 \n",
       "Q 2459 3747 2459 3466 \n",
       "Q 2459 3003 1878 2509 \n",
       "Q 2269 2356 2469 2072 \n",
       "Q 2669 1788 2669 1403 \n",
       "Q 2669 853 2319 450 \n",
       "Q 1863 -75 997 -75 \n",
       "Q 569 -75 414 31 \n",
       "Q 259 138 259 259 \n",
       "Q 259 350 332 419 \n",
       "Q 406 488 509 488 \n",
       "Q 588 488 669 463 \n",
       "Q 722 447 909 348 \n",
       "Q 1097 250 1169 231 \n",
       "Q 1284 197 1416 197 \n",
       "Q 1734 197 1970 444 \n",
       "Q 2206 691 2206 1028 \n",
       "Q 2206 1275 2097 1509 \n",
       "Q 2016 1684 1919 1775 \n",
       "Q 1784 1900 1550 2001 \n",
       "Q 1316 2103 1072 2103 \n",
       "L 972 2103 \n",
       "L 972 2197 \n",
       "Q 1219 2228 1467 2375 \n",
       "Q 1716 2522 1828 2728 \n",
       "Q 1941 2934 1941 3181 \n",
       "Q 1941 3503 1739 3701 \n",
       "Q 1538 3900 1238 3900 \n",
       "Q 753 3900 428 3381 \n",
       "L 325 3431 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-33\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- Epoch -->\n",
       "     <g transform=\"translate(171.509375 255.986562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"TimesNewRomanPSMT-45\" d=\"M 1338 4006 \n",
       "L 1338 2331 \n",
       "L 2269 2331 \n",
       "Q 2631 2331 2753 2441 \n",
       "Q 2916 2584 2934 2947 \n",
       "L 3050 2947 \n",
       "L 3050 1472 \n",
       "L 2934 1472 \n",
       "Q 2891 1781 2847 1869 \n",
       "Q 2791 1978 2662 2040 \n",
       "Q 2534 2103 2269 2103 \n",
       "L 1338 2103 \n",
       "L 1338 706 \n",
       "Q 1338 425 1363 364 \n",
       "Q 1388 303 1450 267 \n",
       "Q 1513 231 1688 231 \n",
       "L 2406 231 \n",
       "Q 2766 231 2928 281 \n",
       "Q 3091 331 3241 478 \n",
       "Q 3434 672 3638 1063 \n",
       "L 3763 1063 \n",
       "L 3397 0 \n",
       "L 131 0 \n",
       "L 131 116 \n",
       "L 281 116 \n",
       "Q 431 116 566 188 \n",
       "Q 666 238 702 338 \n",
       "Q 738 438 738 747 \n",
       "L 738 3500 \n",
       "Q 738 3903 656 3997 \n",
       "Q 544 4122 281 4122 \n",
       "L 131 4122 \n",
       "L 131 4238 \n",
       "L 3397 4238 \n",
       "L 3444 3309 \n",
       "L 3322 3309 \n",
       "Q 3256 3644 3176 3769 \n",
       "Q 3097 3894 2941 3959 \n",
       "Q 2816 4006 2500 4006 \n",
       "L 1338 4006 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-70\" d=\"M -6 2578 \n",
       "L 875 2934 \n",
       "L 994 2934 \n",
       "L 994 2266 \n",
       "Q 1216 2644 1439 2795 \n",
       "Q 1663 2947 1909 2947 \n",
       "Q 2341 2947 2628 2609 \n",
       "Q 2981 2197 2981 1534 \n",
       "Q 2981 794 2556 309 \n",
       "Q 2206 -88 1675 -88 \n",
       "Q 1444 -88 1275 -22 \n",
       "Q 1150 25 994 166 \n",
       "L 994 -706 \n",
       "Q 994 -1000 1030 -1079 \n",
       "Q 1066 -1159 1155 -1206 \n",
       "Q 1244 -1253 1478 -1253 \n",
       "L 1478 -1369 \n",
       "L -22 -1369 \n",
       "L -22 -1253 \n",
       "L 56 -1253 \n",
       "Q 228 -1256 350 -1188 \n",
       "Q 409 -1153 442 -1076 \n",
       "Q 475 -1000 475 -688 \n",
       "L 475 2019 \n",
       "Q 475 2297 450 2372 \n",
       "Q 425 2447 370 2484 \n",
       "Q 316 2522 222 2522 \n",
       "Q 147 2522 31 2478 \n",
       "L -6 2578 \n",
       "z\n",
       "M 994 2081 \n",
       "L 994 1013 \n",
       "Q 994 666 1022 556 \n",
       "Q 1066 375 1236 237 \n",
       "Q 1406 100 1666 100 \n",
       "Q 1978 100 2172 344 \n",
       "Q 2425 663 2425 1241 \n",
       "Q 2425 1897 2138 2250 \n",
       "Q 1938 2494 1663 2494 \n",
       "Q 1513 2494 1366 2419 \n",
       "Q 1253 2363 994 2081 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-6f\" d=\"M 1600 2947 \n",
       "Q 2250 2947 2644 2453 \n",
       "Q 2978 2031 2978 1484 \n",
       "Q 2978 1100 2793 706 \n",
       "Q 2609 313 2286 112 \n",
       "Q 1963 -88 1566 -88 \n",
       "Q 919 -88 538 428 \n",
       "Q 216 863 216 1403 \n",
       "Q 216 1797 411 2186 \n",
       "Q 606 2575 925 2761 \n",
       "Q 1244 2947 1600 2947 \n",
       "z\n",
       "M 1503 2744 \n",
       "Q 1338 2744 1170 2645 \n",
       "Q 1003 2547 900 2300 \n",
       "Q 797 2053 797 1666 \n",
       "Q 797 1041 1045 587 \n",
       "Q 1294 134 1700 134 \n",
       "Q 2003 134 2200 384 \n",
       "Q 2397 634 2397 1244 \n",
       "Q 2397 2006 2069 2444 \n",
       "Q 1847 2744 1503 2744 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-63\" d=\"M 2631 1088 \n",
       "Q 2516 522 2178 217 \n",
       "Q 1841 -88 1431 -88 \n",
       "Q 944 -88 581 321 \n",
       "Q 219 731 219 1428 \n",
       "Q 219 2103 620 2525 \n",
       "Q 1022 2947 1584 2947 \n",
       "Q 2006 2947 2278 2723 \n",
       "Q 2550 2500 2550 2259 \n",
       "Q 2550 2141 2473 2067 \n",
       "Q 2397 1994 2259 1994 \n",
       "Q 2075 1994 1981 2113 \n",
       "Q 1928 2178 1911 2362 \n",
       "Q 1894 2547 1784 2644 \n",
       "Q 1675 2738 1481 2738 \n",
       "Q 1169 2738 978 2506 \n",
       "Q 725 2200 725 1697 \n",
       "Q 725 1184 976 792 \n",
       "Q 1228 400 1656 400 \n",
       "Q 1963 400 2206 609 \n",
       "Q 2378 753 2541 1131 \n",
       "L 2631 1088 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-68\" d=\"M 1041 4444 \n",
       "L 1041 2350 \n",
       "Q 1388 2731 1591 2839 \n",
       "Q 1794 2947 1997 2947 \n",
       "Q 2241 2947 2416 2812 \n",
       "Q 2591 2678 2675 2391 \n",
       "Q 2734 2191 2734 1659 \n",
       "L 2734 647 \n",
       "Q 2734 375 2778 275 \n",
       "Q 2809 200 2884 156 \n",
       "Q 2959 113 3159 113 \n",
       "L 3159 0 \n",
       "L 1753 0 \n",
       "L 1753 113 \n",
       "L 1819 113 \n",
       "Q 2019 113 2097 173 \n",
       "Q 2175 234 2206 353 \n",
       "Q 2216 403 2216 647 \n",
       "L 2216 1659 \n",
       "Q 2216 2128 2167 2275 \n",
       "Q 2119 2422 2012 2495 \n",
       "Q 1906 2569 1756 2569 \n",
       "Q 1603 2569 1437 2487 \n",
       "Q 1272 2406 1041 2159 \n",
       "L 1041 647 \n",
       "Q 1041 353 1073 281 \n",
       "Q 1106 209 1195 161 \n",
       "Q 1284 113 1503 113 \n",
       "L 1503 0 \n",
       "L 84 0 \n",
       "L 84 113 \n",
       "Q 275 113 384 172 \n",
       "Q 447 203 484 290 \n",
       "Q 522 378 522 647 \n",
       "L 522 3238 \n",
       "Q 522 3728 498 3840 \n",
       "Q 475 3953 426 3993 \n",
       "Q 378 4034 297 4034 \n",
       "Q 231 4034 84 3984 \n",
       "L 41 4094 \n",
       "L 897 4444 \n",
       "L 1041 4444 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-45\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-70\" x=\"61.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-6f\" x=\"111.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"161.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-68\" x=\"205.46875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 44.782812 228.96 \n",
       "L 323.782812 228.96 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <defs>\n",
       "       <path id=\"mcd123fd2b1\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"228.96\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(20.282812 232.431875) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-2e\" d=\"M 800 606 \n",
       "Q 947 606 1047 504 \n",
       "Q 1147 403 1147 259 \n",
       "Q 1147 116 1045 14 \n",
       "Q 944 -88 800 -88 \n",
       "Q 656 -88 554 14 \n",
       "Q 453 116 453 259 \n",
       "Q 453 406 554 506 \n",
       "Q 656 606 800 606 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 44.782812 217.872 \n",
       "L 323.782812 217.872 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"217.872\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.282812 221.343875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 44.782812 206.784 \n",
       "L 323.782812 206.784 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"206.784\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.282812 210.255875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 44.782812 195.695999 \n",
       "L 323.782812 195.695999 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"195.695999\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.282812 199.167874) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-31\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 44.782812 184.607999 \n",
       "L 323.782812 184.607999 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"184.607999\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.282812 188.079874) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-32\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 44.782812 173.52 \n",
       "L 323.782812 173.52 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"173.52\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(20.282812 176.991875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-32\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 44.782812 162.431997 \n",
       "L 323.782812 162.431997 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"162.431997\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.30 -->\n",
       "      <g transform=\"translate(20.282812 165.903872) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-33\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 44.782812 151.344001 \n",
       "L 323.782812 151.344001 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"151.344001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.35 -->\n",
       "      <g transform=\"translate(20.282812 154.815876) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-33\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <path d=\"M 44.782812 140.255999 \n",
       "L 323.782812 140.255999 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"140.255999\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.40 -->\n",
       "      <g transform=\"translate(20.282812 143.727874) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-34\" d=\"M 2978 1563 \n",
       "L 2978 1119 \n",
       "L 2409 1119 \n",
       "L 2409 0 \n",
       "L 1894 0 \n",
       "L 1894 1119 \n",
       "L 100 1119 \n",
       "L 100 1519 \n",
       "L 2066 4325 \n",
       "L 2409 4325 \n",
       "L 2409 1563 \n",
       "L 2978 1563 \n",
       "z\n",
       "M 1894 1563 \n",
       "L 1894 3666 \n",
       "L 406 1563 \n",
       "L 1894 1563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-34\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <path d=\"M 44.782812 129.167996 \n",
       "L 323.782812 129.167996 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"129.167996\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 0.45 -->\n",
       "      <g transform=\"translate(20.282812 132.639871) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-34\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <path d=\"M 44.782812 118.08 \n",
       "L 323.782812 118.08 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_34\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(20.282812 121.551875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_35\">\n",
       "      <path d=\"M 44.782812 106.991997 \n",
       "L 323.782812 106.991997 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_36\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"106.991997\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 0.55 -->\n",
       "      <g transform=\"translate(20.282812 110.463872) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_37\">\n",
       "      <path d=\"M 44.782812 95.903995 \n",
       "L 323.782812 95.903995 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_38\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"95.903995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 0.60 -->\n",
       "      <g transform=\"translate(20.282812 99.37587) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-36\" d=\"M 2869 4325 \n",
       "L 2869 4209 \n",
       "Q 2456 4169 2195 4045 \n",
       "Q 1934 3922 1679 3669 \n",
       "Q 1425 3416 1258 3105 \n",
       "Q 1091 2794 978 2366 \n",
       "Q 1428 2675 1881 2675 \n",
       "Q 2316 2675 2634 2325 \n",
       "Q 2953 1975 2953 1425 \n",
       "Q 2953 894 2631 456 \n",
       "Q 2244 -75 1606 -75 \n",
       "Q 1172 -75 869 213 \n",
       "Q 275 772 275 1663 \n",
       "Q 275 2231 503 2743 \n",
       "Q 731 3256 1154 3653 \n",
       "Q 1578 4050 1965 4187 \n",
       "Q 2353 4325 2688 4325 \n",
       "L 2869 4325 \n",
       "z\n",
       "M 925 2138 \n",
       "Q 869 1716 869 1456 \n",
       "Q 869 1156 980 804 \n",
       "Q 1091 453 1309 247 \n",
       "Q 1469 100 1697 100 \n",
       "Q 1969 100 2183 356 \n",
       "Q 2397 613 2397 1088 \n",
       "Q 2397 1622 2184 2012 \n",
       "Q 1972 2403 1581 2403 \n",
       "Q 1463 2403 1327 2353 \n",
       "Q 1191 2303 925 2138 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-36\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <path d=\"M 44.782812 84.816005 \n",
       "L 323.782812 84.816005 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_40\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"84.816005\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 0.65 -->\n",
       "      <g transform=\"translate(20.282812 88.28788) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-36\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <path d=\"M 44.782812 73.728003 \n",
       "L 323.782812 73.728003 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_42\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"73.728003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.70 -->\n",
       "      <g transform=\"translate(20.282812 77.199878) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-37\" d=\"M 644 4238 \n",
       "L 2916 4238 \n",
       "L 2916 4119 \n",
       "L 1503 -88 \n",
       "L 1153 -88 \n",
       "L 2419 3728 \n",
       "L 1253 3728 \n",
       "Q 900 3728 750 3644 \n",
       "Q 488 3500 328 3200 \n",
       "L 238 3234 \n",
       "L 644 4238 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-37\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_43\">\n",
       "      <path d=\"M 44.782812 62.64 \n",
       "L 323.782812 62.64 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_44\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"62.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(20.282812 66.111875) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-37\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_17\">\n",
       "     <g id=\"line2d_45\">\n",
       "      <path d=\"M 44.782812 51.551997 \n",
       "L 323.782812 51.551997 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_46\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"51.551997\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.80 -->\n",
       "      <g transform=\"translate(20.282812 55.023872) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-38\" d=\"M 1228 2134 \n",
       "Q 725 2547 579 2797 \n",
       "Q 434 3047 434 3316 \n",
       "Q 434 3728 753 4026 \n",
       "Q 1072 4325 1600 4325 \n",
       "Q 2113 4325 2425 4047 \n",
       "Q 2738 3769 2738 3413 \n",
       "Q 2738 3175 2569 2928 \n",
       "Q 2400 2681 1866 2347 \n",
       "Q 2416 1922 2594 1678 \n",
       "Q 2831 1359 2831 1006 \n",
       "Q 2831 559 2490 242 \n",
       "Q 2150 -75 1597 -75 \n",
       "Q 994 -75 656 303 \n",
       "Q 388 606 388 966 \n",
       "Q 388 1247 577 1523 \n",
       "Q 766 1800 1228 2134 \n",
       "z\n",
       "M 1719 2469 \n",
       "Q 2094 2806 2194 3001 \n",
       "Q 2294 3197 2294 3444 \n",
       "Q 2294 3772 2109 3958 \n",
       "Q 1925 4144 1606 4144 \n",
       "Q 1288 4144 1088 3959 \n",
       "Q 888 3775 888 3528 \n",
       "Q 888 3366 970 3203 \n",
       "Q 1053 3041 1206 2894 \n",
       "L 1719 2469 \n",
       "z\n",
       "M 1375 2016 \n",
       "Q 1116 1797 991 1539 \n",
       "Q 866 1281 866 981 \n",
       "Q 866 578 1086 336 \n",
       "Q 1306 94 1647 94 \n",
       "Q 1984 94 2187 284 \n",
       "Q 2391 475 2391 747 \n",
       "Q 2391 972 2272 1150 \n",
       "Q 2050 1481 1375 2016 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-38\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_18\">\n",
       "     <g id=\"line2d_47\">\n",
       "      <path d=\"M 44.782812 40.463995 \n",
       "L 323.782812 40.463995 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_48\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"40.463995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 0.85 -->\n",
       "      <g transform=\"translate(20.282812 43.93587) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-38\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_19\">\n",
       "     <g id=\"line2d_49\">\n",
       "      <path d=\"M 44.782812 29.376005 \n",
       "L 323.782812 29.376005 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_50\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"29.376005\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 0.90 -->\n",
       "      <g transform=\"translate(20.282812 32.84788) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"TimesNewRomanPSMT-39\" d=\"M 338 -88 \n",
       "L 338 28 \n",
       "Q 744 34 1094 217 \n",
       "Q 1444 400 1770 856 \n",
       "Q 2097 1313 2225 1859 \n",
       "Q 1734 1544 1338 1544 \n",
       "Q 891 1544 572 1889 \n",
       "Q 253 2234 253 2806 \n",
       "Q 253 3363 572 3797 \n",
       "Q 956 4325 1575 4325 \n",
       "Q 2097 4325 2469 3894 \n",
       "Q 2925 3359 2925 2575 \n",
       "Q 2925 1869 2578 1258 \n",
       "Q 2231 647 1613 244 \n",
       "Q 1109 -88 516 -88 \n",
       "L 338 -88 \n",
       "z\n",
       "M 2275 2091 \n",
       "Q 2331 2497 2331 2741 \n",
       "Q 2331 3044 2228 3395 \n",
       "Q 2125 3747 1936 3934 \n",
       "Q 1747 4122 1506 4122 \n",
       "Q 1228 4122 1018 3872 \n",
       "Q 809 3622 809 3128 \n",
       "Q 809 2469 1088 2097 \n",
       "Q 1291 1828 1588 1828 \n",
       "Q 1731 1828 1928 1897 \n",
       "Q 2125 1966 2275 2091 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-39\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_20\">\n",
       "     <g id=\"line2d_51\">\n",
       "      <path d=\"M 44.782812 18.288003 \n",
       "L 323.782812 18.288003 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_52\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcd123fd2b1\" x=\"44.782812\" y=\"18.288003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 0.95 -->\n",
       "      <g transform=\"translate(20.282812 21.759878) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-30\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-2e\" x=\"50\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-39\" x=\"75\"/>\n",
       "       <use xlink:href=\"#TimesNewRomanPSMT-35\" x=\"125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_28\">\n",
       "     <!-- Values -->\n",
       "     <g transform=\"translate(14.14375 131.408906) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"TimesNewRomanPSMT-56\" d=\"M 4544 4238 \n",
       "L 4544 4122 \n",
       "Q 4319 4081 4203 3978 \n",
       "Q 4038 3825 3909 3509 \n",
       "L 2431 -97 \n",
       "L 2316 -97 \n",
       "L 728 3556 \n",
       "Q 606 3838 556 3900 \n",
       "Q 478 3997 364 4051 \n",
       "Q 250 4106 56 4122 \n",
       "L 56 4238 \n",
       "L 1788 4238 \n",
       "L 1788 4122 \n",
       "Q 1494 4094 1406 4022 \n",
       "Q 1319 3950 1319 3838 \n",
       "Q 1319 3681 1463 3350 \n",
       "L 2541 866 \n",
       "L 3541 3319 \n",
       "Q 3688 3681 3688 3822 \n",
       "Q 3688 3913 3597 3995 \n",
       "Q 3506 4078 3291 4113 \n",
       "Q 3275 4116 3238 4122 \n",
       "L 3238 4238 \n",
       "L 4544 4238 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-61\" d=\"M 1822 413 \n",
       "Q 1381 72 1269 19 \n",
       "Q 1100 -59 909 -59 \n",
       "Q 613 -59 420 144 \n",
       "Q 228 347 228 678 \n",
       "Q 228 888 322 1041 \n",
       "Q 450 1253 767 1440 \n",
       "Q 1084 1628 1822 1897 \n",
       "L 1822 2009 \n",
       "Q 1822 2438 1686 2597 \n",
       "Q 1550 2756 1291 2756 \n",
       "Q 1094 2756 978 2650 \n",
       "Q 859 2544 859 2406 \n",
       "L 866 2225 \n",
       "Q 866 2081 792 2003 \n",
       "Q 719 1925 600 1925 \n",
       "Q 484 1925 411 2006 \n",
       "Q 338 2088 338 2228 \n",
       "Q 338 2497 613 2722 \n",
       "Q 888 2947 1384 2947 \n",
       "Q 1766 2947 2009 2819 \n",
       "Q 2194 2722 2281 2516 \n",
       "Q 2338 2381 2338 1966 \n",
       "L 2338 994 \n",
       "Q 2338 584 2353 492 \n",
       "Q 2369 400 2405 369 \n",
       "Q 2441 338 2488 338 \n",
       "Q 2538 338 2575 359 \n",
       "Q 2641 400 2828 588 \n",
       "L 2828 413 \n",
       "Q 2478 -56 2159 -56 \n",
       "Q 2006 -56 1915 50 \n",
       "Q 1825 156 1822 413 \n",
       "z\n",
       "M 1822 616 \n",
       "L 1822 1706 \n",
       "Q 1350 1519 1213 1441 \n",
       "Q 966 1303 859 1153 \n",
       "Q 753 1003 753 825 \n",
       "Q 753 600 887 451 \n",
       "Q 1022 303 1197 303 \n",
       "Q 1434 303 1822 616 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-6c\" d=\"M 1184 4444 \n",
       "L 1184 647 \n",
       "Q 1184 378 1223 290 \n",
       "Q 1263 203 1344 158 \n",
       "Q 1425 113 1647 113 \n",
       "L 1647 0 \n",
       "L 244 0 \n",
       "L 244 113 \n",
       "Q 441 113 512 153 \n",
       "Q 584 194 625 287 \n",
       "Q 666 381 666 647 \n",
       "L 666 3247 \n",
       "Q 666 3731 644 3842 \n",
       "Q 622 3953 573 3993 \n",
       "Q 525 4034 450 4034 \n",
       "Q 369 4034 244 3984 \n",
       "L 191 4094 \n",
       "L 1044 4444 \n",
       "L 1184 4444 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-75\" d=\"M 2709 2863 \n",
       "L 2709 1128 \n",
       "Q 2709 631 2732 520 \n",
       "Q 2756 409 2807 365 \n",
       "Q 2859 322 2928 322 \n",
       "Q 3025 322 3147 375 \n",
       "L 3191 266 \n",
       "L 2334 -88 \n",
       "L 2194 -88 \n",
       "L 2194 519 \n",
       "Q 1825 119 1631 15 \n",
       "Q 1438 -88 1222 -88 \n",
       "Q 981 -88 804 51 \n",
       "Q 628 191 559 409 \n",
       "Q 491 628 491 1028 \n",
       "L 491 2306 \n",
       "Q 491 2509 447 2587 \n",
       "Q 403 2666 317 2708 \n",
       "Q 231 2750 6 2747 \n",
       "L 6 2863 \n",
       "L 1009 2863 \n",
       "L 1009 947 \n",
       "Q 1009 547 1148 422 \n",
       "Q 1288 297 1484 297 \n",
       "Q 1619 297 1789 381 \n",
       "Q 1959 466 2194 703 \n",
       "L 2194 2325 \n",
       "Q 2194 2569 2105 2655 \n",
       "Q 2016 2741 1734 2747 \n",
       "L 1734 2863 \n",
       "L 2709 2863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-65\" d=\"M 681 1784 \n",
       "Q 678 1147 991 784 \n",
       "Q 1303 422 1725 422 \n",
       "Q 2006 422 2214 576 \n",
       "Q 2422 731 2563 1106 \n",
       "L 2659 1044 \n",
       "Q 2594 616 2278 264 \n",
       "Q 1963 -88 1488 -88 \n",
       "Q 972 -88 605 314 \n",
       "Q 238 716 238 1394 \n",
       "Q 238 2128 614 2539 \n",
       "Q 991 2950 1559 2950 \n",
       "Q 2041 2950 2350 2633 \n",
       "Q 2659 2316 2659 1784 \n",
       "L 681 1784 \n",
       "z\n",
       "M 681 1966 \n",
       "L 2006 1966 \n",
       "Q 1991 2241 1941 2353 \n",
       "Q 1863 2528 1708 2628 \n",
       "Q 1553 2728 1384 2728 \n",
       "Q 1125 2728 920 2526 \n",
       "Q 716 2325 681 1966 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-73\" d=\"M 2050 2947 \n",
       "L 2050 1972 \n",
       "L 1947 1972 \n",
       "Q 1828 2431 1642 2597 \n",
       "Q 1456 2763 1169 2763 \n",
       "Q 950 2763 815 2647 \n",
       "Q 681 2531 681 2391 \n",
       "Q 681 2216 781 2091 \n",
       "Q 878 1963 1175 1819 \n",
       "L 1631 1597 \n",
       "Q 2266 1288 2266 781 \n",
       "Q 2266 391 1970 151 \n",
       "Q 1675 -88 1309 -88 \n",
       "Q 1047 -88 709 6 \n",
       "Q 606 38 541 38 \n",
       "Q 469 38 428 -44 \n",
       "L 325 -44 \n",
       "L 325 978 \n",
       "L 428 978 \n",
       "Q 516 541 762 319 \n",
       "Q 1009 97 1316 97 \n",
       "Q 1531 97 1667 223 \n",
       "Q 1803 350 1803 528 \n",
       "Q 1803 744 1651 891 \n",
       "Q 1500 1038 1047 1263 \n",
       "Q 594 1488 453 1669 \n",
       "Q 313 1847 313 2119 \n",
       "Q 313 2472 555 2709 \n",
       "Q 797 2947 1181 2947 \n",
       "Q 1350 2947 1591 2875 \n",
       "Q 1750 2828 1803 2828 \n",
       "Q 1853 2828 1881 2850 \n",
       "Q 1909 2872 1947 2947 \n",
       "L 2050 2947 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-56\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"61.091797\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-6c\" x=\"105.476562\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-75\" x=\"133.259766\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-65\" x=\"183.259766\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-73\" x=\"227.644531\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_53\">\n",
       "    <path d=\"M 44.782812 27.317327 \n",
       "L 54.403502 23.913314 \n",
       "L 64.024192 23.163025 \n",
       "L 73.644881 21.640267 \n",
       "L 83.265571 20.716268 \n",
       "L 92.886261 20.383626 \n",
       "L 102.50695 19.07895 \n",
       "L 112.12764 18.720427 \n",
       "L 121.74833 18.398875 \n",
       "L 131.369019 17.707722 \n",
       "L 140.989709 18.288003 \n",
       "L 150.610399 17.219862 \n",
       "L 160.231088 16.84286 \n",
       "L 169.851778 16.813292 \n",
       "L 179.472468 16.550876 \n",
       "L 189.093157 16.354987 \n",
       "L 198.713847 16.84286 \n",
       "L 208.334537 16.32173 \n",
       "L 217.955226 16.107362 \n",
       "L 227.575916 16.151708 \n",
       "L 237.196606 15.778421 \n",
       "L 246.817295 16.007567 \n",
       "L 256.437985 15.815378 \n",
       "L 266.058675 15.545574 \n",
       "L 275.679364 15.353372 \n",
       "L 285.300054 15.368163 \n",
       "L 294.920744 15.04661 \n",
       "L 304.541433 15.334893 \n",
       "L 314.162123 15.212931 \n",
       "L 323.782812 14.965294 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_54\">\n",
       "    <path d=\"M 44.782812 27.114048 \n",
       "L 54.403502 23.499355 \n",
       "L 64.024192 23.299778 \n",
       "L 73.644881 21.547874 \n",
       "L 83.265571 20.727358 \n",
       "L 92.886261 21.281757 \n",
       "L 102.50695 19.951201 \n",
       "L 112.12764 19.662918 \n",
       "L 121.74833 19.24157 \n",
       "L 131.369019 18.642811 \n",
       "L 140.989709 19.552033 \n",
       "L 150.610399 18.376708 \n",
       "L 160.231088 18.044066 \n",
       "L 169.851778 18.376708 \n",
       "L 179.472468 18.110592 \n",
       "L 189.093157 17.511846 \n",
       "L 198.713847 17.999706 \n",
       "L 208.334537 17.866655 \n",
       "L 217.955226 17.711423 \n",
       "L 227.575916 17.866655 \n",
       "L 237.196606 17.090498 \n",
       "L 246.817295 17.711423 \n",
       "L 256.437985 17.777949 \n",
       "L 266.058675 17.445307 \n",
       "L 275.679364 16.735676 \n",
       "L 285.300054 17.334435 \n",
       "L 294.920744 16.447393 \n",
       "L 304.541433 16.868741 \n",
       "L 314.162123 17.068319 \n",
       "L 323.782812 16.71351 \n",
       "\" clip-path=\"url(#pef7e57143d)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 44.782812 228.96 \n",
       "L 44.782812 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 323.782812 228.96 \n",
       "L 323.782812 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 44.782812 228.96 \n",
       "L 323.782812 228.96 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 44.782812 7.2 \n",
       "L 323.782812 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 51.782812 223.96 \n",
       "L 120.425 223.96 \n",
       "Q 122.425 223.96 122.425 221.96 \n",
       "L 122.425 194.644375 \n",
       "Q 122.425 192.644375 120.425 192.644375 \n",
       "L 51.782812 192.644375 \n",
       "Q 49.782812 192.644375 49.782812 194.644375 \n",
       "L 49.782812 221.96 \n",
       "Q 49.782812 223.96 51.782812 223.96 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_55\">\n",
       "     <path d=\"M 53.782812 200.144375 \n",
       "L 63.782812 200.144375 \n",
       "L 73.782813 200.144375 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_29\">\n",
       "     <!-- train_acc -->\n",
       "     <g transform=\"translate(81.782813 203.644375) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"TimesNewRomanPSMT-74\" d=\"M 1031 3803 \n",
       "L 1031 2863 \n",
       "L 1700 2863 \n",
       "L 1700 2644 \n",
       "L 1031 2644 \n",
       "L 1031 788 \n",
       "Q 1031 509 1111 412 \n",
       "Q 1191 316 1316 316 \n",
       "Q 1419 316 1516 380 \n",
       "Q 1613 444 1666 569 \n",
       "L 1788 569 \n",
       "Q 1678 263 1478 108 \n",
       "Q 1278 -47 1066 -47 \n",
       "Q 922 -47 784 33 \n",
       "Q 647 113 581 261 \n",
       "Q 516 409 516 719 \n",
       "L 516 2644 \n",
       "L 63 2644 \n",
       "L 63 2747 \n",
       "Q 234 2816 414 2980 \n",
       "Q 594 3144 734 3369 \n",
       "Q 806 3488 934 3803 \n",
       "L 1031 3803 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-72\" d=\"M 1038 2947 \n",
       "L 1038 2303 \n",
       "Q 1397 2947 1775 2947 \n",
       "Q 1947 2947 2059 2842 \n",
       "Q 2172 2738 2172 2600 \n",
       "Q 2172 2478 2090 2393 \n",
       "Q 2009 2309 1897 2309 \n",
       "Q 1788 2309 1652 2417 \n",
       "Q 1516 2525 1450 2525 \n",
       "Q 1394 2525 1328 2463 \n",
       "Q 1188 2334 1038 2041 \n",
       "L 1038 669 \n",
       "Q 1038 431 1097 309 \n",
       "Q 1138 225 1241 169 \n",
       "Q 1344 113 1538 113 \n",
       "L 1538 0 \n",
       "L 72 0 \n",
       "L 72 113 \n",
       "Q 291 113 397 181 \n",
       "Q 475 231 506 341 \n",
       "Q 522 394 522 644 \n",
       "L 522 1753 \n",
       "Q 522 2253 501 2348 \n",
       "Q 481 2444 426 2487 \n",
       "Q 372 2531 291 2531 \n",
       "Q 194 2531 72 2484 \n",
       "L 41 2597 \n",
       "L 906 2947 \n",
       "L 1038 2947 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-69\" d=\"M 928 4444 \n",
       "Q 1059 4444 1151 4351 \n",
       "Q 1244 4259 1244 4128 \n",
       "Q 1244 3997 1151 3903 \n",
       "Q 1059 3809 928 3809 \n",
       "Q 797 3809 703 3903 \n",
       "Q 609 3997 609 4128 \n",
       "Q 609 4259 701 4351 \n",
       "Q 794 4444 928 4444 \n",
       "z\n",
       "M 1188 2947 \n",
       "L 1188 647 \n",
       "Q 1188 378 1227 289 \n",
       "Q 1266 200 1342 156 \n",
       "Q 1419 113 1622 113 \n",
       "L 1622 0 \n",
       "L 231 0 \n",
       "L 231 113 \n",
       "Q 441 113 512 153 \n",
       "Q 584 194 626 287 \n",
       "Q 669 381 669 647 \n",
       "L 669 1750 \n",
       "Q 669 2216 641 2353 \n",
       "Q 619 2453 572 2492 \n",
       "Q 525 2531 444 2531 \n",
       "Q 356 2531 231 2484 \n",
       "L 188 2597 \n",
       "L 1050 2947 \n",
       "L 1188 2947 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-6e\" d=\"M 1034 2341 \n",
       "Q 1538 2947 1994 2947 \n",
       "Q 2228 2947 2397 2830 \n",
       "Q 2566 2713 2666 2444 \n",
       "Q 2734 2256 2734 1869 \n",
       "L 2734 647 \n",
       "Q 2734 375 2778 278 \n",
       "Q 2813 200 2889 156 \n",
       "Q 2966 113 3172 113 \n",
       "L 3172 0 \n",
       "L 1756 0 \n",
       "L 1756 113 \n",
       "L 1816 113 \n",
       "Q 2016 113 2095 173 \n",
       "Q 2175 234 2206 353 \n",
       "Q 2219 400 2219 647 \n",
       "L 2219 1819 \n",
       "Q 2219 2209 2117 2386 \n",
       "Q 2016 2563 1775 2563 \n",
       "Q 1403 2563 1034 2156 \n",
       "L 1034 647 \n",
       "Q 1034 356 1069 288 \n",
       "Q 1113 197 1189 155 \n",
       "Q 1266 113 1500 113 \n",
       "L 1500 0 \n",
       "L 84 0 \n",
       "L 84 113 \n",
       "L 147 113 \n",
       "Q 366 113 442 223 \n",
       "Q 519 334 519 647 \n",
       "L 519 1709 \n",
       "Q 519 2225 495 2337 \n",
       "Q 472 2450 423 2490 \n",
       "Q 375 2531 294 2531 \n",
       "Q 206 2531 84 2484 \n",
       "L 38 2597 \n",
       "L 900 2947 \n",
       "L 1034 2947 \n",
       "L 1034 2341 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"TimesNewRomanPSMT-5f\" d=\"M 3256 -1381 \n",
       "L -53 -1381 \n",
       "L -53 -1119 \n",
       "L 3256 -1119 \n",
       "L 3256 -1381 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-74\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-72\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"61.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-69\" x=\"105.46875\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-6e\" x=\"133.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-5f\" x=\"183.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"233.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"277.636719\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"322.021484\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_56\">\n",
       "     <path d=\"M 53.782812 214.302187 \n",
       "L 63.782812 214.302187 \n",
       "L 73.782813 214.302187 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_30\">\n",
       "     <!-- test_acc -->\n",
       "     <g transform=\"translate(81.782813 217.802187) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-74\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-65\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-73\" x=\"72.167969\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-74\" x=\"111.083984\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-5f\" x=\"138.867188\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-61\" x=\"188.867188\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"233.251953\"/>\n",
       "      <use xlink:href=\"#TimesNewRomanPSMT-63\" x=\"277.636719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pef7e57143d\">\n",
       "   <rect x=\"44.782812\" y=\"7.2\" width=\"279\" height=\"221.76\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "# 开始训练\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.network = nn.Sequential(nn.Flatten(),\n",
    "        #                              nn.Linear(28*28, 2**5), nn.ReLU(),\n",
    "        #                              nn.Linear(2**5, 10), nn.Softmax())\n",
    "        self.num_hidden = 2**5\n",
    "        self.layer1 = nn.Flatten()\n",
    "        self.layer2 = nn.Linear(28*28, self.num_hidden)\n",
    "        self.layer3 = nn.Linear(self.num_hidden, self.num_hidden)\n",
    "        self.ac = nn.ReLU()\n",
    "        # self.ac = nn.Tanh()\n",
    "        self.dp = nn.Dropout()\n",
    "        self.bn = nn.BatchNorm1d(self.num_hidden)\n",
    "        self.layer4 = nn.Linear(self.num_hidden, 10)\n",
    "        self.layer5 = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.layer1(X)\n",
    "        y = self.layer2(y)\n",
    "        \n",
    "        for i in range(2):\n",
    "            y = y + self.dp(self.ac(self.bn(self.layer3(y))))\n",
    "                   \n",
    "        y = self.layer4(y)\n",
    "        y = self.layer5(y)\n",
    "        return y\n",
    "\n",
    "net = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)\n",
    "\n",
    "train_steps(\n",
    "    epochs=30, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device, \n",
    "    train_figure=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8.3. <a id='toc8_8_3_'></a>[K折交叉验证](#toc0_)\n",
    "- 简述：把数据分成K份，分别只取1份做Test_data，（K-1）做Train_data，做K次，计算Test_acc的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1 # k必须大于1\n",
    "    fold_size = X.shape[0] // k # 窗口大小：X一维数据长度除以k向下取整数\n",
    "    print('fold_size: ', fold_size)\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size) # 切片范围 (窗口大小)\n",
    "        print(idx)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11],\n",
       "         [12, 13, 14]]),\n",
       " tensor([[  0,  -1,  -2],\n",
       "         [ -3,  -4,  -5],\n",
       "         [ -6,  -7,  -8],\n",
       "         [ -9, -10, -11],\n",
       "         [-12, -13, -14]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(15).reshape(5, 3)\n",
    "y = torch.negative(torch.arange(15).reshape(5, 3))\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_size:  2\n",
      "slice(0, 2, None)\n",
      "slice(2, 4, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 0, -1, -2],\n",
       "         [-3, -4, -5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]),\n",
       " tensor([[ -6,  -7,  -8],\n",
       "         [ -9, -10, -11]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_fold_data(2, 1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.9. <a id='toc8_9_'></a>[可视化训练过程](#toc0_)\n",
    "* 清理上一次\n",
    "    * figure plt.clf() # 只是清理figure内容\n",
    "    * figure `plt.colse()` # 关闭（释放）figure\n",
    "    * axes plt.cla() # 只是清理axes内容\n",
    "* 绘图plot\n",
    "* 用jupyter的display来显示\n",
    "* 保持yupyter上的display直至下一次展示再清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印图片耗时： 2.1511104106903076 seconds\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"256.140024pt\" height=\"183.35625pt\" viewBox=\"0 0 256.140024 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-19T19:07:56.544742</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 256.140024 183.35625 \n",
       "L 256.140024 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 247.460938 145.8 \n",
       "L 247.460938 7.2 \n",
       "L 52.160938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m4bcc2df402\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4bcc2df402\" x=\"61.03821\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(57.85696 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4bcc2df402\" x=\"97.346073\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(90.983573 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4bcc2df402\" x=\"133.653936\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(127.291436 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4bcc2df402\" x=\"169.961799\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(163.599299 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4bcc2df402\" x=\"206.269661\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(199.907161 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4bcc2df402\" x=\"242.577524\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(236.215024 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(134.582813 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m2c5f01160e\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c5f01160e\" x=\"52.160938\" y=\"139.500616\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(20.878125 143.299835) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c5f01160e\" x=\"52.160938\" y=\"108.000363\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 111.799582) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c5f01160e\" x=\"52.160938\" y=\"76.500111\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 80.29933) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c5f01160e\" x=\"52.160938\" y=\"44.999858\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 48.799077) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c5f01160e\" x=\"52.160938\" y=\"13.499606\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(29.257812 17.298824) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 61.03821 76.500111 \n",
       "L 62.853603 46.296059 \n",
       "L 63.942839 31.306315 \n",
       "L 64.668997 23.487016 \n",
       "L 65.395154 17.781178 \n",
       "L 66.121311 14.41628 \n",
       "L 66.48439 13.657422 \n",
       "L 66.847468 13.52647 \n",
       "L 67.210547 14.024726 \n",
       "L 67.573626 15.147218 \n",
       "L 67.936704 16.882727 \n",
       "L 68.662861 22.117481 \n",
       "L 69.389018 29.520304 \n",
       "L 70.478255 44.023271 \n",
       "L 71.930569 67.609479 \n",
       "L 74.47212 109.880058 \n",
       "L 75.561355 124.17905 \n",
       "L 76.287512 131.409817 \n",
       "L 77.01367 136.451524 \n",
       "L 77.376748 138.085002 \n",
       "L 77.739827 139.103144 \n",
       "L 78.102905 139.495779 \n",
       "L 78.465985 139.258981 \n",
       "L 78.829063 138.395119 \n",
       "L 79.192142 136.912826 \n",
       "L 79.9183 132.158191 \n",
       "L 80.644456 125.184661 \n",
       "L 81.733691 111.193588 \n",
       "L 83.186006 87.976446 \n",
       "L 85.727558 45.37071 \n",
       "L 86.816792 30.574697 \n",
       "L 87.542949 22.922185 \n",
       "L 88.269107 17.405639 \n",
       "L 88.632186 15.520684 \n",
       "L 88.995264 14.245016 \n",
       "L 89.358342 13.591377 \n",
       "L 89.72142 13.5663 \n",
       "L 90.0845 14.170042 \n",
       "L 90.44758 15.39657 \n",
       "L 90.810657 17.233606 \n",
       "L 91.536814 22.659935 \n",
       "L 92.262974 30.23274 \n",
       "L 93.35221 44.935574 \n",
       "L 94.804523 68.659431 \n",
       "L 97.346073 110.773717 \n",
       "L 98.43531 124.864713 \n",
       "L 99.161466 131.921389 \n",
       "L 99.887623 136.768595 \n",
       "L 100.250699 138.299581 \n",
       "L 100.613779 139.213094 \n",
       "L 100.976859 139.5 \n",
       "L 101.339936 139.157431 \n",
       "L 101.703016 138.188805 \n",
       "L 102.066096 136.603802 \n",
       "L 102.792252 131.654039 \n",
       "L 103.518409 124.505468 \n",
       "L 104.607646 110.304477 \n",
       "L 106.059959 86.933281 \n",
       "L 108.601512 44.45416 \n",
       "L 109.690748 29.85603 \n",
       "L 110.416905 22.372469 \n",
       "L 111.143062 17.046804 \n",
       "L 111.506142 15.263139 \n",
       "L 111.869218 14.091345 \n",
       "L 112.232298 13.543113 \n",
       "L 112.595378 13.62393 \n",
       "L 112.958455 14.33298 \n",
       "L 113.321531 15.663176 \n",
       "L 114.047688 20.127799 \n",
       "L 114.773844 26.839803 \n",
       "L 115.500004 35.531647 \n",
       "L 116.589241 51.505265 \n",
       "L 118.767711 88.524274 \n",
       "L 120.220024 111.657646 \n",
       "L 121.309264 125.5367 \n",
       "L 122.035417 132.41726 \n",
       "L 122.761577 137.068637 \n",
       "L 123.12465 138.496688 \n",
       "L 123.48773 139.305312 \n",
       "L 123.85081 139.486406 \n",
       "L 124.21389 139.038157 \n",
       "L 124.57697 137.965041 \n",
       "L 124.94005 136.277786 \n",
       "L 125.66621 131.134251 \n",
       "L 126.392363 123.812686 \n",
       "L 127.481603 109.405756 \n",
       "L 128.933909 85.887196 \n",
       "L 131.47547 43.546622 \n",
       "L 132.564703 29.150574 \n",
       "L 133.290863 21.838047 \n",
       "L 134.017016 16.704776 \n",
       "L 134.380096 15.022916 \n",
       "L 134.743169 13.955331 \n",
       "L 135.106249 13.512655 \n",
       "L 135.469329 13.699329 \n",
       "L 135.832409 14.513496 \n",
       "L 136.195489 15.947018 \n",
       "L 136.921642 20.608729 \n",
       "L 137.647795 27.498624 \n",
       "L 138.373955 36.34213 \n",
       "L 139.463195 52.481159 \n",
       "L 143.820135 122.080613 \n",
       "L 144.546295 129.812422 \n",
       "L 145.272455 135.418824 \n",
       "L 145.635535 137.35157 \n",
       "L 145.998615 138.676306 \n",
       "L 146.361688 139.379784 \n",
       "L 146.724768 139.455002 \n",
       "L 147.087848 138.901193 \n",
       "L 147.450928 137.723887 \n",
       "L 147.814008 135.934849 \n",
       "L 148.540161 130.599075 \n",
       "L 149.266314 123.106569 \n",
       "L 150.355554 108.497831 \n",
       "L 151.807867 84.838338 \n",
       "L 154.34942 42.648502 \n",
       "L 155.438653 28.458543 \n",
       "L 156.164813 21.319138 \n",
       "L 156.890967 16.379676 \n",
       "L 157.254047 14.800084 \n",
       "L 157.617127 13.836985 \n",
       "L 157.9802 13.5 \n",
       "L 158.34328 13.792483 \n",
       "L 158.70636 14.711527 \n",
       "L 159.06944 16.247942 \n",
       "L 159.7956 21.105487 \n",
       "L 160.52176 28.171456 \n",
       "L 161.610993 42.273508 \n",
       "L 163.063306 65.572561 \n",
       "L 165.604852 108.113047 \n",
       "L 166.694085 122.805393 \n",
       "L 167.420245 130.369284 \n",
       "L 168.146405 135.785564 \n",
       "L 168.509485 137.617268 \n",
       "L 168.872565 138.838302 \n",
       "L 169.235645 139.436471 \n",
       "L 169.598719 139.405803 \n",
       "L 169.961799 138.746602 \n",
       "L 170.324879 137.465452 \n",
       "L 170.687959 135.575146 \n",
       "L 171.414112 130.048604 \n",
       "L 172.140272 122.387189 \n",
       "L 173.229505 107.580864 \n",
       "L 174.681825 83.787122 \n",
       "L 176.860298 47.180317 \n",
       "L 177.949531 32.011043 \n",
       "L 178.675691 24.035789 \n",
       "L 179.401837 18.152229 \n",
       "L 180.127997 14.594697 \n",
       "L 180.49107 13.736377 \n",
       "L 180.854157 13.50516 \n",
       "L 181.21723 13.903364 \n",
       "L 181.580317 14.927048 \n",
       "L 181.943391 16.565904 \n",
       "L 182.669551 21.617851 \n",
       "L 183.395697 28.857637 \n",
       "L 184.484944 43.167632 \n",
       "L 185.937264 66.617405 \n",
       "L 188.478817 109.025016 \n",
       "L 189.56805 123.517244 \n",
       "L 190.29421 130.911035 \n",
       "L 191.020356 136.135545 \n",
       "L 191.383443 137.865714 \n",
       "L 191.746516 138.982679 \n",
       "L 192.109589 139.475359 \n",
       "L 192.472676 139.338811 \n",
       "L 192.835749 138.574415 \n",
       "L 193.198836 137.189747 \n",
       "L 193.924982 132.621265 \n",
       "L 194.651142 125.81533 \n",
       "L 195.377303 117.043337 \n",
       "L 196.466536 100.982161 \n",
       "L 200.823482 31.267348 \n",
       "L 201.549642 23.456775 \n",
       "L 202.275802 17.760889 \n",
       "L 203.001962 14.406768 \n",
       "L 203.365035 13.653487 \n",
       "L 203.728108 13.528126 \n",
       "L 204.091195 14.031973 \n",
       "L 204.454268 15.159956 \n",
       "L 204.817355 16.900887 \n",
       "L 205.543501 22.145731 \n",
       "L 206.269661 29.557607 \n",
       "L 207.358894 44.071183 \n",
       "L 208.811215 67.664925 \n",
       "L 211.352754 109.927376 \n",
       "L 212.441987 124.215479 \n",
       "L 213.168147 131.437166 \n",
       "L 213.894307 136.468666 \n",
       "L 214.25738 138.096733 \n",
       "L 214.620467 139.109392 \n",
       "L 214.983554 139.496451 \n",
       "L 215.346627 139.254062 \n",
       "L 215.709714 138.384635 \n",
       "L 216.072787 136.896915 \n",
       "L 216.798947 132.131916 \n",
       "L 217.525107 125.14904 \n",
       "L 218.61434 111.146777 \n",
       "L 220.066646 87.921465 \n",
       "L 222.6082 45.322085 \n",
       "L 223.697433 30.53644 \n",
       "L 224.423593 22.89276 \n",
       "L 225.149753 17.386248 \n",
       "L 225.512826 15.506659 \n",
       "L 225.875899 14.236477 \n",
       "L 226.238986 13.588381 \n",
       "L 226.602059 13.568895 \n",
       "L 226.965132 14.178179 \n",
       "L 227.328219 15.410201 \n",
       "L 227.691292 17.252562 \n",
       "L 228.417452 22.688977 \n",
       "L 229.143612 30.270686 \n",
       "L 230.232859 44.984106 \n",
       "L 231.685165 68.714942 \n",
       "L 234.226718 110.820693 \n",
       "L 235.315952 124.900541 \n",
       "L 236.042112 131.947998 \n",
       "L 236.768272 136.784904 \n",
       "L 237.131345 138.310437 \n",
       "L 237.494418 139.218403 \n",
       "L 237.857505 139.499726 \n",
       "L 238.220578 139.151573 \n",
       "L 238.583665 138.177401 \n",
       "L 238.583665 138.177401 \n",
       "\" clip-path=\"url(#p2e814b87ad)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 52.160938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 247.460938 145.8 \n",
       "L 247.460938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 247.460938 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 7.2 \n",
       "L 247.460938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2e814b87ad\">\n",
       "   <rect x=\"52.160938\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def dl_plot(x, y):\n",
    "    '''再jupyter中持续刷新展示图片'''\n",
    "    plt.close()                                 # close figure （推荐）\n",
    "    fig = plt.figure(figsize=(3.5, 2.5))\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    # plt.show()                                # 普通展示\n",
    "    display.display(fig)                        # 在jupyter中展示 （推荐）\n",
    "    display.clear_output(wait=True)             # 等待 （必须）\n",
    "\n",
    "    \n",
    "start = time.time()\n",
    "for epoch in range(50):\n",
    "    x = torch.arange(0, epoch+1, 0.1)\n",
    "    y = torch.sin(x)\n",
    "    if epoch % 2 == 0:\n",
    "        dl_plot(x, y)\n",
    "stop = time.time()\n",
    "print(f\"打印图片耗时： {stop - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10: \t train_loss=0.8999999761581421 \t train_acc=0.7833268642425537\n",
      "2/10: \t train_loss=1.899999976158142 \t train_acc=0.9463000893592834\n",
      "3/10: \t train_loss=2.9000000953674316 \t train_acc=0.23924924433231354\n",
      "4/10: \t train_loss=3.9000000953674316 \t train_acc=-0.6877662539482117\n",
      "5/10: \t train_loss=4.900000095367432 \t train_acc=-0.9824525713920593\n",
      "6/10: \t train_loss=5.900000095367432 \t train_acc=-0.37387657165527344\n",
      "7/10: \t train_loss=6.900000095367432 \t train_acc=0.5784398317337036\n",
      "8/10: \t train_loss=7.899999618530273 \t train_acc=0.9989413619041443\n",
      "9/10: \t train_loss=8.899999618530273 \t train_acc=0.5010212063789368\n",
      "10/10: \t train_loss=9.899999618530273 \t train_acc=-0.4575355648994446\n",
      "打印数值耗时： 0.0017552375793457031 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    x = torch.arange(0, epoch+1, 0.1)\n",
    "    y = torch.sin(x)\n",
    "    # dl_plot(x, y)\n",
    "    print(f\"{epoch+1}/{10}: \\t train_loss={x[-1]} \\t train_acc={y[-1]}\")\n",
    "    \n",
    "stop = time.time()\n",
    "print(f\"打印数值耗时： {stop - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. <a id='toc9_'></a>[在 GPU 上训练](#toc0_)\n",
    "- 要实行运算的Tensor`必须`在同一张GPU卡上：\n",
    "\n",
    "|操作|函数|\n",
    "|:-|:-|\n",
    "|1. 张量传到GPU上 |x_gpu = x`.to`('cuda:0')|\n",
    "|2. 神经网络传到GPU上|net = net`.to`('cuda:0')|\n",
    "\n",
    "- CPU和GPU之间数据传输总结：\n",
    "\n",
    "|对象|方法一|方法二|\n",
    "|:-|:-|:-|\n",
    "|模型上GPU：|model.cuda()|model.`to(device)`|\n",
    "|数据上GPU：|data.cuda()|data.`to(device)`|\n",
    "|输出下GPU：|output=model(data)|output`.detach().cpu().numpy()`|\n",
    "||解释：||\n",
    "||output`.detach()`|将变量output从计算图中分离，使其不具有梯度，不进行反向传播|\n",
    "||`.cpu()`|将GPU数据转CPU|\n",
    "||.numpy()|将Tensor转numpy|\n",
    "||`.item()`|将只有`一个元素`的Tensor转为python数值|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. <a id='toc9_1_'></a>[查看GPU配置](#toc0_)\n",
    "都在`torch.cuda`模块中."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 是否有可用的GPU\n",
    "torch.cuda.is_available()           \n",
    "# True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可用的GPU数量\n",
    "torch.cuda.device_count()     \n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-SXM4-40GB'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回gpu名字，设备索引默认从0开始；\n",
    "torch.cuda.get_device_name(0)\n",
    "# \"Tesla T4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回当前设备索引；\n",
    "torch.cuda.current_device()\n",
    "# 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单机2卡: ['NVIDIA A100-SXM4-40GB', 'NVIDIA A100-SXM4-40GB']\n"
     ]
    }
   ],
   "source": [
    "def check_device():\n",
    "    '''判断是否有GPU，并列出GPU的代号/名称'''\n",
    "    if torch.cuda.is_available(): # 判断是否支持cuda/GPU\n",
    "        gpu_num = torch.cuda.device_count() # cuda/GPU计数\n",
    "        if gpu_num == 1:\n",
    "            print(f\"单机单卡: {[torch.cuda.get_device_name(gpu_name) for gpu_name in range(gpu_num)]}\")\n",
    "        else:\n",
    "            print(f\"单机{gpu_num}卡: {[torch.cuda.get_device_name(gpu_name) for gpu_name in range(gpu_num)]}\")\n",
    "    else:\n",
    "        print(f\"只有CPU\")\n",
    "    return None \n",
    "\n",
    "check_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda:0', 'cuda:1']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = [ 'cpu' if not torch.cuda.is_available() else ]\n",
    "device = [f'cuda:{i}' for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else ['cpu']\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. <a id='toc9_2_'></a>[单机单卡（GPU）](#toc0_)\n",
    "所有的张量必须存在于同一个设备上（同一个CPU或同一个GPU），才能正确计算，否则可能会出现异常错误。  \n",
    "1. 模型上GPU：model.cuda() 或 model.to(device)   \n",
    "2. 数据上GPU：data_gpu = data.cuda() 或 data_gpu = data.to(device)   \n",
    "3. 输出下GPU：output = model(data)  output.detach().cpu().numpy()，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.]]),\n",
       " tensor([[1.],\n",
       "         [1.]]),\n",
       " device(type='cpu'),\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((2, 1))\n",
    "y = torch.ones((2, 1))\n",
    "\n",
    "x, y, x.device, y.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.]], device='cuda:0'),\n",
       " tensor([[1.],\n",
       "         [1.]], device='cuda:0'),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x.to(device)\n",
    "y1 = y.to(device)\n",
    "\n",
    "x1, y1, x1.device, y1.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. <a id='toc9_3_'></a>[单机多卡（GPU）](#toc0_)\n",
    "\n",
    "目前PyTorch的单机多卡训练，主要有两种方式：\n",
    "\n",
    "|方法|函数|注释|\n",
    "|:-|:-|:-|\n",
    "|第一种：|torch.nn.`DataParallel`(module=net, device_ids=[0, 1], output_device=[0])|# 单机两卡|\n",
    "|第二种：|torch.nn.parallel.`DistributedDataParallel`()|# 单机多卡、多机多卡|\n",
    "\n",
    "\n",
    "DataParallel (DP) 和 DistributedDataParallel (DDP) 都是用于在多GPU上进行训练的工具，但它们有一些关键的区别：\n",
    "\n",
    "1. **目标环境：**\n",
    "   - `DataParallel` 适用于单机多卡的情况，通过将模型复制到每个GPU上，每个GPU计算不同的批次，最后通过梯度累积或平均来更新模型参数。\n",
    "   - `DistributedDataParallel` 适用于分布式环境，可以在单机或多台机器上的多个GPU上运行，每个GPU计算不同的批次，并通过分布式通信来同步梯度和更新模型参数。\n",
    "\n",
    "2. **通信方式：**\n",
    "   - `DataParallel` 使用单个进程内的多个GPU，通信相对较简单，仅涉及到进程内的数据传输。\n",
    "   - `DistributedDataParallel` 通过分布式通信协议，如NCCL或Gloo，实现跨进程和可能跨机器的通信，因此需要更复杂的设置。\n",
    "\n",
    "3. **启动方式：**\n",
    "   - `DataParallel` 只需在模型实例上调用 `nn.DataParallel(model)` 即可。\n",
    "   - `DistributedDataParallel` 需要在训练脚本中设置分布式环境变量，如`torch.distributed.launch` 或手动设置`os.environ`。\n",
    "\n",
    "4. **维护性：**\n",
    "   - `DataParallel` 更容易使用，因为它不涉及复杂的分布式设置。\n",
    "   - `DistributedDataParallel` 适用于更复杂的分布式场景，但需要更多的设置和管理。\n",
    "\n",
    "在单机多卡的情况下，如果简单性和易用性是首要考虑的因素，可以使用 DataParallel。在需要更高级的分布式设置时，或者在多机多卡的环境中，DistributedDataParallel 提供了更大的灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1. <a id='toc9_3_1_'></a>[DP](#toc0_)\n",
    "- 单机多线程\n",
    "\n",
    "- 参数详情\n",
    "```python\n",
    "torch.nn.DataParallel(module, device_ids, output_device)  \n",
    "\n",
    "Parameters\n",
    "    module (Module) – module to be parallelized                                                 # 神经网络\n",
    "    device_ids (list of int or torch.device) – CUDA devices (default: all devices)              # 默认使用所用GPU\n",
    "    output_device (int or torch.device) – device location of output (default: device_ids[0])    # 在cuda:0上进行参数分配、计算、汇总、更新\n",
    "Variables\n",
    "    module (Module) – the module to be parallelized\n",
    "```\n",
    "\n",
    "- 前提\n",
    " \n",
    "    1. 有一个前提: net模型被复制到cuda:[0, 1, 2等等]上，但是X, y必须提前在cuda:0上，而不能在cuda:1、cuda:2等等上；\n",
    "\n",
    "    2. 那如果cuda:0有其他人占满了，怎么办？那就需要手动指定其他GPU为cuda:0了：\n",
    "\n",
    "        - os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"      # 一定一定要放在所有访问显卡的代码之前，否则则无效，给我困扰了好一段时间才发现了。我之前看到有一个说法是放到import os之后并且在import torch之前。\n",
    "\n",
    "        - os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2, 3\"         # 只识别2、3而抛弃了其他GPU，把2当成pytorch逻辑上的cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=1.5717055797576904, train_acc=90.85833740234375, test_acc=90.91999816894531\n",
      "epoch 2/10: train_loss=1.5450035333633423, train_acc=92.63500213623047, test_acc=92.77999877929688\n",
      "epoch 3/10: train_loss=1.5318394899368286, train_acc=93.75666809082031, test_acc=93.83999633789062\n",
      "epoch 4/10: train_loss=1.5233505964279175, train_acc=94.5, test_acc=94.18000030517578\n",
      "epoch 5/10: train_loss=1.5167455673217773, train_acc=95.13500213623047, test_acc=94.83999633789062\n",
      "epoch 6/10: train_loss=1.5123507976531982, train_acc=95.55833435058594, test_acc=95.25\n",
      "epoch 7/10: train_loss=1.5058631896972656, train_acc=96.14500427246094, test_acc=95.44000244140625\n",
      "epoch 8/10: train_loss=1.502568006515503, train_acc=96.41999816894531, test_acc=95.80000305175781\n",
      "epoch 9/10: train_loss=1.4993404150009155, train_acc=96.69999694824219, test_acc=96.18000030517578\n",
      "epoch 10/10: train_loss=1.4963492155075073, train_acc=97.02667236328125, test_acc=96.30000305175781\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 5.24815821647644 s\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import time \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据准备\n",
    "dbs = './Pytorch_datasets/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(dataset=train_dataset, batch_size=128,  shuffle=True)\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练\n",
    "\n",
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "    print('='*100, '\\n', f\"Runing on {device}\", '\\n','='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "    net = nn.DataParallel(module=net)\n",
    "    # net = nn.DataParallel(module=net, device_ids=[0, 1], output_device=[0]) # 多GPU并行计算，等价于net = nn.DataParallel(module=net)\n",
    "    net.to(device)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)# 计算loss\n",
    "            loss.backward()         # 计算梯度\n",
    "            opt.step()              # 更新网络参数\n",
    "\n",
    "        net.eval()  # 切换至评估模式\n",
    "                    # 模型默认是net.train()\n",
    "                    # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                    # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None\n",
    "\n",
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "net = Net()  \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. <a id='toc9_3_2_'></a>[DDP](#toc0_)\n",
    "```shell\n",
    "1. 与 DataParallel 的单进程控制多 GPU 不同，在 distributed 的帮助下，我们只需要编写一份代码，torch 就会自动将其分配给 \n",
    " 个进程，分别在 n 个 GPU 上运行。\n",
    "2. 单机多进程\n",
    "```\n",
    "```python\n",
    "详解\n",
    "torch.nn.parallel.DistributedDataParallel(module, device_ids, output_device)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import os \n",
    "\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    '''\n",
    "    Args:\n",
    "        rank: unique identifier of each process\n",
    "        world_size: Total number of process\n",
    "    '''\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12357\"\n",
    "    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2.1. <a id='toc9_3_2_1_'></a>[在colab上测试可用](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.multiprocessing import Process\n",
    "import os\n",
    "\n",
    "\n",
    "# 定义卷积神经网络模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(local_rank, world_size):\n",
    "  \n",
    "  os.environ[\"MASTER_PORT\"] = \"12357\"\n",
    "  os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "\n",
    "  # 设置每个进程的GPU\n",
    "  torch.cuda.set_device(local_rank)\n",
    "  device = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "  # 初始化进程组\n",
    "  dist.init_process_group(backend='nccl', world_size=world_size, rank=local_rank)\n",
    "\n",
    "  # 数据预处理和加载\n",
    "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "  trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "  # 使用DistributedSampler来对数据进行分布式采样\n",
    "  train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=False, sampler=train_sampler)\n",
    "\n",
    "  # 创建CNN模型实例，并放入多个GPU上\n",
    "  model = CNN().to(device)\n",
    "  model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "\n",
    "  # 定义损失函数和优化器\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "  # 训练模型\n",
    "  num_epochs = 5\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      model.train()\n",
    "      running_loss = 0.0\n",
    "\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "          inputs, labels = data\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          running_loss += loss.item()\n",
    "\n",
    "      print(f\"Local Rank {local_rank}, Epoch {epoch + 1}/{num_epochs}, Training Loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "  dist.destroy_process_group()\n",
    "\n",
    "# Process格式：\n",
    "if __name__ == \"__main__\":\n",
    "  # size = torch.cuda.device_count()\n",
    "  size = 10\n",
    "  processes = []\n",
    "  world_size = 1\n",
    "  for rank in range(size):\n",
    "      p = Process(target=train, args=(rank, world_size))\n",
    "      p.start()\n",
    "      processes.append(p)\n",
    "\n",
    "  for p in processes:\n",
    "      p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. <a id='toc9_4_'></a>[多机多卡（GPU）- 分布式训练](#toc0_)\n",
    "```shell\n",
    "目前PyTorch的多机多卡训练，主要有两种方式：   \n",
    "    1. torch.nn.parallel.DistributedDataParallel()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. <a id='toc10_'></a>[模型和参数的保存与加载](#toc0_)\n",
    "\n",
    "* torch.save( 张量名, 位置 )\n",
    "\n",
    "* 张量名称 = torch.load( 位置 )\n",
    "\n",
    "* torch.save会保存数据的`很多关系`，会有条件限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. <a id='toc10_1_'></a>[加载和保存-张量](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.ones((3, 5))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save()\n",
    "torch.save(x, './Pytorch_params/tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.load()\n",
    "x1 = torch.load('./Pytorch_params/tensor.pt', weights_only=True)\n",
    "\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. <a id='toc10_2_'></a>[加载和保存-模型参数](#toc0_)\n",
    "保存单个权重向量（或其他张量）确实有用， 但是如果我们想保存整个模型，并在以后加载它们， 单独保存每个向量则会变得很麻烦。 毕竟，我们可能有数百个参数散布在各处。 因此，深度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。 例如，如果我们有一个3层多层感知机，我们需要单独指定架构。 因为模型本身可以包含任意代码，所以模型本身难以序列化。 因此，为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。\n",
    "\n",
    "1. save和load函数可用于张量对象的文件读写。\n",
    "2. 我们可以通过参数字典保存和加载网络的全部参数。\n",
    "3. 保存架构必须在代码中完成，而不是在参数中完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save()\n",
    "# 接下来，我们将模型的参数存储在一个叫做“mlp.params”的文件中。\n",
    "torch.save(net.state_dict(), './Pytorch_params/mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.load()\n",
    "# 为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 \n",
    "# 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。\n",
    "net_params = torch.load('./Pytorch_params/mlp.params', weights_only=True)\n",
    "clone = MLP()\n",
    "\n",
    "clone.load_state_dict(net_params)\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 完整的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3885055/2144141008.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load('./Pytorch_params/ckpt.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存\n",
    "torch.save(\n",
    "    {\n",
    "        'epoch': '10', \n",
    "        'model_state_dict': net.state_dict(), \n",
    "        # 'opt_state_dict': opt.state_dict(), \n",
    "        'loss': 'loss'\n",
    "    }, \n",
    "    './Pytorch_params/ckpt.pt'\n",
    ")\n",
    "\n",
    "# 重载\n",
    "check_point = torch.load('./Pytorch_params/ckpt.pt')\n",
    "\n",
    "check_point['model_state_dict']\n",
    "check_point['loss']\n",
    "check_point['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. <a id='toc10_3_'></a>[safetensor](#toc0_)\n",
    "是有huggingface推出的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        net = nn.Sequential(nn.Linear(12, 128), nn.ReLU(), nn.Linear(128, 2))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return net(X)\n",
    "    \n",
    "\n",
    "net = DemoModel()\n",
    "\n",
    "state_dicts1 = net.state_dict()\n",
    "\n",
    "safetensors.torch.save_file(state_dicts1, './Pytorch_params/demo.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import safetensors \n",
    "\n",
    "\n",
    "state_dicts2 = safetensors.torch.load_file('./Pytorch_params/demo.safetensors')\n",
    "\n",
    "state_dicts2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. <a id='toc11_'></a>[神经网络类型](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. <a id='toc11_1_'></a>[CNN](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1. <a id='toc11_1_1_'></a>[概述](#toc0_)\n",
    "\n",
    "\n",
    "CBAPD: 卷积，批量归一化，激活，池化，丢弃\n",
    "\n",
    "卷积层就是特征提取，随后将特征传入FC（全连接层）；\n",
    "\n",
    "卷积本身是线性的，但是经过激活函数后可以编程非线性的。\n",
    "\n",
    "- 为什么要用CNN？\n",
    "  - 利用MLP处理图片像素矩阵，太占内存\n",
    "  - 解决办法，顶层设计一个新的算法具备如下特点：\n",
    "    - 局部性\n",
    "    - 平移不变性\n",
    "  - 刚好来自“信号处理中的卷积”符合此类特征：\n",
    "    - 局部性 （固定/通用的卷积核）\n",
    "    - 平移不变性 （特征图在整个图片的位置不固定，可以平移）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2. <a id='toc11_1_2_'></a>[简单CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.1. <a id='toc11_1_2_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1.2.1.1. <a id='toc11_1_2_1_1_'></a>[卷积计算过程](#toc0_)\n",
    "\n",
    " <img src=\"./Pytorch_Pictures/convolution/conv.gif\" width = \"500\" height = \"300\" alt=\"图片名称\" align=center />\n",
    "\n",
    "- 内积后求和\n",
    "\n",
    "- 输出大小：(Xh - Kh + 1, Xw - Kw + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "def cov2d(X, kernel)-> torch.Tensor:\n",
    "    '''\n",
    "    手写二维convolution计算过程 (二维互关运算)\n",
    "    \n",
    "    Args: \n",
    "        X (2d): 输入图片像素矩阵\n",
    "        kernel (int): 卷积核\n",
    "\n",
    "    Return: \n",
    "        Y: 卷积计算结果\n",
    "    '''\n",
    "    h, w = kernel.shape\n",
    "    Y = torch.zeros(size=(X.shape[0] - h + 1, X.shape[1] - w + 1))   # 输出形状，暂时用0填充\n",
    "    # print(Y)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * kernel).sum()      # X取子集 * kernel 最后在求和\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(9, dtype=torch.float32).reshape(3, 3)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = torch.arange(4, dtype=torch.float32).reshape(2, 2)\n",
    "\n",
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov2d(X=X, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1.2.1.2. <a id='toc11_1_2_1_2_'></a>[从头卷积层](#toc0_)\n",
    "- 卷积层对输入和卷积核进行互关运算，并添加偏置；\n",
    "- 所以卷积层中两个被训练的参数是卷积核与偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Cov2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        # self.bias = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return cov2d(X, kernel=self.kernel) + self.bias                   # 将conv2d计算添加进来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]),\n",
       " tensor([[ 4.2942,  6.8109],\n",
       "         [11.8445, 14.3613]], grad_fn=<AddBackward0>),\n",
       " Parameter containing:\n",
       " tensor([[0.7042, 0.7619],\n",
       "         [0.6707, 0.3800]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov2d1 = Cov2d(kernel_size=(2, 2))\n",
    "\n",
    "X, cov2d1(X=X), cov2d1.kernel, cov2d1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.2. <a id='toc11_1_2_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5187, -0.3951],\n",
       "          [-0.1480, -0.0244]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    kernel_size = (2, 2), \n",
    "    bias = True\n",
    ")\n",
    "\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "conv2d(X.reshape(shape=(1,1,3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.3. <a id='toc11_1_2_3_'></a>[填充和步幅](#toc0_)\n",
    "\n",
    "- 填充 (padding)\n",
    "\n",
    "  - 输出大小：(Xh - Kh + Ph + 1, Xw - Kw + Pw + 1)\n",
    "\n",
    "  - 一般情况下Kh和Kw为奇数(1,3,5,7) 可得 (输入和输出形状一致)：\n",
    "\n",
    "    - Ph设置为：Kh - 1\n",
    "\n",
    "    - Pw设置为：Kw - 1\n",
    "\n",
    "  - padding填写时写一半 (输入和输出形状一致)：\n",
    "    - padding = (Ph/2, Pw/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]),\n",
       " torch.Size([1, 1, 3, 3]),\n",
       " tensor([[[[-0.8907, -0.1869,  0.2284],\n",
       "           [-0.5685,  0.4497,  0.2547],\n",
       "           [ 4.9647,  3.9560,  0.3975]]]], grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 1, 3, 3]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn  \n",
    "\n",
    "\n",
    "conv2d1 = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    kernel_size = (3, 3), \n",
    "    bias = True, \n",
    "    padding = (1, 1),           # ((3 - 1)/2, (3 - 1)/2)\n",
    "    stride = 1\n",
    ")\n",
    "\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "X = torch.arange(9, dtype=torch.float32).reshape(3, 3)\n",
    "Y = conv2d1(X.reshape(shape=(1,1,3,3)))\n",
    "\n",
    "X, X.reshape(shape=(1,1,3,3)).shape, Y, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步幅 (stride)\n",
    "\n",
    "  - 输出大小为：( (Xh - Kh + Ph + Sh)/Sh, (Xw - Kw + Pw + Sw)/Sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19., 20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27., 28., 29., 30., 31.],\n",
       "         [32., 33., 34., 35., 36., 37., 38., 39.],\n",
       "         [40., 41., 42., 43., 44., 45., 46., 47.],\n",
       "         [48., 49., 50., 51., 52., 53., 54., 55.],\n",
       "         [56., 57., 58., 59., 60., 61., 62., 63.]]),\n",
       " torch.Size([1, 1, 8, 8]),\n",
       " tensor([[[[ -2.2047,  -5.6431,  -6.8988,  -8.1545],\n",
       "           [-13.6358, -17.9951, -19.6276, -21.2601],\n",
       "           [-28.1641, -31.0550, -32.6875, -34.3200],\n",
       "           [-42.6923, -44.1149, -45.7474, -47.3799]]]],\n",
       "        grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 1, 4, 4]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "conv2d1 = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 1, \n",
    "    kernel_size = (3, 3), \n",
    "    bias = True, \n",
    "    padding = (1, 1),           # ((3 - 1)/2, (3 - 1)/2)\n",
    "    stride = 2                  # (8 - 3 + 1 + 2 )/2 = 4\n",
    ")\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "X = torch.arange(64, dtype=torch.float32).reshape(8, 8)\n",
    "Y = conv2d1(X.reshape((1,1,8,8)))\n",
    "\n",
    "X, X.reshape((1,1,8,8)).shape, Y, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.4. <a id='toc11_1_2_4_'></a>[多输入和多输出通道](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19., 20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27., 28., 29., 30., 31.],\n",
       "         [32., 33., 34., 35., 36., 37., 38., 39.],\n",
       "         [40., 41., 42., 43., 44., 45., 46., 47.],\n",
       "         [48., 49., 50., 51., 52., 53., 54., 55.],\n",
       "         [56., 57., 58., 59., 60., 61., 62., 63.]]),\n",
       " torch.Size([1, 1, 8, 8]),\n",
       " tensor([[[[ -2.3830,  -4.8751,  -5.6074,  -6.3396],\n",
       "           [ -4.1800,  -7.9979,  -8.1388,  -8.2797],\n",
       "           [ -4.5518,  -9.1252,  -9.2662,  -9.4071],\n",
       "           [ -4.9237, -10.2526, -10.3935, -10.5344]],\n",
       " \n",
       "          [[ -0.6794,  -3.0746,  -3.6831,  -4.2917],\n",
       "           [  2.9294,  -2.9314,  -2.5534,  -2.1754],\n",
       "           [  8.4542,   0.0925,   0.4705,   0.8484],\n",
       "           [ 13.9789,   3.1163,   3.4943,   3.8723]],\n",
       " \n",
       "          [[ -1.6030,  -2.0202,  -2.3736,  -2.7271],\n",
       "           [ -5.1094,  -4.1152,  -4.3205,  -4.5257],\n",
       "           [ -8.3311,  -5.7571,  -5.9623,  -6.1675],\n",
       "           [-11.5528,  -7.3989,  -7.6042,  -7.8094]]]],\n",
       "        grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 3, 4, 4]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "conv2d1 = nn.Conv2d(\n",
    "    in_channels = 1, \n",
    "    out_channels = 3, \n",
    "    kernel_size = (3, 3), \n",
    "    bias = True, \n",
    "    padding = (1, 1),           # ((3 - 1)/2, (3 - 1)/2)\n",
    "    stride = 2                  # (8 - 3 + 1 + 2 )/2 = 4\n",
    ")\n",
    "\n",
    "# nn.Conv2d的输入和输出都是：批量大小、通道数、高度和宽度\n",
    "X = torch.arange(64, dtype=torch.float32).reshape(8, 8)\n",
    "Y = conv2d1(X.reshape((1,1,8,8)))\n",
    "\n",
    "X, X.reshape((1,1,8,8)).shape, Y, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2.5. <a id='toc11_1_2_5_'></a>[Pooling (汇聚层)](#toc0_)\n",
    "pooling层不包含参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 11.1.2.5.1. <a id='toc11_1_2_5_1_'></a>[平均Pooling](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AvgPool2d(kernel_size=(2, 2), stride=1, padding=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "nn.AvgPool2d(\n",
    "    kernel_size = (2, 2), \n",
    "    padding = 0, \n",
    "    stride = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1.2.5.2. <a id='toc11_1_2_5_2_'></a>[最大Pooling](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "nn.MaxPool2d(\n",
    "    kernel_size = (2, 2), \n",
    "    padding = 0, \n",
    "    stride = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3. <a id='toc11_1_3_'></a>[LeNet](#toc0_)\n",
    "\n",
    "- 最早被Yann LeCun用来识别手写数字的算法\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/LeNet.jpg\" width = \"700\" height = \"300\" alt=\"图片名称\" align=center >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4869, -0.4271, -0.2250,  0.0145, -0.0824, -0.5031, -0.2721,  0.5979,\n",
       "         -0.1851, -0.4968]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5), \n",
    "            nn.Sigmoid(), \n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(16 * 5 * 5, 120), \n",
    "            nn.Sigmoid(), \n",
    "            nn.Linear(120, 84), \n",
    "            nn.Sigmoid(), \n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "\n",
    "# 测试\n",
    "lenet = LeNet()\n",
    "\n",
    "X = torch.arange(28*28, dtype=torch.float32).reshape((1, 1, 28, 28))\n",
    "# X = torch.rand(size=(1,1,28,28), dtype=torch.float32)\n",
    "# X.shape\n",
    "\n",
    "lenet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.4. <a id='toc11_1_4_'></a>[AlexNet](#toc0_)\n",
    "\n",
    "- 第一个在大规模视觉比赛 (ImageNet) 中战胜传统给算法 (如支持向量机 supportvectormachines) 的**大型神经网络**\n",
    "\n",
    "- 证明算法学习的特征可以超越手动设计的特征\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/AlexNet.jpg\" width = \"500\" height = \"700\" alt=\"图片名称\" align=center >  \n",
    "\n",
    "- LeNet VS AlexNet：\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/LeNetVSAlexNet.jpg\" width = \"1000\" height = \"300\" alt=\"图片名称\" align=center >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import AlexNet\n",
    "\n",
    "\n",
    "alexnet = AlexNet()\n",
    "\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.5. <a id='toc11_1_5_'></a>[VGG](#toc0_)\n",
    "\n",
    "- 利用重复的神经网络块\n",
    "\n",
    "  - 卷积层，如Conv2d()\n",
    "  - 非线性激活，如nn.Relu()\n",
    "  - 汇聚层，如nn.MaxPooling()\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/convolution/VGG.jpg\" width = \"500\" height = \"500\" alt=\"图片名称\" align=center >  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模块设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (5): Flatten(start_dim=1, end_dim=-1)\n",
       "  (6): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Dropout(p=0.5, inplace=False)\n",
       "  (9): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.5, inplace=False)\n",
       "  (12): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "conv_arch = (\n",
    "    (1, 64), \n",
    "    (1, 128), \n",
    "    (2, 256), \n",
    "    (2, 512), \n",
    "    (2, 512)\n",
    ")\n",
    "\n",
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "        \n",
    "    return nn.Sequential(\n",
    "        *conv_blks, \n",
    "        nn.Flatten(), \n",
    "        # 全连接部分\n",
    "        nn.Linear(out_channels*7*7, 4096), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(p=0.5), \n",
    "        nn.Linear(4096, 4096), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(p=0.5), \n",
    "        nn.Linear(4096, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "net = vgg(conv_arch)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vgg11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vgg11\n",
    "\n",
    "\n",
    "vgg = vgg11()\n",
    "\n",
    "vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.6. <a id='toc11_1_6_'></a>[NiN](#toc0_)\n",
    "\n",
    "- 使用1 x 1卷积层来替代全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.7. <a id='toc11_1_7_'></a>[GoogLeNet](#toc0_)\n",
    "\n",
    "- 2014年的ImageNet挑战赛中，GoogLeNet大放异彩；\n",
    "\n",
    "- 解决了到底选多大的卷积核的问题？结论是：使用不同大小的卷积核组合更加有利。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.8. <a id='toc11_1_8_'></a>[批量规范化](#toc0_)\n",
    "\n",
    "- batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.9. <a id='toc11_1_9_'></a>[ResNet](#toc0_)\n",
    "```shell\n",
    "如果，CNN只需要弄懂一个神经网络模型的话，那就是ResNet。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.9.1. <a id='toc11_1_9_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.LSTM(), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet34 \n",
    "\n",
    "\n",
    "resnet = resnet34()\n",
    "\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. <a id='toc11_2_'></a>[序列数据](#toc0_)\n",
    "### 11.2.1. <a id='toc11_2_1_'></a>[什么是序列](#toc0_)\n",
    "\n",
    "在深度学习中，**序列**是一段具有连续关系的数据，通常带有时间先后顺序。例如，文本、语音、股票价格、气温、DNA序列等都可以被视为序列数据。为了处理不定长的数据，我们常常使用循环神经网络（RNN）来处理序列信息。总之，序列数据在许多领域中都有广泛的应用，包括自然语言处理、时间序列分析、音频处理和图像处理等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2. <a id='toc11_2_2_'></a>[语言模型](#toc0_)\n",
    "\n",
    "语言模型 (language model) 是定义在单词序列上的概率模型，可以用来计算一个句子或一段文字的概率。\n",
    "\n",
    "常见的语言模型包括：\n",
    "  - `n-gram模型`：基于统计的方法，通过计算n个连续词出现的概率来预测下一个词。\n",
    "  - `神经网络语言模型`：使用神经网络（如RNN、LSTM、Transformer等）来捕捉语言的复杂模式和长距离依赖关系。\n",
    "  - `预训练语言模型`：如GPT（生成式预训练变换器）和BERT（双向编码器表示）等，这些模型在大量文本上进行预训练，然后可以通过微调应用于特定任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3. <a id='toc11_2_3_'></a>[文本预处理](#toc0_)\n",
    "* token：最小单位（字符/单词/词组）\n",
    "* vocab：（token：indice）对照（查询）列表\n",
    "* cropus：token转化为indice后的文本，也称之为语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.1. <a id='toc11_2_3_1_'></a>[下载《Time machine》并读取数据](#toc0_)\n",
    "首先，我们从H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)中加载文本。\n",
    "这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，\n",
    "而现实中的文档集合可能会包含数十亿个单词。\n",
    "下面的函数 (**将数据集读取到由多条文本行组成的列表中**)，其中每条文本行都是一个字符串。\n",
    "为简单起见，我们在这里忽略了标点符号和字母大写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines: ['the time machine by h g wells', '', '', '', '', 'i', '', '', 'the time traveller for so it will be convenient to speak of him', 'was expounding a recondite matter to us his grey eyes shone and', 'twinkled and his usually pale face was flushed and animated the', 'fire burned brightly and the soft radiance of the incandescent', 'lights in the lilies of silver caught the bubbles that flashed and', 'passed in our glasses our chairs being his patents embraced and', 'caressed us rather than submitted to be sat upon and there was that', 'luxurious after dinner atmosphere when thought roams gracefully', 'free of the trammels of precision and he put it to us in this', 'way marking the points with a lean forefinger as we sat and lazily', 'admired his earnestness over this new paradox as we thought it', 'and his fecundity', '', 'you must follow me carefully i shall have to controvert one or two', 'ideas that are almost universally accepted the geometry for', 'instance they taught you at school is founded on a misconception', '', 'is not that rather a large thing to expect us to begin upon', 'said filby an argumentative person with red hair', '', 'i do not mean to ask you to accept anything without reasonable', 'ground for it you will soon admit as much as i need from you you', 'know of course that a mathematical line a line of thickness nil', 'has no real existence they taught you that neither has a', 'mathematical plane these things are mere abstractions', '', 'that is all right said the psychologist', '', 'nor having only length breadth and thickness can a cube have a', 'real existence', '', 'there i object said filby of course a solid body may exist all', 'real things', '', 'so most people think but wait a moment can an instantaneous', 'cube exist', '', 'don t follow you said filby', '', 'can a cube that does not last for any time at all have a real', 'existence', '', 'filby became pensive clearly the time traveller proceeded any', 'real body must have extension in four directions it must have', 'length breadth thickness and duration but through a natural', 'infirmity of the flesh which i will explain to you in a moment we', 'incline to overlook this fact there are really four dimensions', 'three which we call the three planes of space and a fourth time', 'there is however a tendency to draw an unreal distinction between', 'the former three dimensions and the latter because it happens that', 'our consciousness moves intermittently in one direction along the', 'latter from the beginning to the end of our lives', '', 'that said a very young man making spasmodic efforts to relight', 'his cigar over the lamp that very clear indeed', '', 'now it is very remarkable that this is so extensively overlooked', 'continued the time traveller with a slight accession of', 'cheerfulness really this is what is meant by the fourth dimension', 'though some people who talk about the fourth dimension do not know', 'they mean it it is only another way of looking at time there is', 'no difference between time and any of the three dimensions of space', 'except that our consciousness moves along it but some foolish', 'people have got hold of the wrong side of that idea you have all', 'heard what they have to say about this fourth dimension', '', 'i have not said the provincial mayor', '', 'it is simply this that space as our mathematicians have it is', 'spoken of as having three dimensions which one may call length', 'breadth and thickness and is always definable by reference to', 'three planes each at right angles to the others but some', 'philosophical people have been asking why three dimensions', 'particularly why not another direction at right angles to the other', 'three and have even tried to construct a four dimension geometry', 'professor simon newcomb was expounding this to the new york', 'mathematical society only a month or so ago you know how on a flat', 'surface which has only two dimensions we can represent a figure of', 'a three dimensional solid and similarly they think that by models', 'of three dimensions they could represent one of four if they could', 'master the perspective of the thing see', '', 'i think so murmured the provincial mayor and knitting his', 'brows he lapsed into an introspective state his lips moving as one', 'who repeats mystic words yes i think i see it now he said after', 'some time brightening in a quite transitory manner', '', 'well i do not mind telling you i have been at work upon this', 'geometry of four dimensions for some time some of my results', 'are curious for instance here is a portrait of a man at eight', 'years old another at fifteen another at seventeen another at', 'twenty three and so on all these are evidently sections as it', 'were three dimensional representations of his four dimensioned', 'being which is a fixed and unalterable thing', '', 'scientific people proceeded the time traveller after the pause', 'required for the proper assimilation of this know very well that', 'time is only a kind of space here is a popular scientific diagram', 'a weather record this line i trace with my finger shows the', 'movement of the barometer yesterday it was so high yesterday night', 'it fell then this morning it rose again and so gently upward to', 'here surely the mercury did not trace this line in any of the', 'dimensions of space generally recognized but certainly it traced', 'such a line and that line therefore we must conclude was along', 'the time dimension', '', 'but said the medical man staring hard at a coal in the fire if', 'time is really only a fourth dimension of space why is it and why', 'has it always been regarded as something different and why cannot', 'we move in time as we move about in the other dimensions of space', '', 'the time traveller smiled are you sure we can move freely in', 'space right and left we can go backward and forward freely enough', 'and men always have done so i admit we move freely in two', 'dimensions but how about up and down gravitation limits us there', '', 'not exactly said the medical man there are balloons', '', 'but before the balloons save for spasmodic jumping and the', 'inequalities of the surface man had no freedom of vertical', 'movement', '', 'still they could move a little up and down said the medical man', '', 'easier far easier down than up', '', 'and you cannot move at all in time you cannot get away from the', 'present moment', '', 'my dear sir that is just where you are wrong that is just where', 'the whole world has gone wrong we are always getting away from the', 'present moment our mental existences which are immaterial and have', 'no dimensions are passing along the time dimension with a uniform', 'velocity from the cradle to the grave just as we should travel down', 'if we began our existence fifty miles above the earth s surface', '', 'but the great difficulty is this interrupted the psychologist', 'you can move about in all directions of space but you cannot', 'move about in time', '', 'that is the germ of my great discovery but you are wrong to say', 'that we cannot move about in time for instance if i am recalling', 'an incident very vividly i go back to the instant of its occurrence', 'i become absent minded as you say i jump back for a moment of', 'course we have no means of staying back for any length of time any', 'more than a savage or an animal has of staying six feet above the', 'ground but a civilized man is better off than the savage in this', 'respect he can go up against gravitation in a balloon and why', 'should he not hope that ultimately he may be able to stop or', 'accelerate his drift along the time dimension or even turn about', 'and travel the other way', '', 'oh this began filby is all', '', 'why not said the time traveller', '', 'it s against reason said filby', '', 'what reason said the time traveller', '', 'you can show black is white by argument said filby but you will', 'never convince me', '', 'possibly not said the time traveller but now you begin to see', 'the object of my investigations into the geometry of four', 'dimensions long ago i had a vague inkling of a machine', '', 'to travel through time exclaimed the very young man', '', 'that shall travel indifferently in any direction of space and time', 'as the driver determines', '', 'filby contented himself with laughter', '', 'but i have experimental verification said the time traveller', '', 'it would be remarkably convenient for the historian the', 'psychologist suggested one might travel back and verify the', 'accepted account of the battle of hastings for instance', '', 'don t you think you would attract attention said the medical man', 'our ancestors had no great tolerance for anachronisms', '', 'one might get one s greek from the very lips of homer and plato', 'the very young man thought', '', 'in which case they would certainly plough you for the little go', 'the german scholars have improved greek so much', '', 'then there is the future said the very young man just think', 'one might invest all one s money leave it to accumulate at', 'interest and hurry on ahead', '', 'to discover a society said i erected on a strictly communistic', 'basis', '', 'of all the wild extravagant theories began the psychologist', '', 'yes so it seemed to me and so i never talked of it until', '', 'experimental verification cried i you are going to verify', 'that', '', 'the experiment cried filby who was getting brain weary', '', 'let s see your experiment anyhow said the psychologist though', 'it s all humbug you know', '', 'the time traveller smiled round at us then still smiling faintly', 'and with his hands deep in his trousers pockets he walked slowly', 'out of the room and we heard his slippers shuffling down the long', 'passage to his laboratory', '', 'the psychologist looked at us i wonder what he s got', '', 'some sleight of hand trick or other said the medical man and', 'filby tried to tell us about a conjurer he had seen at burslem but', 'before he had finished his preface the time traveller came back and', 'filby s anecdote collapsed', '', 'the thing the time traveller held in his hand was a glittering', 'metallic framework scarcely larger than a small clock and very', 'delicately made there was ivory in it and some transparent', 'crystalline substance and now i must be explicit for this that', 'follows unless his explanation is to be accepted is an absolutely', 'unaccountable thing he took one of the small octagonal tables that', 'were scattered about the room and set it in front of the fire with', 'two legs on the hearthrug on this table he placed the mechanism', 'then he drew up a chair and sat down the only other object on the', 'table was a small shaded lamp the bright light of which fell upon', 'the model there were also perhaps a dozen candles about two in', 'brass candlesticks upon the mantel and several in sconces so that', 'the room was brilliantly illuminated i sat in a low arm chair', 'nearest the fire and i drew this forward so as to be almost between', 'the time traveller and the fireplace filby sat behind him looking', 'over his shoulder the medical man and the provincial mayor watched', 'him in profile from the right the psychologist from the left the', 'very young man stood behind the psychologist we were all on the', 'alert it appears incredible to me that any kind of trick however', 'subtly conceived and however adroitly done could have been played', 'upon us under these conditions', '', 'the time traveller looked at us and then at the mechanism well', 'said the psychologist', '', 'this little affair said the time traveller resting his elbows', 'upon the table and pressing his hands together above the apparatus', 'is only a model it is my plan for a machine to travel through', 'time you will notice that it looks singularly askew and that there', 'is an odd twinkling appearance about this bar as though it was in', 'some way unreal he pointed to the part with his finger also', 'here is one little white lever and here is another', '', 'the medical man got up out of his chair and peered into the thing', 'it s beautifully made he said', '', 'it took two years to make retorted the time traveller then when', 'we had all imitated the action of the medical man he said now i', 'want you clearly to understand that this lever being pressed over', 'sends the machine gliding into the future and this other reverses', 'the motion this saddle represents the seat of a time traveller', 'presently i am going to press the lever and off the machine will', 'go it will vanish pass into future time and disappear have a', 'good look at the thing look at the table too and satisfy', 'yourselves there is no trickery i don t want to waste this model', 'and then be told i m a quack', '', 'there was a minute s pause perhaps the psychologist seemed about to', 'speak to me but changed his mind then the time traveller put forth', 'his finger towards the lever no he said suddenly lend me your', 'hand and turning to the psychologist he took that individual s', 'hand in his own and told him to put out his forefinger so that it', 'was the psychologist himself who sent forth the model time machine', 'on its interminable voyage we all saw the lever turn i am', 'absolutely certain there was no trickery there was a breath of', 'wind and the lamp flame jumped one of the candles on the mantel', 'was blown out and the little machine suddenly swung round became', 'indistinct was seen as a ghost for a second perhaps as an eddy of', 'faintly glittering brass and ivory and it was gone vanished save', 'for the lamp the table was bare', '', 'everyone was silent for a minute then filby said he was damned', '', 'the psychologist recovered from his stupor and suddenly looked', 'under the table at that the time traveller laughed cheerfully', 'well he said with a reminiscence of the psychologist then', 'getting up he went to the tobacco jar on the mantel and with his', 'back to us began to fill his pipe', '', 'we stared at each other look here said the medical man are you', 'in earnest about this do you seriously believe that that machine', 'has travelled into time', '', 'certainly said the time traveller stooping to light a spill at', 'the fire then he turned lighting his pipe to look at the', 'psychologist s face the psychologist to show that he was not', 'unhinged helped himself to a cigar and tried to light it uncut', 'what is more i have a big machine nearly finished in there he', 'indicated the laboratory and when that is put together i mean to', 'have a journey on my own account', '', 'you mean to say that that machine has travelled into the future', 'said filby', '', 'into the future or the past i don t for certain know which', '', 'after an interval the psychologist had an inspiration it must have', 'gone into the past if it has gone anywhere he said', '', 'why said the time traveller', '', 'because i presume that it has not moved in space and if it', 'travelled into the future it would still be here all this time', 'since it must have travelled through this time', '', 'but i said if it travelled into the past it would have been', 'visible when we came first into this room and last thursday when we', 'were here and the thursday before that and so forth', '', 'serious objections remarked the provincial mayor with an air of', 'impartiality turning towards the time traveller', '', 'not a bit said the time traveller and to the psychologist you', 'think you can explain that it s presentation below the threshold', 'you know diluted presentation', '', 'of course said the psychologist and reassured us that s a', 'simple point of psychology i should have thought of it it s plain', 'enough and helps the paradox delightfully we cannot see it nor', 'can we appreciate this machine any more than we can the spoke of', 'a wheel spinning or a bullet flying through the air if it is', 'travelling through time fifty times or a hundred times faster than', 'we are if it gets through a minute while we get through a second', 'the impression it creates will of course be only one fiftieth or', 'one hundredth of what it would make if it were not travelling in', 'time that s plain enough he passed his hand through the space in', 'which the machine had been you see he said laughing', '', 'we sat and stared at the vacant table for a minute or so then the', 'time traveller asked us what we thought of it all', '', 'it sounds plausible enough to night said the medical man but', 'wait until to morrow wait for the common sense of the morning', '', 'would you like to see the time machine itself asked the time', 'traveller and therewith taking the lamp in his hand he led the', 'way down the long draughty corridor to his laboratory i remember', 'vividly the flickering light his queer broad head in silhouette', 'the dance of the shadows how we all followed him puzzled but', 'incredulous and how there in the laboratory we beheld a larger', 'edition of the little mechanism which we had seen vanish from before', 'our eyes parts were of nickel parts of ivory parts had certainly', 'been filed or sawn out of rock crystal the thing was generally', 'complete but the twisted crystalline bars lay unfinished upon the', 'bench beside some sheets of drawings and i took one up for a better', 'look at it quartz it seemed to be', '', 'look here said the medical man are you perfectly serious', 'or is this a trick like that ghost you showed us last christmas', '', 'upon that machine said the time traveller holding the lamp', 'aloft i intend to explore time is that plain i was never more', 'serious in my life', '', 'none of us quite knew how to take it', '', 'i caught filby s eye over the shoulder of the medical man and he', 'winked at me solemnly', '', '', '', '', 'ii', '', '', 'i think that at that time none of us quite believed in the time', 'machine the fact is the time traveller was one of those men who', 'are too clever to be believed you never felt that you saw all round', 'him you always suspected some subtle reserve some ingenuity in', 'ambush behind his lucid frankness had filby shown the model and', 'explained the matter in the time traveller s words we should have', 'shown him far less scepticism for we should have perceived his', 'motives a pork butcher could understand filby but the time', 'traveller had more than a touch of whim among his elements and we', 'distrusted him things that would have made the frame of a less', 'clever man seemed tricks in his hands it is a mistake to do things', 'too easily the serious people who took him seriously never felt', 'quite sure of his deportment they were somehow aware that trusting', 'their reputations for judgment with him was like furnishing a', 'nursery with egg shell china so i don t think any of us said very', 'much about time travelling in the interval between that thursday and', 'the next though its odd potentialities ran no doubt in most of', 'our minds its plausibility that is its practical incredibleness', 'the curious possibilities of anachronism and of utter confusion it', 'suggested for my own part i was particularly preoccupied with the', 'trick of the model that i remember discussing with the medical man', 'whom i met on friday at the linnaean he said he had seen a similar', 'thing at tubingen and laid considerable stress on the blowing out', 'of the candle but how the trick was done he could not explain', '', 'the next thursday i went again to richmond i suppose i was one of', 'the time traveller s most constant guests and arriving late found', 'four or five men already assembled in his drawing room the medical', 'man was standing before the fire with a sheet of paper in one hand', 'and his watch in the other i looked round for the time traveller', 'and it s half past seven now said the medical man i suppose', 'we d better have dinner', '', 'where s said i naming our host', '', 'you ve just come it s rather odd he s unavoidably detained he', 'asks me in this note to lead off with dinner at seven if he s not', 'back says he ll explain when he comes', '', 'it seems a pity to let the dinner spoil said the editor of a', 'well known daily paper and thereupon the doctor rang the bell', '', 'the psychologist was the only person besides the doctor and myself', 'who had attended the previous dinner the other men were blank the', 'editor aforementioned a certain journalist and another a quiet', 'shy man with a beard whom i didn t know and who as far as my', 'observation went never opened his mouth all the evening there was', 'some speculation at the dinner table about the time traveller s', 'absence and i suggested time travelling in a half jocular spirit', 'the editor wanted that explained to him and the psychologist', 'volunteered a wooden account of the ingenious paradox and trick we', 'had witnessed that day week he was in the midst of his exposition', 'when the door from the corridor opened slowly and without noise i', 'was facing the door and saw it first hallo i said at last', 'and the door opened wider and the time traveller stood before us', 'i gave a cry of surprise good heavens man what s the matter', 'cried the medical man who saw him next and the whole tableful', 'turned towards the door', '', 'he was in an amazing plight his coat was dusty and dirty and', 'smeared with green down the sleeves his hair disordered and as it', 'seemed to me greyer either with dust and dirt or because its colour', 'had actually faded his face was ghastly pale his chin had a brown', 'cut on it a cut half healed his expression was haggard and drawn', 'as by intense suffering for a moment he hesitated in the doorway', 'as if he had been dazzled by the light then he came into the room', 'he walked with just such a limp as i have seen in footsore tramps', 'we stared at him in silence expecting him to speak', '', 'he said not a word but came painfully to the table and made a', 'motion towards the wine the editor filled a glass of champagne and', 'pushed it towards him he drained it and it seemed to do him good', 'for he looked round the table and the ghost of his old smile', 'flickered across his face what on earth have you been up to man', 'said the doctor the time traveller did not seem to hear don t let', 'me disturb you he said with a certain faltering articulation', 'i m all right he stopped held out his glass for more and took', 'it off at a draught that s good he said his eyes grew brighter', 'and a faint colour came into his cheeks his glance flickered over', 'our faces with a certain dull approval and then went round the warm', 'and comfortable room then he spoke again still as it were feeling', 'his way among his words i m going to wash and dress and then i ll', 'come down and explain things save me some of that mutton i m', 'starving for a bit of meat', '', 'he looked across at the editor who was a rare visitor and hoped he', 'was all right the editor began a question tell you presently', 'said the time traveller i m funny be all right in a minute', '', 'he put down his glass and walked towards the staircase door again', 'i remarked his lameness and the soft padding sound of his footfall', 'and standing up in my place i saw his feet as he went out he had', 'nothing on them but a pair of tattered blood stained socks then the', 'door closed upon him i had half a mind to follow till i remembered', 'how he detested any fuss about himself for a minute perhaps my', 'mind was wool gathering then remarkable behaviour of an eminent', 'scientist i heard the editor say thinking after his wont in', 'headlines and this brought my attention back to the bright', 'dinner table', '', 'what s the game said the journalist has he been doing the', 'amateur cadger i don t follow i met the eye of the psychologist', 'and read my own interpretation in his face i thought of the time', 'traveller limping painfully upstairs i don t think any one else had', 'noticed his lameness', '', 'the first to recover completely from this surprise was the medical', 'man who rang the bell the time traveller hated to have servants', 'waiting at dinner for a hot plate at that the editor turned to his', 'knife and fork with a grunt and the silent man followed suit the', 'dinner was resumed conversation was exclamatory for a little while', 'with gaps of wonderment and then the editor got fervent in his', 'curiosity does our friend eke out his modest income with a', 'crossing or has he his nebuchadnezzar phases he inquired i feel', 'assured it s this business of the time machine i said and took up', 'the psychologist s account of our previous meeting the new guests', 'were frankly incredulous the editor raised objections what was', 'this time travelling a man couldn t cover himself with dust by', 'rolling in a paradox could he and then as the idea came home to', 'him he resorted to caricature hadn t they any clothes brushes in', 'the future the journalist too would not believe at any price and', 'joined the editor in the easy work of heaping ridicule on the whole', 'thing they were both the new kind of journalist very joyous', 'irreverent young men our special correspondent in the day', 'after to morrow reports the journalist was saying or rather', 'shouting when the time traveller came back he was dressed in', 'ordinary evening clothes and nothing save his haggard look remained', 'of the change that had startled me', '', 'i say said the editor hilariously these chaps here say you have', 'been travelling into the middle of next week tell us all about', 'little rosebery will you what will you take for the lot', '', 'the time traveller came to the place reserved for him without a', 'word he smiled quietly in his old way where s my mutton he', 'said what a treat it is to stick a fork into meat again', '', 'story cried the editor', '', 'story be damned said the time traveller i want something to', 'eat i won t say a word until i get some peptone into my arteries', 'thanks and the salt', '', 'one word said i have you been time travelling', '', 'yes said the time traveller with his mouth full nodding his', 'head', '', 'i d give a shilling a line for a verbatim note said the editor', 'the time traveller pushed his glass towards the silent man and rang', 'it with his fingernail at which the silent man who had been', 'staring at his face started convulsively and poured him wine', 'the rest of the dinner was uncomfortable for my own part sudden', 'questions kept on rising to my lips and i dare say it was the same', 'with the others the journalist tried to relieve the tension by', 'telling anecdotes of hettie potter the time traveller devoted his', 'attention to his dinner and displayed the appetite of a tramp', 'the medical man smoked a cigarette and watched the time traveller', 'through his eyelashes the silent man seemed even more clumsy than', 'usual and drank champagne with regularity and determination out of', 'sheer nervousness at last the time traveller pushed his plate away', 'and looked round us i suppose i must apologize he said i was', 'simply starving i ve had a most amazing time he reached out his', 'hand for a cigar and cut the end but come into the smoking room', 'it s too long a story to tell over greasy plates and ringing the', 'bell in passing he led the way into the adjoining room', '', 'you have told blank and dash and chose about the machine he', 'said to me leaning back in his easy chair and naming the three new', 'guests', '', 'but the thing s a mere paradox said the editor', '', 'i can t argue to night i don t mind telling you the story but', 'i can t argue i will he went on tell you the story of what', 'has happened to me if you like but you must refrain from', 'interruptions i want to tell it badly most of it will sound like', 'lying so be it it s true every word of it all the same i was in', 'my laboratory at four o clock and since then i ve lived eight', 'days such days as no human being ever lived before i m nearly', 'worn out but i shan t sleep till i ve told this thing over to you', 'then i shall go to bed but no interruptions is it agreed', '', 'agreed said the editor and the rest of us echoed agreed and', 'with that the time traveller began his story as i have set it forth', 'he sat back in his chair at first and spoke like a weary man', 'afterwards he got more animated in writing it down i feel with only', 'too much keenness the inadequacy of pen and ink and above all my', 'own inadequacy to express its quality you read i will suppose', 'attentively enough but you cannot see the speaker s white', 'sincere face in the bright circle of the little lamp nor hear the', 'intonation of his voice you cannot know how his expression followed', 'the turns of his story most of us hearers were in shadow for the', 'candles in the smoking room had not been lighted and only the face', 'of the journalist and the legs of the silent man from the knees', 'downward were illuminated at first we glanced now and again at each', 'other after a time we ceased to do that and looked only at the', 'time traveller s face', '', '', '', '', 'iii', '', '', 'i told some of you last thursday of the principles of the time', 'machine and showed you the actual thing itself incomplete in the', 'workshop there it is now a little travel worn truly and one of', 'the ivory bars is cracked and a brass rail bent but the rest of', 'it s sound enough i expected to finish it on friday but on friday', 'when the putting together was nearly done i found that one of the', 'nickel bars was exactly one inch too short and this i had to get', 'remade so that the thing was not complete until this morning it', 'was at ten o clock to day that the first of all time machines began', 'its career i gave it a last tap tried all the screws again put', 'one more drop of oil on the quartz rod and sat myself in the', 'saddle i suppose a suicide who holds a pistol to his skull feels', 'much the same wonder at what will come next as i felt then i took', 'the starting lever in one hand and the stopping one in the other', 'pressed the first and almost immediately the second i seemed to', 'reel i felt a nightmare sensation of falling and looking round', 'i saw the laboratory exactly as before had anything happened for', 'a moment i suspected that my intellect had tricked me then i noted', 'the clock a moment before as it seemed it had stood at a minute', 'or so past ten now it was nearly half past three', '', 'i drew a breath set my teeth gripped the starting lever with both', 'hands and went off with a thud the laboratory got hazy and went', 'dark mrs watchett came in and walked apparently without seeing', 'me towards the garden door i suppose it took her a minute or so to', 'traverse the place but to me she seemed to shoot across the room', 'like a rocket i pressed the lever over to its extreme position the', 'night came like the turning out of a lamp and in another moment', 'came to morrow the laboratory grew faint and hazy then fainter', 'and ever fainter to morrow night came black then day again night', 'again day again faster and faster still an eddying murmur filled', 'my ears and a strange dumb confusedness descended on my mind', '', 'i am afraid i cannot convey the peculiar sensations of time', 'travelling they are excessively unpleasant there is a feeling', 'exactly like that one has upon a switchback of a helpless headlong', 'motion i felt the same horrible anticipation too of an imminent', 'smash as i put on pace night followed day like the flapping of a', 'black wing the dim suggestion of the laboratory seemed presently to', 'fall away from me and i saw the sun hopping swiftly across the sky', 'leaping it every minute and every minute marking a day i supposed', 'the laboratory had been destroyed and i had come into the open air', 'i had a dim impression of scaffolding but i was already going too', 'fast to be conscious of any moving things the slowest snail that', 'ever crawled dashed by too fast for me the twinkling succession of', 'darkness and light was excessively painful to the eye then in the', 'intermittent darknesses i saw the moon spinning swiftly through her', 'quarters from new to full and had a faint glimpse of the circling', 'stars presently as i went on still gaining velocity the', 'palpitation of night and day merged into one continuous greyness', 'the sky took on a wonderful deepness of blue a splendid luminous', 'color like that of early twilight the jerking sun became a streak', 'of fire a brilliant arch in space the moon a fainter fluctuating', 'band and i could see nothing of the stars save now and then a', 'brighter circle flickering in the blue', '', 'the landscape was misty and vague i was still on the hill side', 'upon which this house now stands and the shoulder rose above me', 'grey and dim i saw trees growing and changing like puffs of vapour', 'now brown now green they grew spread shivered and passed away', 'i saw huge buildings rise up faint and fair and pass like dreams', 'the whole surface of the earth seemed changed melting and flowing', 'under my eyes the little hands upon the dials that registered my', 'speed raced round faster and faster presently i noted that the sun', 'belt swayed up and down from solstice to solstice in a minute or', 'less and that consequently my pace was over a year a minute and', 'minute by minute the white snow flashed across the world and', 'vanished and was followed by the bright brief green of spring', '', 'the unpleasant sensations of the start were less poignant now they', 'merged at last into a kind of hysterical exhilaration i remarked', 'indeed a clumsy swaying of the machine for which i was unable to', 'account but my mind was too confused to attend to it so with a', 'kind of madness growing upon me i flung myself into futurity at', 'first i scarce thought of stopping scarce thought of anything but', 'these new sensations but presently a fresh series of impressions', 'grew up in my mind a certain curiosity and therewith a certain', 'dread until at last they took complete possession of me what', 'strange developments of humanity what wonderful advances upon our', 'rudimentary civilization i thought might not appear when i came to', 'look nearly into the dim elusive world that raced and fluctuated', 'before my eyes i saw great and splendid architecture rising about', 'me more massive than any buildings of our own time and yet as it', 'seemed built of glimmer and mist i saw a richer green flow up the', 'hill side and remain there without any wintry intermission even', 'through the veil of my confusion the earth seemed very fair and so', 'my mind came round to the business of stopping', '', 'the peculiar risk lay in the possibility of my finding some', 'substance in the space which i or the machine occupied so long', 'as i travelled at a high velocity through time this scarcely', 'mattered i was so to speak attenuated was slipping like a vapour', 'through the interstices of intervening substances but to come to', 'a stop involved the jamming of myself molecule by molecule into', 'whatever lay in my way meant bringing my atoms into such intimate', 'contact with those of the obstacle that a profound chemical', 'reaction possibly a far reaching explosion would result and blow', 'myself and my apparatus out of all possible dimensions into the', 'unknown this possibility had occurred to me again and again while i', 'was making the machine but then i had cheerfully accepted it as an', 'unavoidable risk one of the risks a man has got to take now the', 'risk was inevitable i no longer saw it in the same cheerful light', 'the fact is that insensibly the absolute strangeness of everything', 'the sickly jarring and swaying of the machine above all the', 'feeling of prolonged falling had absolutely upset my nerve i told', 'myself that i could never stop and with a gust of petulance i', 'resolved to stop forthwith like an impatient fool i lugged over', 'the lever and incontinently the thing went reeling over and i was', 'flung headlong through the air', '', 'there was the sound of a clap of thunder in my ears i may have', 'been stunned for a moment a pitiless hail was hissing round me', 'and i was sitting on soft turf in front of the overset machine', 'everything still seemed grey but presently i remarked that the', 'confusion in my ears was gone i looked round me i was on what', 'seemed to be a little lawn in a garden surrounded by rhododendron', 'bushes and i noticed that their mauve and purple blossoms were', 'dropping in a shower under the beating of the hail stones the', 'rebounding dancing hail hung in a cloud over the machine and drove', 'along the ground like smoke in a moment i was wet to the skin', 'fine hospitality said i to a man who has travelled innumerable', 'years to see you', '', 'presently i thought what a fool i was to get wet i stood up and', 'looked round me a colossal figure carved apparently in some white', 'stone loomed indistinctly beyond the rhododendrons through the hazy', 'downpour but all else of the world was invisible', '', 'my sensations would be hard to describe as the columns of hail', 'grew thinner i saw the white figure more distinctly it was very', 'large for a silver birch tree touched its shoulder it was of white', 'marble in shape something like a winged sphinx but the wings', 'instead of being carried vertically at the sides were spread so', 'that it seemed to hover the pedestal it appeared to me was of', 'bronze and was thick with verdigris it chanced that the face was', 'towards me the sightless eyes seemed to watch me there was the', 'faint shadow of a smile on the lips it was greatly weather worn', 'and that imparted an unpleasant suggestion of disease i stood', 'looking at it for a little space half a minute perhaps or half an', 'hour it seemed to advance and to recede as the hail drove before it', 'denser or thinner at last i tore my eyes from it for a moment and', 'saw that the hail curtain had worn threadbare and that the sky was', 'lightening with the promise of the sun', '', 'i looked up again at the crouching white shape and the full', 'temerity of my voyage came suddenly upon me what might appear when', 'that hazy curtain was altogether withdrawn what might not have', 'happened to men what if cruelty had grown into a common passion', 'what if in this interval the race had lost its manliness and had', 'developed into something inhuman unsympathetic and overwhelmingly', 'powerful i might seem some old world savage animal only the more', 'dreadful and disgusting for our common likeness a foul creature to', 'be incontinently slain', '', 'already i saw other vast shapes huge buildings with intricate', 'parapets and tall columns with a wooded hill side dimly creeping', 'in upon me through the lessening storm i was seized with a panic', 'fear i turned frantically to the time machine and strove hard to', 'readjust it as i did so the shafts of the sun smote through the', 'thunderstorm the grey downpour was swept aside and vanished like', 'the trailing garments of a ghost above me in the intense blue', 'of the summer sky some faint brown shreds of cloud whirled into', 'nothingness the great buildings about me stood out clear and', 'distinct shining with the wet of the thunderstorm and picked out', 'in white by the unmelted hailstones piled along their courses i', 'felt naked in a strange world i felt as perhaps a bird may feel in', 'the clear air knowing the hawk wings above and will swoop my fear', 'grew to frenzy i took a breathing space set my teeth and again', 'grappled fiercely wrist and knee with the machine it gave under', 'my desperate onset and turned over it struck my chin violently one', 'hand on the saddle the other on the lever i stood panting heavily', 'in attitude to mount again', '', 'but with this recovery of a prompt retreat my courage recovered i', 'looked more curiously and less fearfully at this world of the remote', 'future in a circular opening high up in the wall of the nearer', 'house i saw a group of figures clad in rich soft robes they had', 'seen me and their faces were directed towards me', '', 'then i heard voices approaching me coming through the bushes by', 'the white sphinx were the heads and shoulders of men running one of', 'these emerged in a pathway leading straight to the little lawn upon', 'which i stood with my machine he was a slight creature perhaps', 'four feet high clad in a purple tunic girdled at the waist with a', 'leather belt sandals or buskins i could not clearly distinguish', 'which were on his feet his legs were bare to the knees and his', 'head was bare noticing that i noticed for the first time how warm', 'the air was', '', 'he struck me as being a very beautiful and graceful creature but', 'indescribably frail his flushed face reminded me of the more', 'beautiful kind of consumptive that hectic beauty of which we used', 'to hear so much at the sight of him i suddenly regained confidence', 'i took my hands from the machine', '', '', '', '', 'iv', '', '', 'in another moment we were standing face to face i and this fragile', 'thing out of futurity he came straight up to me and laughed into my', 'eyes the absence from his bearing of any sign of fear struck me at', 'once then he turned to the two others who were following him and', 'spoke to them in a strange and very sweet and liquid tongue', '', 'there were others coming and presently a little group of perhaps', 'eight or ten of these exquisite creatures were about me one of them', 'addressed me it came into my head oddly enough that my voice was', 'too harsh and deep for them so i shook my head and pointing to my', 'ears shook it again he came a step forward hesitated and then', 'touched my hand then i felt other soft little tentacles upon my', 'back and shoulders they wanted to make sure i was real there was', 'nothing in this at all alarming indeed there was something in', 'these pretty little people that inspired confidence a graceful', 'gentleness a certain childlike ease and besides they looked so', 'frail that i could fancy myself flinging the whole dozen of them', 'about like nine pins but i made a sudden motion to warn them when i', 'saw their little pink hands feeling at the time machine happily', 'then when it was not too late i thought of a danger i had hitherto', 'forgotten and reaching over the bars of the machine i unscrewed the', 'little levers that would set it in motion and put these in my', 'pocket then i turned again to see what i could do in the way of', 'communication', '', 'and then looking more nearly into their features i saw some', 'further peculiarities in their dresden china type of prettiness', 'their hair which was uniformly curly came to a sharp end at the', 'neck and cheek there was not the faintest suggestion of it on the', 'face and their ears were singularly minute the mouths were small', 'with bright red rather thin lips and the little chins ran to a', 'point the eyes were large and mild and this may seem egotism on', 'my part i fancied even that there was a certain lack of the', 'interest i might have expected in them', '', 'as they made no effort to communicate with me but simply stood', 'round me smiling and speaking in soft cooing notes to each other i', 'began the conversation i pointed to the time machine and to myself', 'then hesitating for a moment how to express time i pointed to the', 'sun at once a quaintly pretty little figure in chequered purple and', 'white followed my gesture and then astonished me by imitating the', 'sound of thunder', '', 'for a moment i was staggered though the import of his gesture was', 'plain enough the question had come into my mind abruptly were', 'these creatures fools you may hardly understand how it took me', 'you see i had always anticipated that the people of the year eight', 'hundred and two thousand odd would be incredibly in front of us in', 'knowledge art everything then one of them suddenly asked me a', 'question that showed him to be on the intellectual level of one of', 'our five year old children asked me in fact if i had come from', 'the sun in a thunderstorm it let loose the judgment i had suspended', 'upon their clothes their frail light limbs and fragile features', 'a flow of disappointment rushed across my mind for a moment i felt', 'that i had built the time machine in vain', '', 'i nodded pointed to the sun and gave them such a vivid rendering', 'of a thunderclap as startled them they all withdrew a pace or so', 'and bowed then came one laughing towards me carrying a chain of', 'beautiful flowers altogether new to me and put it about my neck', 'the idea was received with melodious applause and presently they', 'were all running to and fro for flowers and laughingly flinging', 'them upon me until i was almost smothered with blossom you who', 'have never seen the like can scarcely imagine what delicate and', 'wonderful flowers countless years of culture had created then', 'someone suggested that their plaything should be exhibited in the', 'nearest building and so i was led past the sphinx of white marble', 'which had seemed to watch me all the while with a smile at my', 'astonishment towards a vast grey edifice of fretted stone as i', 'went with them the memory of my confident anticipations of a', 'profoundly grave and intellectual posterity came with irresistible', 'merriment to my mind', '', 'the building had a huge entry and was altogether of colossal', 'dimensions i was naturally most occupied with the growing crowd of', 'little people and with the big open portals that yawned before me', 'shadowy and mysterious my general impression of the world i saw', 'over their heads was a tangled waste of beautiful bushes and', 'flowers a long neglected and yet weedless garden i saw a number', 'of tall spikes of strange white flowers measuring a foot perhaps', 'across the spread of the waxen petals they grew scattered as if', 'wild among the variegated shrubs but as i say i did not examine', 'them closely at this time the time machine was left deserted on the', 'turf among the rhododendrons', '', 'the arch of the doorway was richly carved but naturally i did', 'not observe the carving very narrowly though i fancied i saw', 'suggestions of old phoenician decorations as i passed through and', 'it struck me that they were very badly broken and weather worn', 'several more brightly clad people met me in the doorway and so we', 'entered i dressed in dingy nineteenth century garments looking', 'grotesque enough garlanded with flowers and surrounded by an', 'eddying mass of bright soft colored robes and shining white limbs', 'in a melodious whirl of laughter and laughing speech', '', 'the big doorway opened into a proportionately great hall hung with', 'brown the roof was in shadow and the windows partially glazed', 'with coloured glass and partially unglazed admitted a tempered', 'light the floor was made up of huge blocks of some very hard white', 'metal not plates nor slabs blocks and it was so much worn as i', 'judged by the going to and fro of past generations as to be deeply', 'channelled along the more frequented ways transverse to the length', 'were innumerable tables made of slabs of polished stone raised', 'perhaps a foot from the floor and upon these were heaps of fruits', 'some i recognized as a kind of hypertrophied raspberry and orange', 'but for the most part they were strange', '', 'between the tables was scattered a great number of cushions', 'upon these my conductors seated themselves signing for me to do', 'likewise with a pretty absence of ceremony they began to eat the', 'fruit with their hands flinging peel and stalks and so forth into', 'the round openings in the sides of the tables i was not loath to', 'follow their example for i felt thirsty and hungry as i did so i', 'surveyed the hall at my leisure', '', 'and perhaps the thing that struck me most was its dilapidated look', 'the stained glass windows which displayed only a geometrical', 'pattern were broken in many places and the curtains that hung', 'across the lower end were thick with dust and it caught my eye that', 'the corner of the marble table near me was fractured nevertheless', 'the general effect was extremely rich and picturesque there were', 'perhaps a couple of hundred people dining in the hall and most of', 'them seated as near to me as they could come were watching me with', 'interest their little eyes shining over the fruit they were eating', 'all were clad in the same soft and yet strong silky material', '', 'fruit by the by was all their diet these people of the remote', 'future were strict vegetarians and while i was with them in spite', 'of some carnal cravings i had to be frugivorous also indeed i', 'found afterwards that horses cattle sheep dogs had followed the', 'ichthyosaurus into extinction but the fruits were very delightful', 'one in particular that seemed to be in season all the time i was', 'there a floury thing in a three sided husk was especially good', 'and i made it my staple at first i was puzzled by all these strange', 'fruits and by the strange flowers i saw but later i began to', 'perceive their import', '', 'however i am telling you of my fruit dinner in the distant future', 'now so soon as my appetite was a little checked i determined to', 'make a resolute attempt to learn the speech of these new men of', 'mine clearly that was the next thing to do the fruits seemed a', 'convenient thing to begin upon and holding one of these up i began', 'a series of interrogative sounds and gestures i had some', 'considerable difficulty in conveying my meaning at first my efforts', 'met with a stare of surprise or inextinguishable laughter but', 'presently a fair haired little creature seemed to grasp my intention', 'and repeated a name they had to chatter and explain the business', 'at great length to each other and my first attempts to make the', 'exquisite little sounds of their language caused an immense amount', 'of amusement however i felt like a schoolmaster amidst children', 'and persisted and presently i had a score of noun substantives at', 'least at my command and then i got to demonstrative pronouns and', 'even the verb to eat but it was slow work and the little people', 'soon tired and wanted to get away from my interrogations so i', 'determined rather of necessity to let them give their lessons in', 'little doses when they felt inclined and very little doses i found', 'they were before long for i never met people more indolent or more', 'easily fatigued', '', 'a queer thing i soon discovered about my little hosts and that was', 'their lack of interest they would come to me with eager cries of', 'astonishment like children but like children they would soon stop', 'examining me and wander away after some other toy the dinner and my', 'conversational beginnings ended i noted for the first time that', 'almost all those who had surrounded me at first were gone it is', 'odd too how speedily i came to disregard these little people i', 'went out through the portal into the sunlit world again as soon as', 'my hunger was satisfied i was continually meeting more of these men', 'of the future who would follow me a little distance chatter and', 'laugh about me and having smiled and gesticulated in a friendly', 'way leave me again to my own devices', '', 'the calm of evening was upon the world as i emerged from the great', 'hall and the scene was lit by the warm glow of the setting sun', 'at first things were very confusing everything was so entirely', 'different from the world i had known even the flowers the big', 'building i had left was situated on the slope of a broad river', 'valley but the thames had shifted perhaps a mile from its present', 'position i resolved to mount to the summit of a crest perhaps a', 'mile and a half away from which i could get a wider view of this', 'our planet in the year eight hundred and two thousand seven hundred', 'and one a d for that i should explain was the date the little', 'dials of my machine recorded', '', 'as i walked i was watching for every impression that could possibly', 'help to explain the condition of ruinous splendour in which i', 'found the world for ruinous it was a little way up the hill for', 'instance was a great heap of granite bound together by masses of', 'aluminium a vast labyrinth of precipitous walls and crumpled', 'heaps amidst which were thick heaps of very beautiful pagoda like', 'plants nettles possibly but wonderfully tinted with brown about', 'the leaves and incapable of stinging it was evidently the derelict', 'remains of some vast structure to what end built i could not', 'determine it was here that i was destined at a later date to have', 'a very strange experience the first intimation of a still stranger', 'discovery but of that i will speak in its proper place', '', 'looking round with a sudden thought from a terrace on which i', 'rested for a while i realized that there were no small houses to be', 'seen apparently the single house and possibly even the household', 'had vanished here and there among the greenery were palace like', 'buildings but the house and the cottage which form such', 'characteristic features of our own english landscape had', 'disappeared', '', 'communism said i to myself', '', 'and on the heels of that came another thought i looked at the', 'half dozen little figures that were following me then in a flash', 'i perceived that all had the same form of costume the same soft', 'hairless visage and the same girlish rotundity of limb it may seem', 'strange perhaps that i had not noticed this before but everything', 'was so strange now i saw the fact plainly enough in costume and', 'in all the differences of texture and bearing that now mark off the', 'sexes from each other these people of the future were alike and', 'the children seemed to my eyes to be but the miniatures of their', 'parents i judged then that the children of that time were', 'extremely precocious physically at least and i found afterwards', 'abundant verification of my opinion', '', 'seeing the ease and security in which these people were living i', 'felt that this close resemblance of the sexes was after all what', 'one would expect for the strength of a man and the softness of a', 'woman the institution of the family and the differentiation of', 'occupations are mere militant necessities of an age of physical', 'force where population is balanced and abundant much childbearing', 'becomes an evil rather than a blessing to the state where', 'violence comes but rarely and off spring are secure there is less', 'necessity indeed there is no necessity for an efficient family', 'and the specialization of the sexes with reference to their', 'children s needs disappears we see some beginnings of this even', 'in our own time and in this future age it was complete this i', 'must remind you was my speculation at the time later i was to', 'appreciate how far it fell short of the reality', '', 'while i was musing upon these things my attention was attracted by', 'a pretty little structure like a well under a cupola i thought in', 'a transitory way of the oddness of wells still existing and then', 'resumed the thread of my speculations there were no large buildings', 'towards the top of the hill and as my walking powers were evidently', 'miraculous i was presently left alone for the first time with a', 'strange sense of freedom and adventure i pushed on up to the crest', '', 'there i found a seat of some yellow metal that i did not recognize', 'corroded in places with a kind of pinkish rust and half smothered', 'in soft moss the arm rests cast and filed into the resemblance of', 'griffins heads i sat down on it and i surveyed the broad view of', 'our old world under the sunset of that long day it was as sweet and', 'fair a view as i have ever seen the sun had already gone below the', 'horizon and the west was flaming gold touched with some horizontal', 'bars of purple and crimson below was the valley of the thames in', 'which the river lay like a band of burnished steel i have already', 'spoken of the great palaces dotted about among the variegated', 'greenery some in ruins and some still occupied here and there rose', 'a white or silvery figure in the waste garden of the earth here and', 'there came the sharp vertical line of some cupola or obelisk there', 'were no hedges no signs of proprietary rights no evidences of', 'agriculture the whole earth had become a garden', '', 'so watching i began to put my interpretation upon the things i had', 'seen and as it shaped itself to me that evening my interpretation', 'was something in this way afterwards i found i had got only a', 'half truth or only a glimpse of one facet of the truth', '', 'it seemed to me that i had happened upon humanity upon the wane', 'the ruddy sunset set me thinking of the sunset of mankind for the', 'first time i began to realize an odd consequence of the social', 'effort in which we are at present engaged and yet come to think', 'it is a logical consequence enough strength is the outcome of need', 'security sets a premium on feebleness the work of ameliorating the', 'conditions of life the true civilizing process that makes life more', 'and more secure had gone steadily on to a climax one triumph of a', 'united humanity over nature had followed another things that are', 'now mere dreams had become projects deliberately put in hand and', 'carried forward and the harvest was what i saw', '', 'after all the sanitation and the agriculture of to day are still', 'in the rudimentary stage the science of our time has attacked but', 'a little department of the field of human disease but even so', 'it spreads its operations very steadily and persistently our', 'agriculture and horticulture destroy a weed just here and there and', 'cultivate perhaps a score or so of wholesome plants leaving the', 'greater number to fight out a balance as they can we improve our', 'favourite plants and animals and how few they are gradually by', 'selective breeding now a new and better peach now a seedless', 'grape now a sweeter and larger flower now a more convenient breed', 'of cattle we improve them gradually because our ideals are vague', 'and tentative and our knowledge is very limited because nature', 'too is shy and slow in our clumsy hands some day all this will', 'be better organized and still better that is the drift of the', 'current in spite of the eddies the whole world will be intelligent', 'educated and co operating things will move faster and faster', 'towards the subjugation of nature in the end wisely and carefully', 'we shall readjust the balance of animal and vegetable life to suit', 'our human needs', '', 'this adjustment i say must have been done and done well done', 'indeed for all time in the space of time across which my machine', 'had leaped the air was free from gnats the earth from weeds or', 'fungi everywhere were fruits and sweet and delightful flowers', 'brilliant butterflies flew hither and thither the ideal of', 'preventive medicine was attained diseases had been stamped out i', 'saw no evidence of any contagious diseases during all my stay and i', 'shall have to tell you later that even the processes of putrefaction', 'and decay had been profoundly affected by these changes', '', 'social triumphs too had been effected i saw mankind housed in', 'splendid shelters gloriously clothed and as yet i had found them', 'engaged in no toil there were no signs of struggle neither social', 'nor economical struggle the shop the advertisement traffic all', 'that commerce which constitutes the body of our world was gone it', 'was natural on that golden evening that i should jump at the idea of', 'a social paradise the difficulty of increasing population had been', 'met i guessed and population had ceased to increase', '', 'but with this change in condition comes inevitably adaptations to', 'the change what unless biological science is a mass of errors is', 'the cause of human intelligence and vigour hardship and freedom', 'conditions under which the active strong and subtle survive and', 'the weaker go to the wall conditions that put a premium upon the', 'loyal alliance of capable men upon self restraint patience and', 'decision and the institution of the family and the emotions that', 'arise therein the fierce jealousy the tenderness for offspring', 'parental self devotion all found their justification and support in', 'the imminent dangers of the young now where are these imminent', 'dangers there is a sentiment arising and it will grow against', 'connubial jealousy against fierce maternity against passion', 'of all sorts unnecessary things now and things that make us', 'uncomfortable savage survivals discords in a refined and pleasant', 'life', '', 'i thought of the physical slightness of the people their lack of', 'intelligence and those big abundant ruins and it strengthened my', 'belief in a perfect conquest of nature for after the battle comes', 'quiet humanity had been strong energetic and intelligent and had', 'used all its abundant vitality to alter the conditions under which', 'it lived and now came the reaction of the altered conditions', '', 'under the new conditions of perfect comfort and security that', 'restless energy that with us is strength would become weakness', 'even in our own time certain tendencies and desires once necessary', 'to survival are a constant source of failure physical courage and', 'the love of battle for instance are no great help may even be', 'hindrances to a civilized man and in a state of physical balance', 'and security power intellectual as well as physical would be out', 'of place for countless years i judged there had been no danger of', 'war or solitary violence no danger from wild beasts no wasting', 'disease to require strength of constitution no need of toil for', 'such a life what we should call the weak are as well equipped as', 'the strong are indeed no longer weak better equipped indeed they', 'are for the strong would be fretted by an energy for which there', 'was no outlet no doubt the exquisite beauty of the buildings i saw', 'was the outcome of the last surgings of the now purposeless energy', 'of mankind before it settled down into perfect harmony with the', 'conditions under which it lived the flourish of that triumph which', 'began the last great peace this has ever been the fate of energy in', 'security it takes to art and to eroticism and then come languor', 'and decay', '', 'even this artistic impetus would at last die away had almost died', 'in the time i saw to adorn themselves with flowers to dance to', 'sing in the sunlight so much was left of the artistic spirit and', 'no more even that would fade in the end into a contented', 'inactivity we are kept keen on the grindstone of pain and', 'necessity and it seemed to me that here was that hateful', 'grindstone broken at last', '', 'as i stood there in the gathering dark i thought that in this', 'simple explanation i had mastered the problem of the world mastered', 'the whole secret of these delicious people possibly the checks they', 'had devised for the increase of population had succeeded too well', 'and their numbers had rather diminished than kept stationary', 'that would account for the abandoned ruins very simple was my', 'explanation and plausible enough as most wrong theories are', '', '', '', '', 'v', '', '', 'as i stood there musing over this too perfect triumph of man the', 'full moon yellow and gibbous came up out of an overflow of silver', 'light in the north east the bright little figures ceased to move', 'about below a noiseless owl flitted by and i shivered with the', 'chill of the night i determined to descend and find where i could', 'sleep', '', 'i looked for the building i knew then my eye travelled along to', 'the figure of the white sphinx upon the pedestal of bronze growing', 'distinct as the light of the rising moon grew brighter i could see', 'the silver birch against it there was the tangle of rhododendron', 'bushes black in the pale light and there was the little lawn', 'i looked at the lawn again a queer doubt chilled my complacency', 'no said i stoutly to myself that was not the lawn', '', 'but it was the lawn for the white leprous face of the sphinx was', 'towards it can you imagine what i felt as this conviction came', 'home to me but you cannot the time machine was gone', '', 'at once like a lash across the face came the possibility of', 'losing my own age of being left helpless in this strange new world', 'the bare thought of it was an actual physical sensation i could', 'feel it grip me at the throat and stop my breathing in another', 'moment i was in a passion of fear and running with great leaping', 'strides down the slope once i fell headlong and cut my face i lost', 'no time in stanching the blood but jumped up and ran on with a', 'warm trickle down my cheek and chin all the time i ran i was saying', 'to myself they have moved it a little pushed it under the bushes', 'out of the way nevertheless i ran with all my might all the', 'time with the certainty that sometimes comes with excessive dread', 'i knew that such assurance was folly knew instinctively that the', 'machine was removed out of my reach my breath came with pain i', 'suppose i covered the whole distance from the hill crest to the', 'little lawn two miles perhaps in ten minutes and i am not a young', 'man i cursed aloud as i ran at my confident folly in leaving the', 'machine wasting good breath thereby i cried aloud and none', 'answered not a creature seemed to be stirring in that moonlit', 'world', '', 'when i reached the lawn my worst fears were realized not a trace', 'of the thing was to be seen i felt faint and cold when i faced the', 'empty space among the black tangle of bushes i ran round it', 'furiously as if the thing might be hidden in a corner and then', 'stopped abruptly with my hands clutching my hair above me towered', 'the sphinx upon the bronze pedestal white shining leprous in', 'the light of the rising moon it seemed to smile in mockery of my', 'dismay', '', 'i might have consoled myself by imagining the little people had put', 'the mechanism in some shelter for me had i not felt assured of', 'their physical and intellectual inadequacy that is what dismayed', 'me the sense of some hitherto unsuspected power through whose', 'intervention my invention had vanished yet for one thing i felt', 'assured unless some other age had produced its exact duplicate', 'the machine could not have moved in time the attachment of the', 'levers i will show you the method later prevented any one from', 'tampering with it in that way when they were removed it had moved', 'and was hid only in space but then where could it be', '', 'i think i must have had a kind of frenzy i remember running', 'violently in and out among the moonlit bushes all round the sphinx', 'and startling some white animal that in the dim light i took for a', 'small deer i remember too late that night beating the bushes', 'with my clenched fist until my knuckles were gashed and bleeding', 'from the broken twigs then sobbing and raving in my anguish of', 'mind i went down to the great building of stone the big hall was', 'dark silent and deserted i slipped on the uneven floor and fell', 'over one of the malachite tables almost breaking my shin i lit a', 'match and went on past the dusty curtains of which i have told you', '', 'there i found a second great hall covered with cushions upon', 'which perhaps a score or so of the little people were sleeping i', 'have no doubt they found my second appearance strange enough coming', 'suddenly out of the quiet darkness with inarticulate noises and the', 'splutter and flare of a match for they had forgotten about matches', 'where is my time machine i began bawling like an angry child', 'laying hands upon them and shaking them up together it must have', 'been very queer to them some laughed most of them looked sorely', 'frightened when i saw them standing round me it came into my head', 'that i was doing as foolish a thing as it was possible for me to do', 'under the circumstances in trying to revive the sensation of fear', 'for reasoning from their daylight behaviour i thought that fear', 'must be forgotten', '', 'abruptly i dashed down the match and knocking one of the people', 'over in my course went blundering across the big dining hall again', 'out under the moonlight i heard cries of terror and their little', 'feet running and stumbling this way and that i do not remember all', 'i did as the moon crept up the sky i suppose it was the unexpected', 'nature of my loss that maddened me i felt hopelessly cut off from', 'my own kind a strange animal in an unknown world i must have raved', 'to and fro screaming and crying upon god and fate i have a memory', 'of horrible fatigue as the long night of despair wore away of', 'looking in this impossible place and that of groping among moon lit', 'ruins and touching strange creatures in the black shadows at last', 'of lying on the ground near the sphinx and weeping with absolute', 'wretchedness i had nothing left but misery then i slept and when', 'i woke again it was full day and a couple of sparrows were hopping', 'round me on the turf within reach of my arm', '', 'i sat up in the freshness of the morning trying to remember how', 'i had got there and why i had such a profound sense of desertion', 'and despair then things came clear in my mind with the plain', 'reasonable daylight i could look my circumstances fairly in the', 'face i saw the wild folly of my frenzy overnight and i could', 'reason with myself suppose the worst i said suppose the', 'machine altogether lost perhaps destroyed it behoves me to be', 'calm and patient to learn the way of the people to get a clear', 'idea of the method of my loss and the means of getting materials', 'and tools so that in the end perhaps i may make another that', 'would be my only hope perhaps but better than despair and after', 'all it was a beautiful and curious world', '', 'but probably the machine had only been taken away still i must', 'be calm and patient find its hiding place and recover it by force', 'or cunning and with that i scrambled to my feet and looked about', 'me wondering where i could bathe i felt weary stiff and', 'travel soiled the freshness of the morning made me desire an equal', 'freshness i had exhausted my emotion indeed as i went about', 'my business i found myself wondering at my intense excitement', 'overnight i made a careful examination of the ground about the', 'little lawn i wasted some time in futile questionings conveyed as', 'well as i was able to such of the little people as came by they', 'all failed to understand my gestures some were simply stolid some', 'thought it was a jest and laughed at me i had the hardest task in', 'the world to keep my hands off their pretty laughing faces it was', 'a foolish impulse but the devil begotten of fear and blind anger', 'was ill curbed and still eager to take advantage of my perplexity', 'the turf gave better counsel i found a groove ripped in it about', 'midway between the pedestal of the sphinx and the marks of my feet', 'where on arrival i had struggled with the overturned machine', 'there were other signs of removal about with queer narrow', 'footprints like those i could imagine made by a sloth this directed', 'my closer attention to the pedestal it was as i think i have said', 'of bronze it was not a mere block but highly decorated with deep', 'framed panels on either side i went and rapped at these the', 'pedestal was hollow examining the panels with care i found them', 'discontinuous with the frames there were no handles or keyholes', 'but possibly the panels if they were doors as i supposed opened', 'from within one thing was clear enough to my mind it took no very', 'great mental effort to infer that my time machine was inside that', 'pedestal but how it got there was a different problem', '', 'i saw the heads of two orange clad people coming through the bushes', 'and under some blossom covered apple trees towards me i turned', 'smiling to them and beckoned them to me they came and then', 'pointing to the bronze pedestal i tried to intimate my wish to open', 'it but at my first gesture towards this they behaved very oddly i', 'don t know how to convey their expression to you suppose you were', 'to use a grossly improper gesture to a delicate minded woman it is', 'how she would look they went off as if they had received the last', 'possible insult i tried a sweet looking little chap in white next', 'with exactly the same result somehow his manner made me feel', 'ashamed of myself but as you know i wanted the time machine and', 'i tried him once more as he turned off like the others my temper', 'got the better of me in three strides i was after him had him by', 'the loose part of his robe round the neck and began dragging him', 'towards the sphinx then i saw the horror and repugnance of his', 'face and all of a sudden i let him go', '', 'but i was not beaten yet i banged with my fist at the bronze', 'panels i thought i heard something stir inside to be explicit', 'i thought i heard a sound like a chuckle but i must have been', 'mistaken then i got a big pebble from the river and came and', 'hammered till i had flattened a coil in the decorations and the', 'verdigris came off in powdery flakes the delicate little people', 'must have heard me hammering in gusty outbreaks a mile away on', 'either hand but nothing came of it i saw a crowd of them upon the', 'slopes looking furtively at me at last hot and tired i sat down', 'to watch the place but i was too restless to watch long i am too', 'occidental for a long vigil i could work at a problem for years', 'but to wait inactive for twenty four hours that is another matter', '', 'i got up after a time and began walking aimlessly through the', 'bushes towards the hill again patience said i to myself if you', 'want your machine again you must leave that sphinx alone if they', 'mean to take your machine away it s little good your wrecking their', 'bronze panels and if they don t you will get it back as soon as', 'you can ask for it to sit among all those unknown things before a', 'puzzle like that is hopeless that way lies monomania face this', 'world learn its ways watch it be careful of too hasty guesses', 'at its meaning in the end you will find clues to it all then', 'suddenly the humour of the situation came into my mind the thought', 'of the years i had spent in study and toil to get into the future', 'age and now my passion of anxiety to get out of it i had made', 'myself the most complicated and the most hopeless trap that ever a', 'man devised although it was at my own expense i could not help', 'myself i laughed aloud', '', 'going through the big palace it seemed to me that the little', 'people avoided me it may have been my fancy or it may have had', 'something to do with my hammering at the gates of bronze yet i felt', 'tolerably sure of the avoidance i was careful however to show no', 'concern and to abstain from any pursuit of them and in the course', 'of a day or two things got back to the old footing i made what', 'progress i could in the language and in addition i pushed my', 'explorations here and there either i missed some subtle point or', 'their language was excessively simple almost exclusively composed', 'of concrete substantives and verbs there seemed to be few if any', 'abstract terms or little use of figurative language their', 'sentences were usually simple and of two words and i failed to', 'convey or understand any but the simplest propositions i determined', 'to put the thought of my time machine and the mystery of the bronze', 'doors under the sphinx as much as possible in a corner of memory', 'until my growing knowledge would lead me back to them in a natural', 'way yet a certain feeling you may understand tethered me in a', 'circle of a few miles round the point of my arrival', '', 'so far as i could see all the world displayed the same exuberant', 'richness as the thames valley from every hill i climbed i saw the', 'same abundance of splendid buildings endlessly varied in material', 'and style the same clustering thickets of evergreens the same', 'blossom laden trees and tree ferns here and there water shone like', 'silver and beyond the land rose into blue undulating hills and', 'so faded into the serenity of the sky a peculiar feature which', 'presently attracted my attention was the presence of certain', 'circular wells several as it seemed to me of a very great depth', 'one lay by the path up the hill which i had followed during my', 'first walk like the others it was rimmed with bronze curiously', 'wrought and protected by a little cupola from the rain sitting by', 'the side of these wells and peering down into the shafted darkness', 'i could see no gleam of water nor could i start any reflection', 'with a lighted match but in all of them i heard a certain sound', 'a thud thud thud like the beating of some big engine and i', 'discovered from the flaring of my matches that a steady current of', 'air set down the shafts further i threw a scrap of paper into the', 'throat of one and instead of fluttering slowly down it was at', 'once sucked swiftly out of sight', '', 'after a time too i came to connect these wells with tall towers', 'standing here and there upon the slopes for above them there was', 'often just such a flicker in the air as one sees on a hot day above', 'a sun scorched beach putting things together i reached a strong', 'suggestion of an extensive system of subterranean ventilation whose', 'true import it was difficult to imagine i was at first inclined to', 'associate it with the sanitary apparatus of these people it was an', 'obvious conclusion but it was absolutely wrong', '', 'and here i must admit that i learned very little of drains and', 'bells and modes of conveyance and the like conveniences during my', 'time in this real future in some of these visions of utopias and', 'coming times which i have read there is a vast amount of detail', 'about building and social arrangements and so forth but while', 'such details are easy enough to obtain when the whole world is', 'contained in one s imagination they are altogether inaccessible to', 'a real traveller amid such realities as i found here conceive the', 'tale of london which a negro fresh from central africa would take', 'back to his tribe what would he know of railway companies of', 'social movements of telephone and telegraph wires of the parcels', 'delivery company and postal orders and the like yet we at least', 'should be willing enough to explain these things to him and even of', 'what he knew how much could he make his untravelled friend either', 'apprehend or believe then think how narrow the gap between a negro', 'and a white man of our own times and how wide the interval between', 'myself and these of the golden age i was sensible of much which was', 'unseen and which contributed to my comfort but save for a general', 'impression of automatic organization i fear i can convey very', 'little of the difference to your mind', '', 'in the matter of sepulture for instance i could see no signs of', 'crematoria nor anything suggestive of tombs but it occurred to me', 'that possibly there might be cemeteries or crematoria somewhere', 'beyond the range of my explorings this again was a question i', 'deliberately put to myself and my curiosity was at first entirely', 'defeated upon the point the thing puzzled me and i was led to make', 'a further remark which puzzled me still more that aged and infirm', 'among this people there were none', '', 'i must confess that my satisfaction with my first theories of an', 'automatic civilization and a decadent humanity did not long endure', 'yet i could think of no other let me put my difficulties the', 'several big palaces i had explored were mere living places great', 'dining halls and sleeping apartments i could find no machinery no', 'appliances of any kind yet these people were clothed in pleasant', 'fabrics that must at times need renewal and their sandals though', 'undecorated were fairly complex specimens of metalwork somehow', 'such things must be made and the little people displayed no vestige', 'of a creative tendency there were no shops no workshops no sign', 'of importations among them they spent all their time in playing', 'gently in bathing in the river in making love in a half playful', 'fashion in eating fruit and sleeping i could not see how things', 'were kept going', '', 'then again about the time machine something i knew not what', 'had taken it into the hollow pedestal of the white sphinx why for', 'the life of me i could not imagine those waterless wells too', 'those flickering pillars i felt i lacked a clue i felt how shall', 'i put it suppose you found an inscription with sentences here and', 'there in excellent plain english and interpolated therewith others', 'made up of words of letters even absolutely unknown to you well', 'on the third day of my visit that was how the world of eight', 'hundred and two thousand seven hundred and one presented itself to', 'me', '', 'that day too i made a friend of a sort it happened that as i', 'was watching some of the little people bathing in a shallow one of', 'them was seized with cramp and began drifting downstream the main', 'current ran rather swiftly but not too strongly for even a moderate', 'swimmer it will give you an idea therefore of the strange', 'deficiency in these creatures when i tell you that none made the', 'slightest attempt to rescue the weakly crying little thing which', 'was drowning before their eyes when i realized this i hurriedly', 'slipped off my clothes and wading in at a point lower down i', 'caught the poor mite and drew her safe to land a little rubbing of', 'the limbs soon brought her round and i had the satisfaction of', 'seeing she was all right before i left her i had got to such a low', 'estimate of her kind that i did not expect any gratitude from her', 'in that however i was wrong', '', 'this happened in the morning in the afternoon i met my little', 'woman as i believe it was as i was returning towards my centre', 'from an exploration and she received me with cries of delight and', 'presented me with a big garland of flowers evidently made for me', 'and me alone the thing took my imagination very possibly i had', 'been feeling desolate at any rate i did my best to display my', 'appreciation of the gift we were soon seated together in a little', 'stone arbour engaged in conversation chiefly of smiles the', 'creature s friendliness affected me exactly as a child s might have', 'done we passed each other flowers and she kissed my hands i did', 'the same to hers then i tried talk and found that her name was', 'weena which though i don t know what it meant somehow seemed', 'appropriate enough that was the beginning of a queer friendship', 'which lasted a week and ended as i will tell you', '', 'she was exactly like a child she wanted to be with me always she', 'tried to follow me everywhere and on my next journey out and about', 'it went to my heart to tire her down and leave her at last', 'exhausted and calling after me rather plaintively but the problems', 'of the world had to be mastered i had not i said to myself come', 'into the future to carry on a miniature flirtation yet her distress', 'when i left her was very great her expostulations at the parting', 'were sometimes frantic and i think altogether i had as much', 'trouble as comfort from her devotion nevertheless she was somehow', 'a very great comfort i thought it was mere childish affection that', 'made her cling to me until it was too late i did not clearly know', 'what i had inflicted upon her when i left her nor until it was too', 'late did i clearly understand what she was to me for by merely', 'seeming fond of me and showing in her weak futile way that she', 'cared for me the little doll of a creature presently gave my return', 'to the neighbourhood of the white sphinx almost the feeling of', 'coming home and i would watch for her tiny figure of white and gold', 'so soon as i came over the hill', '', 'it was from her too that i learned that fear had not yet left the', 'world she was fearless enough in the daylight and she had the', 'oddest confidence in me for once in a foolish moment i made', 'threatening grimaces at her and she simply laughed at them but she', 'dreaded the dark dreaded shadows dreaded black things darkness', 'to her was the one thing dreadful it was a singularly passionate', 'emotion and it set me thinking and observing i discovered then', 'among other things that these little people gathered into the great', 'houses after dark and slept in droves to enter upon them without a', 'light was to put them into a tumult of apprehension i never found', 'one out of doors or one sleeping alone within doors after dark', 'yet i was still such a blockhead that i missed the lesson of that', 'fear and in spite of weena s distress i insisted upon sleeping away', 'from these slumbering multitudes', '', 'it troubled her greatly but in the end her odd affection for me', 'triumphed and for five of the nights of our acquaintance including', 'the last night of all she slept with her head pillowed on my arm', 'but my story slips away from me as i speak of her it must have been', 'the night before her rescue that i was awakened about dawn i had', 'been restless dreaming most disagreeably that i was drowned and', 'that sea anemones were feeling over my face with their soft palps', 'i woke with a start and with an odd fancy that some greyish animal', 'had just rushed out of the chamber i tried to get to sleep again', 'but i felt restless and uncomfortable it was that dim grey hour', 'when things are just creeping out of darkness when everything is', 'colourless and clear cut and yet unreal i got up and went down', 'into the great hall and so out upon the flagstones in front of the', 'palace i thought i would make a virtue of necessity and see the', 'sunrise', '', 'the moon was setting and the dying moonlight and the first pallor', 'of dawn were mingled in a ghastly half light the bushes were inky', 'black the ground a sombre grey the sky colourless and cheerless', 'and up the hill i thought i could see ghosts there several times', 'as i scanned the slope i saw white figures twice i fancied i saw', 'a solitary white ape like creature running rather quickly up the', 'hill and once near the ruins i saw a leash of them carrying some', 'dark body they moved hastily i did not see what became of them', 'it seemed that they vanished among the bushes the dawn was still', 'indistinct you must understand i was feeling that chill', 'uncertain early morning feeling you may have known i doubted', 'my eyes', '', 'as the eastern sky grew brighter and the light of the day came on', 'and its vivid colouring returned upon the world once more i scanned', 'the view keenly but i saw no vestige of my white figures they were', 'mere creatures of the half light they must have been ghosts i', 'said i wonder whence they dated for a queer notion of grant', 'allen s came into my head and amused me if each generation die and', 'leave ghosts he argued the world at last will get overcrowded with', 'them on that theory they would have grown innumerable some eight', 'hundred thousand years hence and it was no great wonder to see four', 'at once but the jest was unsatisfying and i was thinking of these', 'figures all the morning until weena s rescue drove them out of my', 'head i associated them in some indefinite way with the white animal', 'i had startled in my first passionate search for the time machine', 'but weena was a pleasant substitute yet all the same they were', 'soon destined to take far deadlier possession of my mind', '', 'i think i have said how much hotter than our own was the weather', 'of this golden age i cannot account for it it may be that the sun', 'was hotter or the earth nearer the sun it is usual to assume that', 'the sun will go on cooling steadily in the future but people', 'unfamiliar with such speculations as those of the younger darwin', 'forget that the planets must ultimately fall back one by one into', 'the parent body as these catastrophes occur the sun will blaze', 'with renewed energy and it may be that some inner planet had', 'suffered this fate whatever the reason the fact remains that the', 'sun was very much hotter than we know it', '', 'well one very hot morning my fourth i think as i was seeking', 'shelter from the heat and glare in a colossal ruin near the great', 'house where i slept and fed there happened this strange thing', 'clambering among these heaps of masonry i found a narrow gallery', 'whose end and side windows were blocked by fallen masses of stone', 'by contrast with the brilliancy outside it seemed at first', 'impenetrably dark to me i entered it groping for the change from', 'light to blackness made spots of colour swim before me suddenly i', 'halted spellbound a pair of eyes luminous by reflection against', 'the daylight without was watching me out of the darkness', '', 'the old instinctive dread of wild beasts came upon me i clenched', 'my hands and steadfastly looked into the glaring eyeballs i was', 'afraid to turn then the thought of the absolute security in which', 'humanity appeared to be living came to my mind and then i', 'remembered that strange terror of the dark overcoming my fear to', 'some extent i advanced a step and spoke i will admit that my', 'voice was harsh and ill controlled i put out my hand and touched', 'something soft at once the eyes darted sideways and something', 'white ran past me i turned with my heart in my mouth and saw a', 'queer little ape like figure its head held down in a peculiar', 'manner running across the sunlit space behind me it blundered', 'against a block of granite staggered aside and in a moment was', 'hidden in a black shadow beneath another pile of ruined masonry', '', 'my impression of it is of course imperfect but i know it was a', 'dull white and had strange large greyish red eyes also that there', 'was flaxen hair on its head and down its back but as i say it', 'went too fast for me to see distinctly i cannot even say whether it', 'ran on all fours or only with its forearms held very low after an', 'instant s pause i followed it into the second heap of ruins i could', 'not find it at first but after a time in the profound obscurity i', 'came upon one of those round well like openings of which i have told', 'you half closed by a fallen pillar a sudden thought came to me', 'could this thing have vanished down the shaft i lit a match and', 'looking down i saw a small white moving creature with large', 'bright eyes which regarded me steadfastly as it retreated it made', 'me shudder it was so like a human spider it was clambering down', 'the wall and now i saw for the first time a number of metal foot', 'and hand rests forming a kind of ladder down the shaft then the', 'light burned my fingers and fell out of my hand going out as it', 'dropped and when i had lit another the little monster had', 'disappeared', '', 'i do not know how long i sat peering down that well it was not for', 'some time that i could succeed in persuading myself that the thing i', 'had seen was human but gradually the truth dawned on me that', 'man had not remained one species but had differentiated into two', 'distinct animals that my graceful children of the upper world were', 'not the sole descendants of our generation but that this bleached', 'obscene nocturnal thing which had flashed before me was also heir', 'to all the ages', '', 'i thought of the flickering pillars and of my theory of an', 'underground ventilation i began to suspect their true import and', 'what i wondered was this lemur doing in my scheme of a perfectly', 'balanced organization how was it related to the indolent serenity', 'of the beautiful upper worlders and what was hidden down there', 'at the foot of that shaft i sat upon the edge of the well telling', 'myself that at any rate there was nothing to fear and that there', 'i must descend for the solution of my difficulties and withal i', 'was absolutely afraid to go as i hesitated two of the beautiful', 'upper world people came running in their amorous sport across the', 'daylight in the shadow the male pursued the female flinging', 'flowers at her as he ran', '', 'they seemed distressed to find me my arm against the overturned', 'pillar peering down the well apparently it was considered bad form', 'to remark these apertures for when i pointed to this one and tried', 'to frame a question about it in their tongue they were still more', 'visibly distressed and turned away but they were interested by my', 'matches and i struck some to amuse them i tried them again about', 'the well and again i failed so presently i left them meaning to', 'go back to weena and see what i could get from her but my mind was', 'already in revolution my guesses and impressions were slipping and', 'sliding to a new adjustment i had now a clue to the import of these', 'wells to the ventilating towers to the mystery of the ghosts to', 'say nothing of a hint at the meaning of the bronze gates and the', 'fate of the time machine and very vaguely there came a suggestion', 'towards the solution of the economic problem that had puzzled me', '', 'here was the new view plainly this second species of man was', 'subterranean there were three circumstances in particular which', 'made me think that its rare emergence above ground was the outcome', 'of a long continued underground habit in the first place there was', 'the bleached look common in most animals that live largely in the', 'dark the white fish of the kentucky caves for instance then', 'those large eyes with that capacity for reflecting light are', 'common features of nocturnal things witness the owl and the cat', 'and last of all that evident confusion in the sunshine that hasty', 'yet fumbling awkward flight towards dark shadow and that peculiar', 'carriage of the head while in the light all reinforced the theory', 'of an extreme sensitiveness of the retina', '', 'beneath my feet then the earth must be tunnelled enormously and', 'these tunnellings were the habitat of the new race the presence of', 'ventilating shafts and wells along the hill slopes everywhere in', 'fact except along the river valley showed how universal were its', 'ramifications what so natural then as to assume that it was in', 'this artificial underworld that such work as was necessary to the', 'comfort of the daylight race was done the notion was so plausible', 'that i at once accepted it and went on to assume the how of this', 'splitting of the human species i dare say you will anticipate the', 'shape of my theory though for myself i very soon felt that it', 'fell far short of the truth', '', 'at first proceeding from the problems of our own age it seemed', 'clear as daylight to me that the gradual widening of the present', 'merely temporary and social difference between the capitalist and', 'the labourer was the key to the whole position no doubt it will', 'seem grotesque enough to you and wildly incredible and yet even', 'now there are existing circumstances to point that way there is', 'a tendency to utilize underground space for the less ornamental', 'purposes of civilization there is the metropolitan railway in', 'london for instance there are new electric railways there are', 'subways there are underground workrooms and restaurants and they', 'increase and multiply evidently i thought this tendency had', 'increased till industry had gradually lost its birthright in the', 'sky i mean that it had gone deeper and deeper into larger and ever', 'larger underground factories spending a still increasing amount of', 'its time therein till in the end even now does not an east end', 'worker live in such artificial conditions as practically to be cut', 'off from the natural surface of the earth', '', 'again the exclusive tendency of richer people due no doubt to', 'the increasing refinement of their education and the widening gulf', 'between them and the rude violence of the poor is already leading', 'to the closing in their interest of considerable portions of the', 'surface of the land about london for instance perhaps half the', 'prettier country is shut in against intrusion and this same', 'widening gulf which is due to the length and expense of the higher', 'educational process and the increased facilities for and temptations', 'towards refined habits on the part of the rich will make that', 'exchange between class and class that promotion by intermarriage', 'which at present retards the splitting of our species along lines', 'of social stratification less and less frequent so in the end', 'above ground you must have the haves pursuing pleasure and comfort', 'and beauty and below ground the have nots the workers getting', 'continually adapted to the conditions of their labour once they', 'were there they would no doubt have to pay rent and not a little', 'of it for the ventilation of their caverns and if they refused', 'they would starve or be suffocated for arrears such of them as were', 'so constituted as to be miserable and rebellious would die and in', 'the end the balance being permanent the survivors would become as', 'well adapted to the conditions of underground life and as happy in', 'their way as the upper world people were to theirs as it seemed to', 'me the refined beauty and the etiolated pallor followed naturally', 'enough', '', 'the great triumph of humanity i had dreamed of took a different', 'shape in my mind it had been no such triumph of moral education and', 'general co operation as i had imagined instead i saw a real', 'aristocracy armed with a perfected science and working to a logical', 'conclusion the industrial system of to day its triumph had not been', 'simply a triumph over nature but a triumph over nature and the', 'fellow man this i must warn you was my theory at the time i had', 'no convenient cicerone in the pattern of the utopian books my', 'explanation may be absolutely wrong i still think it is the', 'most plausible one but even on this supposition the balanced', 'civilization that was at last attained must have long since passed', 'its zenith and was now far fallen into decay the too perfect', 'security of the upper worlders had led them to a slow movement of', 'degeneration to a general dwindling in size strength and', 'intelligence that i could see clearly enough already what had', 'happened to the under grounders i did not yet suspect but from what', 'i had seen of the morlocks that by the by was the name by which', 'these creatures were called i could imagine that the modification', 'of the human type was even far more profound than among the eloi', 'the beautiful race that i already knew', '', 'then came troublesome doubts why had the morlocks taken my time', 'machine for i felt sure it was they who had taken it why too if', 'the eloi were masters could they not restore the machine to me and', 'why were they so terribly afraid of the dark i proceeded as i have', 'said to question weena about this under world but here again i was', 'disappointed at first she would not understand my questions and', 'presently she refused to answer them she shivered as though the', 'topic was unendurable and when i pressed her perhaps a little', 'harshly she burst into tears they were the only tears except my', 'own i ever saw in that golden age when i saw them i ceased', 'abruptly to trouble about the morlocks and was only concerned in', 'banishing these signs of the human inheritance from weena s eyes', 'and very soon she was smiling and clapping her hands while i', 'solemnly burned a match', '', '', '', '', 'vi', '', '', 'it may seem odd to you but it was two days before i could follow', 'up the new found clue in what was manifestly the proper way i felt', 'a peculiar shrinking from those pallid bodies they were just the', 'half bleached colour of the worms and things one sees preserved in', 'spirit in a zoological museum and they were filthily cold to the', 'touch probably my shrinking was largely due to the sympathetic', 'influence of the eloi whose disgust of the morlocks i now began', 'to appreciate', '', 'the next night i did not sleep well probably my health was a', 'little disordered i was oppressed with perplexity and doubt once', 'or twice i had a feeling of intense fear for which i could perceive', 'no definite reason i remember creeping noiselessly into the great', 'hall where the little people were sleeping in the moonlight that', 'night weena was among them and feeling reassured by their presence', 'it occurred to me even then that in the course of a few days the', 'moon must pass through its last quarter and the nights grow dark', 'when the appearances of these unpleasant creatures from below these', 'whitened lemurs this new vermin that had replaced the old might be', 'more abundant and on both these days i had the restless feeling of', 'one who shirks an inevitable duty i felt assured that the time', 'machine was only to be recovered by boldly penetrating these', 'underground mysteries yet i could not face the mystery if only i', 'had had a companion it would have been different but i was so', 'horribly alone and even to clamber down into the darkness of the', 'well appalled me i don t know if you will understand my feeling', 'but i never felt quite safe at my back', '', 'it was this restlessness this insecurity perhaps that drove me', 'further and further afield in my exploring expeditions going to the', 'south westward towards the rising country that is now called combe', 'wood i observed far off in the direction of nineteenth century', 'banstead a vast green structure different in character from any', 'i had hitherto seen it was larger than the largest of the palaces', 'or ruins i knew and the facade had an oriental look the face', 'of it having the lustre as well as the pale green tint a kind', 'of bluish green of a certain type of chinese porcelain this', 'difference in aspect suggested a difference in use and i was minded', 'to push on and explore but the day was growing late and i had come', 'upon the sight of the place after a long and tiring circuit so i', 'resolved to hold over the adventure for the following day and i', 'returned to the welcome and the caresses of little weena but next', 'morning i perceived clearly enough that my curiosity regarding the', 'palace of green porcelain was a piece of self deception to enable', 'me to shirk by another day an experience i dreaded i resolved i', 'would make the descent without further waste of time and started', 'out in the early morning towards a well near the ruins of granite', 'and aluminium', '', 'little weena ran with me she danced beside me to the well but', 'when she saw me lean over the mouth and look downward she seemed', 'strangely disconcerted good bye little weena i said kissing', 'her and then putting her down i began to feel over the parapet', 'for the climbing hooks rather hastily i may as well confess for', 'i feared my courage might leak away at first she watched me in', 'amazement then she gave a most piteous cry and running to me she', 'began to pull at me with her little hands i think her opposition', 'nerved me rather to proceed i shook her off perhaps a little', 'roughly and in another moment i was in the throat of the well i', 'saw her agonized face over the parapet and smiled to reassure her', 'then i had to look down at the unstable hooks to which i clung', '', 'i had to clamber down a shaft of perhaps two hundred yards the', 'descent was effected by means of metallic bars projecting from', 'the sides of the well and these being adapted to the needs of', 'a creature much smaller and lighter than myself i was speedily', 'cramped and fatigued by the descent and not simply fatigued one of', 'the bars bent suddenly under my weight and almost swung me off into', 'the blackness beneath for a moment i hung by one hand and after', 'that experience i did not dare to rest again though my arms and', 'back were presently acutely painful i went on clambering down the', 'sheer descent with as quick a motion as possible glancing upward', 'i saw the aperture a small blue disk in which a star was visible', 'while little weena s head showed as a round black projection the', 'thudding sound of a machine below grew louder and more oppressive', 'everything save that little disk above was profoundly dark and when', 'i looked up again weena had disappeared', '', 'i was in an agony of discomfort i had some thought of trying to go', 'up the shaft again and leave the under world alone but even while', 'i turned this over in my mind i continued to descend at last with', 'intense relief i saw dimly coming up a foot to the right of me a', 'slender loophole in the wall swinging myself in i found it was the', 'aperture of a narrow horizontal tunnel in which i could lie down and', 'rest it was not too soon my arms ached my back was cramped and i', 'was trembling with the prolonged terror of a fall besides this the', 'unbroken darkness had had a distressing effect upon my eyes the air', 'was full of the throb and hum of machinery pumping air down the', 'shaft', '', 'i do not know how long i lay i was roused by a soft hand touching', 'my face starting up in the darkness i snatched at my matches and', 'hastily striking one i saw three stooping white creatures similar', 'to the one i had seen above ground in the ruin hastily retreating', 'before the light living as they did in what appeared to me', 'impenetrable darkness their eyes were abnormally large and', 'sensitive just as are the pupils of the abysmal fishes and they', 'reflected the light in the same way i have no doubt they could see', 'me in that rayless obscurity and they did not seem to have any fear', 'of me apart from the light but so soon as i struck a match in', 'order to see them they fled incontinently vanishing into dark', 'gutters and tunnels from which their eyes glared at me in the', 'strangest fashion', '', 'i tried to call to them but the language they had was apparently', 'different from that of the over world people so that i was needs', 'left to my own unaided efforts and the thought of flight before', 'exploration was even then in my mind but i said to myself you are', 'in for it now and feeling my way along the tunnel i found the', 'noise of machinery grow louder presently the walls fell away from', 'me and i came to a large open space and striking another match', 'saw that i had entered a vast arched cavern which stretched into', 'utter darkness beyond the range of my light the view i had of it', 'was as much as one could see in the burning of a match', '', 'necessarily my memory is vague great shapes like big machines rose', 'out of the dimness and cast grotesque black shadows in which dim', 'spectral morlocks sheltered from the glare the place by the by', 'was very stuffy and oppressive and the faint halitus of freshly', 'shed blood was in the air some way down the central vista was a', 'little table of white metal laid with what seemed a meal the', 'morlocks at any rate were carnivorous even at the time i remember', 'wondering what large animal could have survived to furnish the red', 'joint i saw it was all very indistinct the heavy smell the big', 'unmeaning shapes the obscene figures lurking in the shadows and', 'only waiting for the darkness to come at me again then the match', 'burned down and stung my fingers and fell a wriggling red spot', 'in the blackness', '', 'i have thought since how particularly ill equipped i was for such', 'an experience when i had started with the time machine i had', 'started with the absurd assumption that the men of the future would', 'certainly be infinitely ahead of ourselves in all their appliances', 'i had come without arms without medicine without anything to', 'smoke at times i missed tobacco frightfully even without enough', 'matches if only i had thought of a kodak i could have flashed that', 'glimpse of the underworld in a second and examined it at leisure', 'but as it was i stood there with only the weapons and the powers', 'that nature had endowed me with hands feet and teeth these and', 'four safety matches that still remained to me', '', 'i was afraid to push my way in among all this machinery in the', 'dark and it was only with my last glimpse of light i discovered', 'that my store of matches had run low it had never occurred to me', 'until that moment that there was any need to economize them and i', 'had wasted almost half the box in astonishing the upper worlders to', 'whom fire was a novelty now as i say i had four left and while i', 'stood in the dark a hand touched mine lank fingers came feeling', 'over my face and i was sensible of a peculiar unpleasant odour i', 'fancied i heard the breathing of a crowd of those dreadful little', 'beings about me i felt the box of matches in my hand being gently', 'disengaged and other hands behind me plucking at my clothing the', 'sense of these unseen creatures examining me was indescribably', 'unpleasant the sudden realization of my ignorance of their ways of', 'thinking and doing came home to me very vividly in the darkness i', 'shouted at them as loudly as i could they started away and then', 'i could feel them approaching me again they clutched at me more', 'boldly whispering odd sounds to each other i shivered violently', 'and shouted again rather discordantly this time they were not so', 'seriously alarmed and they made a queer laughing noise as they came', 'back at me i will confess i was horribly frightened i determined', 'to strike another match and escape under the protection of its', 'glare i did so and eking out the flicker with a scrap of paper', 'from my pocket i made good my retreat to the narrow tunnel but i', 'had scarce entered this when my light was blown out and in the', 'blackness i could hear the morlocks rustling like wind among leaves', 'and pattering like the rain as they hurried after me', '', 'in a moment i was clutched by several hands and there was no', 'mistaking that they were trying to haul me back i struck another', 'light and waved it in their dazzled faces you can scarce imagine', 'how nauseatingly inhuman they looked those pale chinless faces', 'and great lidless pinkish grey eyes as they stared in their', 'blindness and bewilderment but i did not stay to look i promise', 'you i retreated again and when my second match had ended i struck', 'my third it had almost burned through when i reached the opening', 'into the shaft i lay down on the edge for the throb of the great', 'pump below made me giddy then i felt sideways for the projecting', 'hooks and as i did so my feet were grasped from behind and i', 'was violently tugged backward i lit my last match and it', 'incontinently went out but i had my hand on the climbing bars now', 'and kicking violently i disengaged myself from the clutches of the', 'morlocks and was speedily clambering up the shaft while they stayed', 'peering and blinking up at me all but one little wretch who', 'followed me for some way and well nigh secured my boot as a trophy', '', 'that climb seemed interminable to me with the last twenty or', 'thirty feet of it a deadly nausea came upon me i had the greatest', 'difficulty in keeping my hold the last few yards was a frightful', 'struggle against this faintness several times my head swam and i', 'felt all the sensations of falling at last however i got over the', 'well mouth somehow and staggered out of the ruin into the blinding', 'sunlight i fell upon my face even the soil smelt sweet and clean', 'then i remember weena kissing my hands and ears and the voices of', 'others among the eloi then for a time i was insensible', '', '', '', '', 'vii', '', '', 'now indeed i seemed in a worse case than before hitherto', 'except during my night s anguish at the loss of the time machine', 'i had felt a sustaining hope of ultimate escape but that hope was', 'staggered by these new discoveries hitherto i had merely thought', 'myself impeded by the childish simplicity of the little people and', 'by some unknown forces which i had only to understand to overcome', 'but there was an altogether new element in the sickening quality of', 'the morlocks a something inhuman and malign instinctively i', 'loathed them before i had felt as a man might feel who had fallen', 'into a pit my concern was with the pit and how to get out of it', 'now i felt like a beast in a trap whose enemy would come upon him', 'soon', '', 'the enemy i dreaded may surprise you it was the darkness of the', 'new moon weena had put this into my head by some at first', 'incomprehensible remarks about the dark nights it was not now', 'such a very difficult problem to guess what the coming dark nights', 'might mean the moon was on the wane each night there was a longer', 'interval of darkness and i now understood to some slight degree at', 'least the reason of the fear of the little upper world people for', 'the dark i wondered vaguely what foul villainy it might be that', 'the morlocks did under the new moon i felt pretty sure now that', 'my second hypothesis was all wrong the upper world people might', 'once have been the favoured aristocracy and the morlocks their', 'mechanical servants but that had long since passed away the two', 'species that had resulted from the evolution of man were sliding', 'down towards or had already arrived at an altogether new', 'relationship the eloi like the carolingian kings had decayed', 'to a mere beautiful futility they still possessed the earth on', 'sufferance since the morlocks subterranean for innumerable', 'generations had come at last to find the daylit surface', 'intolerable and the morlocks made their garments i inferred and', 'maintained them in their habitual needs perhaps through the', 'survival of an old habit of service they did it as a standing horse', 'paws with his foot or as a man enjoys killing animals in sport', 'because ancient and departed necessities had impressed it on the', 'organism but clearly the old order was already in part reversed', 'the nemesis of the delicate ones was creeping on apace ages ago', 'thousands of generations ago man had thrust his brother man out of', 'the ease and the sunshine and now that brother was coming back', 'changed already the eloi had begun to learn one old lesson anew', 'they were becoming reacquainted with fear and suddenly there came', 'into my head the memory of the meat i had seen in the under world', 'it seemed odd how it floated into my mind not stirred up as it', 'were by the current of my meditations but coming in almost like a', 'question from outside i tried to recall the form of it i had a', 'vague sense of something familiar but i could not tell what it was', 'at the time', '', 'still however helpless the little people in the presence of their', 'mysterious fear i was differently constituted i came out of this', 'age of ours this ripe prime of the human race when fear does not', 'paralyse and mystery has lost its terrors i at least would defend', 'myself without further delay i determined to make myself arms and a', 'fastness where i might sleep with that refuge as a base i could', 'face this strange world with some of that confidence i had lost in', 'realizing to what creatures night by night i lay exposed i felt', 'i could never sleep again until my bed was secure from them i', 'shuddered with horror to think how they must already have examined', 'me', '', 'i wandered during the afternoon along the valley of the thames but', 'found nothing that commended itself to my mind as inaccessible all', 'the buildings and trees seemed easily practicable to such dexterous', 'climbers as the morlocks to judge by their wells must be then the', 'tall pinnacles of the palace of green porcelain and the polished', 'gleam of its walls came back to my memory and in the evening', 'taking weena like a child upon my shoulder i went up the hills', 'towards the south west the distance i had reckoned was seven or', 'eight miles but it must have been nearer eighteen i had first seen', 'the place on a moist afternoon when distances are deceptively', 'diminished in addition the heel of one of my shoes was loose and', 'a nail was working through the sole they were comfortable old shoes', 'i wore about indoors so that i was lame and it was already long', 'past sunset when i came in sight of the palace silhouetted black', 'against the pale yellow of the sky', '', 'weena had been hugely delighted when i began to carry her but', 'after a while she desired me to let her down and ran along by the', 'side of me occasionally darting off on either hand to pick flowers', 'to stick in my pockets my pockets had always puzzled weena but at', 'the last she had concluded that they were an eccentric kind of vase', 'for floral decoration at least she utilized them for that purpose', 'and that reminds me in changing my jacket i found', '', 'the time traveller paused put his hand into his pocket and', 'silently placed two withered flowers not unlike very large white', 'mallows upon the little table then he resumed his narrative', '', 'as the hush of evening crept over the world and we proceeded over', 'the hill crest towards wimbledon weena grew tired and wanted to', 'return to the house of grey stone but i pointed out the distant', 'pinnacles of the palace of green porcelain to her and contrived to', 'make her understand that we were seeking a refuge there from her', 'fear you know that great pause that comes upon things before the', 'dusk even the breeze stops in the trees to me there is always an', 'air of expectation about that evening stillness the sky was clear', 'remote and empty save for a few horizontal bars far down in the', 'sunset well that night the expectation took the colour of my', 'fears in that darkling calm my senses seemed preternaturally', 'sharpened i fancied i could even feel the hollowness of the ground', 'beneath my feet could indeed almost see through it the morlocks', 'on their ant hill going hither and thither and waiting for the dark', 'in my excitement i fancied that they would receive my invasion of', 'their burrows as a declaration of war and why had they taken my', 'time machine', '', 'so we went on in the quiet and the twilight deepened into night', 'the clear blue of the distance faded and one star after another', 'came out the ground grew dim and the trees black weena s fears and', 'her fatigue grew upon her i took her in my arms and talked to her', 'and caressed her then as the darkness grew deeper she put her', 'arms round my neck and closing her eyes tightly pressed her face', 'against my shoulder so we went down a long slope into a valley and', 'there in the dimness i almost walked into a little river this i', 'waded and went up the opposite side of the valley past a number', 'of sleeping houses and by a statue a faun or some such figure', 'minus the head here too were acacias so far i had seen nothing of', 'the morlocks but it was yet early in the night and the darker hours', 'before the old moon rose were still to come', '', 'from the brow of the next hill i saw a thick wood spreading wide', 'and black before me i hesitated at this i could see no end to', 'it either to the right or the left feeling tired my feet in', 'particular were very sore i carefully lowered weena from my', 'shoulder as i halted and sat down upon the turf i could no', 'longer see the palace of green porcelain and i was in doubt of my', 'direction i looked into the thickness of the wood and thought of', 'what it might hide under that dense tangle of branches one would', 'be out of sight of the stars even were there no other lurking', 'danger a danger i did not care to let my imagination loose', 'upon there would still be all the roots to stumble over and the', 'tree boles to strike against', '', 'i was very tired too after the excitements of the day so i', 'decided that i would not face it but would pass the night upon the', 'open hill', '', 'weena i was glad to find was fast asleep i carefully wrapped her', 'in my jacket and sat down beside her to wait for the moonrise the', 'hill side was quiet and deserted but from the black of the wood', 'there came now and then a stir of living things above me shone the', 'stars for the night was very clear i felt a certain sense of', 'friendly comfort in their twinkling all the old constellations', 'had gone from the sky however that slow movement which is', 'imperceptible in a hundred human lifetimes had long since', 'rearranged them in unfamiliar groupings but the milky way it', 'seemed to me was still the same tattered streamer of star dust as', 'of yore southward as i judged it was a very bright red star that', 'was new to me it was even more splendid than our own green sirius', 'and amid all these scintillating points of light one bright planet', 'shone kindly and steadily like the face of an old friend', '', 'looking at these stars suddenly dwarfed my own troubles and all', 'the gravities of terrestrial life i thought of their unfathomable', 'distance and the slow inevitable drift of their movements out of', 'the unknown past into the unknown future i thought of the great', 'precessional cycle that the pole of the earth describes only forty', 'times had that silent revolution occurred during all the years that', 'i had traversed and during these few revolutions all the activity', 'all the traditions the complex organizations the nations', 'languages literatures aspirations even the mere memory of man as', 'i knew him had been swept out of existence instead were these', 'frail creatures who had forgotten their high ancestry and the white', 'things of which i went in terror then i thought of the great fear', 'that was between the two species and for the first time with a', 'sudden shiver came the clear knowledge of what the meat i had seen', 'might be yet it was too horrible i looked at little weena sleeping', 'beside me her face white and starlike under the stars and', 'forthwith dismissed the thought', '', 'through that long night i held my mind off the morlocks as well as', 'i could and whiled away the time by trying to fancy i could find', 'signs of the old constellations in the new confusion the sky kept', 'very clear except for a hazy cloud or so no doubt i dozed at', 'times then as my vigil wore on came a faintness in the eastward', 'sky like the reflection of some colourless fire and the old moon', 'rose thin and peaked and white and close behind and overtaking', 'it and overflowing it the dawn came pale at first and then', 'growing pink and warm no morlocks had approached us indeed i had', 'seen none upon the hill that night and in the confidence of renewed', 'day it almost seemed to me that my fear had been unreasonable i', 'stood up and found my foot with the loose heel swollen at the ankle', 'and painful under the heel so i sat down again took off my shoes', 'and flung them away', '', 'i awakened weena and we went down into the wood now green and', 'pleasant instead of black and forbidding we found some fruit', 'wherewith to break our fast we soon met others of the dainty ones', 'laughing and dancing in the sunlight as though there was no such', 'thing in nature as the night and then i thought once more of the', 'meat that i had seen i felt assured now of what it was and from', 'the bottom of my heart i pitied this last feeble rill from the great', 'flood of humanity clearly at some time in the long ago of human', 'decay the morlocks food had run short possibly they had lived on', 'rats and such like vermin even now man is far less discriminating', 'and exclusive in his food than he was far less than any monkey his', 'prejudice against human flesh is no deep seated instinct and so', 'these inhuman sons of men i tried to look at the thing in a', 'scientific spirit after all they were less human and more remote', 'than our cannibal ancestors of three or four thousand years ago', 'and the intelligence that would have made this state of things a', 'torment had gone why should i trouble myself these eloi were mere', 'fatted cattle which the ant like morlocks preserved and preyed', 'upon probably saw to the breeding of and there was weena dancing', 'at my side', '', 'then i tried to preserve myself from the horror that was coming', 'upon me by regarding it as a rigorous punishment of human', 'selfishness man had been content to live in ease and delight upon', 'the labours of his fellow man had taken necessity as his watchword', 'and excuse and in the fullness of time necessity had come home to', 'him i even tried a carlyle like scorn of this wretched aristocracy', 'in decay but this attitude of mind was impossible however great', 'their intellectual degradation the eloi had kept too much of the', 'human form not to claim my sympathy and to make me perforce a', 'sharer in their degradation and their fear', '', 'i had at that time very vague ideas as to the course i should', 'pursue my first was to secure some safe place of refuge and to', 'make myself such arms of metal or stone as i could contrive that', 'necessity was immediate in the next place i hoped to procure some', 'means of fire so that i should have the weapon of a torch at hand', 'for nothing i knew would be more efficient against these morlocks', 'then i wanted to arrange some contrivance to break open the doors of', 'bronze under the white sphinx i had in mind a battering ram i had', 'a persuasion that if i could enter those doors and carry a blaze of', 'light before me i should discover the time machine and escape i', 'could not imagine the morlocks were strong enough to move it far', 'away weena i had resolved to bring with me to our own time and', 'turning such schemes over in my mind i pursued our way towards the', 'building which my fancy had chosen as our dwelling', '', '', '', '', 'viii', '', '', 'i found the palace of green porcelain when we approached it about', 'noon deserted and falling into ruin only ragged vestiges of glass', 'remained in its windows and great sheets of the green facing had', 'fallen away from the corroded metallic framework it lay very high', 'upon a turfy down and looking north eastward before i entered it i', 'was surprised to see a large estuary or even creek where i judged', 'wandsworth and battersea must once have been i thought then though', 'i never followed up the thought of what might have happened or', 'might be happening to the living things in the sea', '', 'the material of the palace proved on examination to be indeed', 'porcelain and along the face of it i saw an inscription in some', 'unknown character i thought rather foolishly that weena might', 'help me to interpret this but i only learned that the bare idea of', 'writing had never entered her head she always seemed to me i', 'fancy more human than she was perhaps because her affection was so', 'human', '', 'within the big valves of the door which were open and broken we', 'found instead of the customary hall a long gallery lit by many', 'side windows at the first glance i was reminded of a museum', 'the tiled floor was thick with dust and a remarkable array of', 'miscellaneous objects was shrouded in the same grey covering then', 'i perceived standing strange and gaunt in the centre of the hall', 'what was clearly the lower part of a huge skeleton i recognized', 'by the oblique feet that it was some extinct creature after the', 'fashion of the megatherium the skull and the upper bones lay', 'beside it in the thick dust and in one place where rain water had', 'dropped through a leak in the roof the thing itself had been worn', 'away further in the gallery was the huge skeleton barrel of a', 'brontosaurus my museum hypothesis was confirmed going towards the', 'side i found what appeared to be sloping shelves and clearing away', 'the thick dust i found the old familiar glass cases of our own', 'time but they must have been air tight to judge from the fair', 'preservation of some of their contents', '', 'clearly we stood among the ruins of some latter day south', 'kensington here apparently was the palaeontological section', 'and a very splendid array of fossils it must have been though the', 'inevitable process of decay that had been staved off for a time and', 'had through the extinction of bacteria and fungi lost ninety nine', 'hundredths of its force was nevertheless with extreme sureness if', 'with extreme slowness at work again upon all its treasures here and', 'there i found traces of the little people in the shape of rare', 'fossils broken to pieces or threaded in strings upon reeds and the', 'cases had in some instances been bodily removed by the morlocks as', 'i judged the place was very silent the thick dust deadened our', 'footsteps weena who had been rolling a sea urchin down the sloping', 'glass of a case presently came as i stared about me and very', 'quietly took my hand and stood beside me', '', 'and at first i was so much surprised by this ancient monument of an', 'intellectual age that i gave no thought to the possibilities it', 'presented even my preoccupation about the time machine receded a', 'little from my mind', '', 'to judge from the size of the place this palace of green porcelain', 'had a great deal more in it than a gallery of palaeontology', 'possibly historical galleries it might be even a library to me', 'at least in my present circumstances these would be vastly more', 'interesting than this spectacle of oldtime geology in decay', 'exploring i found another short gallery running transversely to the', 'first this appeared to be devoted to minerals and the sight of a', 'block of sulphur set my mind running on gunpowder but i could find', 'no saltpeter indeed no nitrates of any kind doubtless they had', 'deliquesced ages ago yet the sulphur hung in my mind and set up a', 'train of thinking as for the rest of the contents of that gallery', 'though on the whole they were the best preserved of all i saw i had', 'little interest i am no specialist in mineralogy and i went on', 'down a very ruinous aisle running parallel to the first hall i had', 'entered apparently this section had been devoted to natural', 'history but everything had long since passed out of recognition a', 'few shrivelled and blackened vestiges of what had once been stuffed', 'animals desiccated mummies in jars that had once held spirit a', 'brown dust of departed plants that was all i was sorry for that', 'because i should have been glad to trace the patent readjustments by', 'which the conquest of animated nature had been attained then we', 'came to a gallery of simply colossal proportions but singularly', 'ill lit the floor of it running downward at a slight angle from the', 'end at which i entered at intervals white globes hung from the', 'ceiling many of them cracked and smashed which suggested that', 'originally the place had been artificially lit here i was more in', 'my element for rising on either side of me were the huge bulks of', 'big machines all greatly corroded and many broken down but some', 'still fairly complete you know i have a certain weakness for', 'mechanism and i was inclined to linger among these the more so as', 'for the most part they had the interest of puzzles and i could make', 'only the vaguest guesses at what they were for i fancied that if', 'i could solve their puzzles i should find myself in possession of', 'powers that might be of use against the morlocks', '', 'suddenly weena came very close to my side so suddenly that she', 'startled me had it not been for her i do not think i should have', 'noticed that the floor of the gallery sloped at all footnote it', 'may be of course that the floor did not slope but that the museum', 'was built into the side of a hill ed the end i had come in at', 'was quite above ground and was lit by rare slit like windows as', 'you went down the length the ground came up against these windows', 'until at last there was a pit like the area of a london house', 'before each and only a narrow line of daylight at the top i went', 'slowly along puzzling about the machines and had been too intent', 'upon them to notice the gradual diminution of the light until', 'weena s increasing apprehensions drew my attention then i saw that', 'the gallery ran down at last into a thick darkness i hesitated and', 'then as i looked round me i saw that the dust was less abundant', 'and its surface less even further away towards the dimness it', 'appeared to be broken by a number of small narrow footprints my', 'sense of the immediate presence of the morlocks revived at that', 'i felt that i was wasting my time in the academic examination of', 'machinery i called to mind that it was already far advanced in the', 'afternoon and that i had still no weapon no refuge and no means', 'of making a fire and then down in the remote blackness of the', 'gallery i heard a peculiar pattering and the same odd noises i had', 'heard down the well', '', 'i took weena s hand then struck with a sudden idea i left her', 'and turned to a machine from which projected a lever not unlike', 'those in a signal box clambering upon the stand and grasping this', 'lever in my hands i put all my weight upon it sideways suddenly', 'weena deserted in the central aisle began to whimper i had judged', 'the strength of the lever pretty correctly for it snapped after a', 'minute s strain and i rejoined her with a mace in my hand more than', 'sufficient i judged for any morlock skull i might encounter and i', 'longed very much to kill a morlock or so very inhuman you may', 'think to want to go killing one s own descendants but it was', 'impossible somehow to feel any humanity in the things only my', 'disinclination to leave weena and a persuasion that if i began to', 'slake my thirst for murder my time machine might suffer restrained', 'me from going straight down the gallery and killing the brutes i', 'heard', '', 'well mace in one hand and weena in the other i went out of that', 'gallery and into another and still larger one which at the first', 'glance reminded me of a military chapel hung with tattered flags', 'the brown and charred rags that hung from the sides of it i', 'presently recognized as the decaying vestiges of books they had', 'long since dropped to pieces and every semblance of print had left', 'them but here and there were warped boards and cracked metallic', 'clasps that told the tale well enough had i been a literary man i', 'might perhaps have moralized upon the futility of all ambition', 'but as it was the thing that struck me with keenest force was the', 'enormous waste of labour to which this sombre wilderness of rotting', 'paper testified at the time i will confess that i thought chiefly', 'of the philosophical transactions and my own seventeen papers upon', 'physical optics', '', 'then going up a broad staircase we came to what may once have', 'been a gallery of technical chemistry and here i had not a little', 'hope of useful discoveries except at one end where the roof had', 'collapsed this gallery was well preserved i went eagerly to every', 'unbroken case and at last in one of the really air tight cases', 'i found a box of matches very eagerly i tried them they were', 'perfectly good they were not even damp i turned to weena dance', 'i cried to her in her own tongue for now i had a weapon indeed', 'against the horrible creatures we feared and so in that derelict', 'museum upon the thick soft carpeting of dust to weena s huge', 'delight i solemnly performed a kind of composite dance whistling', 'the land of the leal as cheerfully as i could in part it was a', 'modest cancan in part a step dance in part a skirt dance so far', 'as my tail coat permitted and in part original for i am naturally', 'inventive as you know', '', 'now i still think that for this box of matches to have escaped', 'the wear of time for immemorial years was a most strange as for', 'me it was a most fortunate thing yet oddly enough i found a far', 'unlikelier substance and that was camphor i found it in a sealed', 'jar that by chance i suppose had been really hermetically sealed', 'i fancied at first that it was paraffin wax and smashed the glass', 'accordingly but the odour of camphor was unmistakable in the', 'universal decay this volatile substance had chanced to survive', 'perhaps through many thousands of centuries it reminded me of a', 'sepia painting i had once seen done from the ink of a fossil', 'belemnite that must have perished and become fossilized millions', 'of years ago i was about to throw it away but i remembered that', 'it was inflammable and burned with a good bright flame was in', 'fact an excellent candle and i put it in my pocket i found no', 'explosives however nor any means of breaking down the bronze', 'doors as yet my iron crowbar was the most helpful thing i had', 'chanced upon nevertheless i left that gallery greatly elated', '', 'i cannot tell you all the story of that long afternoon it would', 'require a great effort of memory to recall my explorations in at all', 'the proper order i remember a long gallery of rusting stands of', 'arms and how i hesitated between my crowbar and a hatchet or a', 'sword i could not carry both however and my bar of iron promised', 'best against the bronze gates there were numbers of guns pistols', 'and rifles the most were masses of rust but many were of some', 'new metal and still fairly sound but any cartridges or powder', 'there may once have been had rotted into dust one corner i saw was', 'charred and shattered perhaps i thought by an explosion among the', 'specimens in another place was a vast array of idols polynesian', 'mexican grecian phoenician every country on earth i should think', 'and here yielding to an irresistible impulse i wrote my name upon', 'the nose of a steatite monster from south america that particularly', 'took my fancy', '', 'as the evening drew on my interest waned i went through gallery', 'after gallery dusty silent often ruinous the exhibits sometimes', 'mere heaps of rust and lignite sometimes fresher in one place i', 'suddenly found myself near the model of a tin mine and then by the', 'merest accident i discovered in an air tight case two dynamite', 'cartridges i shouted eureka and smashed the case with joy then', 'came a doubt i hesitated then selecting a little side gallery', 'i made my essay i never felt such a disappointment as i did in', 'waiting five ten fifteen minutes for an explosion that never came', 'of course the things were dummies as i might have guessed from', 'their presence i really believe that had they not been so i should', 'have rushed off incontinently and blown sphinx bronze doors and', 'as it proved my chances of finding the time machine all together', 'into non existence', '', 'it was after that i think that we came to a little open court', 'within the palace it was turfed and had three fruit trees so we', 'rested and refreshed ourselves towards sunset i began to consider', 'our position night was creeping upon us and my inaccessible', 'hiding place had still to be found but that troubled me very little', 'now i had in my possession a thing that was perhaps the best of', 'all defences against the morlocks i had matches i had the camphor', 'in my pocket too if a blaze were needed it seemed to me that', 'the best thing we could do would be to pass the night in the open', 'protected by a fire in the morning there was the getting of the', 'time machine towards that as yet i had only my iron mace but', 'now with my growing knowledge i felt very differently towards', 'those bronze doors up to this i had refrained from forcing them', 'largely because of the mystery on the other side they had never', 'impressed me as being very strong and i hoped to find my bar of', 'iron not altogether inadequate for the work', '', '', '', '', 'ix', '', '', 'we emerged from the palace while the sun was still in part above', 'the horizon i was determined to reach the white sphinx early the', 'next morning and ere the dusk i purposed pushing through the woods', 'that had stopped me on the previous journey my plan was to go as', 'far as possible that night and then building a fire to sleep', 'in the protection of its glare accordingly as we went along i', 'gathered any sticks or dried grass i saw and presently had my arms', 'full of such litter thus loaded our progress was slower than i had', 'anticipated and besides weena was tired and i began to suffer from', 'sleepiness too so that it was full night before we reached the', 'wood upon the shrubby hill of its edge weena would have stopped', 'fearing the darkness before us but a singular sense of impending', 'calamity that should indeed have served me as a warning drove me', 'onward i had been without sleep for a night and two days and i was', 'feverish and irritable i felt sleep coming upon me and the', 'morlocks with it', '', 'while we hesitated among the black bushes behind us and dim', 'against their blackness i saw three crouching figures there was', 'scrub and long grass all about us and i did not feel safe from', 'their insidious approach the forest i calculated was rather', 'less than a mile across if we could get through it to the bare', 'hill side there as it seemed to me was an altogether safer', 'resting place i thought that with my matches and my camphor i could', 'contrive to keep my path illuminated through the woods yet it was', 'evident that if i was to flourish matches with my hands i should', 'have to abandon my firewood so rather reluctantly i put it down', 'and then it came into my head that i would amaze our friends behind', 'by lighting it i was to discover the atrocious folly of this', 'proceeding but it came to my mind as an ingenious move for covering', 'our retreat', '', 'i don t know if you have ever thought what a rare thing flame must', 'be in the absence of man and in a temperate climate the sun s', 'heat is rarely strong enough to burn even when it is focused by', 'dewdrops as is sometimes the case in more tropical districts', 'lightning may blast and blacken but it rarely gives rise to', 'widespread fire decaying vegetation may occasionally smoulder with', 'the heat of its fermentation but this rarely results in flame in', 'this decadence too the art of fire making had been forgotten on', 'the earth the red tongues that went licking up my heap of wood were', 'an altogether new and strange thing to weena', '', 'she wanted to run to it and play with it i believe she would have', 'cast herself into it had i not restrained her but i caught her up', 'and in spite of her struggles plunged boldly before me into the', 'wood for a little way the glare of my fire lit the path looking', 'back presently i could see through the crowded stems that from my', 'heap of sticks the blaze had spread to some bushes adjacent and a', 'curved line of fire was creeping up the grass of the hill i laughed', 'at that and turned again to the dark trees before me it was very', 'black and weena clung to me convulsively but there was still as', 'my eyes grew accustomed to the darkness sufficient light for me to', 'avoid the stems overhead it was simply black except where a gap of', 'remote blue sky shone down upon us here and there i struck none of', 'my matches because i had no hand free upon my left arm i carried my', 'little one in my right hand i had my iron bar', '', 'for some way i heard nothing but the crackling twigs under my feet', 'the faint rustle of the breeze above and my own breathing and the', 'throb of the blood vessels in my ears then i seemed to know of a', 'pattering about me i pushed on grimly the pattering grew more', 'distinct and then i caught the same queer sound and voices i had', 'heard in the under world there were evidently several of the', 'morlocks and they were closing in upon me indeed in another', 'minute i felt a tug at my coat then something at my arm and weena', 'shivered violently and became quite still', '', 'it was time for a match but to get one i must put her down i did', 'so and as i fumbled with my pocket a struggle began in the', 'darkness about my knees perfectly silent on her part and with the', 'same peculiar cooing sounds from the morlocks soft little hands', 'too were creeping over my coat and back touching even my neck', 'then the match scratched and fizzed i held it flaring and saw the', 'white backs of the morlocks in flight amid the trees i hastily took', 'a lump of camphor from my pocket and prepared to light it as soon', 'as the match should wane then i looked at weena she was lying', 'clutching my feet and quite motionless with her face to the ground', 'with a sudden fright i stooped to her she seemed scarcely to', 'breathe i lit the block of camphor and flung it to the ground', 'and as it split and flared up and drove back the morlocks and the', 'shadows i knelt down and lifted her the wood behind seemed full of', 'the stir and murmur of a great company', '', 'she seemed to have fainted i put her carefully upon my shoulder', 'and rose to push on and then there came a horrible realization in', 'manoeuvring with my matches and weena i had turned myself about', 'several times and now i had not the faintest idea in what direction', 'lay my path for all i knew i might be facing back towards the', 'palace of green porcelain i found myself in a cold sweat i had to', 'think rapidly what to do i determined to build a fire and encamp', 'where we were i put weena still motionless down upon a turfy', 'bole and very hastily as my first lump of camphor waned i began', 'collecting sticks and leaves here and there out of the darkness', 'round me the morlocks eyes shone like carbuncles', '', 'the camphor flickered and went out i lit a match and as i did so', 'two white forms that had been approaching weena dashed hastily away', 'one was so blinded by the light that he came straight for me and i', 'felt his bones grind under the blow of my fist he gave a whoop of', 'dismay staggered a little way and fell down i lit another piece', 'of camphor and went on gathering my bonfire presently i noticed', 'how dry was some of the foliage above me for since my arrival', 'on the time machine a matter of a week no rain had fallen so', 'instead of casting about among the trees for fallen twigs i began', 'leaping up and dragging down branches very soon i had a choking', 'smoky fire of green wood and dry sticks and could economize my', 'camphor then i turned to where weena lay beside my iron mace i', 'tried what i could to revive her but she lay like one dead i could', 'not even satisfy myself whether or not she breathed', '', 'now the smoke of the fire beat over towards me and it must have', 'made me heavy of a sudden moreover the vapour of camphor was in', 'the air my fire would not need replenishing for an hour or so i', 'felt very weary after my exertion and sat down the wood too was', 'full of a slumbrous murmur that i did not understand i seemed just', 'to nod and open my eyes but all was dark and the morlocks had', 'their hands upon me flinging off their clinging fingers i hastily', 'felt in my pocket for the match box and it had gone then they', 'gripped and closed with me again in a moment i knew what had', 'happened i had slept and my fire had gone out and the bitterness', 'of death came over my soul the forest seemed full of the smell of', 'burning wood i was caught by the neck by the hair by the arms', 'and pulled down it was indescribably horrible in the darkness to', 'feel all these soft creatures heaped upon me i felt as if i was in', 'a monstrous spider s web i was overpowered and went down i felt', 'little teeth nipping at my neck i rolled over and as i did so my', 'hand came against my iron lever it gave me strength i struggled', 'up shaking the human rats from me and holding the bar short', 'i thrust where i judged their faces might be i could feel the', 'succulent giving of flesh and bone under my blows and for a moment', 'i was free', '', 'the strange exultation that so often seems to accompany hard', 'fighting came upon me i knew that both i and weena were lost but i', 'determined to make the morlocks pay for their meat i stood with my', 'back to a tree swinging the iron bar before me the whole wood was', 'full of the stir and cries of them a minute passed their voices', 'seemed to rise to a higher pitch of excitement and their movements', 'grew faster yet none came within reach i stood glaring at the', 'blackness then suddenly came hope what if the morlocks were', 'afraid and close on the heels of that came a strange thing the', 'darkness seemed to grow luminous very dimly i began to see the', 'morlocks about me three battered at my feet and then i recognized', 'with incredulous surprise that the others were running in an', 'incessant stream as it seemed from behind me and away through the', 'wood in front and their backs seemed no longer white but reddish', 'as i stood agape i saw a little red spark go drifting across a gap', 'of starlight between the branches and vanish and at that i', 'understood the smell of burning wood the slumbrous murmur that was', 'growing now into a gusty roar the red glow and the morlocks', 'flight', '', 'stepping out from behind my tree and looking back i saw through', 'the black pillars of the nearer trees the flames of the burning', 'forest it was my first fire coming after me with that i looked for', 'weena but she was gone the hissing and crackling behind me the', 'explosive thud as each fresh tree burst into flame left little', 'time for reflection my iron bar still gripped i followed in the', 'morlocks path it was a close race once the flames crept forward', 'so swiftly on my right as i ran that i was outflanked and had to', 'strike off to the left but at last i emerged upon a small open', 'space and as i did so a morlock came blundering towards me and', 'past me and went on straight into the fire', '', 'and now i was to see the most weird and horrible thing i think of', 'all that i beheld in that future age this whole space was as bright', 'as day with the reflection of the fire in the centre was a hillock', 'or tumulus surmounted by a scorched hawthorn beyond this was', 'another arm of the burning forest with yellow tongues already', 'writhing from it completely encircling the space with a fence of', 'fire upon the hill side were some thirty or forty morlocks dazzled', 'by the light and heat and blundering hither and thither against', 'each other in their bewilderment at first i did not realize their', 'blindness and struck furiously at them with my bar in a frenzy of', 'fear as they approached me killing one and crippling several more', 'but when i had watched the gestures of one of them groping under the', 'hawthorn against the red sky and heard their moans i was assured', 'of their absolute helplessness and misery in the glare and i struck', 'no more of them', '', 'yet every now and then one would come straight towards me setting', 'loose a quivering horror that made me quick to elude him at one', 'time the flames died down somewhat and i feared the foul creatures', 'would presently be able to see me i was thinking of beginning the', 'fight by killing some of them before this should happen but the', 'fire burst out again brightly and i stayed my hand i walked about', 'the hill among them and avoided them looking for some trace of', 'weena but weena was gone', '', 'at last i sat down on the summit of the hillock and watched this', 'strange incredible company of blind things groping to and fro and', 'making uncanny noises to each other as the glare of the fire beat', 'on them the coiling uprush of smoke streamed across the sky and', 'through the rare tatters of that red canopy remote as though they', 'belonged to another universe shone the little stars two or three', 'morlocks came blundering into me and i drove them off with blows', 'of my fists trembling as i did so', '', 'for the most part of that night i was persuaded it was a nightmare', 'i bit myself and screamed in a passionate desire to awake i beat', 'the ground with my hands and got up and sat down again and', 'wandered here and there and again sat down then i would fall to', 'rubbing my eyes and calling upon god to let me awake thrice i saw', 'morlocks put their heads down in a kind of agony and rush into the', 'flames but at last above the subsiding red of the fire above the', 'streaming masses of black smoke and the whitening and blackening', 'tree stumps and the diminishing numbers of these dim creatures', 'came the white light of the day', '', 'i searched again for traces of weena but there were none it was', 'plain that they had left her poor little body in the forest i', 'cannot describe how it relieved me to think that it had escaped the', 'awful fate to which it seemed destined as i thought of that i was', 'almost moved to begin a massacre of the helpless abominations about', 'me but i contained myself the hillock as i have said was a kind', 'of island in the forest from its summit i could now make out', 'through a haze of smoke the palace of green porcelain and from that', 'i could get my bearings for the white sphinx and so leaving the', 'remnant of these damned souls still going hither and thither and', 'moaning as the day grew clearer i tied some grass about my feet', 'and limped on across smoking ashes and among black stems that still', 'pulsated internally with fire towards the hiding place of the time', 'machine i walked slowly for i was almost exhausted as well as', 'lame and i felt the intensest wretchedness for the horrible death', 'of little weena it seemed an overwhelming calamity now in this', 'old familiar room it is more like the sorrow of a dream than an', 'actual loss but that morning it left me absolutely lonely', 'again terribly alone i began to think of this house of mine of', 'this fireside of some of you and with such thoughts came a longing', 'that was pain', '', 'but as i walked over the smoking ashes under the bright morning', 'sky i made a discovery in my trouser pocket were still some loose', 'matches the box must have leaked before it was lost', '', '', '', '', 'x', '', '', 'about eight or nine in the morning i came to the same seat of', 'yellow metal from which i had viewed the world upon the evening of', 'my arrival i thought of my hasty conclusions upon that evening and', 'could not refrain from laughing bitterly at my confidence here', 'was the same beautiful scene the same abundant foliage the same', 'splendid palaces and magnificent ruins the same silver river', 'running between its fertile banks the gay robes of the beautiful', 'people moved hither and thither among the trees some were bathing', 'in exactly the place where i had saved weena and that suddenly gave', 'me a keen stab of pain and like blots upon the landscape rose the', 'cupolas above the ways to the under world i understood now what all', 'the beauty of the over world people covered very pleasant was their', 'day as pleasant as the day of the cattle in the field like the', 'cattle they knew of no enemies and provided against no needs and', 'their end was the same', '', 'i grieved to think how brief the dream of the human intellect had', 'been it had committed suicide it had set itself steadfastly', 'towards comfort and ease a balanced society with security and', 'permanency as its watchword it had attained its hopes to come', 'to this at last once life and property must have reached almost', 'absolute safety the rich had been assured of his wealth and', 'comfort the toiler assured of his life and work no doubt in that', 'perfect world there had been no unemployed problem no social', 'question left unsolved and a great quiet had followed', '', 'it is a law of nature we overlook that intellectual versatility', 'is the compensation for change danger and trouble an animal', 'perfectly in harmony with its environment is a perfect mechanism', 'nature never appeals to intelligence until habit and instinct are', 'useless there is no intelligence where there is no change and no', 'need of change only those animals partake of intelligence that have', 'to meet a huge variety of needs and dangers', '', 'so as i see it the upper world man had drifted towards his', 'feeble prettiness and the under world to mere mechanical industry', 'but that perfect state had lacked one thing even for mechanical', 'perfection absolute permanency apparently as time went on the', 'feeding of the under world however it was effected had become', 'disjointed mother necessity who had been staved off for a', 'few thousand years came back again and she began below the', 'under world being in contact with machinery which however perfect', 'still needs some little thought outside habit had probably retained', 'perforce rather more initiative if less of every other human', 'character than the upper and when other meat failed them they', 'turned to what old habit had hitherto forbidden so i say i saw it', 'in my last view of the world of eight hundred and two thousand seven', 'hundred and one it may be as wrong an explanation as mortal wit', 'could invent it is how the thing shaped itself to me and as that i', 'give it to you', '', 'after the fatigues excitements and terrors of the past days and', 'in spite of my grief this seat and the tranquil view and the warm', 'sunlight were very pleasant i was very tired and sleepy and soon', 'my theorizing passed into dozing catching myself at that i took my', 'own hint and spreading myself out upon the turf i had a long and', 'refreshing sleep', '', 'i awoke a little before sunsetting i now felt safe against being', 'caught napping by the morlocks and stretching myself i came on', 'down the hill towards the white sphinx i had my crowbar in one', 'hand and the other hand played with the matches in my pocket', '', 'and now came a most unexpected thing as i approached the pedestal', 'of the sphinx i found the bronze valves were open they had slid', 'down into grooves', '', 'at that i stopped short before them hesitating to enter', '', 'within was a small apartment and on a raised place in the corner', 'of this was the time machine i had the small levers in my pocket', 'so here after all my elaborate preparations for the siege of the', 'white sphinx was a meek surrender i threw my iron bar away almost', 'sorry not to use it', '', 'a sudden thought came into my head as i stooped towards the portal', 'for once at least i grasped the mental operations of the morlocks', 'suppressing a strong inclination to laugh i stepped through the', 'bronze frame and up to the time machine i was surprised to find it', 'had been carefully oiled and cleaned i have suspected since that', 'the morlocks had even partially taken it to pieces while trying in', 'their dim way to grasp its purpose', '', 'now as i stood and examined it finding a pleasure in the mere', 'touch of the contrivance the thing i had expected happened the', 'bronze panels suddenly slid up and struck the frame with a clang', 'i was in the dark trapped so the morlocks thought at that i', 'chuckled gleefully', '', 'i could already hear their murmuring laughter as they came towards', 'me very calmly i tried to strike the match i had only to fix on', 'the levers and depart then like a ghost but i had overlooked one', 'little thing the matches were of that abominable kind that light', 'only on the box', '', 'you may imagine how all my calm vanished the little brutes were', 'close upon me one touched me i made a sweeping blow in the dark at', 'them with the levers and began to scramble into the saddle of the', 'machine then came one hand upon me and then another then i had', 'simply to fight against their persistent fingers for my levers and', 'at the same time feel for the studs over which these fitted one', 'indeed they almost got away from me as it slipped from my hand', 'i had to butt in the dark with my head i could hear the morlock s', 'skull ring to recover it it was a nearer thing than the fight in', 'the forest i think this last scramble', '', 'but at last the lever was fitted and pulled over the clinging', 'hands slipped from me the darkness presently fell from my eyes', 'i found myself in the same grey light and tumult i have already', 'described', '', '', '', '', 'xi', '', '', 'i have already told you of the sickness and confusion that comes', 'with time travelling and this time i was not seated properly in the', 'saddle but sideways and in an unstable fashion for an indefinite', 'time i clung to the machine as it swayed and vibrated quite', 'unheeding how i went and when i brought myself to look at the dials', 'again i was amazed to find where i had arrived one dial records', 'days and another thousands of days another millions of days and', 'another thousands of millions now instead of reversing the levers', 'i had pulled them over so as to go forward with them and when i', 'came to look at these indicators i found that the thousands hand was', 'sweeping round as fast as the seconds hand of a watch into', 'futurity', '', 'as i drove on a peculiar change crept over the appearance of', 'things the palpitating greyness grew darker then though i was', 'still travelling with prodigious velocity the blinking succession', 'of day and night which was usually indicative of a slower pace', 'returned and grew more and more marked this puzzled me very much', 'at first the alternations of night and day grew slower and slower', 'and so did the passage of the sun across the sky until they seemed', 'to stretch through centuries at last a steady twilight brooded over', 'the earth a twilight only broken now and then when a comet glared', 'across the darkling sky the band of light that had indicated the', 'sun had long since disappeared for the sun had ceased to set it', 'simply rose and fell in the west and grew ever broader and more', 'red all trace of the moon had vanished the circling of the stars', 'growing slower and slower had given place to creeping points of', 'light at last some time before i stopped the sun red and very', 'large halted motionless upon the horizon a vast dome glowing with', 'a dull heat and now and then suffering a momentary extinction at', 'one time it had for a little while glowed more brilliantly again', 'but it speedily reverted to its sullen red heat i perceived by this', 'slowing down of its rising and setting that the work of the tidal', 'drag was done the earth had come to rest with one face to the sun', 'even as in our own time the moon faces the earth very cautiously', 'for i remembered my former headlong fall i began to reverse', 'my motion slower and slower went the circling hands until the', 'thousands one seemed motionless and the daily one was no longer a', 'mere mist upon its scale still slower until the dim outlines of a', 'desolate beach grew visible', '', 'i stopped very gently and sat upon the time machine looking round', 'the sky was no longer blue north eastward it was inky black', 'and out of the blackness shone brightly and steadily the pale', 'white stars overhead it was a deep indian red and starless and', 'south eastward it grew brighter to a glowing scarlet where cut by', 'the horizon lay the huge hull of the sun red and motionless the', 'rocks about me were of a harsh reddish colour and all the trace of', 'life that i could see at first was the intensely green vegetation', 'that covered every projecting point on their south eastern face it', 'was the same rich green that one sees on forest moss or on the', 'lichen in caves plants which like these grow in a perpetual', 'twilight', '', 'the machine was standing on a sloping beach the sea stretched away', 'to the south west to rise into a sharp bright horizon against the', 'wan sky there were no breakers and no waves for not a breath of', 'wind was stirring only a slight oily swell rose and fell like a', 'gentle breathing and showed that the eternal sea was still moving', 'and living and along the margin where the water sometimes broke was', 'a thick incrustation of salt pink under the lurid sky there was a', 'sense of oppression in my head and i noticed that i was breathing', 'very fast the sensation reminded me of my only experience of', 'mountaineering and from that i judged the air to be more rarefied', 'than it is now', '', 'far away up the desolate slope i heard a harsh scream and saw a', 'thing like a huge white butterfly go slanting and fluttering up into', 'the sky and circling disappear over some low hillocks beyond the', 'sound of its voice was so dismal that i shivered and seated myself', 'more firmly upon the machine looking round me again i saw that', 'quite near what i had taken to be a reddish mass of rock was moving', 'slowly towards me then i saw the thing was really a monstrous', 'crab like creature can you imagine a crab as large as yonder table', 'with its many legs moving slowly and uncertainly its big claws', 'swaying its long antennae like carters whips waving and feeling', 'and its stalked eyes gleaming at you on either side of its metallic', 'front its back was corrugated and ornamented with ungainly bosses', 'and a greenish incrustation blotched it here and there i could see', 'the many palps of its complicated mouth flickering and feeling as it', 'moved', '', 'as i stared at this sinister apparition crawling towards me i felt', 'a tickling on my cheek as though a fly had lighted there i tried to', 'brush it away with my hand but in a moment it returned and almost', 'immediately came another by my ear i struck at this and caught', 'something threadlike it was drawn swiftly out of my hand with a', 'frightful qualm i turned and i saw that i had grasped the antenna', 'of another monster crab that stood just behind me its evil eyes', 'were wriggling on their stalks its mouth was all alive with', 'appetite and its vast ungainly claws smeared with an algal slime', 'were descending upon me in a moment my hand was on the lever and', 'i had placed a month between myself and these monsters but i was', 'still on the same beach and i saw them distinctly now as soon as i', 'stopped dozens of them seemed to be crawling here and there in the', 'sombre light among the foliated sheets of intense green', '', 'i cannot convey the sense of abominable desolation that hung over', 'the world the red eastern sky the northward blackness the salt', 'dead sea the stony beach crawling with these foul slow stirring', 'monsters the uniform poisonous looking green of the lichenous', 'plants the thin air that hurts one s lungs all contributed to an', 'appalling effect i moved on a hundred years and there was the same', 'red sun a little larger a little duller the same dying sea the', 'same chill air and the same crowd of earthy crustacea creeping in', 'and out among the green weed and the red rocks and in the westward', 'sky i saw a curved pale line like a vast new moon', '', 'so i travelled stopping ever and again in great strides of a', 'thousand years or more drawn on by the mystery of the earth s fate', 'watching with a strange fascination the sun grow larger and duller', 'in the westward sky and the life of the old earth ebb away at', 'last more than thirty million years hence the huge red hot dome of', 'the sun had come to obscure nearly a tenth part of the darkling', 'heavens then i stopped once more for the crawling multitude of', 'crabs had disappeared and the red beach save for its livid green', 'liverworts and lichens seemed lifeless and now it was flecked with', 'white a bitter cold assailed me rare white flakes ever and again', 'came eddying down to the north eastward the glare of snow lay', 'under the starlight of the sable sky and i could see an undulating', 'crest of hillocks pinkish white there were fringes of ice along the', 'sea margin with drifting masses further out but the main expanse', 'of that salt ocean all bloody under the eternal sunset was still', 'unfrozen', '', 'i looked about me to see if any traces of animal life remained a', 'certain indefinable apprehension still kept me in the saddle of the', 'machine but i saw nothing moving in earth or sky or sea the green', 'slime on the rocks alone testified that life was not extinct a', 'shallow sandbank had appeared in the sea and the water had receded', 'from the beach i fancied i saw some black object flopping about', 'upon this bank but it became motionless as i looked at it and i', 'judged that my eye had been deceived and that the black object was', 'merely a rock the stars in the sky were intensely bright and seemed', 'to me to twinkle very little', '', 'suddenly i noticed that the circular westward outline of the sun', 'had changed that a concavity a bay had appeared in the curve i', 'saw this grow larger for a minute perhaps i stared aghast at this', 'blackness that was creeping over the day and then i realized that', 'an eclipse was beginning either the moon or the planet mercury was', 'passing across the sun s disk naturally at first i took it to be', 'the moon but there is much to incline me to believe that what i', 'really saw was the transit of an inner planet passing very near to', 'the earth', '', 'the darkness grew apace a cold wind began to blow in freshening', 'gusts from the east and the showering white flakes in the air', 'increased in number from the edge of the sea came a ripple and', 'whisper beyond these lifeless sounds the world was silent silent', 'it would be hard to convey the stillness of it all the sounds of', 'man the bleating of sheep the cries of birds the hum of insects', 'the stir that makes the background of our lives all that was over', 'as the darkness thickened the eddying flakes grew more abundant', 'dancing before my eyes and the cold of the air more intense at', 'last one by one swiftly one after the other the white peaks of', 'the distant hills vanished into blackness the breeze rose to a', 'moaning wind i saw the black central shadow of the eclipse sweeping', 'towards me in another moment the pale stars alone were visible all', 'else was rayless obscurity the sky was absolutely black', '', 'a horror of this great darkness came on me the cold that smote', 'to my marrow and the pain i felt in breathing overcame me i', 'shivered and a deadly nausea seized me then like a red hot bow', 'in the sky appeared the edge of the sun i got off the machine to', 'recover myself i felt giddy and incapable of facing the return', 'journey as i stood sick and confused i saw again the moving thing', 'upon the shoal there was no mistake now that it was a moving', 'thing against the red water of the sea it was a round thing the', 'size of a football perhaps or it may be bigger and tentacles', 'trailed down from it it seemed black against the weltering', 'blood red water and it was hopping fitfully about then i felt i', 'was fainting but a terrible dread of lying helpless in that remote', 'and awful twilight sustained me while i clambered upon the saddle', '', '', '', '', 'xii', '', '', 'so i came back for a long time i must have been insensible upon', 'the machine the blinking succession of the days and nights was', 'resumed the sun got golden again the sky blue i breathed with', 'greater freedom the fluctuating contours of the land ebbed and', 'flowed the hands spun backward upon the dials at last i saw again', 'the dim shadows of houses the evidences of decadent humanity', 'these too changed and passed and others came presently when the', 'million dial was at zero i slackened speed i began to recognize', 'our own pretty and familiar architecture the thousands hand ran back', 'to the starting point the night and day flapped slower and slower', 'then the old walls of the laboratory came round me very gently', 'now i slowed the mechanism down', '', 'i saw one little thing that seemed odd to me i think i have told', 'you that when i set out before my velocity became very high mrs', 'watchett had walked across the room travelling as it seemed to me', 'like a rocket as i returned i passed again across that minute when', 'she traversed the laboratory but now her every motion appeared to', 'be the exact inversion of her previous ones the door at the lower', 'end opened and she glided quietly up the laboratory back foremost', 'and disappeared behind the door by which she had previously entered', 'just before that i seemed to see hillyer for a moment but he passed', 'like a flash', '', 'then i stopped the machine and saw about me again the old familiar', 'laboratory my tools my appliances just as i had left them i got', 'off the thing very shakily and sat down upon my bench for several', 'minutes i trembled violently then i became calmer around me was', 'my old workshop again exactly as it had been i might have slept', 'there and the whole thing have been a dream', '', 'and yet not exactly the thing had started from the south east', 'corner of the laboratory it had come to rest again in the', 'north west against the wall where you saw it that gives you the', 'exact distance from my little lawn to the pedestal of the white', 'sphinx into which the morlocks had carried my machine', '', 'for a time my brain went stagnant presently i got up and came', 'through the passage here limping because my heel was still', 'painful and feeling sorely begrimed i saw the pall mall gazette', 'on the table by the door i found the date was indeed to day and', 'looking at the timepiece saw the hour was almost eight o clock i', 'heard your voices and the clatter of plates i hesitated i felt so', 'sick and weak then i sniffed good wholesome meat and opened the', 'door on you you know the rest i washed and dined and now i am', 'telling you the story', '', 'i know he said after a pause that all this will be absolutely', 'incredible to you to me the one incredible thing is that i am here', 'to night in this old familiar room looking into your friendly faces', 'and telling you these strange adventures', '', 'he looked at the medical man no i cannot expect you to believe', 'it take it as a lie or a prophecy say i dreamed it in the', 'workshop consider i have been speculating upon the destinies of our', 'race until i have hatched this fiction treat my assertion of its', 'truth as a mere stroke of art to enhance its interest and taking', 'it as a story what do you think of it', '', 'he took up his pipe and began in his old accustomed manner to tap', 'with it nervously upon the bars of the grate there was a momentary', 'stillness then chairs began to creak and shoes to scrape upon the', 'carpet i took my eyes off the time traveller s face and looked', 'round at his audience they were in the dark and little spots of', 'colour swam before them the medical man seemed absorbed in the', 'contemplation of our host the editor was looking hard at the end', 'of his cigar the sixth the journalist fumbled for his watch the', 'others as far as i remember were motionless', '', 'the editor stood up with a sigh what a pity it is you re not', 'a writer of stories he said putting his hand on the time', 'traveller s shoulder', '', 'you don t believe it', '', 'well', '', 'i thought not', '', 'the time traveller turned to us where are the matches he said', 'he lit one and spoke over his pipe puffing to tell you the truth', 'i hardly believe it myself and yet', '', 'his eye fell with a mute inquiry upon the withered white flowers', 'upon the little table then he turned over the hand holding his', 'pipe and i saw he was looking at some half healed scars on his', 'knuckles', '', 'the medical man rose came to the lamp and examined the flowers', 'the gynaeceum s odd he said the psychologist leant forward to', 'see holding out his hand for a specimen', '', 'i m hanged if it isn t a quarter to one said the journalist', 'how shall we get home', '', 'plenty of cabs at the station said the psychologist', '', 'it s a curious thing said the medical man but i certainly don t', 'know the natural order of these flowers may i have them', '', 'the time traveller hesitated then suddenly certainly not', '', 'where did you really get them said the medical man', '', 'the time traveller put his hand to his head he spoke like one who', 'was trying to keep hold of an idea that eluded him they were put', 'into my pocket by weena when i travelled into time he stared', 'round the room i m damned if it isn t all going this room and you', 'and the atmosphere of every day is too much for my memory did i', 'ever make a time machine or a model of a time machine or is it all', 'only a dream they say life is a dream a precious poor dream at', 'times but i can t stand another that won t fit it s madness and', 'where did the dream come from i must look at that machine if', 'there is one', '', 'he caught up the lamp swiftly and carried it flaring red through', 'the door into the corridor we followed him there in the flickering', 'light of the lamp was the machine sure enough squat ugly and', 'askew a thing of brass ebony ivory and translucent glimmering', 'quartz solid to the touch for i put out my hand and felt the rail', 'of it and with brown spots and smears upon the ivory and bits of', 'grass and moss upon the lower parts and one rail bent awry', '', 'the time traveller put the lamp down on the bench and ran his hand', 'along the damaged rail it s all right now he said the story i', 'told you was true i m sorry to have brought you out here in the', 'cold he took up the lamp and in an absolute silence we', 'returned to the smoking room', '', 'he came into the hall with us and helped the editor on with his', 'coat the medical man looked into his face and with a certain', 'hesitation told him he was suffering from overwork at which he', 'laughed hugely i remember him standing in the open doorway bawling', 'good night', '', 'i shared a cab with the editor he thought the tale a gaudy lie', 'for my own part i was unable to come to a conclusion the story was', 'so fantastic and incredible the telling so credible and sober i', 'lay awake most of the night thinking about it i determined to go', 'next day and see the time traveller again i was told he was in the', 'laboratory and being on easy terms in the house i went up to him', 'the laboratory however was empty i stared for a minute at the', 'time machine and put out my hand and touched the lever at that the', 'squat substantial looking mass swayed like a bough shaken by the', 'wind its instability startled me extremely and i had a queer', 'reminiscence of the childish days when i used to be forbidden to', 'meddle i came back through the corridor the time traveller met me', 'in the smoking room he was coming from the house he had a small', 'camera under one arm and a knapsack under the other he laughed when', 'he saw me and gave me an elbow to shake i m frightfully busy', 'said he with that thing in there', '', 'but is it not some hoax i said do you really travel through', 'time', '', 'really and truly i do and he looked frankly into my eyes he', 'hesitated his eye wandered about the room i only want half an', 'hour he said i know why you came and it s awfully good of you', 'there s some magazines here if you ll stop to lunch i ll prove you', 'this time travelling up to the hilt specimen and all if you ll', 'forgive my leaving you now', '', 'i consented hardly comprehending then the full import of his words', 'and he nodded and went on down the corridor i heard the door of', 'the laboratory slam seated myself in a chair and took up a daily', 'paper what was he going to do before lunch time then suddenly', 'i was reminded by an advertisement that i had promised to meet', 'richardson the publisher at two i looked at my watch and saw', 'that i could barely save that engagement i got up and went down the', 'passage to tell the time traveller', '', 'as i took hold of the handle of the door i heard an exclamation', 'oddly truncated at the end and a click and a thud a gust of air', 'whirled round me as i opened the door and from within came the', 'sound of broken glass falling on the floor the time traveller was', 'not there i seemed to see a ghostly indistinct figure sitting in', 'a whirling mass of black and brass for a moment a figure so', 'transparent that the bench behind with its sheets of drawings was', 'absolutely distinct but this phantasm vanished as i rubbed my eyes', 'the time machine had gone save for a subsiding stir of dust the', 'further end of the laboratory was empty a pane of the skylight had', 'apparently just been blown in', '', 'i felt an unreasonable amazement i knew that something strange had', 'happened and for the moment could not distinguish what the strange', 'thing might be as i stood staring the door into the garden opened', 'and the man servant appeared', '', 'we looked at each other then ideas began to come has mr', 'gone out that way said i', '', 'no sir no one has come out this way i was expecting to find him', 'here', '', 'at that i understood at the risk of disappointing richardson i', 'stayed on waiting for the time traveller waiting for the second', 'perhaps still stranger story and the specimens and photographs he', 'would bring with him but i am beginning now to fear that i must', 'wait a lifetime the time traveller vanished three years ago and', 'as everybody knows now he has never returned', '', '', '', '', 'epilogue', '', '', 'one cannot choose but wonder will he ever return it may be that he', 'swept back into the past and fell among the blood drinking hairy', 'savages of the age of unpolished stone into the abysses of the', 'cretaceous sea or among the grotesque saurians the huge reptilian', 'brutes of the jurassic times he may even now if i may use the', 'phrase be wandering on some plesiosaurus haunted oolitic coral', 'reef or beside the lonely saline lakes of the triassic age or did', 'he go forward into one of the nearer ages in which men are still', 'men but with the riddles of our own time answered and its wearisome', 'problems solved into the manhood of the race for i for my own', 'part cannot think that these latter days of weak experiment', 'fragmentary theory and mutual discord are indeed man s culminating', 'time i say for my own part he i know for the question had been', 'discussed among us long before the time machine was made thought', 'but cheerlessly of the advancement of mankind and saw in the', 'growing pile of civilization only a foolish heaping that must', 'inevitably fall back upon and destroy its makers in the end if that', 'is so it remains for us to live as though it were not so but to me', 'the future is still black and blank is a vast ignorance lit at a', 'few casual places by the memory of his story and i have by me for', 'my comfort two strange white flowers shrivelled now and brown and', 'flat and brittle to witness that even when mind and strength had', 'gone gratitude and a mutual tenderness still lived on in the heart', 'of man']\n",
      "# 文本总行数: 3221\n",
      "0: the time machine by h g wells\n",
      "1: \n",
      "2: \n",
      "3: \n",
      "4: \n",
      "5: i\n",
      "6: \n",
      "7: \n",
      "8: the time traveller for so it will be convenient to speak of him\n",
      "9: was expounding a recondite matter to us his grey eyes shone and\n",
      "10: twinkled and his usually pale face was flushed and animated the\n",
      "11: fire burned brightly and the soft radiance of the incandescent\n",
      "12: lights in the lilies of silver caught the bubbles that flashed and\n",
      "13: passed in our glasses our chairs being his patents embraced and\n",
      "14: caressed us rather than submitted to be sat upon and there was that\n",
      "15: luxurious after dinner atmosphere when thought roams gracefully\n",
      "16: free of the trammels of precision and he put it to us in this\n",
      "17: way marking the points with a lean forefinger as we sat and lazily\n",
      "18: admired his earnestness over this new paradox as we thought it\n",
      "19: and his fecundity\n",
      "20: \n",
      "21: you must follow me carefully i shall have to controvert one or two\n",
      "22: ideas that are almost universally accepted the geometry for\n",
      "23: instance they taught you at school is founded on a misconception\n",
      "24: \n",
      "25: is not that rather a large thing to expect us to begin upon\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "# 下载到../data/timemachine.txt\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "\n",
    "# ['The machine is haha', '', '', ...]\n",
    "lines = read_time_machine()\n",
    "\n",
    "print('lines:', lines)\n",
    "print(f'# 文本总行数: {len(lines)}')\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    print(f'{i}: {line}')\n",
    "    if i == 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.2. <a id='toc11_2_3_2_'></a>[词元化（Tokenization）](#toc0_)\n",
    "分词的方法\n",
    "  - 基于`规则`的分词：使用预定义的规则或词典进行分割，适用于规则明确的语言（如英语）。  \n",
    "  - `统计学`分词：基于词频和共现统计进行分割，适用于无明显分词标志的语言（如中文）。\n",
    "  - `机器学习`分词：利用监督学习模型进行分割，能够学习上下文信息进行更准确的分词。\n",
    "\n",
    "词元化的类型\n",
    "  * 基于`词`的分词（Word-Based Tokenization）：按照word拆分成列表格式\n",
    "  * `子词`分词（Subword Tokenization）：词根表示。\n",
    "    - BPE（Byte Pair Encoding）\n",
    "    - WordPiece，主要用于Google的模型，如BERT。\n",
    "    - Unigram，主要用于BERT。\n",
    "  * 基于`字符`的分词（Character-Based Tokenization）：按照char拆分成列表格式。\n",
    "  * `语素`分词（Morpheme-Based Tokenization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: [['the', 'time', 'machine', 'by', 'h', 'g', 'wells'], [], [], [], [], ['i'], [], [], ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him'], ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and'], ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the'], ['fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent'], ['lights', 'in', 'the', 'lilies', 'of', 'silver', 'caught', 'the', 'bubbles', 'that', 'flashed', 'and'], ['passed', 'in', 'our', 'glasses', 'our', 'chairs', 'being', 'his', 'patents', 'embraced', 'and'], ['caressed', 'us', 'rather', 'than', 'submitted', 'to', 'be', 'sat', 'upon', 'and', 'there', 'was', 'that'], ['luxurious', 'after', 'dinner', 'atmosphere', 'when', 'thought', 'roams', 'gracefully'], ['free', 'of', 'the', 'trammels', 'of', 'precision', 'and', 'he', 'put', 'it', 'to', 'us', 'in', 'this'], ['way', 'marking', 'the', 'points', 'with', 'a', 'lean', 'forefinger', 'as', 'we', 'sat', 'and', 'lazily'], ['admired', 'his', 'earnestness', 'over', 'this', 'new', 'paradox', 'as', 'we', 'thought', 'it'], ['and', 'his', 'fecundity'], [], ['you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two'], ['ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for'], ['instance', 'they', 'taught', 'you', 'at', 'school', 'is', 'founded', 'on', 'a', 'misconception'], [], ['is', 'not', 'that', 'rather', 'a', 'large', 'thing', 'to', 'expect', 'us', 'to', 'begin', 'upon'], ['said', 'filby', 'an', 'argumentative', 'person', 'with', 'red', 'hair'], [], ['i', 'do', 'not', 'mean', 'to', 'ask', 'you', 'to', 'accept', 'anything', 'without', 'reasonable'], ['ground', 'for', 'it', 'you', 'will', 'soon', 'admit', 'as', 'much', 'as', 'i', 'need', 'from', 'you', 'you'], ['know', 'of', 'course', 'that', 'a', 'mathematical', 'line', 'a', 'line', 'of', 'thickness', 'nil'], ['has', 'no', 'real', 'existence', 'they', 'taught', 'you', 'that', 'neither', 'has', 'a'], ['mathematical', 'plane', 'these', 'things', 'are', 'mere', 'abstractions'], [], ['that', 'is', 'all', 'right', 'said', 'the', 'psychologist'], [], ['nor', 'having', 'only', 'length', 'breadth', 'and', 'thickness', 'can', 'a', 'cube', 'have', 'a'], ['real', 'existence'], [], ['there', 'i', 'object', 'said', 'filby', 'of', 'course', 'a', 'solid', 'body', 'may', 'exist', 'all'], ['real', 'things'], [], ['so', 'most', 'people', 'think', 'but', 'wait', 'a', 'moment', 'can', 'an', 'instantaneous'], ['cube', 'exist'], [], ['don', 't', 'follow', 'you', 'said', 'filby'], [], ['can', 'a', 'cube', 'that', 'does', 'not', 'last', 'for', 'any', 'time', 'at', 'all', 'have', 'a', 'real'], ['existence'], [], ['filby', 'became', 'pensive', 'clearly', 'the', 'time', 'traveller', 'proceeded', 'any'], ['real', 'body', 'must', 'have', 'extension', 'in', 'four', 'directions', 'it', 'must', 'have'], ['length', 'breadth', 'thickness', 'and', 'duration', 'but', 'through', 'a', 'natural'], ['infirmity', 'of', 'the', 'flesh', 'which', 'i', 'will', 'explain', 'to', 'you', 'in', 'a', 'moment', 'we'], ['incline', 'to', 'overlook', 'this', 'fact', 'there', 'are', 'really', 'four', 'dimensions'], ['three', 'which', 'we', 'call', 'the', 'three', 'planes', 'of', 'space', 'and', 'a', 'fourth', 'time'], ['there', 'is', 'however', 'a', 'tendency', 'to', 'draw', 'an', 'unreal', 'distinction', 'between'], ['the', 'former', 'three', 'dimensions', 'and', 'the', 'latter', 'because', 'it', 'happens', 'that'], ['our', 'consciousness', 'moves', 'intermittently', 'in', 'one', 'direction', 'along', 'the'], ['latter', 'from', 'the', 'beginning', 'to', 'the', 'end', 'of', 'our', 'lives'], [], ['that', 'said', 'a', 'very', 'young', 'man', 'making', 'spasmodic', 'efforts', 'to', 'relight'], ['his', 'cigar', 'over', 'the', 'lamp', 'that', 'very', 'clear', 'indeed'], [], ['now', 'it', 'is', 'very', 'remarkable', 'that', 'this', 'is', 'so', 'extensively', 'overlooked'], ['continued', 'the', 'time', 'traveller', 'with', 'a', 'slight', 'accession', 'of'], ['cheerfulness', 'really', 'this', 'is', 'what', 'is', 'meant', 'by', 'the', 'fourth', 'dimension'], ['though', 'some', 'people', 'who', 'talk', 'about', 'the', 'fourth', 'dimension', 'do', 'not', 'know'], ['they', 'mean', 'it', 'it', 'is', 'only', 'another', 'way', 'of', 'looking', 'at', 'time', 'there', 'is'], ['no', 'difference', 'between', 'time', 'and', 'any', 'of', 'the', 'three', 'dimensions', 'of', 'space'], ['except', 'that', 'our', 'consciousness', 'moves', 'along', 'it', 'but', 'some', 'foolish'], ['people', 'have', 'got', 'hold', 'of', 'the', 'wrong', 'side', 'of', 'that', 'idea', 'you', 'have', 'all'], ['heard', 'what', 'they', 'have', 'to', 'say', 'about', 'this', 'fourth', 'dimension'], [], ['i', 'have', 'not', 'said', 'the', 'provincial', 'mayor'], [], ['it', 'is', 'simply', 'this', 'that', 'space', 'as', 'our', 'mathematicians', 'have', 'it', 'is'], ['spoken', 'of', 'as', 'having', 'three', 'dimensions', 'which', 'one', 'may', 'call', 'length'], ['breadth', 'and', 'thickness', 'and', 'is', 'always', 'definable', 'by', 'reference', 'to'], ['three', 'planes', 'each', 'at', 'right', 'angles', 'to', 'the', 'others', 'but', 'some'], ['philosophical', 'people', 'have', 'been', 'asking', 'why', 'three', 'dimensions'], ['particularly', 'why', 'not', 'another', 'direction', 'at', 'right', 'angles', 'to', 'the', 'other'], ['three', 'and', 'have', 'even', 'tried', 'to', 'construct', 'a', 'four', 'dimension', 'geometry'], ['professor', 'simon', 'newcomb', 'was', 'expounding', 'this', 'to', 'the', 'new', 'york'], ['mathematical', 'society', 'only', 'a', 'month', 'or', 'so', 'ago', 'you', 'know', 'how', 'on', 'a', 'flat'], ['surface', 'which', 'has', 'only', 'two', 'dimensions', 'we', 'can', 'represent', 'a', 'figure', 'of'], ['a', 'three', 'dimensional', 'solid', 'and', 'similarly', 'they', 'think', 'that', 'by', 'models'], ['of', 'three', 'dimensions', 'they', 'could', 'represent', 'one', 'of', 'four', 'if', 'they', 'could'], ['master', 'the', 'perspective', 'of', 'the', 'thing', 'see'], [], ['i', 'think', 'so', 'murmured', 'the', 'provincial', 'mayor', 'and', 'knitting', 'his'], ['brows', 'he', 'lapsed', 'into', 'an', 'introspective', 'state', 'his', 'lips', 'moving', 'as', 'one'], ['who', 'repeats', 'mystic', 'words', 'yes', 'i', 'think', 'i', 'see', 'it', 'now', 'he', 'said', 'after'], ['some', 'time', 'brightening', 'in', 'a', 'quite', 'transitory', 'manner'], [], ['well', 'i', 'do', 'not', 'mind', 'telling', 'you', 'i', 'have', 'been', 'at', 'work', 'upon', 'this'], ['geometry', 'of', 'four', 'dimensions', 'for', 'some', 'time', 'some', 'of', 'my', 'results'], ['are', 'curious', 'for', 'instance', 'here', 'is', 'a', 'portrait', 'of', 'a', 'man', 'at', 'eight'], ['years', 'old', 'another', 'at', 'fifteen', 'another', 'at', 'seventeen', 'another', 'at'], ['twenty', 'three', 'and', 'so', 'on', 'all', 'these', 'are', 'evidently', 'sections', 'as', 'it'], ['were', 'three', 'dimensional', 'representations', 'of', 'his', 'four', 'dimensioned'], ['being', 'which', 'is', 'a', 'fixed', 'and', 'unalterable', 'thing'], [], ['scientific', 'people', 'proceeded', 'the', 'time', 'traveller', 'after', 'the', 'pause'], ['required', 'for', 'the', 'proper', 'assimilation', 'of', 'this', 'know', 'very', 'well', 'that'], ['time', 'is', 'only', 'a', 'kind', 'of', 'space', 'here', 'is', 'a', 'popular', 'scientific', 'diagram'], ['a', 'weather', 'record', 'this', 'line', 'i', 'trace', 'with', 'my', 'finger', 'shows', 'the'], ['movement', 'of', 'the', 'barometer', 'yesterday', 'it', 'was', 'so', 'high', 'yesterday', 'night'], ['it', 'fell', 'then', 'this', 'morning', 'it', 'rose', 'again', 'and', 'so', 'gently', 'upward', 'to'], ['here', 'surely', 'the', 'mercury', 'did', 'not', 'trace', 'this', 'line', 'in', 'any', 'of', 'the'], ['dimensions', 'of', 'space', 'generally', 'recognized', 'but', 'certainly', 'it', 'traced'], ['such', 'a', 'line', 'and', 'that', 'line', 'therefore', 'we', 'must', 'conclude', 'was', 'along'], ['the', 'time', 'dimension'], [], ['but', 'said', 'the', 'medical', 'man', 'staring', 'hard', 'at', 'a', 'coal', 'in', 'the', 'fire', 'if'], ['time', 'is', 'really', 'only', 'a', 'fourth', 'dimension', 'of', 'space', 'why', 'is', 'it', 'and', 'why'], ['has', 'it', 'always', 'been', 'regarded', 'as', 'something', 'different', 'and', 'why', 'cannot'], ['we', 'move', 'in', 'time', 'as', 'we', 'move', 'about', 'in', 'the', 'other', 'dimensions', 'of', 'space'], [], ['the', 'time', 'traveller', 'smiled', 'are', 'you', 'sure', 'we', 'can', 'move', 'freely', 'in'], ['space', 'right', 'and', 'left', 'we', 'can', 'go', 'backward', 'and', 'forward', 'freely', 'enough'], ['and', 'men', 'always', 'have', 'done', 'so', 'i', 'admit', 'we', 'move', 'freely', 'in', 'two'], ['dimensions', 'but', 'how', 'about', 'up', 'and', 'down', 'gravitation', 'limits', 'us', 'there'], [], ['not', 'exactly', 'said', 'the', 'medical', 'man', 'there', 'are', 'balloons'], [], ['but', 'before', 'the', 'balloons', 'save', 'for', 'spasmodic', 'jumping', 'and', 'the'], ['inequalities', 'of', 'the', 'surface', 'man', 'had', 'no', 'freedom', 'of', 'vertical'], ['movement'], [], ['still', 'they', 'could', 'move', 'a', 'little', 'up', 'and', 'down', 'said', 'the', 'medical', 'man'], [], ['easier', 'far', 'easier', 'down', 'than', 'up'], [], ['and', 'you', 'cannot', 'move', 'at', 'all', 'in', 'time', 'you', 'cannot', 'get', 'away', 'from', 'the'], ['present', 'moment'], [], ['my', 'dear', 'sir', 'that', 'is', 'just', 'where', 'you', 'are', 'wrong', 'that', 'is', 'just', 'where'], ['the', 'whole', 'world', 'has', 'gone', 'wrong', 'we', 'are', 'always', 'getting', 'away', 'from', 'the'], ['present', 'moment', 'our', 'mental', 'existences', 'which', 'are', 'immaterial', 'and', 'have'], ['no', 'dimensions', 'are', 'passing', 'along', 'the', 'time', 'dimension', 'with', 'a', 'uniform'], ['velocity', 'from', 'the', 'cradle', 'to', 'the', 'grave', 'just', 'as', 'we', 'should', 'travel', 'down'], ['if', 'we', 'began', 'our', 'existence', 'fifty', 'miles', 'above', 'the', 'earth', 's', 'surface'], [], ['but', 'the', 'great', 'difficulty', 'is', 'this', 'interrupted', 'the', 'psychologist'], ['you', 'can', 'move', 'about', 'in', 'all', 'directions', 'of', 'space', 'but', 'you', 'cannot'], ['move', 'about', 'in', 'time'], [], ['that', 'is', 'the', 'germ', 'of', 'my', 'great', 'discovery', 'but', 'you', 'are', 'wrong', 'to', 'say'], ['that', 'we', 'cannot', 'move', 'about', 'in', 'time', 'for', 'instance', 'if', 'i', 'am', 'recalling'], ['an', 'incident', 'very', 'vividly', 'i', 'go', 'back', 'to', 'the', 'instant', 'of', 'its', 'occurrence'], ['i', 'become', 'absent', 'minded', 'as', 'you', 'say', 'i', 'jump', 'back', 'for', 'a', 'moment', 'of'], ['course', 'we', 'have', 'no', 'means', 'of', 'staying', 'back', 'for', 'any', 'length', 'of', 'time', 'any'], ['more', 'than', 'a', 'savage', 'or', 'an', 'animal', 'has', 'of', 'staying', 'six', 'feet', 'above', 'the'], ['ground', 'but', 'a', 'civilized', 'man', 'is', 'better', 'off', 'than', 'the', 'savage', 'in', 'this'], ['respect', 'he', 'can', 'go', 'up', 'against', 'gravitation', 'in', 'a', 'balloon', 'and', 'why'], ['should', 'he', 'not', 'hope', 'that', 'ultimately', 'he', 'may', 'be', 'able', 'to', 'stop', 'or'], ['accelerate', 'his', 'drift', 'along', 'the', 'time', 'dimension', 'or', 'even', 'turn', 'about'], ['and', 'travel', 'the', 'other', 'way'], [], ['oh', 'this', 'began', 'filby', 'is', 'all'], [], ['why', 'not', 'said', 'the', 'time', 'traveller'], [], ['it', 's', 'against', 'reason', 'said', 'filby'], [], ['what', 'reason', 'said', 'the', 'time', 'traveller'], [], ['you', 'can', 'show', 'black', 'is', 'white', 'by', 'argument', 'said', 'filby', 'but', 'you', 'will'], ['never', 'convince', 'me'], [], ['possibly', 'not', 'said', 'the', 'time', 'traveller', 'but', 'now', 'you', 'begin', 'to', 'see'], ['the', 'object', 'of', 'my', 'investigations', 'into', 'the', 'geometry', 'of', 'four'], ['dimensions', 'long', 'ago', 'i', 'had', 'a', 'vague', 'inkling', 'of', 'a', 'machine'], [], ['to', 'travel', 'through', 'time', 'exclaimed', 'the', 'very', 'young', 'man'], [], ['that', 'shall', 'travel', 'indifferently', 'in', 'any', 'direction', 'of', 'space', 'and', 'time'], ['as', 'the', 'driver', 'determines'], [], ['filby', 'contented', 'himself', 'with', 'laughter'], [], ['but', 'i', 'have', 'experimental', 'verification', 'said', 'the', 'time', 'traveller'], [], ['it', 'would', 'be', 'remarkably', 'convenient', 'for', 'the', 'historian', 'the'], ['psychologist', 'suggested', 'one', 'might', 'travel', 'back', 'and', 'verify', 'the'], ['accepted', 'account', 'of', 'the', 'battle', 'of', 'hastings', 'for', 'instance'], [], ['don', 't', 'you', 'think', 'you', 'would', 'attract', 'attention', 'said', 'the', 'medical', 'man'], ['our', 'ancestors', 'had', 'no', 'great', 'tolerance', 'for', 'anachronisms'], [], ['one', 'might', 'get', 'one', 's', 'greek', 'from', 'the', 'very', 'lips', 'of', 'homer', 'and', 'plato'], ['the', 'very', 'young', 'man', 'thought'], [], ['in', 'which', 'case', 'they', 'would', 'certainly', 'plough', 'you', 'for', 'the', 'little', 'go'], ['the', 'german', 'scholars', 'have', 'improved', 'greek', 'so', 'much'], [], ['then', 'there', 'is', 'the', 'future', 'said', 'the', 'very', 'young', 'man', 'just', 'think'], ['one', 'might', 'invest', 'all', 'one', 's', 'money', 'leave', 'it', 'to', 'accumulate', 'at'], ['interest', 'and', 'hurry', 'on', 'ahead'], [], ['to', 'discover', 'a', 'society', 'said', 'i', 'erected', 'on', 'a', 'strictly', 'communistic'], ['basis'], [], ['of', 'all', 'the', 'wild', 'extravagant', 'theories', 'began', 'the', 'psychologist'], [], ['yes', 'so', 'it', 'seemed', 'to', 'me', 'and', 'so', 'i', 'never', 'talked', 'of', 'it', 'until'], [], ['experimental', 'verification', 'cried', 'i', 'you', 'are', 'going', 'to', 'verify'], ['that'], [], ['the', 'experiment', 'cried', 'filby', 'who', 'was', 'getting', 'brain', 'weary'], [], ['let', 's', 'see', 'your', 'experiment', 'anyhow', 'said', 'the', 'psychologist', 'though'], ['it', 's', 'all', 'humbug', 'you', 'know'], [], ['the', 'time', 'traveller', 'smiled', 'round', 'at', 'us', 'then', 'still', 'smiling', 'faintly'], ['and', 'with', 'his', 'hands', 'deep', 'in', 'his', 'trousers', 'pockets', 'he', 'walked', 'slowly'], ['out', 'of', 'the', 'room', 'and', 'we', 'heard', 'his', 'slippers', 'shuffling', 'down', 'the', 'long'], ['passage', 'to', 'his', 'laboratory'], [], ['the', 'psychologist', 'looked', 'at', 'us', 'i', 'wonder', 'what', 'he', 's', 'got'], [], ['some', 'sleight', 'of', 'hand', 'trick', 'or', 'other', 'said', 'the', 'medical', 'man', 'and'], ['filby', 'tried', 'to', 'tell', 'us', 'about', 'a', 'conjurer', 'he', 'had', 'seen', 'at', 'burslem', 'but'], ['before', 'he', 'had', 'finished', 'his', 'preface', 'the', 'time', 'traveller', 'came', 'back', 'and'], ['filby', 's', 'anecdote', 'collapsed'], [], ['the', 'thing', 'the', 'time', 'traveller', 'held', 'in', 'his', 'hand', 'was', 'a', 'glittering'], ['metallic', 'framework', 'scarcely', 'larger', 'than', 'a', 'small', 'clock', 'and', 'very'], ['delicately', 'made', 'there', 'was', 'ivory', 'in', 'it', 'and', 'some', 'transparent'], ['crystalline', 'substance', 'and', 'now', 'i', 'must', 'be', 'explicit', 'for', 'this', 'that'], ['follows', 'unless', 'his', 'explanation', 'is', 'to', 'be', 'accepted', 'is', 'an', 'absolutely'], ['unaccountable', 'thing', 'he', 'took', 'one', 'of', 'the', 'small', 'octagonal', 'tables', 'that'], ['were', 'scattered', 'about', 'the', 'room', 'and', 'set', 'it', 'in', 'front', 'of', 'the', 'fire', 'with'], ['two', 'legs', 'on', 'the', 'hearthrug', 'on', 'this', 'table', 'he', 'placed', 'the', 'mechanism'], ['then', 'he', 'drew', 'up', 'a', 'chair', 'and', 'sat', 'down', 'the', 'only', 'other', 'object', 'on', 'the'], ['table', 'was', 'a', 'small', 'shaded', 'lamp', 'the', 'bright', 'light', 'of', 'which', 'fell', 'upon'], ['the', 'model', 'there', 'were', 'also', 'perhaps', 'a', 'dozen', 'candles', 'about', 'two', 'in'], ['brass', 'candlesticks', 'upon', 'the', 'mantel', 'and', 'several', 'in', 'sconces', 'so', 'that'], ['the', 'room', 'was', 'brilliantly', 'illuminated', 'i', 'sat', 'in', 'a', 'low', 'arm', 'chair'], ['nearest', 'the', 'fire', 'and', 'i', 'drew', 'this', 'forward', 'so', 'as', 'to', 'be', 'almost', 'between'], ['the', 'time', 'traveller', 'and', 'the', 'fireplace', 'filby', 'sat', 'behind', 'him', 'looking'], ['over', 'his', 'shoulder', 'the', 'medical', 'man', 'and', 'the', 'provincial', 'mayor', 'watched'], ['him', 'in', 'profile', 'from', 'the', 'right', 'the', 'psychologist', 'from', 'the', 'left', 'the'], ['very', 'young', 'man', 'stood', 'behind', 'the', 'psychologist', 'we', 'were', 'all', 'on', 'the'], ['alert', 'it', 'appears', 'incredible', 'to', 'me', 'that', 'any', 'kind', 'of', 'trick', 'however'], ['subtly', 'conceived', 'and', 'however', 'adroitly', 'done', 'could', 'have', 'been', 'played'], ['upon', 'us', 'under', 'these', 'conditions'], [], ['the', 'time', 'traveller', 'looked', 'at', 'us', 'and', 'then', 'at', 'the', 'mechanism', 'well'], ['said', 'the', 'psychologist'], [], ['this', 'little', 'affair', 'said', 'the', 'time', 'traveller', 'resting', 'his', 'elbows'], ['upon', 'the', 'table', 'and', 'pressing', 'his', 'hands', 'together', 'above', 'the', 'apparatus'], ['is', 'only', 'a', 'model', 'it', 'is', 'my', 'plan', 'for', 'a', 'machine', 'to', 'travel', 'through'], ['time', 'you', 'will', 'notice', 'that', 'it', 'looks', 'singularly', 'askew', 'and', 'that', 'there'], ['is', 'an', 'odd', 'twinkling', 'appearance', 'about', 'this', 'bar', 'as', 'though', 'it', 'was', 'in'], ['some', 'way', 'unreal', 'he', 'pointed', 'to', 'the', 'part', 'with', 'his', 'finger', 'also'], ['here', 'is', 'one', 'little', 'white', 'lever', 'and', 'here', 'is', 'another'], [], ['the', 'medical', 'man', 'got', 'up', 'out', 'of', 'his', 'chair', 'and', 'peered', 'into', 'the', 'thing'], ['it', 's', 'beautifully', 'made', 'he', 'said'], [], ['it', 'took', 'two', 'years', 'to', 'make', 'retorted', 'the', 'time', 'traveller', 'then', 'when'], ['we', 'had', 'all', 'imitated', 'the', 'action', 'of', 'the', 'medical', 'man', 'he', 'said', 'now', 'i'], ['want', 'you', 'clearly', 'to', 'understand', 'that', 'this', 'lever', 'being', 'pressed', 'over'], ['sends', 'the', 'machine', 'gliding', 'into', 'the', 'future', 'and', 'this', 'other', 'reverses'], ['the', 'motion', 'this', 'saddle', 'represents', 'the', 'seat', 'of', 'a', 'time', 'traveller'], ['presently', 'i', 'am', 'going', 'to', 'press', 'the', 'lever', 'and', 'off', 'the', 'machine', 'will'], ['go', 'it', 'will', 'vanish', 'pass', 'into', 'future', 'time', 'and', 'disappear', 'have', 'a'], ['good', 'look', 'at', 'the', 'thing', 'look', 'at', 'the', 'table', 'too', 'and', 'satisfy'], ['yourselves', 'there', 'is', 'no', 'trickery', 'i', 'don', 't', 'want', 'to', 'waste', 'this', 'model'], ['and', 'then', 'be', 'told', 'i', 'm', 'a', 'quack'], [], ['there', 'was', 'a', 'minute', 's', 'pause', 'perhaps', 'the', 'psychologist', 'seemed', 'about', 'to'], ['speak', 'to', 'me', 'but', 'changed', 'his', 'mind', 'then', 'the', 'time', 'traveller', 'put', 'forth'], ['his', 'finger', 'towards', 'the', 'lever', 'no', 'he', 'said', 'suddenly', 'lend', 'me', 'your'], ['hand', 'and', 'turning', 'to', 'the', 'psychologist', 'he', 'took', 'that', 'individual', 's'], ['hand', 'in', 'his', 'own', 'and', 'told', 'him', 'to', 'put', 'out', 'his', 'forefinger', 'so', 'that', 'it'], ['was', 'the', 'psychologist', 'himself', 'who', 'sent', 'forth', 'the', 'model', 'time', 'machine'], ['on', 'its', 'interminable', 'voyage', 'we', 'all', 'saw', 'the', 'lever', 'turn', 'i', 'am'], ['absolutely', 'certain', 'there', 'was', 'no', 'trickery', 'there', 'was', 'a', 'breath', 'of'], ['wind', 'and', 'the', 'lamp', 'flame', 'jumped', 'one', 'of', 'the', 'candles', 'on', 'the', 'mantel'], ['was', 'blown', 'out', 'and', 'the', 'little', 'machine', 'suddenly', 'swung', 'round', 'became'], ['indistinct', 'was', 'seen', 'as', 'a', 'ghost', 'for', 'a', 'second', 'perhaps', 'as', 'an', 'eddy', 'of'], ['faintly', 'glittering', 'brass', 'and', 'ivory', 'and', 'it', 'was', 'gone', 'vanished', 'save'], ['for', 'the', 'lamp', 'the', 'table', 'was', 'bare'], [], ['everyone', 'was', 'silent', 'for', 'a', 'minute', 'then', 'filby', 'said', 'he', 'was', 'damned'], [], ['the', 'psychologist', 'recovered', 'from', 'his', 'stupor', 'and', 'suddenly', 'looked'], ['under', 'the', 'table', 'at', 'that', 'the', 'time', 'traveller', 'laughed', 'cheerfully'], ['well', 'he', 'said', 'with', 'a', 'reminiscence', 'of', 'the', 'psychologist', 'then'], ['getting', 'up', 'he', 'went', 'to', 'the', 'tobacco', 'jar', 'on', 'the', 'mantel', 'and', 'with', 'his'], ['back', 'to', 'us', 'began', 'to', 'fill', 'his', 'pipe'], [], ['we', 'stared', 'at', 'each', 'other', 'look', 'here', 'said', 'the', 'medical', 'man', 'are', 'you'], ['in', 'earnest', 'about', 'this', 'do', 'you', 'seriously', 'believe', 'that', 'that', 'machine'], ['has', 'travelled', 'into', 'time'], [], ['certainly', 'said', 'the', 'time', 'traveller', 'stooping', 'to', 'light', 'a', 'spill', 'at'], ['the', 'fire', 'then', 'he', 'turned', 'lighting', 'his', 'pipe', 'to', 'look', 'at', 'the'], ['psychologist', 's', 'face', 'the', 'psychologist', 'to', 'show', 'that', 'he', 'was', 'not'], ['unhinged', 'helped', 'himself', 'to', 'a', 'cigar', 'and', 'tried', 'to', 'light', 'it', 'uncut'], ['what', 'is', 'more', 'i', 'have', 'a', 'big', 'machine', 'nearly', 'finished', 'in', 'there', 'he'], ['indicated', 'the', 'laboratory', 'and', 'when', 'that', 'is', 'put', 'together', 'i', 'mean', 'to'], ['have', 'a', 'journey', 'on', 'my', 'own', 'account'], [], ['you', 'mean', 'to', 'say', 'that', 'that', 'machine', 'has', 'travelled', 'into', 'the', 'future'], ['said', 'filby'], [], ['into', 'the', 'future', 'or', 'the', 'past', 'i', 'don', 't', 'for', 'certain', 'know', 'which'], [], ['after', 'an', 'interval', 'the', 'psychologist', 'had', 'an', 'inspiration', 'it', 'must', 'have'], ['gone', 'into', 'the', 'past', 'if', 'it', 'has', 'gone', 'anywhere', 'he', 'said'], [], ['why', 'said', 'the', 'time', 'traveller'], [], ['because', 'i', 'presume', 'that', 'it', 'has', 'not', 'moved', 'in', 'space', 'and', 'if', 'it'], ['travelled', 'into', 'the', 'future', 'it', 'would', 'still', 'be', 'here', 'all', 'this', 'time'], ['since', 'it', 'must', 'have', 'travelled', 'through', 'this', 'time'], [], ['but', 'i', 'said', 'if', 'it', 'travelled', 'into', 'the', 'past', 'it', 'would', 'have', 'been'], ['visible', 'when', 'we', 'came', 'first', 'into', 'this', 'room', 'and', 'last', 'thursday', 'when', 'we'], ['were', 'here', 'and', 'the', 'thursday', 'before', 'that', 'and', 'so', 'forth'], [], ['serious', 'objections', 'remarked', 'the', 'provincial', 'mayor', 'with', 'an', 'air', 'of'], ['impartiality', 'turning', 'towards', 'the', 'time', 'traveller'], [], ['not', 'a', 'bit', 'said', 'the', 'time', 'traveller', 'and', 'to', 'the', 'psychologist', 'you'], ['think', 'you', 'can', 'explain', 'that', 'it', 's', 'presentation', 'below', 'the', 'threshold'], ['you', 'know', 'diluted', 'presentation'], [], ['of', 'course', 'said', 'the', 'psychologist', 'and', 'reassured', 'us', 'that', 's', 'a'], ['simple', 'point', 'of', 'psychology', 'i', 'should', 'have', 'thought', 'of', 'it', 'it', 's', 'plain'], ['enough', 'and', 'helps', 'the', 'paradox', 'delightfully', 'we', 'cannot', 'see', 'it', 'nor'], ['can', 'we', 'appreciate', 'this', 'machine', 'any', 'more', 'than', 'we', 'can', 'the', 'spoke', 'of'], ['a', 'wheel', 'spinning', 'or', 'a', 'bullet', 'flying', 'through', 'the', 'air', 'if', 'it', 'is'], ['travelling', 'through', 'time', 'fifty', 'times', 'or', 'a', 'hundred', 'times', 'faster', 'than'], ['we', 'are', 'if', 'it', 'gets', 'through', 'a', 'minute', 'while', 'we', 'get', 'through', 'a', 'second'], ['the', 'impression', 'it', 'creates', 'will', 'of', 'course', 'be', 'only', 'one', 'fiftieth', 'or'], ['one', 'hundredth', 'of', 'what', 'it', 'would', 'make', 'if', 'it', 'were', 'not', 'travelling', 'in'], ['time', 'that', 's', 'plain', 'enough', 'he', 'passed', 'his', 'hand', 'through', 'the', 'space', 'in'], ['which', 'the', 'machine', 'had', 'been', 'you', 'see', 'he', 'said', 'laughing'], [], ['we', 'sat', 'and', 'stared', 'at', 'the', 'vacant', 'table', 'for', 'a', 'minute', 'or', 'so', 'then', 'the'], ['time', 'traveller', 'asked', 'us', 'what', 'we', 'thought', 'of', 'it', 'all'], [], ['it', 'sounds', 'plausible', 'enough', 'to', 'night', 'said', 'the', 'medical', 'man', 'but'], ['wait', 'until', 'to', 'morrow', 'wait', 'for', 'the', 'common', 'sense', 'of', 'the', 'morning'], [], ['would', 'you', 'like', 'to', 'see', 'the', 'time', 'machine', 'itself', 'asked', 'the', 'time'], ['traveller', 'and', 'therewith', 'taking', 'the', 'lamp', 'in', 'his', 'hand', 'he', 'led', 'the'], ['way', 'down', 'the', 'long', 'draughty', 'corridor', 'to', 'his', 'laboratory', 'i', 'remember'], ['vividly', 'the', 'flickering', 'light', 'his', 'queer', 'broad', 'head', 'in', 'silhouette'], ['the', 'dance', 'of', 'the', 'shadows', 'how', 'we', 'all', 'followed', 'him', 'puzzled', 'but'], ['incredulous', 'and', 'how', 'there', 'in', 'the', 'laboratory', 'we', 'beheld', 'a', 'larger'], ['edition', 'of', 'the', 'little', 'mechanism', 'which', 'we', 'had', 'seen', 'vanish', 'from', 'before'], ['our', 'eyes', 'parts', 'were', 'of', 'nickel', 'parts', 'of', 'ivory', 'parts', 'had', 'certainly'], ['been', 'filed', 'or', 'sawn', 'out', 'of', 'rock', 'crystal', 'the', 'thing', 'was', 'generally'], ['complete', 'but', 'the', 'twisted', 'crystalline', 'bars', 'lay', 'unfinished', 'upon', 'the'], ['bench', 'beside', 'some', 'sheets', 'of', 'drawings', 'and', 'i', 'took', 'one', 'up', 'for', 'a', 'better'], ['look', 'at', 'it', 'quartz', 'it', 'seemed', 'to', 'be'], [], ['look', 'here', 'said', 'the', 'medical', 'man', 'are', 'you', 'perfectly', 'serious'], ['or', 'is', 'this', 'a', 'trick', 'like', 'that', 'ghost', 'you', 'showed', 'us', 'last', 'christmas'], [], ['upon', 'that', 'machine', 'said', 'the', 'time', 'traveller', 'holding', 'the', 'lamp'], ['aloft', 'i', 'intend', 'to', 'explore', 'time', 'is', 'that', 'plain', 'i', 'was', 'never', 'more'], ['serious', 'in', 'my', 'life'], [], ['none', 'of', 'us', 'quite', 'knew', 'how', 'to', 'take', 'it'], [], ['i', 'caught', 'filby', 's', 'eye', 'over', 'the', 'shoulder', 'of', 'the', 'medical', 'man', 'and', 'he'], ['winked', 'at', 'me', 'solemnly'], [], [], [], [], ['ii'], [], [], ['i', 'think', 'that', 'at', 'that', 'time', 'none', 'of', 'us', 'quite', 'believed', 'in', 'the', 'time'], ['machine', 'the', 'fact', 'is', 'the', 'time', 'traveller', 'was', 'one', 'of', 'those', 'men', 'who'], ['are', 'too', 'clever', 'to', 'be', 'believed', 'you', 'never', 'felt', 'that', 'you', 'saw', 'all', 'round'], ['him', 'you', 'always', 'suspected', 'some', 'subtle', 'reserve', 'some', 'ingenuity', 'in'], ['ambush', 'behind', 'his', 'lucid', 'frankness', 'had', 'filby', 'shown', 'the', 'model', 'and'], ['explained', 'the', 'matter', 'in', 'the', 'time', 'traveller', 's', 'words', 'we', 'should', 'have'], ['shown', 'him', 'far', 'less', 'scepticism', 'for', 'we', 'should', 'have', 'perceived', 'his'], ['motives', 'a', 'pork', 'butcher', 'could', 'understand', 'filby', 'but', 'the', 'time'], ['traveller', 'had', 'more', 'than', 'a', 'touch', 'of', 'whim', 'among', 'his', 'elements', 'and', 'we'], ['distrusted', 'him', 'things', 'that', 'would', 'have', 'made', 'the', 'frame', 'of', 'a', 'less'], ['clever', 'man', 'seemed', 'tricks', 'in', 'his', 'hands', 'it', 'is', 'a', 'mistake', 'to', 'do', 'things'], ['too', 'easily', 'the', 'serious', 'people', 'who', 'took', 'him', 'seriously', 'never', 'felt'], ['quite', 'sure', 'of', 'his', 'deportment', 'they', 'were', 'somehow', 'aware', 'that', 'trusting'], ['their', 'reputations', 'for', 'judgment', 'with', 'him', 'was', 'like', 'furnishing', 'a'], ['nursery', 'with', 'egg', 'shell', 'china', 'so', 'i', 'don', 't', 'think', 'any', 'of', 'us', 'said', 'very'], ['much', 'about', 'time', 'travelling', 'in', 'the', 'interval', 'between', 'that', 'thursday', 'and'], ['the', 'next', 'though', 'its', 'odd', 'potentialities', 'ran', 'no', 'doubt', 'in', 'most', 'of'], ['our', 'minds', 'its', 'plausibility', 'that', 'is', 'its', 'practical', 'incredibleness'], ['the', 'curious', 'possibilities', 'of', 'anachronism', 'and', 'of', 'utter', 'confusion', 'it'], ['suggested', 'for', 'my', 'own', 'part', 'i', 'was', 'particularly', 'preoccupied', 'with', 'the'], ['trick', 'of', 'the', 'model', 'that', 'i', 'remember', 'discussing', 'with', 'the', 'medical', 'man'], ['whom', 'i', 'met', 'on', 'friday', 'at', 'the', 'linnaean', 'he', 'said', 'he', 'had', 'seen', 'a', 'similar'], ['thing', 'at', 'tubingen', 'and', 'laid', 'considerable', 'stress', 'on', 'the', 'blowing', 'out'], ['of', 'the', 'candle', 'but', 'how', 'the', 'trick', 'was', 'done', 'he', 'could', 'not', 'explain'], [], ['the', 'next', 'thursday', 'i', 'went', 'again', 'to', 'richmond', 'i', 'suppose', 'i', 'was', 'one', 'of'], ['the', 'time', 'traveller', 's', 'most', 'constant', 'guests', 'and', 'arriving', 'late', 'found'], ['four', 'or', 'five', 'men', 'already', 'assembled', 'in', 'his', 'drawing', 'room', 'the', 'medical'], ['man', 'was', 'standing', 'before', 'the', 'fire', 'with', 'a', 'sheet', 'of', 'paper', 'in', 'one', 'hand'], ['and', 'his', 'watch', 'in', 'the', 'other', 'i', 'looked', 'round', 'for', 'the', 'time', 'traveller'], ['and', 'it', 's', 'half', 'past', 'seven', 'now', 'said', 'the', 'medical', 'man', 'i', 'suppose'], ['we', 'd', 'better', 'have', 'dinner'], [], ['where', 's', 'said', 'i', 'naming', 'our', 'host'], [], ['you', 've', 'just', 'come', 'it', 's', 'rather', 'odd', 'he', 's', 'unavoidably', 'detained', 'he'], ['asks', 'me', 'in', 'this', 'note', 'to', 'lead', 'off', 'with', 'dinner', 'at', 'seven', 'if', 'he', 's', 'not'], ['back', 'says', 'he', 'll', 'explain', 'when', 'he', 'comes'], [], ['it', 'seems', 'a', 'pity', 'to', 'let', 'the', 'dinner', 'spoil', 'said', 'the', 'editor', 'of', 'a'], ['well', 'known', 'daily', 'paper', 'and', 'thereupon', 'the', 'doctor', 'rang', 'the', 'bell'], [], ['the', 'psychologist', 'was', 'the', 'only', 'person', 'besides', 'the', 'doctor', 'and', 'myself'], ['who', 'had', 'attended', 'the', 'previous', 'dinner', 'the', 'other', 'men', 'were', 'blank', 'the'], ['editor', 'aforementioned', 'a', 'certain', 'journalist', 'and', 'another', 'a', 'quiet'], ['shy', 'man', 'with', 'a', 'beard', 'whom', 'i', 'didn', 't', 'know', 'and', 'who', 'as', 'far', 'as', 'my'], ['observation', 'went', 'never', 'opened', 'his', 'mouth', 'all', 'the', 'evening', 'there', 'was'], ['some', 'speculation', 'at', 'the', 'dinner', 'table', 'about', 'the', 'time', 'traveller', 's'], ['absence', 'and', 'i', 'suggested', 'time', 'travelling', 'in', 'a', 'half', 'jocular', 'spirit'], ['the', 'editor', 'wanted', 'that', 'explained', 'to', 'him', 'and', 'the', 'psychologist'], ['volunteered', 'a', 'wooden', 'account', 'of', 'the', 'ingenious', 'paradox', 'and', 'trick', 'we'], ['had', 'witnessed', 'that', 'day', 'week', 'he', 'was', 'in', 'the', 'midst', 'of', 'his', 'exposition'], ['when', 'the', 'door', 'from', 'the', 'corridor', 'opened', 'slowly', 'and', 'without', 'noise', 'i'], ['was', 'facing', 'the', 'door', 'and', 'saw', 'it', 'first', 'hallo', 'i', 'said', 'at', 'last'], ['and', 'the', 'door', 'opened', 'wider', 'and', 'the', 'time', 'traveller', 'stood', 'before', 'us'], ['i', 'gave', 'a', 'cry', 'of', 'surprise', 'good', 'heavens', 'man', 'what', 's', 'the', 'matter'], ['cried', 'the', 'medical', 'man', 'who', 'saw', 'him', 'next', 'and', 'the', 'whole', 'tableful'], ['turned', 'towards', 'the', 'door'], [], ['he', 'was', 'in', 'an', 'amazing', 'plight', 'his', 'coat', 'was', 'dusty', 'and', 'dirty', 'and'], ['smeared', 'with', 'green', 'down', 'the', 'sleeves', 'his', 'hair', 'disordered', 'and', 'as', 'it'], ['seemed', 'to', 'me', 'greyer', 'either', 'with', 'dust', 'and', 'dirt', 'or', 'because', 'its', 'colour'], ['had', 'actually', 'faded', 'his', 'face', 'was', 'ghastly', 'pale', 'his', 'chin', 'had', 'a', 'brown'], ['cut', 'on', 'it', 'a', 'cut', 'half', 'healed', 'his', 'expression', 'was', 'haggard', 'and', 'drawn'], ['as', 'by', 'intense', 'suffering', 'for', 'a', 'moment', 'he', 'hesitated', 'in', 'the', 'doorway'], ['as', 'if', 'he', 'had', 'been', 'dazzled', 'by', 'the', 'light', 'then', 'he', 'came', 'into', 'the', 'room'], ['he', 'walked', 'with', 'just', 'such', 'a', 'limp', 'as', 'i', 'have', 'seen', 'in', 'footsore', 'tramps'], ['we', 'stared', 'at', 'him', 'in', 'silence', 'expecting', 'him', 'to', 'speak'], [], ['he', 'said', 'not', 'a', 'word', 'but', 'came', 'painfully', 'to', 'the', 'table', 'and', 'made', 'a'], ['motion', 'towards', 'the', 'wine', 'the', 'editor', 'filled', 'a', 'glass', 'of', 'champagne', 'and'], ['pushed', 'it', 'towards', 'him', 'he', 'drained', 'it', 'and', 'it', 'seemed', 'to', 'do', 'him', 'good'], ['for', 'he', 'looked', 'round', 'the', 'table', 'and', 'the', 'ghost', 'of', 'his', 'old', 'smile'], ['flickered', 'across', 'his', 'face', 'what', 'on', 'earth', 'have', 'you', 'been', 'up', 'to', 'man'], ['said', 'the', 'doctor', 'the', 'time', 'traveller', 'did', 'not', 'seem', 'to', 'hear', 'don', 't', 'let'], ['me', 'disturb', 'you', 'he', 'said', 'with', 'a', 'certain', 'faltering', 'articulation'], ['i', 'm', 'all', 'right', 'he', 'stopped', 'held', 'out', 'his', 'glass', 'for', 'more', 'and', 'took'], ['it', 'off', 'at', 'a', 'draught', 'that', 's', 'good', 'he', 'said', 'his', 'eyes', 'grew', 'brighter'], ['and', 'a', 'faint', 'colour', 'came', 'into', 'his', 'cheeks', 'his', 'glance', 'flickered', 'over'], ['our', 'faces', 'with', 'a', 'certain', 'dull', 'approval', 'and', 'then', 'went', 'round', 'the', 'warm'], ['and', 'comfortable', 'room', 'then', 'he', 'spoke', 'again', 'still', 'as', 'it', 'were', 'feeling'], ['his', 'way', 'among', 'his', 'words', 'i', 'm', 'going', 'to', 'wash', 'and', 'dress', 'and', 'then', 'i', 'll'], ['come', 'down', 'and', 'explain', 'things', 'save', 'me', 'some', 'of', 'that', 'mutton', 'i', 'm'], ['starving', 'for', 'a', 'bit', 'of', 'meat'], [], ['he', 'looked', 'across', 'at', 'the', 'editor', 'who', 'was', 'a', 'rare', 'visitor', 'and', 'hoped', 'he'], ['was', 'all', 'right', 'the', 'editor', 'began', 'a', 'question', 'tell', 'you', 'presently'], ['said', 'the', 'time', 'traveller', 'i', 'm', 'funny', 'be', 'all', 'right', 'in', 'a', 'minute'], [], ['he', 'put', 'down', 'his', 'glass', 'and', 'walked', 'towards', 'the', 'staircase', 'door', 'again'], ['i', 'remarked', 'his', 'lameness', 'and', 'the', 'soft', 'padding', 'sound', 'of', 'his', 'footfall'], ['and', 'standing', 'up', 'in', 'my', 'place', 'i', 'saw', 'his', 'feet', 'as', 'he', 'went', 'out', 'he', 'had'], ['nothing', 'on', 'them', 'but', 'a', 'pair', 'of', 'tattered', 'blood', 'stained', 'socks', 'then', 'the'], ['door', 'closed', 'upon', 'him', 'i', 'had', 'half', 'a', 'mind', 'to', 'follow', 'till', 'i', 'remembered'], ['how', 'he', 'detested', 'any', 'fuss', 'about', 'himself', 'for', 'a', 'minute', 'perhaps', 'my'], ['mind', 'was', 'wool', 'gathering', 'then', 'remarkable', 'behaviour', 'of', 'an', 'eminent'], ['scientist', 'i', 'heard', 'the', 'editor', 'say', 'thinking', 'after', 'his', 'wont', 'in'], ['headlines', 'and', 'this', 'brought', 'my', 'attention', 'back', 'to', 'the', 'bright'], ['dinner', 'table'], [], ['what', 's', 'the', 'game', 'said', 'the', 'journalist', 'has', 'he', 'been', 'doing', 'the'], ['amateur', 'cadger', 'i', 'don', 't', 'follow', 'i', 'met', 'the', 'eye', 'of', 'the', 'psychologist'], ['and', 'read', 'my', 'own', 'interpretation', 'in', 'his', 'face', 'i', 'thought', 'of', 'the', 'time'], ['traveller', 'limping', 'painfully', 'upstairs', 'i', 'don', 't', 'think', 'any', 'one', 'else', 'had'], ['noticed', 'his', 'lameness'], [], ['the', 'first', 'to', 'recover', 'completely', 'from', 'this', 'surprise', 'was', 'the', 'medical'], ['man', 'who', 'rang', 'the', 'bell', 'the', 'time', 'traveller', 'hated', 'to', 'have', 'servants'], ['waiting', 'at', 'dinner', 'for', 'a', 'hot', 'plate', 'at', 'that', 'the', 'editor', 'turned', 'to', 'his'], ['knife', 'and', 'fork', 'with', 'a', 'grunt', 'and', 'the', 'silent', 'man', 'followed', 'suit', 'the'], ['dinner', 'was', 'resumed', 'conversation', 'was', 'exclamatory', 'for', 'a', 'little', 'while'], ['with', 'gaps', 'of', 'wonderment', 'and', 'then', 'the', 'editor', 'got', 'fervent', 'in', 'his'], ['curiosity', 'does', 'our', 'friend', 'eke', 'out', 'his', 'modest', 'income', 'with', 'a'], ['crossing', 'or', 'has', 'he', 'his', 'nebuchadnezzar', 'phases', 'he', 'inquired', 'i', 'feel'], ['assured', 'it', 's', 'this', 'business', 'of', 'the', 'time', 'machine', 'i', 'said', 'and', 'took', 'up'], ['the', 'psychologist', 's', 'account', 'of', 'our', 'previous', 'meeting', 'the', 'new', 'guests'], ['were', 'frankly', 'incredulous', 'the', 'editor', 'raised', 'objections', 'what', 'was'], ['this', 'time', 'travelling', 'a', 'man', 'couldn', 't', 'cover', 'himself', 'with', 'dust', 'by'], ['rolling', 'in', 'a', 'paradox', 'could', 'he', 'and', 'then', 'as', 'the', 'idea', 'came', 'home', 'to'], ['him', 'he', 'resorted', 'to', 'caricature', 'hadn', 't', 'they', 'any', 'clothes', 'brushes', 'in'], ['the', 'future', 'the', 'journalist', 'too', 'would', 'not', 'believe', 'at', 'any', 'price', 'and'], ['joined', 'the', 'editor', 'in', 'the', 'easy', 'work', 'of', 'heaping', 'ridicule', 'on', 'the', 'whole'], ['thing', 'they', 'were', 'both', 'the', 'new', 'kind', 'of', 'journalist', 'very', 'joyous'], ['irreverent', 'young', 'men', 'our', 'special', 'correspondent', 'in', 'the', 'day'], ['after', 'to', 'morrow', 'reports', 'the', 'journalist', 'was', 'saying', 'or', 'rather'], ['shouting', 'when', 'the', 'time', 'traveller', 'came', 'back', 'he', 'was', 'dressed', 'in'], ['ordinary', 'evening', 'clothes', 'and', 'nothing', 'save', 'his', 'haggard', 'look', 'remained'], ['of', 'the', 'change', 'that', 'had', 'startled', 'me'], [], ['i', 'say', 'said', 'the', 'editor', 'hilariously', 'these', 'chaps', 'here', 'say', 'you', 'have'], ['been', 'travelling', 'into', 'the', 'middle', 'of', 'next', 'week', 'tell', 'us', 'all', 'about'], ['little', 'rosebery', 'will', 'you', 'what', 'will', 'you', 'take', 'for', 'the', 'lot'], [], ['the', 'time', 'traveller', 'came', 'to', 'the', 'place', 'reserved', 'for', 'him', 'without', 'a'], ['word', 'he', 'smiled', 'quietly', 'in', 'his', 'old', 'way', 'where', 's', 'my', 'mutton', 'he'], ['said', 'what', 'a', 'treat', 'it', 'is', 'to', 'stick', 'a', 'fork', 'into', 'meat', 'again'], [], ['story', 'cried', 'the', 'editor'], [], ['story', 'be', 'damned', 'said', 'the', 'time', 'traveller', 'i', 'want', 'something', 'to'], ['eat', 'i', 'won', 't', 'say', 'a', 'word', 'until', 'i', 'get', 'some', 'peptone', 'into', 'my', 'arteries'], ['thanks', 'and', 'the', 'salt'], [], ['one', 'word', 'said', 'i', 'have', 'you', 'been', 'time', 'travelling'], [], ['yes', 'said', 'the', 'time', 'traveller', 'with', 'his', 'mouth', 'full', 'nodding', 'his'], ['head'], [], ['i', 'd', 'give', 'a', 'shilling', 'a', 'line', 'for', 'a', 'verbatim', 'note', 'said', 'the', 'editor'], ['the', 'time', 'traveller', 'pushed', 'his', 'glass', 'towards', 'the', 'silent', 'man', 'and', 'rang'], ['it', 'with', 'his', 'fingernail', 'at', 'which', 'the', 'silent', 'man', 'who', 'had', 'been'], ['staring', 'at', 'his', 'face', 'started', 'convulsively', 'and', 'poured', 'him', 'wine'], ['the', 'rest', 'of', 'the', 'dinner', 'was', 'uncomfortable', 'for', 'my', 'own', 'part', 'sudden'], ['questions', 'kept', 'on', 'rising', 'to', 'my', 'lips', 'and', 'i', 'dare', 'say', 'it', 'was', 'the', 'same'], ['with', 'the', 'others', 'the', 'journalist', 'tried', 'to', 'relieve', 'the', 'tension', 'by'], ['telling', 'anecdotes', 'of', 'hettie', 'potter', 'the', 'time', 'traveller', 'devoted', 'his'], ['attention', 'to', 'his', 'dinner', 'and', 'displayed', 'the', 'appetite', 'of', 'a', 'tramp'], ['the', 'medical', 'man', 'smoked', 'a', 'cigarette', 'and', 'watched', 'the', 'time', 'traveller'], ['through', 'his', 'eyelashes', 'the', 'silent', 'man', 'seemed', 'even', 'more', 'clumsy', 'than'], ['usual', 'and', 'drank', 'champagne', 'with', 'regularity', 'and', 'determination', 'out', 'of'], ['sheer', 'nervousness', 'at', 'last', 'the', 'time', 'traveller', 'pushed', 'his', 'plate', 'away'], ['and', 'looked', 'round', 'us', 'i', 'suppose', 'i', 'must', 'apologize', 'he', 'said', 'i', 'was'], ['simply', 'starving', 'i', 've', 'had', 'a', 'most', 'amazing', 'time', 'he', 'reached', 'out', 'his'], ['hand', 'for', 'a', 'cigar', 'and', 'cut', 'the', 'end', 'but', 'come', 'into', 'the', 'smoking', 'room'], ['it', 's', 'too', 'long', 'a', 'story', 'to', 'tell', 'over', 'greasy', 'plates', 'and', 'ringing', 'the'], ['bell', 'in', 'passing', 'he', 'led', 'the', 'way', 'into', 'the', 'adjoining', 'room'], [], ['you', 'have', 'told', 'blank', 'and', 'dash', 'and', 'chose', 'about', 'the', 'machine', 'he'], ['said', 'to', 'me', 'leaning', 'back', 'in', 'his', 'easy', 'chair', 'and', 'naming', 'the', 'three', 'new'], ['guests'], [], ['but', 'the', 'thing', 's', 'a', 'mere', 'paradox', 'said', 'the', 'editor'], [], ['i', 'can', 't', 'argue', 'to', 'night', 'i', 'don', 't', 'mind', 'telling', 'you', 'the', 'story', 'but'], ['i', 'can', 't', 'argue', 'i', 'will', 'he', 'went', 'on', 'tell', 'you', 'the', 'story', 'of', 'what'], ['has', 'happened', 'to', 'me', 'if', 'you', 'like', 'but', 'you', 'must', 'refrain', 'from'], ['interruptions', 'i', 'want', 'to', 'tell', 'it', 'badly', 'most', 'of', 'it', 'will', 'sound', 'like'], ['lying', 'so', 'be', 'it', 'it', 's', 'true', 'every', 'word', 'of', 'it', 'all', 'the', 'same', 'i', 'was', 'in'], ['my', 'laboratory', 'at', 'four', 'o', 'clock', 'and', 'since', 'then', 'i', 've', 'lived', 'eight'], ['days', 'such', 'days', 'as', 'no', 'human', 'being', 'ever', 'lived', 'before', 'i', 'm', 'nearly'], ['worn', 'out', 'but', 'i', 'shan', 't', 'sleep', 'till', 'i', 've', 'told', 'this', 'thing', 'over', 'to', 'you'], ['then', 'i', 'shall', 'go', 'to', 'bed', 'but', 'no', 'interruptions', 'is', 'it', 'agreed'], [], ['agreed', 'said', 'the', 'editor', 'and', 'the', 'rest', 'of', 'us', 'echoed', 'agreed', 'and'], ['with', 'that', 'the', 'time', 'traveller', 'began', 'his', 'story', 'as', 'i', 'have', 'set', 'it', 'forth'], ['he', 'sat', 'back', 'in', 'his', 'chair', 'at', 'first', 'and', 'spoke', 'like', 'a', 'weary', 'man'], ['afterwards', 'he', 'got', 'more', 'animated', 'in', 'writing', 'it', 'down', 'i', 'feel', 'with', 'only'], ['too', 'much', 'keenness', 'the', 'inadequacy', 'of', 'pen', 'and', 'ink', 'and', 'above', 'all', 'my'], ['own', 'inadequacy', 'to', 'express', 'its', 'quality', 'you', 'read', 'i', 'will', 'suppose'], ['attentively', 'enough', 'but', 'you', 'cannot', 'see', 'the', 'speaker', 's', 'white'], ['sincere', 'face', 'in', 'the', 'bright', 'circle', 'of', 'the', 'little', 'lamp', 'nor', 'hear', 'the'], ['intonation', 'of', 'his', 'voice', 'you', 'cannot', 'know', 'how', 'his', 'expression', 'followed'], ['the', 'turns', 'of', 'his', 'story', 'most', 'of', 'us', 'hearers', 'were', 'in', 'shadow', 'for', 'the'], ['candles', 'in', 'the', 'smoking', 'room', 'had', 'not', 'been', 'lighted', 'and', 'only', 'the', 'face'], ['of', 'the', 'journalist', 'and', 'the', 'legs', 'of', 'the', 'silent', 'man', 'from', 'the', 'knees'], ['downward', 'were', 'illuminated', 'at', 'first', 'we', 'glanced', 'now', 'and', 'again', 'at', 'each'], ['other', 'after', 'a', 'time', 'we', 'ceased', 'to', 'do', 'that', 'and', 'looked', 'only', 'at', 'the'], ['time', 'traveller', 's', 'face'], [], [], [], [], ['iii'], [], [], ['i', 'told', 'some', 'of', 'you', 'last', 'thursday', 'of', 'the', 'principles', 'of', 'the', 'time'], ['machine', 'and', 'showed', 'you', 'the', 'actual', 'thing', 'itself', 'incomplete', 'in', 'the'], ['workshop', 'there', 'it', 'is', 'now', 'a', 'little', 'travel', 'worn', 'truly', 'and', 'one', 'of'], ['the', 'ivory', 'bars', 'is', 'cracked', 'and', 'a', 'brass', 'rail', 'bent', 'but', 'the', 'rest', 'of'], ['it', 's', 'sound', 'enough', 'i', 'expected', 'to', 'finish', 'it', 'on', 'friday', 'but', 'on', 'friday'], ['when', 'the', 'putting', 'together', 'was', 'nearly', 'done', 'i', 'found', 'that', 'one', 'of', 'the'], ['nickel', 'bars', 'was', 'exactly', 'one', 'inch', 'too', 'short', 'and', 'this', 'i', 'had', 'to', 'get'], ['remade', 'so', 'that', 'the', 'thing', 'was', 'not', 'complete', 'until', 'this', 'morning', 'it'], ['was', 'at', 'ten', 'o', 'clock', 'to', 'day', 'that', 'the', 'first', 'of', 'all', 'time', 'machines', 'began'], ['its', 'career', 'i', 'gave', 'it', 'a', 'last', 'tap', 'tried', 'all', 'the', 'screws', 'again', 'put'], ['one', 'more', 'drop', 'of', 'oil', 'on', 'the', 'quartz', 'rod', 'and', 'sat', 'myself', 'in', 'the'], ['saddle', 'i', 'suppose', 'a', 'suicide', 'who', 'holds', 'a', 'pistol', 'to', 'his', 'skull', 'feels'], ['much', 'the', 'same', 'wonder', 'at', 'what', 'will', 'come', 'next', 'as', 'i', 'felt', 'then', 'i', 'took'], ['the', 'starting', 'lever', 'in', 'one', 'hand', 'and', 'the', 'stopping', 'one', 'in', 'the', 'other'], ['pressed', 'the', 'first', 'and', 'almost', 'immediately', 'the', 'second', 'i', 'seemed', 'to'], ['reel', 'i', 'felt', 'a', 'nightmare', 'sensation', 'of', 'falling', 'and', 'looking', 'round'], ['i', 'saw', 'the', 'laboratory', 'exactly', 'as', 'before', 'had', 'anything', 'happened', 'for'], ['a', 'moment', 'i', 'suspected', 'that', 'my', 'intellect', 'had', 'tricked', 'me', 'then', 'i', 'noted'], ['the', 'clock', 'a', 'moment', 'before', 'as', 'it', 'seemed', 'it', 'had', 'stood', 'at', 'a', 'minute'], ['or', 'so', 'past', 'ten', 'now', 'it', 'was', 'nearly', 'half', 'past', 'three'], [], ['i', 'drew', 'a', 'breath', 'set', 'my', 'teeth', 'gripped', 'the', 'starting', 'lever', 'with', 'both'], ['hands', 'and', 'went', 'off', 'with', 'a', 'thud', 'the', 'laboratory', 'got', 'hazy', 'and', 'went'], ['dark', 'mrs', 'watchett', 'came', 'in', 'and', 'walked', 'apparently', 'without', 'seeing'], ['me', 'towards', 'the', 'garden', 'door', 'i', 'suppose', 'it', 'took', 'her', 'a', 'minute', 'or', 'so', 'to'], ['traverse', 'the', 'place', 'but', 'to', 'me', 'she', 'seemed', 'to', 'shoot', 'across', 'the', 'room'], ['like', 'a', 'rocket', 'i', 'pressed', 'the', 'lever', 'over', 'to', 'its', 'extreme', 'position', 'the'], ['night', 'came', 'like', 'the', 'turning', 'out', 'of', 'a', 'lamp', 'and', 'in', 'another', 'moment'], ['came', 'to', 'morrow', 'the', 'laboratory', 'grew', 'faint', 'and', 'hazy', 'then', 'fainter'], ['and', 'ever', 'fainter', 'to', 'morrow', 'night', 'came', 'black', 'then', 'day', 'again', 'night'], ['again', 'day', 'again', 'faster', 'and', 'faster', 'still', 'an', 'eddying', 'murmur', 'filled'], ['my', 'ears', 'and', 'a', 'strange', 'dumb', 'confusedness', 'descended', 'on', 'my', 'mind'], [], ['i', 'am', 'afraid', 'i', 'cannot', 'convey', 'the', 'peculiar', 'sensations', 'of', 'time'], ['travelling', 'they', 'are', 'excessively', 'unpleasant', 'there', 'is', 'a', 'feeling'], ['exactly', 'like', 'that', 'one', 'has', 'upon', 'a', 'switchback', 'of', 'a', 'helpless', 'headlong'], ['motion', 'i', 'felt', 'the', 'same', 'horrible', 'anticipation', 'too', 'of', 'an', 'imminent'], ['smash', 'as', 'i', 'put', 'on', 'pace', 'night', 'followed', 'day', 'like', 'the', 'flapping', 'of', 'a'], ['black', 'wing', 'the', 'dim', 'suggestion', 'of', 'the', 'laboratory', 'seemed', 'presently', 'to'], ['fall', 'away', 'from', 'me', 'and', 'i', 'saw', 'the', 'sun', 'hopping', 'swiftly', 'across', 'the', 'sky'], ['leaping', 'it', 'every', 'minute', 'and', 'every', 'minute', 'marking', 'a', 'day', 'i', 'supposed'], ['the', 'laboratory', 'had', 'been', 'destroyed', 'and', 'i', 'had', 'come', 'into', 'the', 'open', 'air'], ['i', 'had', 'a', 'dim', 'impression', 'of', 'scaffolding', 'but', 'i', 'was', 'already', 'going', 'too'], ['fast', 'to', 'be', 'conscious', 'of', 'any', 'moving', 'things', 'the', 'slowest', 'snail', 'that'], ['ever', 'crawled', 'dashed', 'by', 'too', 'fast', 'for', 'me', 'the', 'twinkling', 'succession', 'of'], ['darkness', 'and', 'light', 'was', 'excessively', 'painful', 'to', 'the', 'eye', 'then', 'in', 'the'], ['intermittent', 'darknesses', 'i', 'saw', 'the', 'moon', 'spinning', 'swiftly', 'through', 'her'], ['quarters', 'from', 'new', 'to', 'full', 'and', 'had', 'a', 'faint', 'glimpse', 'of', 'the', 'circling'], ['stars', 'presently', 'as', 'i', 'went', 'on', 'still', 'gaining', 'velocity', 'the'], ['palpitation', 'of', 'night', 'and', 'day', 'merged', 'into', 'one', 'continuous', 'greyness'], ['the', 'sky', 'took', 'on', 'a', 'wonderful', 'deepness', 'of', 'blue', 'a', 'splendid', 'luminous'], ['color', 'like', 'that', 'of', 'early', 'twilight', 'the', 'jerking', 'sun', 'became', 'a', 'streak'], ['of', 'fire', 'a', 'brilliant', 'arch', 'in', 'space', 'the', 'moon', 'a', 'fainter', 'fluctuating'], ['band', 'and', 'i', 'could', 'see', 'nothing', 'of', 'the', 'stars', 'save', 'now', 'and', 'then', 'a'], ['brighter', 'circle', 'flickering', 'in', 'the', 'blue'], [], ['the', 'landscape', 'was', 'misty', 'and', 'vague', 'i', 'was', 'still', 'on', 'the', 'hill', 'side'], ['upon', 'which', 'this', 'house', 'now', 'stands', 'and', 'the', 'shoulder', 'rose', 'above', 'me'], ['grey', 'and', 'dim', 'i', 'saw', 'trees', 'growing', 'and', 'changing', 'like', 'puffs', 'of', 'vapour'], ['now', 'brown', 'now', 'green', 'they', 'grew', 'spread', 'shivered', 'and', 'passed', 'away'], ['i', 'saw', 'huge', 'buildings', 'rise', 'up', 'faint', 'and', 'fair', 'and', 'pass', 'like', 'dreams'], ['the', 'whole', 'surface', 'of', 'the', 'earth', 'seemed', 'changed', 'melting', 'and', 'flowing'], ['under', 'my', 'eyes', 'the', 'little', 'hands', 'upon', 'the', 'dials', 'that', 'registered', 'my'], ['speed', 'raced', 'round', 'faster', 'and', 'faster', 'presently', 'i', 'noted', 'that', 'the', 'sun'], ['belt', 'swayed', 'up', 'and', 'down', 'from', 'solstice', 'to', 'solstice', 'in', 'a', 'minute', 'or'], ['less', 'and', 'that', 'consequently', 'my', 'pace', 'was', 'over', 'a', 'year', 'a', 'minute', 'and'], ['minute', 'by', 'minute', 'the', 'white', 'snow', 'flashed', 'across', 'the', 'world', 'and'], ['vanished', 'and', 'was', 'followed', 'by', 'the', 'bright', 'brief', 'green', 'of', 'spring'], [], ['the', 'unpleasant', 'sensations', 'of', 'the', 'start', 'were', 'less', 'poignant', 'now', 'they'], ['merged', 'at', 'last', 'into', 'a', 'kind', 'of', 'hysterical', 'exhilaration', 'i', 'remarked'], ['indeed', 'a', 'clumsy', 'swaying', 'of', 'the', 'machine', 'for', 'which', 'i', 'was', 'unable', 'to'], ['account', 'but', 'my', 'mind', 'was', 'too', 'confused', 'to', 'attend', 'to', 'it', 'so', 'with', 'a'], ['kind', 'of', 'madness', 'growing', 'upon', 'me', 'i', 'flung', 'myself', 'into', 'futurity', 'at'], ['first', 'i', 'scarce', 'thought', 'of', 'stopping', 'scarce', 'thought', 'of', 'anything', 'but'], ['these', 'new', 'sensations', 'but', 'presently', 'a', 'fresh', 'series', 'of', 'impressions'], ['grew', 'up', 'in', 'my', 'mind', 'a', 'certain', 'curiosity', 'and', 'therewith', 'a', 'certain'], ['dread', 'until', 'at', 'last', 'they', 'took', 'complete', 'possession', 'of', 'me', 'what'], ['strange', 'developments', 'of', 'humanity', 'what', 'wonderful', 'advances', 'upon', 'our'], ['rudimentary', 'civilization', 'i', 'thought', 'might', 'not', 'appear', 'when', 'i', 'came', 'to'], ['look', 'nearly', 'into', 'the', 'dim', 'elusive', 'world', 'that', 'raced', 'and', 'fluctuated'], ['before', 'my', 'eyes', 'i', 'saw', 'great', 'and', 'splendid', 'architecture', 'rising', 'about'], ['me', 'more', 'massive', 'than', 'any', 'buildings', 'of', 'our', 'own', 'time', 'and', 'yet', 'as', 'it'], ['seemed', 'built', 'of', 'glimmer', 'and', 'mist', 'i', 'saw', 'a', 'richer', 'green', 'flow', 'up', 'the'], ['hill', 'side', 'and', 'remain', 'there', 'without', 'any', 'wintry', 'intermission', 'even'], ['through', 'the', 'veil', 'of', 'my', 'confusion', 'the', 'earth', 'seemed', 'very', 'fair', 'and', 'so'], ['my', 'mind', 'came', 'round', 'to', 'the', 'business', 'of', 'stopping'], [], ['the', 'peculiar', 'risk', 'lay', 'in', 'the', 'possibility', 'of', 'my', 'finding', 'some'], ['substance', 'in', 'the', 'space', 'which', 'i', 'or', 'the', 'machine', 'occupied', 'so', 'long'], ['as', 'i', 'travelled', 'at', 'a', 'high', 'velocity', 'through', 'time', 'this', 'scarcely'], ['mattered', 'i', 'was', 'so', 'to', 'speak', 'attenuated', 'was', 'slipping', 'like', 'a', 'vapour'], ['through', 'the', 'interstices', 'of', 'intervening', 'substances', 'but', 'to', 'come', 'to'], ['a', 'stop', 'involved', 'the', 'jamming', 'of', 'myself', 'molecule', 'by', 'molecule', 'into'], ['whatever', 'lay', 'in', 'my', 'way', 'meant', 'bringing', 'my', 'atoms', 'into', 'such', 'intimate'], ['contact', 'with', 'those', 'of', 'the', 'obstacle', 'that', 'a', 'profound', 'chemical'], ['reaction', 'possibly', 'a', 'far', 'reaching', 'explosion', 'would', 'result', 'and', 'blow'], ['myself', 'and', 'my', 'apparatus', 'out', 'of', 'all', 'possible', 'dimensions', 'into', 'the'], ['unknown', 'this', 'possibility', 'had', 'occurred', 'to', 'me', 'again', 'and', 'again', 'while', 'i'], ['was', 'making', 'the', 'machine', 'but', 'then', 'i', 'had', 'cheerfully', 'accepted', 'it', 'as', 'an'], ['unavoidable', 'risk', 'one', 'of', 'the', 'risks', 'a', 'man', 'has', 'got', 'to', 'take', 'now', 'the'], ['risk', 'was', 'inevitable', 'i', 'no', 'longer', 'saw', 'it', 'in', 'the', 'same', 'cheerful', 'light'], ['the', 'fact', 'is', 'that', 'insensibly', 'the', 'absolute', 'strangeness', 'of', 'everything'], ['the', 'sickly', 'jarring', 'and', 'swaying', 'of', 'the', 'machine', 'above', 'all', 'the'], ['feeling', 'of', 'prolonged', 'falling', 'had', 'absolutely', 'upset', 'my', 'nerve', 'i', 'told'], ['myself', 'that', 'i', 'could', 'never', 'stop', 'and', 'with', 'a', 'gust', 'of', 'petulance', 'i'], ['resolved', 'to', 'stop', 'forthwith', 'like', 'an', 'impatient', 'fool', 'i', 'lugged', 'over'], ['the', 'lever', 'and', 'incontinently', 'the', 'thing', 'went', 'reeling', 'over', 'and', 'i', 'was'], ['flung', 'headlong', 'through', 'the', 'air'], [], ['there', 'was', 'the', 'sound', 'of', 'a', 'clap', 'of', 'thunder', 'in', 'my', 'ears', 'i', 'may', 'have'], ['been', 'stunned', 'for', 'a', 'moment', 'a', 'pitiless', 'hail', 'was', 'hissing', 'round', 'me'], ['and', 'i', 'was', 'sitting', 'on', 'soft', 'turf', 'in', 'front', 'of', 'the', 'overset', 'machine'], ['everything', 'still', 'seemed', 'grey', 'but', 'presently', 'i', 'remarked', 'that', 'the'], ['confusion', 'in', 'my', 'ears', 'was', 'gone', 'i', 'looked', 'round', 'me', 'i', 'was', 'on', 'what'], ['seemed', 'to', 'be', 'a', 'little', 'lawn', 'in', 'a', 'garden', 'surrounded', 'by', 'rhododendron'], ['bushes', 'and', 'i', 'noticed', 'that', 'their', 'mauve', 'and', 'purple', 'blossoms', 'were'], ['dropping', 'in', 'a', 'shower', 'under', 'the', 'beating', 'of', 'the', 'hail', 'stones', 'the'], ['rebounding', 'dancing', 'hail', 'hung', 'in', 'a', 'cloud', 'over', 'the', 'machine', 'and', 'drove'], ['along', 'the', 'ground', 'like', 'smoke', 'in', 'a', 'moment', 'i', 'was', 'wet', 'to', 'the', 'skin'], ['fine', 'hospitality', 'said', 'i', 'to', 'a', 'man', 'who', 'has', 'travelled', 'innumerable'], ['years', 'to', 'see', 'you'], [], ['presently', 'i', 'thought', 'what', 'a', 'fool', 'i', 'was', 'to', 'get', 'wet', 'i', 'stood', 'up', 'and'], ['looked', 'round', 'me', 'a', 'colossal', 'figure', 'carved', 'apparently', 'in', 'some', 'white'], ['stone', 'loomed', 'indistinctly', 'beyond', 'the', 'rhododendrons', 'through', 'the', 'hazy'], ['downpour', 'but', 'all', 'else', 'of', 'the', 'world', 'was', 'invisible'], [], ['my', 'sensations', 'would', 'be', 'hard', 'to', 'describe', 'as', 'the', 'columns', 'of', 'hail'], ['grew', 'thinner', 'i', 'saw', 'the', 'white', 'figure', 'more', 'distinctly', 'it', 'was', 'very'], ['large', 'for', 'a', 'silver', 'birch', 'tree', 'touched', 'its', 'shoulder', 'it', 'was', 'of', 'white'], ['marble', 'in', 'shape', 'something', 'like', 'a', 'winged', 'sphinx', 'but', 'the', 'wings'], ['instead', 'of', 'being', 'carried', 'vertically', 'at', 'the', 'sides', 'were', 'spread', 'so'], ['that', 'it', 'seemed', 'to', 'hover', 'the', 'pedestal', 'it', 'appeared', 'to', 'me', 'was', 'of'], ['bronze', 'and', 'was', 'thick', 'with', 'verdigris', 'it', 'chanced', 'that', 'the', 'face', 'was'], ['towards', 'me', 'the', 'sightless', 'eyes', 'seemed', 'to', 'watch', 'me', 'there', 'was', 'the'], ['faint', 'shadow', 'of', 'a', 'smile', 'on', 'the', 'lips', 'it', 'was', 'greatly', 'weather', 'worn'], ['and', 'that', 'imparted', 'an', 'unpleasant', 'suggestion', 'of', 'disease', 'i', 'stood'], ['looking', 'at', 'it', 'for', 'a', 'little', 'space', 'half', 'a', 'minute', 'perhaps', 'or', 'half', 'an'], ['hour', 'it', 'seemed', 'to', 'advance', 'and', 'to', 'recede', 'as', 'the', 'hail', 'drove', 'before', 'it'], ['denser', 'or', 'thinner', 'at', 'last', 'i', 'tore', 'my', 'eyes', 'from', 'it', 'for', 'a', 'moment', 'and'], ['saw', 'that', 'the', 'hail', 'curtain', 'had', 'worn', 'threadbare', 'and', 'that', 'the', 'sky', 'was'], ['lightening', 'with', 'the', 'promise', 'of', 'the', 'sun'], [], ['i', 'looked', 'up', 'again', 'at', 'the', 'crouching', 'white', 'shape', 'and', 'the', 'full'], ['temerity', 'of', 'my', 'voyage', 'came', 'suddenly', 'upon', 'me', 'what', 'might', 'appear', 'when'], ['that', 'hazy', 'curtain', 'was', 'altogether', 'withdrawn', 'what', 'might', 'not', 'have'], ['happened', 'to', 'men', 'what', 'if', 'cruelty', 'had', 'grown', 'into', 'a', 'common', 'passion'], ['what', 'if', 'in', 'this', 'interval', 'the', 'race', 'had', 'lost', 'its', 'manliness', 'and', 'had'], ['developed', 'into', 'something', 'inhuman', 'unsympathetic', 'and', 'overwhelmingly'], ['powerful', 'i', 'might', 'seem', 'some', 'old', 'world', 'savage', 'animal', 'only', 'the', 'more'], ['dreadful', 'and', 'disgusting', 'for', 'our', 'common', 'likeness', 'a', 'foul', 'creature', 'to'], ['be', 'incontinently', 'slain'], [], ['already', 'i', 'saw', 'other', 'vast', 'shapes', 'huge', 'buildings', 'with', 'intricate'], ['parapets', 'and', 'tall', 'columns', 'with', 'a', 'wooded', 'hill', 'side', 'dimly', 'creeping'], ['in', 'upon', 'me', 'through', 'the', 'lessening', 'storm', 'i', 'was', 'seized', 'with', 'a', 'panic'], ['fear', 'i', 'turned', 'frantically', 'to', 'the', 'time', 'machine', 'and', 'strove', 'hard', 'to'], ['readjust', 'it', 'as', 'i', 'did', 'so', 'the', 'shafts', 'of', 'the', 'sun', 'smote', 'through', 'the'], ['thunderstorm', 'the', 'grey', 'downpour', 'was', 'swept', 'aside', 'and', 'vanished', 'like'], ['the', 'trailing', 'garments', 'of', 'a', 'ghost', 'above', 'me', 'in', 'the', 'intense', 'blue'], ['of', 'the', 'summer', 'sky', 'some', 'faint', 'brown', 'shreds', 'of', 'cloud', 'whirled', 'into'], ['nothingness', 'the', 'great', 'buildings', 'about', 'me', 'stood', 'out', 'clear', 'and'], ['distinct', 'shining', 'with', 'the', 'wet', 'of', 'the', 'thunderstorm', 'and', 'picked', 'out'], ['in', 'white', 'by', 'the', 'unmelted', 'hailstones', 'piled', 'along', 'their', 'courses', 'i'], ['felt', 'naked', 'in', 'a', 'strange', 'world', 'i', 'felt', 'as', 'perhaps', 'a', 'bird', 'may', 'feel', 'in'], ['the', 'clear', 'air', 'knowing', 'the', 'hawk', 'wings', 'above', 'and', 'will', 'swoop', 'my', 'fear'], ['grew', 'to', 'frenzy', 'i', 'took', 'a', 'breathing', 'space', 'set', 'my', 'teeth', 'and', 'again'], ['grappled', 'fiercely', 'wrist', 'and', 'knee', 'with', 'the', 'machine', 'it', 'gave', 'under'], ['my', 'desperate', 'onset', 'and', 'turned', 'over', 'it', 'struck', 'my', 'chin', 'violently', 'one'], ['hand', 'on', 'the', 'saddle', 'the', 'other', 'on', 'the', 'lever', 'i', 'stood', 'panting', 'heavily'], ['in', 'attitude', 'to', 'mount', 'again'], [], ['but', 'with', 'this', 'recovery', 'of', 'a', 'prompt', 'retreat', 'my', 'courage', 'recovered', 'i'], ['looked', 'more', 'curiously', 'and', 'less', 'fearfully', 'at', 'this', 'world', 'of', 'the', 'remote'], ['future', 'in', 'a', 'circular', 'opening', 'high', 'up', 'in', 'the', 'wall', 'of', 'the', 'nearer'], ['house', 'i', 'saw', 'a', 'group', 'of', 'figures', 'clad', 'in', 'rich', 'soft', 'robes', 'they', 'had'], ['seen', 'me', 'and', 'their', 'faces', 'were', 'directed', 'towards', 'me'], [], ['then', 'i', 'heard', 'voices', 'approaching', 'me', 'coming', 'through', 'the', 'bushes', 'by'], ['the', 'white', 'sphinx', 'were', 'the', 'heads', 'and', 'shoulders', 'of', 'men', 'running', 'one', 'of'], ['these', 'emerged', 'in', 'a', 'pathway', 'leading', 'straight', 'to', 'the', 'little', 'lawn', 'upon'], ['which', 'i', 'stood', 'with', 'my', 'machine', 'he', 'was', 'a', 'slight', 'creature', 'perhaps'], ['four', 'feet', 'high', 'clad', 'in', 'a', 'purple', 'tunic', 'girdled', 'at', 'the', 'waist', 'with', 'a'], ['leather', 'belt', 'sandals', 'or', 'buskins', 'i', 'could', 'not', 'clearly', 'distinguish'], ['which', 'were', 'on', 'his', 'feet', 'his', 'legs', 'were', 'bare', 'to', 'the', 'knees', 'and', 'his'], ['head', 'was', 'bare', 'noticing', 'that', 'i', 'noticed', 'for', 'the', 'first', 'time', 'how', 'warm'], ['the', 'air', 'was'], [], ['he', 'struck', 'me', 'as', 'being', 'a', 'very', 'beautiful', 'and', 'graceful', 'creature', 'but'], ['indescribably', 'frail', 'his', 'flushed', 'face', 'reminded', 'me', 'of', 'the', 'more'], ['beautiful', 'kind', 'of', 'consumptive', 'that', 'hectic', 'beauty', 'of', 'which', 'we', 'used'], ['to', 'hear', 'so', 'much', 'at', 'the', 'sight', 'of', 'him', 'i', 'suddenly', 'regained', 'confidence'], ['i', 'took', 'my', 'hands', 'from', 'the', 'machine'], [], [], [], [], ['iv'], [], [], ['in', 'another', 'moment', 'we', 'were', 'standing', 'face', 'to', 'face', 'i', 'and', 'this', 'fragile'], ['thing', 'out', 'of', 'futurity', 'he', 'came', 'straight', 'up', 'to', 'me', 'and', 'laughed', 'into', 'my'], ['eyes', 'the', 'absence', 'from', 'his', 'bearing', 'of', 'any', 'sign', 'of', 'fear', 'struck', 'me', 'at'], ['once', 'then', 'he', 'turned', 'to', 'the', 'two', 'others', 'who', 'were', 'following', 'him', 'and'], ['spoke', 'to', 'them', 'in', 'a', 'strange', 'and', 'very', 'sweet', 'and', 'liquid', 'tongue'], [], ['there', 'were', 'others', 'coming', 'and', 'presently', 'a', 'little', 'group', 'of', 'perhaps'], ['eight', 'or', 'ten', 'of', 'these', 'exquisite', 'creatures', 'were', 'about', 'me', 'one', 'of', 'them'], ['addressed', 'me', 'it', 'came', 'into', 'my', 'head', 'oddly', 'enough', 'that', 'my', 'voice', 'was'], ['too', 'harsh', 'and', 'deep', 'for', 'them', 'so', 'i', 'shook', 'my', 'head', 'and', 'pointing', 'to', 'my'], ['ears', 'shook', 'it', 'again', 'he', 'came', 'a', 'step', 'forward', 'hesitated', 'and', 'then'], ['touched', 'my', 'hand', 'then', 'i', 'felt', 'other', 'soft', 'little', 'tentacles', 'upon', 'my'], ['back', 'and', 'shoulders', 'they', 'wanted', 'to', 'make', 'sure', 'i', 'was', 'real', 'there', 'was'], ['nothing', 'in', 'this', 'at', 'all', 'alarming', 'indeed', 'there', 'was', 'something', 'in'], ['these', 'pretty', 'little', 'people', 'that', 'inspired', 'confidence', 'a', 'graceful'], ['gentleness', 'a', 'certain', 'childlike', 'ease', 'and', 'besides', 'they', 'looked', 'so'], ['frail', 'that', 'i', 'could', 'fancy', 'myself', 'flinging', 'the', 'whole', 'dozen', 'of', 'them'], ['about', 'like', 'nine', 'pins', 'but', 'i', 'made', 'a', 'sudden', 'motion', 'to', 'warn', 'them', 'when', 'i'], ['saw', 'their', 'little', 'pink', 'hands', 'feeling', 'at', 'the', 'time', 'machine', 'happily'], ['then', 'when', 'it', 'was', 'not', 'too', 'late', 'i', 'thought', 'of', 'a', 'danger', 'i', 'had', 'hitherto'], ['forgotten', 'and', 'reaching', 'over', 'the', 'bars', 'of', 'the', 'machine', 'i', 'unscrewed', 'the'], ['little', 'levers', 'that', 'would', 'set', 'it', 'in', 'motion', 'and', 'put', 'these', 'in', 'my'], ['pocket', 'then', 'i', 'turned', 'again', 'to', 'see', 'what', 'i', 'could', 'do', 'in', 'the', 'way', 'of'], ['communication'], [], ['and', 'then', 'looking', 'more', 'nearly', 'into', 'their', 'features', 'i', 'saw', 'some'], ['further', 'peculiarities', 'in', 'their', 'dresden', 'china', 'type', 'of', 'prettiness'], ['their', 'hair', 'which', 'was', 'uniformly', 'curly', 'came', 'to', 'a', 'sharp', 'end', 'at', 'the'], ['neck', 'and', 'cheek', 'there', 'was', 'not', 'the', 'faintest', 'suggestion', 'of', 'it', 'on', 'the'], ['face', 'and', 'their', 'ears', 'were', 'singularly', 'minute', 'the', 'mouths', 'were', 'small'], ['with', 'bright', 'red', 'rather', 'thin', 'lips', 'and', 'the', 'little', 'chins', 'ran', 'to', 'a'], ['point', 'the', 'eyes', 'were', 'large', 'and', 'mild', 'and', 'this', 'may', 'seem', 'egotism', 'on'], ['my', 'part', 'i', 'fancied', 'even', 'that', 'there', 'was', 'a', 'certain', 'lack', 'of', 'the'], ['interest', 'i', 'might', 'have', 'expected', 'in', 'them'], [], ['as', 'they', 'made', 'no', 'effort', 'to', 'communicate', 'with', 'me', 'but', 'simply', 'stood'], ['round', 'me', 'smiling', 'and', 'speaking', 'in', 'soft', 'cooing', 'notes', 'to', 'each', 'other', 'i'], ['began', 'the', 'conversation', 'i', 'pointed', 'to', 'the', 'time', 'machine', 'and', 'to', 'myself'], ['then', 'hesitating', 'for', 'a', 'moment', 'how', 'to', 'express', 'time', 'i', 'pointed', 'to', 'the'], ['sun', 'at', 'once', 'a', 'quaintly', 'pretty', 'little', 'figure', 'in', 'chequered', 'purple', 'and'], ['white', 'followed', 'my', 'gesture', 'and', 'then', 'astonished', 'me', 'by', 'imitating', 'the'], ['sound', 'of', 'thunder'], [], ['for', 'a', 'moment', 'i', 'was', 'staggered', 'though', 'the', 'import', 'of', 'his', 'gesture', 'was'], ['plain', 'enough', 'the', 'question', 'had', 'come', 'into', 'my', 'mind', 'abruptly', 'were'], ['these', 'creatures', 'fools', 'you', 'may', 'hardly', 'understand', 'how', 'it', 'took', 'me'], ['you', 'see', 'i', 'had', 'always', 'anticipated', 'that', 'the', 'people', 'of', 'the', 'year', 'eight'], ['hundred', 'and', 'two', 'thousand', 'odd', 'would', 'be', 'incredibly', 'in', 'front', 'of', 'us', 'in'], ['knowledge', 'art', 'everything', 'then', 'one', 'of', 'them', 'suddenly', 'asked', 'me', 'a'], ['question', 'that', 'showed', 'him', 'to', 'be', 'on', 'the', 'intellectual', 'level', 'of', 'one', 'of'], ['our', 'five', 'year', 'old', 'children', 'asked', 'me', 'in', 'fact', 'if', 'i', 'had', 'come', 'from'], ['the', 'sun', 'in', 'a', 'thunderstorm', 'it', 'let', 'loose', 'the', 'judgment', 'i', 'had', 'suspended'], ['upon', 'their', 'clothes', 'their', 'frail', 'light', 'limbs', 'and', 'fragile', 'features'], ['a', 'flow', 'of', 'disappointment', 'rushed', 'across', 'my', 'mind', 'for', 'a', 'moment', 'i', 'felt'], ['that', 'i', 'had', 'built', 'the', 'time', 'machine', 'in', 'vain'], [], ['i', 'nodded', 'pointed', 'to', 'the', 'sun', 'and', 'gave', 'them', 'such', 'a', 'vivid', 'rendering'], ['of', 'a', 'thunderclap', 'as', 'startled', 'them', 'they', 'all', 'withdrew', 'a', 'pace', 'or', 'so'], ['and', 'bowed', 'then', 'came', 'one', 'laughing', 'towards', 'me', 'carrying', 'a', 'chain', 'of'], ['beautiful', 'flowers', 'altogether', 'new', 'to', 'me', 'and', 'put', 'it', 'about', 'my', 'neck'], ['the', 'idea', 'was', 'received', 'with', 'melodious', 'applause', 'and', 'presently', 'they'], ['were', 'all', 'running', 'to', 'and', 'fro', 'for', 'flowers', 'and', 'laughingly', 'flinging'], ['them', 'upon', 'me', 'until', 'i', 'was', 'almost', 'smothered', 'with', 'blossom', 'you', 'who'], ['have', 'never', 'seen', 'the', 'like', 'can', 'scarcely', 'imagine', 'what', 'delicate', 'and'], ['wonderful', 'flowers', 'countless', 'years', 'of', 'culture', 'had', 'created', 'then'], ['someone', 'suggested', 'that', 'their', 'plaything', 'should', 'be', 'exhibited', 'in', 'the'], ['nearest', 'building', 'and', 'so', 'i', 'was', 'led', 'past', 'the', 'sphinx', 'of', 'white', 'marble'], ['which', 'had', 'seemed', 'to', 'watch', 'me', 'all', 'the', 'while', 'with', 'a', 'smile', 'at', 'my'], ['astonishment', 'towards', 'a', 'vast', 'grey', 'edifice', 'of', 'fretted', 'stone', 'as', 'i'], ['went', 'with', 'them', 'the', 'memory', 'of', 'my', 'confident', 'anticipations', 'of', 'a'], ['profoundly', 'grave', 'and', 'intellectual', 'posterity', 'came', 'with', 'irresistible'], ['merriment', 'to', 'my', 'mind'], [], ['the', 'building', 'had', 'a', 'huge', 'entry', 'and', 'was', 'altogether', 'of', 'colossal'], ['dimensions', 'i', 'was', 'naturally', 'most', 'occupied', 'with', 'the', 'growing', 'crowd', 'of'], ['little', 'people', 'and', 'with', 'the', 'big', 'open', 'portals', 'that', 'yawned', 'before', 'me'], ['shadowy', 'and', 'mysterious', 'my', 'general', 'impression', 'of', 'the', 'world', 'i', 'saw'], ['over', 'their', 'heads', 'was', 'a', 'tangled', 'waste', 'of', 'beautiful', 'bushes', 'and'], ['flowers', 'a', 'long', 'neglected', 'and', 'yet', 'weedless', 'garden', 'i', 'saw', 'a', 'number'], ['of', 'tall', 'spikes', 'of', 'strange', 'white', 'flowers', 'measuring', 'a', 'foot', 'perhaps'], ['across', 'the', 'spread', 'of', 'the', 'waxen', 'petals', 'they', 'grew', 'scattered', 'as', 'if'], ['wild', 'among', 'the', 'variegated', 'shrubs', 'but', 'as', 'i', 'say', 'i', 'did', 'not', 'examine'], ['them', 'closely', 'at', 'this', 'time', 'the', 'time', 'machine', 'was', 'left', 'deserted', 'on', 'the'], ['turf', 'among', 'the', 'rhododendrons'], [], ['the', 'arch', 'of', 'the', 'doorway', 'was', 'richly', 'carved', 'but', 'naturally', 'i', 'did'], ['not', 'observe', 'the', 'carving', 'very', 'narrowly', 'though', 'i', 'fancied', 'i', 'saw'], ['suggestions', 'of', 'old', 'phoenician', 'decorations', 'as', 'i', 'passed', 'through', 'and'], ['it', 'struck', 'me', 'that', 'they', 'were', 'very', 'badly', 'broken', 'and', 'weather', 'worn'], ['several', 'more', 'brightly', 'clad', 'people', 'met', 'me', 'in', 'the', 'doorway', 'and', 'so', 'we'], ['entered', 'i', 'dressed', 'in', 'dingy', 'nineteenth', 'century', 'garments', 'looking'], ['grotesque', 'enough', 'garlanded', 'with', 'flowers', 'and', 'surrounded', 'by', 'an'], ['eddying', 'mass', 'of', 'bright', 'soft', 'colored', 'robes', 'and', 'shining', 'white', 'limbs'], ['in', 'a', 'melodious', 'whirl', 'of', 'laughter', 'and', 'laughing', 'speech'], [], ['the', 'big', 'doorway', 'opened', 'into', 'a', 'proportionately', 'great', 'hall', 'hung', 'with'], ['brown', 'the', 'roof', 'was', 'in', 'shadow', 'and', 'the', 'windows', 'partially', 'glazed'], ['with', 'coloured', 'glass', 'and', 'partially', 'unglazed', 'admitted', 'a', 'tempered'], ['light', 'the', 'floor', 'was', 'made', 'up', 'of', 'huge', 'blocks', 'of', 'some', 'very', 'hard', 'white'], ['metal', 'not', 'plates', 'nor', 'slabs', 'blocks', 'and', 'it', 'was', 'so', 'much', 'worn', 'as', 'i'], ['judged', 'by', 'the', 'going', 'to', 'and', 'fro', 'of', 'past', 'generations', 'as', 'to', 'be', 'deeply'], ['channelled', 'along', 'the', 'more', 'frequented', 'ways', 'transverse', 'to', 'the', 'length'], ['were', 'innumerable', 'tables', 'made', 'of', 'slabs', 'of', 'polished', 'stone', 'raised'], ['perhaps', 'a', 'foot', 'from', 'the', 'floor', 'and', 'upon', 'these', 'were', 'heaps', 'of', 'fruits'], ['some', 'i', 'recognized', 'as', 'a', 'kind', 'of', 'hypertrophied', 'raspberry', 'and', 'orange'], ['but', 'for', 'the', 'most', 'part', 'they', 'were', 'strange'], [], ['between', 'the', 'tables', 'was', 'scattered', 'a', 'great', 'number', 'of', 'cushions'], ['upon', 'these', 'my', 'conductors', 'seated', 'themselves', 'signing', 'for', 'me', 'to', 'do'], ['likewise', 'with', 'a', 'pretty', 'absence', 'of', 'ceremony', 'they', 'began', 'to', 'eat', 'the'], ['fruit', 'with', 'their', 'hands', 'flinging', 'peel', 'and', 'stalks', 'and', 'so', 'forth', 'into'], ['the', 'round', 'openings', 'in', 'the', 'sides', 'of', 'the', 'tables', 'i', 'was', 'not', 'loath', 'to'], ['follow', 'their', 'example', 'for', 'i', 'felt', 'thirsty', 'and', 'hungry', 'as', 'i', 'did', 'so', 'i'], ['surveyed', 'the', 'hall', 'at', 'my', 'leisure'], [], ['and', 'perhaps', 'the', 'thing', 'that', 'struck', 'me', 'most', 'was', 'its', 'dilapidated', 'look'], ['the', 'stained', 'glass', 'windows', 'which', 'displayed', 'only', 'a', 'geometrical'], ['pattern', 'were', 'broken', 'in', 'many', 'places', 'and', 'the', 'curtains', 'that', 'hung'], ['across', 'the', 'lower', 'end', 'were', 'thick', 'with', 'dust', 'and', 'it', 'caught', 'my', 'eye', 'that'], ['the', 'corner', 'of', 'the', 'marble', 'table', 'near', 'me', 'was', 'fractured', 'nevertheless'], ['the', 'general', 'effect', 'was', 'extremely', 'rich', 'and', 'picturesque', 'there', 'were'], ['perhaps', 'a', 'couple', 'of', 'hundred', 'people', 'dining', 'in', 'the', 'hall', 'and', 'most', 'of'], ['them', 'seated', 'as', 'near', 'to', 'me', 'as', 'they', 'could', 'come', 'were', 'watching', 'me', 'with'], ['interest', 'their', 'little', 'eyes', 'shining', 'over', 'the', 'fruit', 'they', 'were', 'eating'], ['all', 'were', 'clad', 'in', 'the', 'same', 'soft', 'and', 'yet', 'strong', 'silky', 'material'], [], ['fruit', 'by', 'the', 'by', 'was', 'all', 'their', 'diet', 'these', 'people', 'of', 'the', 'remote'], ['future', 'were', 'strict', 'vegetarians', 'and', 'while', 'i', 'was', 'with', 'them', 'in', 'spite'], ['of', 'some', 'carnal', 'cravings', 'i', 'had', 'to', 'be', 'frugivorous', 'also', 'indeed', 'i'], ['found', 'afterwards', 'that', 'horses', 'cattle', 'sheep', 'dogs', 'had', 'followed', 'the'], ['ichthyosaurus', 'into', 'extinction', 'but', 'the', 'fruits', 'were', 'very', 'delightful'], ['one', 'in', 'particular', 'that', 'seemed', 'to', 'be', 'in', 'season', 'all', 'the', 'time', 'i', 'was'], ['there', 'a', 'floury', 'thing', 'in', 'a', 'three', 'sided', 'husk', 'was', 'especially', 'good'], ['and', 'i', 'made', 'it', 'my', 'staple', 'at', 'first', 'i', 'was', 'puzzled', 'by', 'all', 'these', 'strange'], ['fruits', 'and', 'by', 'the', 'strange', 'flowers', 'i', 'saw', 'but', 'later', 'i', 'began', 'to'], ['perceive', 'their', 'import'], [], ['however', 'i', 'am', 'telling', 'you', 'of', 'my', 'fruit', 'dinner', 'in', 'the', 'distant', 'future'], ['now', 'so', 'soon', 'as', 'my', 'appetite', 'was', 'a', 'little', 'checked', 'i', 'determined', 'to'], ['make', 'a', 'resolute', 'attempt', 'to', 'learn', 'the', 'speech', 'of', 'these', 'new', 'men', 'of'], ['mine', 'clearly', 'that', 'was', 'the', 'next', 'thing', 'to', 'do', 'the', 'fruits', 'seemed', 'a'], ['convenient', 'thing', 'to', 'begin', 'upon', 'and', 'holding', 'one', 'of', 'these', 'up', 'i', 'began'], ['a', 'series', 'of', 'interrogative', 'sounds', 'and', 'gestures', 'i', 'had', 'some'], ['considerable', 'difficulty', 'in', 'conveying', 'my', 'meaning', 'at', 'first', 'my', 'efforts'], ['met', 'with', 'a', 'stare', 'of', 'surprise', 'or', 'inextinguishable', 'laughter', 'but'], ['presently', 'a', 'fair', 'haired', 'little', 'creature', 'seemed', 'to', 'grasp', 'my', 'intention'], ['and', 'repeated', 'a', 'name', 'they', 'had', 'to', 'chatter', 'and', 'explain', 'the', 'business'], ['at', 'great', 'length', 'to', 'each', 'other', 'and', 'my', 'first', 'attempts', 'to', 'make', 'the'], ['exquisite', 'little', 'sounds', 'of', 'their', 'language', 'caused', 'an', 'immense', 'amount'], ['of', 'amusement', 'however', 'i', 'felt', 'like', 'a', 'schoolmaster', 'amidst', 'children'], ['and', 'persisted', 'and', 'presently', 'i', 'had', 'a', 'score', 'of', 'noun', 'substantives', 'at'], ['least', 'at', 'my', 'command', 'and', 'then', 'i', 'got', 'to', 'demonstrative', 'pronouns', 'and'], ['even', 'the', 'verb', 'to', 'eat', 'but', 'it', 'was', 'slow', 'work', 'and', 'the', 'little', 'people'], ['soon', 'tired', 'and', 'wanted', 'to', 'get', 'away', 'from', 'my', 'interrogations', 'so', 'i'], ['determined', 'rather', 'of', 'necessity', 'to', 'let', 'them', 'give', 'their', 'lessons', 'in'], ['little', 'doses', 'when', 'they', 'felt', 'inclined', 'and', 'very', 'little', 'doses', 'i', 'found'], ['they', 'were', 'before', 'long', 'for', 'i', 'never', 'met', 'people', 'more', 'indolent', 'or', 'more'], ['easily', 'fatigued'], [], ['a', 'queer', 'thing', 'i', 'soon', 'discovered', 'about', 'my', 'little', 'hosts', 'and', 'that', 'was'], ['their', 'lack', 'of', 'interest', 'they', 'would', 'come', 'to', 'me', 'with', 'eager', 'cries', 'of'], ['astonishment', 'like', 'children', 'but', 'like', 'children', 'they', 'would', 'soon', 'stop'], ['examining', 'me', 'and', 'wander', 'away', 'after', 'some', 'other', 'toy', 'the', 'dinner', 'and', 'my'], ['conversational', 'beginnings', 'ended', 'i', 'noted', 'for', 'the', 'first', 'time', 'that'], ['almost', 'all', 'those', 'who', 'had', 'surrounded', 'me', 'at', 'first', 'were', 'gone', 'it', 'is'], ['odd', 'too', 'how', 'speedily', 'i', 'came', 'to', 'disregard', 'these', 'little', 'people', 'i'], ['went', 'out', 'through', 'the', 'portal', 'into', 'the', 'sunlit', 'world', 'again', 'as', 'soon', 'as'], ['my', 'hunger', 'was', 'satisfied', 'i', 'was', 'continually', 'meeting', 'more', 'of', 'these', 'men'], ['of', 'the', 'future', 'who', 'would', 'follow', 'me', 'a', 'little', 'distance', 'chatter', 'and'], ['laugh', 'about', 'me', 'and', 'having', 'smiled', 'and', 'gesticulated', 'in', 'a', 'friendly'], ['way', 'leave', 'me', 'again', 'to', 'my', 'own', 'devices'], [], ['the', 'calm', 'of', 'evening', 'was', 'upon', 'the', 'world', 'as', 'i', 'emerged', 'from', 'the', 'great'], ['hall', 'and', 'the', 'scene', 'was', 'lit', 'by', 'the', 'warm', 'glow', 'of', 'the', 'setting', 'sun'], ['at', 'first', 'things', 'were', 'very', 'confusing', 'everything', 'was', 'so', 'entirely'], ['different', 'from', 'the', 'world', 'i', 'had', 'known', 'even', 'the', 'flowers', 'the', 'big'], ['building', 'i', 'had', 'left', 'was', 'situated', 'on', 'the', 'slope', 'of', 'a', 'broad', 'river'], ['valley', 'but', 'the', 'thames', 'had', 'shifted', 'perhaps', 'a', 'mile', 'from', 'its', 'present'], ['position', 'i', 'resolved', 'to', 'mount', 'to', 'the', 'summit', 'of', 'a', 'crest', 'perhaps', 'a'], ['mile', 'and', 'a', 'half', 'away', 'from', 'which', 'i', 'could', 'get', 'a', 'wider', 'view', 'of', 'this'], ['our', 'planet', 'in', 'the', 'year', 'eight', 'hundred', 'and', 'two', 'thousand', 'seven', 'hundred'], ['and', 'one', 'a', 'd', 'for', 'that', 'i', 'should', 'explain', 'was', 'the', 'date', 'the', 'little'], ['dials', 'of', 'my', 'machine', 'recorded'], [], ['as', 'i', 'walked', 'i', 'was', 'watching', 'for', 'every', 'impression', 'that', 'could', 'possibly'], ['help', 'to', 'explain', 'the', 'condition', 'of', 'ruinous', 'splendour', 'in', 'which', 'i'], ['found', 'the', 'world', 'for', 'ruinous', 'it', 'was', 'a', 'little', 'way', 'up', 'the', 'hill', 'for'], ['instance', 'was', 'a', 'great', 'heap', 'of', 'granite', 'bound', 'together', 'by', 'masses', 'of'], ['aluminium', 'a', 'vast', 'labyrinth', 'of', 'precipitous', 'walls', 'and', 'crumpled'], ['heaps', 'amidst', 'which', 'were', 'thick', 'heaps', 'of', 'very', 'beautiful', 'pagoda', 'like'], ['plants', 'nettles', 'possibly', 'but', 'wonderfully', 'tinted', 'with', 'brown', 'about'], ['the', 'leaves', 'and', 'incapable', 'of', 'stinging', 'it', 'was', 'evidently', 'the', 'derelict'], ['remains', 'of', 'some', 'vast', 'structure', 'to', 'what', 'end', 'built', 'i', 'could', 'not'], ['determine', 'it', 'was', 'here', 'that', 'i', 'was', 'destined', 'at', 'a', 'later', 'date', 'to', 'have'], ['a', 'very', 'strange', 'experience', 'the', 'first', 'intimation', 'of', 'a', 'still', 'stranger'], ['discovery', 'but', 'of', 'that', 'i', 'will', 'speak', 'in', 'its', 'proper', 'place'], [], ['looking', 'round', 'with', 'a', 'sudden', 'thought', 'from', 'a', 'terrace', 'on', 'which', 'i'], ['rested', 'for', 'a', 'while', 'i', 'realized', 'that', 'there', 'were', 'no', 'small', 'houses', 'to', 'be'], ['seen', 'apparently', 'the', 'single', 'house', 'and', 'possibly', 'even', 'the', 'household'], ['had', 'vanished', 'here', 'and', 'there', 'among', 'the', 'greenery', 'were', 'palace', 'like'], ['buildings', 'but', 'the', 'house', 'and', 'the', 'cottage', 'which', 'form', 'such'], ['characteristic', 'features', 'of', 'our', 'own', 'english', 'landscape', 'had'], ['disappeared'], [], ['communism', 'said', 'i', 'to', 'myself'], [], ['and', 'on', 'the', 'heels', 'of', 'that', 'came', 'another', 'thought', 'i', 'looked', 'at', 'the'], ['half', 'dozen', 'little', 'figures', 'that', 'were', 'following', 'me', 'then', 'in', 'a', 'flash'], ['i', 'perceived', 'that', 'all', 'had', 'the', 'same', 'form', 'of', 'costume', 'the', 'same', 'soft'], ['hairless', 'visage', 'and', 'the', 'same', 'girlish', 'rotundity', 'of', 'limb', 'it', 'may', 'seem'], ['strange', 'perhaps', 'that', 'i', 'had', 'not', 'noticed', 'this', 'before', 'but', 'everything'], ['was', 'so', 'strange', 'now', 'i', 'saw', 'the', 'fact', 'plainly', 'enough', 'in', 'costume', 'and'], ['in', 'all', 'the', 'differences', 'of', 'texture', 'and', 'bearing', 'that', 'now', 'mark', 'off', 'the'], ['sexes', 'from', 'each', 'other', 'these', 'people', 'of', 'the', 'future', 'were', 'alike', 'and'], ['the', 'children', 'seemed', 'to', 'my', 'eyes', 'to', 'be', 'but', 'the', 'miniatures', 'of', 'their'], ['parents', 'i', 'judged', 'then', 'that', 'the', 'children', 'of', 'that', 'time', 'were'], ['extremely', 'precocious', 'physically', 'at', 'least', 'and', 'i', 'found', 'afterwards'], ['abundant', 'verification', 'of', 'my', 'opinion'], [], ['seeing', 'the', 'ease', 'and', 'security', 'in', 'which', 'these', 'people', 'were', 'living', 'i'], ['felt', 'that', 'this', 'close', 'resemblance', 'of', 'the', 'sexes', 'was', 'after', 'all', 'what'], ['one', 'would', 'expect', 'for', 'the', 'strength', 'of', 'a', 'man', 'and', 'the', 'softness', 'of', 'a'], ['woman', 'the', 'institution', 'of', 'the', 'family', 'and', 'the', 'differentiation', 'of'], ['occupations', 'are', 'mere', 'militant', 'necessities', 'of', 'an', 'age', 'of', 'physical'], ['force', 'where', 'population', 'is', 'balanced', 'and', 'abundant', 'much', 'childbearing'], ['becomes', 'an', 'evil', 'rather', 'than', 'a', 'blessing', 'to', 'the', 'state', 'where'], ['violence', 'comes', 'but', 'rarely', 'and', 'off', 'spring', 'are', 'secure', 'there', 'is', 'less'], ['necessity', 'indeed', 'there', 'is', 'no', 'necessity', 'for', 'an', 'efficient', 'family'], ['and', 'the', 'specialization', 'of', 'the', 'sexes', 'with', 'reference', 'to', 'their'], ['children', 's', 'needs', 'disappears', 'we', 'see', 'some', 'beginnings', 'of', 'this', 'even'], ['in', 'our', 'own', 'time', 'and', 'in', 'this', 'future', 'age', 'it', 'was', 'complete', 'this', 'i'], ['must', 'remind', 'you', 'was', 'my', 'speculation', 'at', 'the', 'time', 'later', 'i', 'was', 'to'], ['appreciate', 'how', 'far', 'it', 'fell', 'short', 'of', 'the', 'reality'], [], ['while', 'i', 'was', 'musing', 'upon', 'these', 'things', 'my', 'attention', 'was', 'attracted', 'by'], ['a', 'pretty', 'little', 'structure', 'like', 'a', 'well', 'under', 'a', 'cupola', 'i', 'thought', 'in'], ['a', 'transitory', 'way', 'of', 'the', 'oddness', 'of', 'wells', 'still', 'existing', 'and', 'then'], ['resumed', 'the', 'thread', 'of', 'my', 'speculations', 'there', 'were', 'no', 'large', 'buildings'], ['towards', 'the', 'top', 'of', 'the', 'hill', 'and', 'as', 'my', 'walking', 'powers', 'were', 'evidently'], ['miraculous', 'i', 'was', 'presently', 'left', 'alone', 'for', 'the', 'first', 'time', 'with', 'a'], ['strange', 'sense', 'of', 'freedom', 'and', 'adventure', 'i', 'pushed', 'on', 'up', 'to', 'the', 'crest'], [], ['there', 'i', 'found', 'a', 'seat', 'of', 'some', 'yellow', 'metal', 'that', 'i', 'did', 'not', 'recognize'], ['corroded', 'in', 'places', 'with', 'a', 'kind', 'of', 'pinkish', 'rust', 'and', 'half', 'smothered'], ['in', 'soft', 'moss', 'the', 'arm', 'rests', 'cast', 'and', 'filed', 'into', 'the', 'resemblance', 'of'], ['griffins', 'heads', 'i', 'sat', 'down', 'on', 'it', 'and', 'i', 'surveyed', 'the', 'broad', 'view', 'of'], ['our', 'old', 'world', 'under', 'the', 'sunset', 'of', 'that', 'long', 'day', 'it', 'was', 'as', 'sweet', 'and'], ['fair', 'a', 'view', 'as', 'i', 'have', 'ever', 'seen', 'the', 'sun', 'had', 'already', 'gone', 'below', 'the'], ['horizon', 'and', 'the', 'west', 'was', 'flaming', 'gold', 'touched', 'with', 'some', 'horizontal'], ['bars', 'of', 'purple', 'and', 'crimson', 'below', 'was', 'the', 'valley', 'of', 'the', 'thames', 'in'], ['which', 'the', 'river', 'lay', 'like', 'a', 'band', 'of', 'burnished', 'steel', 'i', 'have', 'already'], ['spoken', 'of', 'the', 'great', 'palaces', 'dotted', 'about', 'among', 'the', 'variegated'], ['greenery', 'some', 'in', 'ruins', 'and', 'some', 'still', 'occupied', 'here', 'and', 'there', 'rose'], ['a', 'white', 'or', 'silvery', 'figure', 'in', 'the', 'waste', 'garden', 'of', 'the', 'earth', 'here', 'and'], ['there', 'came', 'the', 'sharp', 'vertical', 'line', 'of', 'some', 'cupola', 'or', 'obelisk', 'there'], ['were', 'no', 'hedges', 'no', 'signs', 'of', 'proprietary', 'rights', 'no', 'evidences', 'of'], ['agriculture', 'the', 'whole', 'earth', 'had', 'become', 'a', 'garden'], [], ['so', 'watching', 'i', 'began', 'to', 'put', 'my', 'interpretation', 'upon', 'the', 'things', 'i', 'had'], ['seen', 'and', 'as', 'it', 'shaped', 'itself', 'to', 'me', 'that', 'evening', 'my', 'interpretation'], ['was', 'something', 'in', 'this', 'way', 'afterwards', 'i', 'found', 'i', 'had', 'got', 'only', 'a'], ['half', 'truth', 'or', 'only', 'a', 'glimpse', 'of', 'one', 'facet', 'of', 'the', 'truth'], [], ['it', 'seemed', 'to', 'me', 'that', 'i', 'had', 'happened', 'upon', 'humanity', 'upon', 'the', 'wane'], ['the', 'ruddy', 'sunset', 'set', 'me', 'thinking', 'of', 'the', 'sunset', 'of', 'mankind', 'for', 'the'], ['first', 'time', 'i', 'began', 'to', 'realize', 'an', 'odd', 'consequence', 'of', 'the', 'social'], ['effort', 'in', 'which', 'we', 'are', 'at', 'present', 'engaged', 'and', 'yet', 'come', 'to', 'think'], ['it', 'is', 'a', 'logical', 'consequence', 'enough', 'strength', 'is', 'the', 'outcome', 'of', 'need'], ['security', 'sets', 'a', 'premium', 'on', 'feebleness', 'the', 'work', 'of', 'ameliorating', 'the'], ['conditions', 'of', 'life', 'the', 'true', 'civilizing', 'process', 'that', 'makes', 'life', 'more'], ['and', 'more', 'secure', 'had', 'gone', 'steadily', 'on', 'to', 'a', 'climax', 'one', 'triumph', 'of', 'a'], ['united', 'humanity', 'over', 'nature', 'had', 'followed', 'another', 'things', 'that', 'are'], ['now', 'mere', 'dreams', 'had', 'become', 'projects', 'deliberately', 'put', 'in', 'hand', 'and'], ['carried', 'forward', 'and', 'the', 'harvest', 'was', 'what', 'i', 'saw'], [], ['after', 'all', 'the', 'sanitation', 'and', 'the', 'agriculture', 'of', 'to', 'day', 'are', 'still'], ['in', 'the', 'rudimentary', 'stage', 'the', 'science', 'of', 'our', 'time', 'has', 'attacked', 'but'], ['a', 'little', 'department', 'of', 'the', 'field', 'of', 'human', 'disease', 'but', 'even', 'so'], ['it', 'spreads', 'its', 'operations', 'very', 'steadily', 'and', 'persistently', 'our'], ['agriculture', 'and', 'horticulture', 'destroy', 'a', 'weed', 'just', 'here', 'and', 'there', 'and'], ['cultivate', 'perhaps', 'a', 'score', 'or', 'so', 'of', 'wholesome', 'plants', 'leaving', 'the'], ['greater', 'number', 'to', 'fight', 'out', 'a', 'balance', 'as', 'they', 'can', 'we', 'improve', 'our'], ['favourite', 'plants', 'and', 'animals', 'and', 'how', 'few', 'they', 'are', 'gradually', 'by'], ['selective', 'breeding', 'now', 'a', 'new', 'and', 'better', 'peach', 'now', 'a', 'seedless'], ['grape', 'now', 'a', 'sweeter', 'and', 'larger', 'flower', 'now', 'a', 'more', 'convenient', 'breed'], ['of', 'cattle', 'we', 'improve', 'them', 'gradually', 'because', 'our', 'ideals', 'are', 'vague'], ['and', 'tentative', 'and', 'our', 'knowledge', 'is', 'very', 'limited', 'because', 'nature'], ['too', 'is', 'shy', 'and', 'slow', 'in', 'our', 'clumsy', 'hands', 'some', 'day', 'all', 'this', 'will'], ['be', 'better', 'organized', 'and', 'still', 'better', 'that', 'is', 'the', 'drift', 'of', 'the'], ['current', 'in', 'spite', 'of', 'the', 'eddies', 'the', 'whole', 'world', 'will', 'be', 'intelligent'], ['educated', 'and', 'co', 'operating', 'things', 'will', 'move', 'faster', 'and', 'faster'], ['towards', 'the', 'subjugation', 'of', 'nature', 'in', 'the', 'end', 'wisely', 'and', 'carefully'], ['we', 'shall', 'readjust', 'the', 'balance', 'of', 'animal', 'and', 'vegetable', 'life', 'to', 'suit'], ['our', 'human', 'needs'], [], ['this', 'adjustment', 'i', 'say', 'must', 'have', 'been', 'done', 'and', 'done', 'well', 'done'], ['indeed', 'for', 'all', 'time', 'in', 'the', 'space', 'of', 'time', 'across', 'which', 'my', 'machine'], ['had', 'leaped', 'the', 'air', 'was', 'free', 'from', 'gnats', 'the', 'earth', 'from', 'weeds', 'or'], ['fungi', 'everywhere', 'were', 'fruits', 'and', 'sweet', 'and', 'delightful', 'flowers'], ['brilliant', 'butterflies', 'flew', 'hither', 'and', 'thither', 'the', 'ideal', 'of'], ['preventive', 'medicine', 'was', 'attained', 'diseases', 'had', 'been', 'stamped', 'out', 'i'], ['saw', 'no', 'evidence', 'of', 'any', 'contagious', 'diseases', 'during', 'all', 'my', 'stay', 'and', 'i'], ['shall', 'have', 'to', 'tell', 'you', 'later', 'that', 'even', 'the', 'processes', 'of', 'putrefaction'], ['and', 'decay', 'had', 'been', 'profoundly', 'affected', 'by', 'these', 'changes'], [], ['social', 'triumphs', 'too', 'had', 'been', 'effected', 'i', 'saw', 'mankind', 'housed', 'in'], ['splendid', 'shelters', 'gloriously', 'clothed', 'and', 'as', 'yet', 'i', 'had', 'found', 'them'], ['engaged', 'in', 'no', 'toil', 'there', 'were', 'no', 'signs', 'of', 'struggle', 'neither', 'social'], ['nor', 'economical', 'struggle', 'the', 'shop', 'the', 'advertisement', 'traffic', 'all'], ['that', 'commerce', 'which', 'constitutes', 'the', 'body', 'of', 'our', 'world', 'was', 'gone', 'it'], ['was', 'natural', 'on', 'that', 'golden', 'evening', 'that', 'i', 'should', 'jump', 'at', 'the', 'idea', 'of'], ['a', 'social', 'paradise', 'the', 'difficulty', 'of', 'increasing', 'population', 'had', 'been'], ['met', 'i', 'guessed', 'and', 'population', 'had', 'ceased', 'to', 'increase'], [], ['but', 'with', 'this', 'change', 'in', 'condition', 'comes', 'inevitably', 'adaptations', 'to'], ['the', 'change', 'what', 'unless', 'biological', 'science', 'is', 'a', 'mass', 'of', 'errors', 'is'], ['the', 'cause', 'of', 'human', 'intelligence', 'and', 'vigour', 'hardship', 'and', 'freedom'], ['conditions', 'under', 'which', 'the', 'active', 'strong', 'and', 'subtle', 'survive', 'and'], ['the', 'weaker', 'go', 'to', 'the', 'wall', 'conditions', 'that', 'put', 'a', 'premium', 'upon', 'the'], ['loyal', 'alliance', 'of', 'capable', 'men', 'upon', 'self', 'restraint', 'patience', 'and'], ['decision', 'and', 'the', 'institution', 'of', 'the', 'family', 'and', 'the', 'emotions', 'that'], ['arise', 'therein', 'the', 'fierce', 'jealousy', 'the', 'tenderness', 'for', 'offspring'], ['parental', 'self', 'devotion', 'all', 'found', 'their', 'justification', 'and', 'support', 'in'], ['the', 'imminent', 'dangers', 'of', 'the', 'young', 'now', 'where', 'are', 'these', 'imminent'], ['dangers', 'there', 'is', 'a', 'sentiment', 'arising', 'and', 'it', 'will', 'grow', 'against'], ['connubial', 'jealousy', 'against', 'fierce', 'maternity', 'against', 'passion'], ['of', 'all', 'sorts', 'unnecessary', 'things', 'now', 'and', 'things', 'that', 'make', 'us'], ['uncomfortable', 'savage', 'survivals', 'discords', 'in', 'a', 'refined', 'and', 'pleasant'], ['life'], [], ['i', 'thought', 'of', 'the', 'physical', 'slightness', 'of', 'the', 'people', 'their', 'lack', 'of'], ['intelligence', 'and', 'those', 'big', 'abundant', 'ruins', 'and', 'it', 'strengthened', 'my'], ['belief', 'in', 'a', 'perfect', 'conquest', 'of', 'nature', 'for', 'after', 'the', 'battle', 'comes'], ['quiet', 'humanity', 'had', 'been', 'strong', 'energetic', 'and', 'intelligent', 'and', 'had'], ['used', 'all', 'its', 'abundant', 'vitality', 'to', 'alter', 'the', 'conditions', 'under', 'which'], ['it', 'lived', 'and', 'now', 'came', 'the', 'reaction', 'of', 'the', 'altered', 'conditions'], [], ['under', 'the', 'new', 'conditions', 'of', 'perfect', 'comfort', 'and', 'security', 'that'], ['restless', 'energy', 'that', 'with', 'us', 'is', 'strength', 'would', 'become', 'weakness'], ['even', 'in', 'our', 'own', 'time', 'certain', 'tendencies', 'and', 'desires', 'once', 'necessary'], ['to', 'survival', 'are', 'a', 'constant', 'source', 'of', 'failure', 'physical', 'courage', 'and'], ['the', 'love', 'of', 'battle', 'for', 'instance', 'are', 'no', 'great', 'help', 'may', 'even', 'be'], ['hindrances', 'to', 'a', 'civilized', 'man', 'and', 'in', 'a', 'state', 'of', 'physical', 'balance'], ['and', 'security', 'power', 'intellectual', 'as', 'well', 'as', 'physical', 'would', 'be', 'out'], ['of', 'place', 'for', 'countless', 'years', 'i', 'judged', 'there', 'had', 'been', 'no', 'danger', 'of'], ['war', 'or', 'solitary', 'violence', 'no', 'danger', 'from', 'wild', 'beasts', 'no', 'wasting'], ['disease', 'to', 'require', 'strength', 'of', 'constitution', 'no', 'need', 'of', 'toil', 'for'], ['such', 'a', 'life', 'what', 'we', 'should', 'call', 'the', 'weak', 'are', 'as', 'well', 'equipped', 'as'], ['the', 'strong', 'are', 'indeed', 'no', 'longer', 'weak', 'better', 'equipped', 'indeed', 'they'], ['are', 'for', 'the', 'strong', 'would', 'be', 'fretted', 'by', 'an', 'energy', 'for', 'which', 'there'], ['was', 'no', 'outlet', 'no', 'doubt', 'the', 'exquisite', 'beauty', 'of', 'the', 'buildings', 'i', 'saw'], ['was', 'the', 'outcome', 'of', 'the', 'last', 'surgings', 'of', 'the', 'now', 'purposeless', 'energy'], ['of', 'mankind', 'before', 'it', 'settled', 'down', 'into', 'perfect', 'harmony', 'with', 'the'], ['conditions', 'under', 'which', 'it', 'lived', 'the', 'flourish', 'of', 'that', 'triumph', 'which'], ['began', 'the', 'last', 'great', 'peace', 'this', 'has', 'ever', 'been', 'the', 'fate', 'of', 'energy', 'in'], ['security', 'it', 'takes', 'to', 'art', 'and', 'to', 'eroticism', 'and', 'then', 'come', 'languor'], ['and', 'decay'], [], ['even', 'this', 'artistic', 'impetus', 'would', 'at', 'last', 'die', 'away', 'had', 'almost', 'died'], ['in', 'the', 'time', 'i', 'saw', 'to', 'adorn', 'themselves', 'with', 'flowers', 'to', 'dance', 'to'], ['sing', 'in', 'the', 'sunlight', 'so', 'much', 'was', 'left', 'of', 'the', 'artistic', 'spirit', 'and'], ['no', 'more', 'even', 'that', 'would', 'fade', 'in', 'the', 'end', 'into', 'a', 'contented'], ['inactivity', 'we', 'are', 'kept', 'keen', 'on', 'the', 'grindstone', 'of', 'pain', 'and'], ['necessity', 'and', 'it', 'seemed', 'to', 'me', 'that', 'here', 'was', 'that', 'hateful'], ['grindstone', 'broken', 'at', 'last'], [], ['as', 'i', 'stood', 'there', 'in', 'the', 'gathering', 'dark', 'i', 'thought', 'that', 'in', 'this'], ['simple', 'explanation', 'i', 'had', 'mastered', 'the', 'problem', 'of', 'the', 'world', 'mastered'], ['the', 'whole', 'secret', 'of', 'these', 'delicious', 'people', 'possibly', 'the', 'checks', 'they'], ['had', 'devised', 'for', 'the', 'increase', 'of', 'population', 'had', 'succeeded', 'too', 'well'], ['and', 'their', 'numbers', 'had', 'rather', 'diminished', 'than', 'kept', 'stationary'], ['that', 'would', 'account', 'for', 'the', 'abandoned', 'ruins', 'very', 'simple', 'was', 'my'], ['explanation', 'and', 'plausible', 'enough', 'as', 'most', 'wrong', 'theories', 'are'], [], [], [], [], ['v'], [], [], ['as', 'i', 'stood', 'there', 'musing', 'over', 'this', 'too', 'perfect', 'triumph', 'of', 'man', 'the'], ['full', 'moon', 'yellow', 'and', 'gibbous', 'came', 'up', 'out', 'of', 'an', 'overflow', 'of', 'silver'], ['light', 'in', 'the', 'north', 'east', 'the', 'bright', 'little', 'figures', 'ceased', 'to', 'move'], ['about', 'below', 'a', 'noiseless', 'owl', 'flitted', 'by', 'and', 'i', 'shivered', 'with', 'the'], ['chill', 'of', 'the', 'night', 'i', 'determined', 'to', 'descend', 'and', 'find', 'where', 'i', 'could'], ['sleep'], [], ['i', 'looked', 'for', 'the', 'building', 'i', 'knew', 'then', 'my', 'eye', 'travelled', 'along', 'to'], ['the', 'figure', 'of', 'the', 'white', 'sphinx', 'upon', 'the', 'pedestal', 'of', 'bronze', 'growing'], ['distinct', 'as', 'the', 'light', 'of', 'the', 'rising', 'moon', 'grew', 'brighter', 'i', 'could', 'see'], ['the', 'silver', 'birch', 'against', 'it', 'there', 'was', 'the', 'tangle', 'of', 'rhododendron'], ['bushes', 'black', 'in', 'the', 'pale', 'light', 'and', 'there', 'was', 'the', 'little', 'lawn'], ['i', 'looked', 'at', 'the', 'lawn', 'again', 'a', 'queer', 'doubt', 'chilled', 'my', 'complacency'], ['no', 'said', 'i', 'stoutly', 'to', 'myself', 'that', 'was', 'not', 'the', 'lawn'], [], ['but', 'it', 'was', 'the', 'lawn', 'for', 'the', 'white', 'leprous', 'face', 'of', 'the', 'sphinx', 'was'], ['towards', 'it', 'can', 'you', 'imagine', 'what', 'i', 'felt', 'as', 'this', 'conviction', 'came'], ['home', 'to', 'me', 'but', 'you', 'cannot', 'the', 'time', 'machine', 'was', 'gone'], [], ['at', 'once', 'like', 'a', 'lash', 'across', 'the', 'face', 'came', 'the', 'possibility', 'of'], ['losing', 'my', 'own', 'age', 'of', 'being', 'left', 'helpless', 'in', 'this', 'strange', 'new', 'world'], ['the', 'bare', 'thought', 'of', 'it', 'was', 'an', 'actual', 'physical', 'sensation', 'i', 'could'], ['feel', 'it', 'grip', 'me', 'at', 'the', 'throat', 'and', 'stop', 'my', 'breathing', 'in', 'another'], ['moment', 'i', 'was', 'in', 'a', 'passion', 'of', 'fear', 'and', 'running', 'with', 'great', 'leaping'], ['strides', 'down', 'the', 'slope', 'once', 'i', 'fell', 'headlong', 'and', 'cut', 'my', 'face', 'i', 'lost'], ['no', 'time', 'in', 'stanching', 'the', 'blood', 'but', 'jumped', 'up', 'and', 'ran', 'on', 'with', 'a'], ['warm', 'trickle', 'down', 'my', 'cheek', 'and', 'chin', 'all', 'the', 'time', 'i', 'ran', 'i', 'was', 'saying'], ['to', 'myself', 'they', 'have', 'moved', 'it', 'a', 'little', 'pushed', 'it', 'under', 'the', 'bushes'], ['out', 'of', 'the', 'way', 'nevertheless', 'i', 'ran', 'with', 'all', 'my', 'might', 'all', 'the'], ['time', 'with', 'the', 'certainty', 'that', 'sometimes', 'comes', 'with', 'excessive', 'dread'], ['i', 'knew', 'that', 'such', 'assurance', 'was', 'folly', 'knew', 'instinctively', 'that', 'the'], ['machine', 'was', 'removed', 'out', 'of', 'my', 'reach', 'my', 'breath', 'came', 'with', 'pain', 'i'], ['suppose', 'i', 'covered', 'the', 'whole', 'distance', 'from', 'the', 'hill', 'crest', 'to', 'the'], ['little', 'lawn', 'two', 'miles', 'perhaps', 'in', 'ten', 'minutes', 'and', 'i', 'am', 'not', 'a', 'young'], ['man', 'i', 'cursed', 'aloud', 'as', 'i', 'ran', 'at', 'my', 'confident', 'folly', 'in', 'leaving', 'the'], ['machine', 'wasting', 'good', 'breath', 'thereby', 'i', 'cried', 'aloud', 'and', 'none'], ['answered', 'not', 'a', 'creature', 'seemed', 'to', 'be', 'stirring', 'in', 'that', 'moonlit'], ['world'], [], ['when', 'i', 'reached', 'the', 'lawn', 'my', 'worst', 'fears', 'were', 'realized', 'not', 'a', 'trace'], ['of', 'the', 'thing', 'was', 'to', 'be', 'seen', 'i', 'felt', 'faint', 'and', 'cold', 'when', 'i', 'faced', 'the'], ['empty', 'space', 'among', 'the', 'black', 'tangle', 'of', 'bushes', 'i', 'ran', 'round', 'it'], ['furiously', 'as', 'if', 'the', 'thing', 'might', 'be', 'hidden', 'in', 'a', 'corner', 'and', 'then'], ['stopped', 'abruptly', 'with', 'my', 'hands', 'clutching', 'my', 'hair', 'above', 'me', 'towered'], ['the', 'sphinx', 'upon', 'the', 'bronze', 'pedestal', 'white', 'shining', 'leprous', 'in'], ['the', 'light', 'of', 'the', 'rising', 'moon', 'it', 'seemed', 'to', 'smile', 'in', 'mockery', 'of', 'my'], ['dismay'], [], ['i', 'might', 'have', 'consoled', 'myself', 'by', 'imagining', 'the', 'little', 'people', 'had', 'put'], ['the', 'mechanism', 'in', 'some', 'shelter', 'for', 'me', 'had', 'i', 'not', 'felt', 'assured', 'of'], ['their', 'physical', 'and', 'intellectual', 'inadequacy', 'that', 'is', 'what', 'dismayed'], ['me', 'the', 'sense', 'of', 'some', 'hitherto', 'unsuspected', 'power', 'through', 'whose'], ['intervention', 'my', 'invention', 'had', 'vanished', 'yet', 'for', 'one', 'thing', 'i', 'felt'], ['assured', 'unless', 'some', 'other', 'age', 'had', 'produced', 'its', 'exact', 'duplicate'], ['the', 'machine', 'could', 'not', 'have', 'moved', 'in', 'time', 'the', 'attachment', 'of', 'the'], ['levers', 'i', 'will', 'show', 'you', 'the', 'method', 'later', 'prevented', 'any', 'one', 'from'], ['tampering', 'with', 'it', 'in', 'that', 'way', 'when', 'they', 'were', 'removed', 'it', 'had', 'moved'], ['and', 'was', 'hid', 'only', 'in', 'space', 'but', 'then', 'where', 'could', 'it', 'be'], [], ['i', 'think', 'i', 'must', 'have', 'had', 'a', 'kind', 'of', 'frenzy', 'i', 'remember', 'running'], ['violently', 'in', 'and', 'out', 'among', 'the', 'moonlit', 'bushes', 'all', 'round', 'the', 'sphinx'], ['and', 'startling', 'some', 'white', 'animal', 'that', 'in', 'the', 'dim', 'light', 'i', 'took', 'for', 'a'], ['small', 'deer', 'i', 'remember', 'too', 'late', 'that', 'night', 'beating', 'the', 'bushes'], ['with', 'my', 'clenched', 'fist', 'until', 'my', 'knuckles', 'were', 'gashed', 'and', 'bleeding'], ['from', 'the', 'broken', 'twigs', 'then', 'sobbing', 'and', 'raving', 'in', 'my', 'anguish', 'of'], ['mind', 'i', 'went', 'down', 'to', 'the', 'great', 'building', 'of', 'stone', 'the', 'big', 'hall', 'was'], ['dark', 'silent', 'and', 'deserted', 'i', 'slipped', 'on', 'the', 'uneven', 'floor', 'and', 'fell'], ['over', 'one', 'of', 'the', 'malachite', 'tables', 'almost', 'breaking', 'my', 'shin', 'i', 'lit', 'a'], ['match', 'and', 'went', 'on', 'past', 'the', 'dusty', 'curtains', 'of', 'which', 'i', 'have', 'told', 'you'], [], ['there', 'i', 'found', 'a', 'second', 'great', 'hall', 'covered', 'with', 'cushions', 'upon'], ['which', 'perhaps', 'a', 'score', 'or', 'so', 'of', 'the', 'little', 'people', 'were', 'sleeping', 'i'], ['have', 'no', 'doubt', 'they', 'found', 'my', 'second', 'appearance', 'strange', 'enough', 'coming'], ['suddenly', 'out', 'of', 'the', 'quiet', 'darkness', 'with', 'inarticulate', 'noises', 'and', 'the'], ['splutter', 'and', 'flare', 'of', 'a', 'match', 'for', 'they', 'had', 'forgotten', 'about', 'matches'], ['where', 'is', 'my', 'time', 'machine', 'i', 'began', 'bawling', 'like', 'an', 'angry', 'child'], ['laying', 'hands', 'upon', 'them', 'and', 'shaking', 'them', 'up', 'together', 'it', 'must', 'have'], ['been', 'very', 'queer', 'to', 'them', 'some', 'laughed', 'most', 'of', 'them', 'looked', 'sorely'], ['frightened', 'when', 'i', 'saw', 'them', 'standing', 'round', 'me', 'it', 'came', 'into', 'my', 'head'], ['that', 'i', 'was', 'doing', 'as', 'foolish', 'a', 'thing', 'as', 'it', 'was', 'possible', 'for', 'me', 'to', 'do'], ['under', 'the', 'circumstances', 'in', 'trying', 'to', 'revive', 'the', 'sensation', 'of', 'fear'], ['for', 'reasoning', 'from', 'their', 'daylight', 'behaviour', 'i', 'thought', 'that', 'fear'], ['must', 'be', 'forgotten'], [], ['abruptly', 'i', 'dashed', 'down', 'the', 'match', 'and', 'knocking', 'one', 'of', 'the', 'people'], ['over', 'in', 'my', 'course', 'went', 'blundering', 'across', 'the', 'big', 'dining', 'hall', 'again'], ['out', 'under', 'the', 'moonlight', 'i', 'heard', 'cries', 'of', 'terror', 'and', 'their', 'little'], ['feet', 'running', 'and', 'stumbling', 'this', 'way', 'and', 'that', 'i', 'do', 'not', 'remember', 'all'], ['i', 'did', 'as', 'the', 'moon', 'crept', 'up', 'the', 'sky', 'i', 'suppose', 'it', 'was', 'the', 'unexpected'], ['nature', 'of', 'my', 'loss', 'that', 'maddened', 'me', 'i', 'felt', 'hopelessly', 'cut', 'off', 'from'], ['my', 'own', 'kind', 'a', 'strange', 'animal', 'in', 'an', 'unknown', 'world', 'i', 'must', 'have', 'raved'], ['to', 'and', 'fro', 'screaming', 'and', 'crying', 'upon', 'god', 'and', 'fate', 'i', 'have', 'a', 'memory'], ['of', 'horrible', 'fatigue', 'as', 'the', 'long', 'night', 'of', 'despair', 'wore', 'away', 'of'], ['looking', 'in', 'this', 'impossible', 'place', 'and', 'that', 'of', 'groping', 'among', 'moon', 'lit'], ['ruins', 'and', 'touching', 'strange', 'creatures', 'in', 'the', 'black', 'shadows', 'at', 'last'], ['of', 'lying', 'on', 'the', 'ground', 'near', 'the', 'sphinx', 'and', 'weeping', 'with', 'absolute'], ['wretchedness', 'i', 'had', 'nothing', 'left', 'but', 'misery', 'then', 'i', 'slept', 'and', 'when'], ['i', 'woke', 'again', 'it', 'was', 'full', 'day', 'and', 'a', 'couple', 'of', 'sparrows', 'were', 'hopping'], ['round', 'me', 'on', 'the', 'turf', 'within', 'reach', 'of', 'my', 'arm'], [], ['i', 'sat', 'up', 'in', 'the', 'freshness', 'of', 'the', 'morning', 'trying', 'to', 'remember', 'how'], ['i', 'had', 'got', 'there', 'and', 'why', 'i', 'had', 'such', 'a', 'profound', 'sense', 'of', 'desertion'], ['and', 'despair', 'then', 'things', 'came', 'clear', 'in', 'my', 'mind', 'with', 'the', 'plain'], ['reasonable', 'daylight', 'i', 'could', 'look', 'my', 'circumstances', 'fairly', 'in', 'the'], ['face', 'i', 'saw', 'the', 'wild', 'folly', 'of', 'my', 'frenzy', 'overnight', 'and', 'i', 'could'], ['reason', 'with', 'myself', 'suppose', 'the', 'worst', 'i', 'said', 'suppose', 'the'], ['machine', 'altogether', 'lost', 'perhaps', 'destroyed', 'it', 'behoves', 'me', 'to', 'be'], ['calm', 'and', 'patient', 'to', 'learn', 'the', 'way', 'of', 'the', 'people', 'to', 'get', 'a', 'clear'], ['idea', 'of', 'the', 'method', 'of', 'my', 'loss', 'and', 'the', 'means', 'of', 'getting', 'materials'], ['and', 'tools', 'so', 'that', 'in', 'the', 'end', 'perhaps', 'i', 'may', 'make', 'another', 'that'], ['would', 'be', 'my', 'only', 'hope', 'perhaps', 'but', 'better', 'than', 'despair', 'and', 'after'], ['all', 'it', 'was', 'a', 'beautiful', 'and', 'curious', 'world'], [], ['but', 'probably', 'the', 'machine', 'had', 'only', 'been', 'taken', 'away', 'still', 'i', 'must'], ['be', 'calm', 'and', 'patient', 'find', 'its', 'hiding', 'place', 'and', 'recover', 'it', 'by', 'force'], ['or', 'cunning', 'and', 'with', 'that', 'i', 'scrambled', 'to', 'my', 'feet', 'and', 'looked', 'about'], ['me', 'wondering', 'where', 'i', 'could', 'bathe', 'i', 'felt', 'weary', 'stiff', 'and'], ['travel', 'soiled', 'the', 'freshness', 'of', 'the', 'morning', 'made', 'me', 'desire', 'an', 'equal'], ['freshness', 'i', 'had', 'exhausted', 'my', 'emotion', 'indeed', 'as', 'i', 'went', 'about'], ['my', 'business', 'i', 'found', 'myself', 'wondering', 'at', 'my', 'intense', 'excitement'], ['overnight', 'i', 'made', 'a', 'careful', 'examination', 'of', 'the', 'ground', 'about', 'the'], ['little', 'lawn', 'i', 'wasted', 'some', 'time', 'in', 'futile', 'questionings', 'conveyed', 'as'], ['well', 'as', 'i', 'was', 'able', 'to', 'such', 'of', 'the', 'little', 'people', 'as', 'came', 'by', 'they'], ['all', 'failed', 'to', 'understand', 'my', 'gestures', 'some', 'were', 'simply', 'stolid', 'some'], ['thought', 'it', 'was', 'a', 'jest', 'and', 'laughed', 'at', 'me', 'i', 'had', 'the', 'hardest', 'task', 'in'], ['the', 'world', 'to', 'keep', 'my', 'hands', 'off', 'their', 'pretty', 'laughing', 'faces', 'it', 'was'], ['a', 'foolish', 'impulse', 'but', 'the', 'devil', 'begotten', 'of', 'fear', 'and', 'blind', 'anger'], ['was', 'ill', 'curbed', 'and', 'still', 'eager', 'to', 'take', 'advantage', 'of', 'my', 'perplexity'], ['the', 'turf', 'gave', 'better', 'counsel', 'i', 'found', 'a', 'groove', 'ripped', 'in', 'it', 'about'], ['midway', 'between', 'the', 'pedestal', 'of', 'the', 'sphinx', 'and', 'the', 'marks', 'of', 'my', 'feet'], ['where', 'on', 'arrival', 'i', 'had', 'struggled', 'with', 'the', 'overturned', 'machine'], ['there', 'were', 'other', 'signs', 'of', 'removal', 'about', 'with', 'queer', 'narrow'], ['footprints', 'like', 'those', 'i', 'could', 'imagine', 'made', 'by', 'a', 'sloth', 'this', 'directed'], ['my', 'closer', 'attention', 'to', 'the', 'pedestal', 'it', 'was', 'as', 'i', 'think', 'i', 'have', 'said'], ['of', 'bronze', 'it', 'was', 'not', 'a', 'mere', 'block', 'but', 'highly', 'decorated', 'with', 'deep'], ['framed', 'panels', 'on', 'either', 'side', 'i', 'went', 'and', 'rapped', 'at', 'these', 'the'], ['pedestal', 'was', 'hollow', 'examining', 'the', 'panels', 'with', 'care', 'i', 'found', 'them'], ['discontinuous', 'with', 'the', 'frames', 'there', 'were', 'no', 'handles', 'or', 'keyholes'], ['but', 'possibly', 'the', 'panels', 'if', 'they', 'were', 'doors', 'as', 'i', 'supposed', 'opened'], ['from', 'within', 'one', 'thing', 'was', 'clear', 'enough', 'to', 'my', 'mind', 'it', 'took', 'no', 'very'], ['great', 'mental', 'effort', 'to', 'infer', 'that', 'my', 'time', 'machine', 'was', 'inside', 'that'], ['pedestal', 'but', 'how', 'it', 'got', 'there', 'was', 'a', 'different', 'problem'], [], ['i', 'saw', 'the', 'heads', 'of', 'two', 'orange', 'clad', 'people', 'coming', 'through', 'the', 'bushes'], ['and', 'under', 'some', 'blossom', 'covered', 'apple', 'trees', 'towards', 'me', 'i', 'turned'], ['smiling', 'to', 'them', 'and', 'beckoned', 'them', 'to', 'me', 'they', 'came', 'and', 'then'], ['pointing', 'to', 'the', 'bronze', 'pedestal', 'i', 'tried', 'to', 'intimate', 'my', 'wish', 'to', 'open'], ['it', 'but', 'at', 'my', 'first', 'gesture', 'towards', 'this', 'they', 'behaved', 'very', 'oddly', 'i'], ['don', 't', 'know', 'how', 'to', 'convey', 'their', 'expression', 'to', 'you', 'suppose', 'you', 'were'], ['to', 'use', 'a', 'grossly', 'improper', 'gesture', 'to', 'a', 'delicate', 'minded', 'woman', 'it', 'is'], ['how', 'she', 'would', 'look', 'they', 'went', 'off', 'as', 'if', 'they', 'had', 'received', 'the', 'last'], ['possible', 'insult', 'i', 'tried', 'a', 'sweet', 'looking', 'little', 'chap', 'in', 'white', 'next'], ['with', 'exactly', 'the', 'same', 'result', 'somehow', 'his', 'manner', 'made', 'me', 'feel'], ['ashamed', 'of', 'myself', 'but', 'as', 'you', 'know', 'i', 'wanted', 'the', 'time', 'machine', 'and'], ['i', 'tried', 'him', 'once', 'more', 'as', 'he', 'turned', 'off', 'like', 'the', 'others', 'my', 'temper'], ['got', 'the', 'better', 'of', 'me', 'in', 'three', 'strides', 'i', 'was', 'after', 'him', 'had', 'him', 'by'], ['the', 'loose', 'part', 'of', 'his', 'robe', 'round', 'the', 'neck', 'and', 'began', 'dragging', 'him'], ['towards', 'the', 'sphinx', 'then', 'i', 'saw', 'the', 'horror', 'and', 'repugnance', 'of', 'his'], ['face', 'and', 'all', 'of', 'a', 'sudden', 'i', 'let', 'him', 'go'], [], ['but', 'i', 'was', 'not', 'beaten', 'yet', 'i', 'banged', 'with', 'my', 'fist', 'at', 'the', 'bronze'], ['panels', 'i', 'thought', 'i', 'heard', 'something', 'stir', 'inside', 'to', 'be', 'explicit'], ['i', 'thought', 'i', 'heard', 'a', 'sound', 'like', 'a', 'chuckle', 'but', 'i', 'must', 'have', 'been'], ['mistaken', 'then', 'i', 'got', 'a', 'big', 'pebble', 'from', 'the', 'river', 'and', 'came', 'and'], ['hammered', 'till', 'i', 'had', 'flattened', 'a', 'coil', 'in', 'the', 'decorations', 'and', 'the'], ['verdigris', 'came', 'off', 'in', 'powdery', 'flakes', 'the', 'delicate', 'little', 'people'], ['must', 'have', 'heard', 'me', 'hammering', 'in', 'gusty', 'outbreaks', 'a', 'mile', 'away', 'on'], ['either', 'hand', 'but', 'nothing', 'came', 'of', 'it', 'i', 'saw', 'a', 'crowd', 'of', 'them', 'upon', 'the'], ['slopes', 'looking', 'furtively', 'at', 'me', 'at', 'last', 'hot', 'and', 'tired', 'i', 'sat', 'down'], ['to', 'watch', 'the', 'place', 'but', 'i', 'was', 'too', 'restless', 'to', 'watch', 'long', 'i', 'am', 'too'], ['occidental', 'for', 'a', 'long', 'vigil', 'i', 'could', 'work', 'at', 'a', 'problem', 'for', 'years'], ['but', 'to', 'wait', 'inactive', 'for', 'twenty', 'four', 'hours', 'that', 'is', 'another', 'matter'], [], ['i', 'got', 'up', 'after', 'a', 'time', 'and', 'began', 'walking', 'aimlessly', 'through', 'the'], ['bushes', 'towards', 'the', 'hill', 'again', 'patience', 'said', 'i', 'to', 'myself', 'if', 'you'], ['want', 'your', 'machine', 'again', 'you', 'must', 'leave', 'that', 'sphinx', 'alone', 'if', 'they'], ['mean', 'to', 'take', 'your', 'machine', 'away', 'it', 's', 'little', 'good', 'your', 'wrecking', 'their'], ['bronze', 'panels', 'and', 'if', 'they', 'don', 't', 'you', 'will', 'get', 'it', 'back', 'as', 'soon', 'as'], ['you', 'can', 'ask', 'for', 'it', 'to', 'sit', 'among', 'all', 'those', 'unknown', 'things', 'before', 'a'], ['puzzle', 'like', 'that', 'is', 'hopeless', 'that', 'way', 'lies', 'monomania', 'face', 'this'], ['world', 'learn', 'its', 'ways', 'watch', 'it', 'be', 'careful', 'of', 'too', 'hasty', 'guesses'], ['at', 'its', 'meaning', 'in', 'the', 'end', 'you', 'will', 'find', 'clues', 'to', 'it', 'all', 'then'], ['suddenly', 'the', 'humour', 'of', 'the', 'situation', 'came', 'into', 'my', 'mind', 'the', 'thought'], ['of', 'the', 'years', 'i', 'had', 'spent', 'in', 'study', 'and', 'toil', 'to', 'get', 'into', 'the', 'future'], ['age', 'and', 'now', 'my', 'passion', 'of', 'anxiety', 'to', 'get', 'out', 'of', 'it', 'i', 'had', 'made'], ['myself', 'the', 'most', 'complicated', 'and', 'the', 'most', 'hopeless', 'trap', 'that', 'ever', 'a'], ['man', 'devised', 'although', 'it', 'was', 'at', 'my', 'own', 'expense', 'i', 'could', 'not', 'help'], ['myself', 'i', 'laughed', 'aloud'], [], ['going', 'through', 'the', 'big', 'palace', 'it', 'seemed', 'to', 'me', 'that', 'the', 'little'], ['people', 'avoided', 'me', 'it', 'may', 'have', 'been', 'my', 'fancy', 'or', 'it', 'may', 'have', 'had'], ['something', 'to', 'do', 'with', 'my', 'hammering', 'at', 'the', 'gates', 'of', 'bronze', 'yet', 'i', 'felt'], ['tolerably', 'sure', 'of', 'the', 'avoidance', 'i', 'was', 'careful', 'however', 'to', 'show', 'no'], ['concern', 'and', 'to', 'abstain', 'from', 'any', 'pursuit', 'of', 'them', 'and', 'in', 'the', 'course'], ['of', 'a', 'day', 'or', 'two', 'things', 'got', 'back', 'to', 'the', 'old', 'footing', 'i', 'made', 'what'], ['progress', 'i', 'could', 'in', 'the', 'language', 'and', 'in', 'addition', 'i', 'pushed', 'my'], ['explorations', 'here', 'and', 'there', 'either', 'i', 'missed', 'some', 'subtle', 'point', 'or'], ['their', 'language', 'was', 'excessively', 'simple', 'almost', 'exclusively', 'composed'], ['of', 'concrete', 'substantives', 'and', 'verbs', 'there', 'seemed', 'to', 'be', 'few', 'if', 'any'], ['abstract', 'terms', 'or', 'little', 'use', 'of', 'figurative', 'language', 'their'], ['sentences', 'were', 'usually', 'simple', 'and', 'of', 'two', 'words', 'and', 'i', 'failed', 'to'], ['convey', 'or', 'understand', 'any', 'but', 'the', 'simplest', 'propositions', 'i', 'determined'], ['to', 'put', 'the', 'thought', 'of', 'my', 'time', 'machine', 'and', 'the', 'mystery', 'of', 'the', 'bronze'], ['doors', 'under', 'the', 'sphinx', 'as', 'much', 'as', 'possible', 'in', 'a', 'corner', 'of', 'memory'], ['until', 'my', 'growing', 'knowledge', 'would', 'lead', 'me', 'back', 'to', 'them', 'in', 'a', 'natural'], ['way', 'yet', 'a', 'certain', 'feeling', 'you', 'may', 'understand', 'tethered', 'me', 'in', 'a'], ['circle', 'of', 'a', 'few', 'miles', 'round', 'the', 'point', 'of', 'my', 'arrival'], [], ['so', 'far', 'as', 'i', 'could', 'see', 'all', 'the', 'world', 'displayed', 'the', 'same', 'exuberant'], ['richness', 'as', 'the', 'thames', 'valley', 'from', 'every', 'hill', 'i', 'climbed', 'i', 'saw', 'the'], ['same', 'abundance', 'of', 'splendid', 'buildings', 'endlessly', 'varied', 'in', 'material'], ['and', 'style', 'the', 'same', 'clustering', 'thickets', 'of', 'evergreens', 'the', 'same'], ['blossom', 'laden', 'trees', 'and', 'tree', 'ferns', 'here', 'and', 'there', 'water', 'shone', 'like'], ['silver', 'and', 'beyond', 'the', 'land', 'rose', 'into', 'blue', 'undulating', 'hills', 'and'], ['so', 'faded', 'into', 'the', 'serenity', 'of', 'the', 'sky', 'a', 'peculiar', 'feature', 'which'], ['presently', 'attracted', 'my', 'attention', 'was', 'the', 'presence', 'of', 'certain'], ['circular', 'wells', 'several', 'as', 'it', 'seemed', 'to', 'me', 'of', 'a', 'very', 'great', 'depth'], ['one', 'lay', 'by', 'the', 'path', 'up', 'the', 'hill', 'which', 'i', 'had', 'followed', 'during', 'my'], ['first', 'walk', 'like', 'the', 'others', 'it', 'was', 'rimmed', 'with', 'bronze', 'curiously'], ['wrought', 'and', 'protected', 'by', 'a', 'little', 'cupola', 'from', 'the', 'rain', 'sitting', 'by'], ['the', 'side', 'of', 'these', 'wells', 'and', 'peering', 'down', 'into', 'the', 'shafted', 'darkness'], ['i', 'could', 'see', 'no', 'gleam', 'of', 'water', 'nor', 'could', 'i', 'start', 'any', 'reflection'], ['with', 'a', 'lighted', 'match', 'but', 'in', 'all', 'of', 'them', 'i', 'heard', 'a', 'certain', 'sound'], ['a', 'thud', 'thud', 'thud', 'like', 'the', 'beating', 'of', 'some', 'big', 'engine', 'and', 'i'], ['discovered', 'from', 'the', 'flaring', 'of', 'my', 'matches', 'that', 'a', 'steady', 'current', 'of'], ['air', 'set', 'down', 'the', 'shafts', 'further', 'i', 'threw', 'a', 'scrap', 'of', 'paper', 'into', 'the'], ['throat', 'of', 'one', 'and', 'instead', 'of', 'fluttering', 'slowly', 'down', 'it', 'was', 'at'], ['once', 'sucked', 'swiftly', 'out', 'of', 'sight'], [], ['after', 'a', 'time', 'too', 'i', 'came', 'to', 'connect', 'these', 'wells', 'with', 'tall', 'towers'], ['standing', 'here', 'and', 'there', 'upon', 'the', 'slopes', 'for', 'above', 'them', 'there', 'was'], ['often', 'just', 'such', 'a', 'flicker', 'in', 'the', 'air', 'as', 'one', 'sees', 'on', 'a', 'hot', 'day', 'above'], ['a', 'sun', 'scorched', 'beach', 'putting', 'things', 'together', 'i', 'reached', 'a', 'strong'], ['suggestion', 'of', 'an', 'extensive', 'system', 'of', 'subterranean', 'ventilation', 'whose'], ['true', 'import', 'it', 'was', 'difficult', 'to', 'imagine', 'i', 'was', 'at', 'first', 'inclined', 'to'], ['associate', 'it', 'with', 'the', 'sanitary', 'apparatus', 'of', 'these', 'people', 'it', 'was', 'an'], ['obvious', 'conclusion', 'but', 'it', 'was', 'absolutely', 'wrong'], [], ['and', 'here', 'i', 'must', 'admit', 'that', 'i', 'learned', 'very', 'little', 'of', 'drains', 'and'], ['bells', 'and', 'modes', 'of', 'conveyance', 'and', 'the', 'like', 'conveniences', 'during', 'my'], ['time', 'in', 'this', 'real', 'future', 'in', 'some', 'of', 'these', 'visions', 'of', 'utopias', 'and'], ['coming', 'times', 'which', 'i', 'have', 'read', 'there', 'is', 'a', 'vast', 'amount', 'of', 'detail'], ['about', 'building', 'and', 'social', 'arrangements', 'and', 'so', 'forth', 'but', 'while'], ['such', 'details', 'are', 'easy', 'enough', 'to', 'obtain', 'when', 'the', 'whole', 'world', 'is'], ['contained', 'in', 'one', 's', 'imagination', 'they', 'are', 'altogether', 'inaccessible', 'to'], ['a', 'real', 'traveller', 'amid', 'such', 'realities', 'as', 'i', 'found', 'here', 'conceive', 'the'], ['tale', 'of', 'london', 'which', 'a', 'negro', 'fresh', 'from', 'central', 'africa', 'would', 'take'], ['back', 'to', 'his', 'tribe', 'what', 'would', 'he', 'know', 'of', 'railway', 'companies', 'of'], ['social', 'movements', 'of', 'telephone', 'and', 'telegraph', 'wires', 'of', 'the', 'parcels'], ['delivery', 'company', 'and', 'postal', 'orders', 'and', 'the', 'like', 'yet', 'we', 'at', 'least'], ['should', 'be', 'willing', 'enough', 'to', 'explain', 'these', 'things', 'to', 'him', 'and', 'even', 'of'], ['what', 'he', 'knew', 'how', 'much', 'could', 'he', 'make', 'his', 'untravelled', 'friend', 'either'], ['apprehend', 'or', 'believe', 'then', 'think', 'how', 'narrow', 'the', 'gap', 'between', 'a', 'negro'], ['and', 'a', 'white', 'man', 'of', 'our', 'own', 'times', 'and', 'how', 'wide', 'the', 'interval', 'between'], ['myself', 'and', 'these', 'of', 'the', 'golden', 'age', 'i', 'was', 'sensible', 'of', 'much', 'which', 'was'], ['unseen', 'and', 'which', 'contributed', 'to', 'my', 'comfort', 'but', 'save', 'for', 'a', 'general'], ['impression', 'of', 'automatic', 'organization', 'i', 'fear', 'i', 'can', 'convey', 'very'], ['little', 'of', 'the', 'difference', 'to', 'your', 'mind'], [], ['in', 'the', 'matter', 'of', 'sepulture', 'for', 'instance', 'i', 'could', 'see', 'no', 'signs', 'of'], ['crematoria', 'nor', 'anything', 'suggestive', 'of', 'tombs', 'but', 'it', 'occurred', 'to', 'me'], ['that', 'possibly', 'there', 'might', 'be', 'cemeteries', 'or', 'crematoria', 'somewhere'], ['beyond', 'the', 'range', 'of', 'my', 'explorings', 'this', 'again', 'was', 'a', 'question', 'i'], ['deliberately', 'put', 'to', 'myself', 'and', 'my', 'curiosity', 'was', 'at', 'first', 'entirely'], ['defeated', 'upon', 'the', 'point', 'the', 'thing', 'puzzled', 'me', 'and', 'i', 'was', 'led', 'to', 'make'], ['a', 'further', 'remark', 'which', 'puzzled', 'me', 'still', 'more', 'that', 'aged', 'and', 'infirm'], ['among', 'this', 'people', 'there', 'were', 'none'], [], ['i', 'must', 'confess', 'that', 'my', 'satisfaction', 'with', 'my', 'first', 'theories', 'of', 'an'], ['automatic', 'civilization', 'and', 'a', 'decadent', 'humanity', 'did', 'not', 'long', 'endure'], ['yet', 'i', 'could', 'think', 'of', 'no', 'other', 'let', 'me', 'put', 'my', 'difficulties', 'the'], ['several', 'big', 'palaces', 'i', 'had', 'explored', 'were', 'mere', 'living', 'places', 'great'], ['dining', 'halls', 'and', 'sleeping', 'apartments', 'i', 'could', 'find', 'no', 'machinery', 'no'], ['appliances', 'of', 'any', 'kind', 'yet', 'these', 'people', 'were', 'clothed', 'in', 'pleasant'], ['fabrics', 'that', 'must', 'at', 'times', 'need', 'renewal', 'and', 'their', 'sandals', 'though'], ['undecorated', 'were', 'fairly', 'complex', 'specimens', 'of', 'metalwork', 'somehow'], ['such', 'things', 'must', 'be', 'made', 'and', 'the', 'little', 'people', 'displayed', 'no', 'vestige'], ['of', 'a', 'creative', 'tendency', 'there', 'were', 'no', 'shops', 'no', 'workshops', 'no', 'sign'], ['of', 'importations', 'among', 'them', 'they', 'spent', 'all', 'their', 'time', 'in', 'playing'], ['gently', 'in', 'bathing', 'in', 'the', 'river', 'in', 'making', 'love', 'in', 'a', 'half', 'playful'], ['fashion', 'in', 'eating', 'fruit', 'and', 'sleeping', 'i', 'could', 'not', 'see', 'how', 'things'], ['were', 'kept', 'going'], [], ['then', 'again', 'about', 'the', 'time', 'machine', 'something', 'i', 'knew', 'not', 'what'], ['had', 'taken', 'it', 'into', 'the', 'hollow', 'pedestal', 'of', 'the', 'white', 'sphinx', 'why', 'for'], ['the', 'life', 'of', 'me', 'i', 'could', 'not', 'imagine', 'those', 'waterless', 'wells', 'too'], ['those', 'flickering', 'pillars', 'i', 'felt', 'i', 'lacked', 'a', 'clue', 'i', 'felt', 'how', 'shall'], ['i', 'put', 'it', 'suppose', 'you', 'found', 'an', 'inscription', 'with', 'sentences', 'here', 'and'], ['there', 'in', 'excellent', 'plain', 'english', 'and', 'interpolated', 'therewith', 'others'], ['made', 'up', 'of', 'words', 'of', 'letters', 'even', 'absolutely', 'unknown', 'to', 'you', 'well'], ['on', 'the', 'third', 'day', 'of', 'my', 'visit', 'that', 'was', 'how', 'the', 'world', 'of', 'eight'], ['hundred', 'and', 'two', 'thousand', 'seven', 'hundred', 'and', 'one', 'presented', 'itself', 'to'], ['me'], [], ['that', 'day', 'too', 'i', 'made', 'a', 'friend', 'of', 'a', 'sort', 'it', 'happened', 'that', 'as', 'i'], ['was', 'watching', 'some', 'of', 'the', 'little', 'people', 'bathing', 'in', 'a', 'shallow', 'one', 'of'], ['them', 'was', 'seized', 'with', 'cramp', 'and', 'began', 'drifting', 'downstream', 'the', 'main'], ['current', 'ran', 'rather', 'swiftly', 'but', 'not', 'too', 'strongly', 'for', 'even', 'a', 'moderate'], ['swimmer', 'it', 'will', 'give', 'you', 'an', 'idea', 'therefore', 'of', 'the', 'strange'], ['deficiency', 'in', 'these', 'creatures', 'when', 'i', 'tell', 'you', 'that', 'none', 'made', 'the'], ['slightest', 'attempt', 'to', 'rescue', 'the', 'weakly', 'crying', 'little', 'thing', 'which'], ['was', 'drowning', 'before', 'their', 'eyes', 'when', 'i', 'realized', 'this', 'i', 'hurriedly'], ['slipped', 'off', 'my', 'clothes', 'and', 'wading', 'in', 'at', 'a', 'point', 'lower', 'down', 'i'], ['caught', 'the', 'poor', 'mite', 'and', 'drew', 'her', 'safe', 'to', 'land', 'a', 'little', 'rubbing', 'of'], ['the', 'limbs', 'soon', 'brought', 'her', 'round', 'and', 'i', 'had', 'the', 'satisfaction', 'of'], ['seeing', 'she', 'was', 'all', 'right', 'before', 'i', 'left', 'her', 'i', 'had', 'got', 'to', 'such', 'a', 'low'], ['estimate', 'of', 'her', 'kind', 'that', 'i', 'did', 'not', 'expect', 'any', 'gratitude', 'from', 'her'], ['in', 'that', 'however', 'i', 'was', 'wrong'], [], ['this', 'happened', 'in', 'the', 'morning', 'in', 'the', 'afternoon', 'i', 'met', 'my', 'little'], ['woman', 'as', 'i', 'believe', 'it', 'was', 'as', 'i', 'was', 'returning', 'towards', 'my', 'centre'], ['from', 'an', 'exploration', 'and', 'she', 'received', 'me', 'with', 'cries', 'of', 'delight', 'and'], ['presented', 'me', 'with', 'a', 'big', 'garland', 'of', 'flowers', 'evidently', 'made', 'for', 'me'], ['and', 'me', 'alone', 'the', 'thing', 'took', 'my', 'imagination', 'very', 'possibly', 'i', 'had'], ['been', 'feeling', 'desolate', 'at', 'any', 'rate', 'i', 'did', 'my', 'best', 'to', 'display', 'my'], ['appreciation', 'of', 'the', 'gift', 'we', 'were', 'soon', 'seated', 'together', 'in', 'a', 'little'], ['stone', 'arbour', 'engaged', 'in', 'conversation', 'chiefly', 'of', 'smiles', 'the'], ['creature', 's', 'friendliness', 'affected', 'me', 'exactly', 'as', 'a', 'child', 's', 'might', 'have'], ['done', 'we', 'passed', 'each', 'other', 'flowers', 'and', 'she', 'kissed', 'my', 'hands', 'i', 'did'], ['the', 'same', 'to', 'hers', 'then', 'i', 'tried', 'talk', 'and', 'found', 'that', 'her', 'name', 'was'], ['weena', 'which', 'though', 'i', 'don', 't', 'know', 'what', 'it', 'meant', 'somehow', 'seemed'], ['appropriate', 'enough', 'that', 'was', 'the', 'beginning', 'of', 'a', 'queer', 'friendship'], ['which', 'lasted', 'a', 'week', 'and', 'ended', 'as', 'i', 'will', 'tell', 'you'], [], ['she', 'was', 'exactly', 'like', 'a', 'child', 'she', 'wanted', 'to', 'be', 'with', 'me', 'always', 'she'], ['tried', 'to', 'follow', 'me', 'everywhere', 'and', 'on', 'my', 'next', 'journey', 'out', 'and', 'about'], ['it', 'went', 'to', 'my', 'heart', 'to', 'tire', 'her', 'down', 'and', 'leave', 'her', 'at', 'last'], ['exhausted', 'and', 'calling', 'after', 'me', 'rather', 'plaintively', 'but', 'the', 'problems'], ['of', 'the', 'world', 'had', 'to', 'be', 'mastered', 'i', 'had', 'not', 'i', 'said', 'to', 'myself', 'come'], ['into', 'the', 'future', 'to', 'carry', 'on', 'a', 'miniature', 'flirtation', 'yet', 'her', 'distress'], ['when', 'i', 'left', 'her', 'was', 'very', 'great', 'her', 'expostulations', 'at', 'the', 'parting'], ['were', 'sometimes', 'frantic', 'and', 'i', 'think', 'altogether', 'i', 'had', 'as', 'much'], ['trouble', 'as', 'comfort', 'from', 'her', 'devotion', 'nevertheless', 'she', 'was', 'somehow'], ['a', 'very', 'great', 'comfort', 'i', 'thought', 'it', 'was', 'mere', 'childish', 'affection', 'that'], ['made', 'her', 'cling', 'to', 'me', 'until', 'it', 'was', 'too', 'late', 'i', 'did', 'not', 'clearly', 'know'], ['what', 'i', 'had', 'inflicted', 'upon', 'her', 'when', 'i', 'left', 'her', 'nor', 'until', 'it', 'was', 'too'], ['late', 'did', 'i', 'clearly', 'understand', 'what', 'she', 'was', 'to', 'me', 'for', 'by', 'merely'], ['seeming', 'fond', 'of', 'me', 'and', 'showing', 'in', 'her', 'weak', 'futile', 'way', 'that', 'she'], ['cared', 'for', 'me', 'the', 'little', 'doll', 'of', 'a', 'creature', 'presently', 'gave', 'my', 'return'], ['to', 'the', 'neighbourhood', 'of', 'the', 'white', 'sphinx', 'almost', 'the', 'feeling', 'of'], ['coming', 'home', 'and', 'i', 'would', 'watch', 'for', 'her', 'tiny', 'figure', 'of', 'white', 'and', 'gold'], ['so', 'soon', 'as', 'i', 'came', 'over', 'the', 'hill'], [], ['it', 'was', 'from', 'her', 'too', 'that', 'i', 'learned', 'that', 'fear', 'had', 'not', 'yet', 'left', 'the'], ['world', 'she', 'was', 'fearless', 'enough', 'in', 'the', 'daylight', 'and', 'she', 'had', 'the'], ['oddest', 'confidence', 'in', 'me', 'for', 'once', 'in', 'a', 'foolish', 'moment', 'i', 'made'], ['threatening', 'grimaces', 'at', 'her', 'and', 'she', 'simply', 'laughed', 'at', 'them', 'but', 'she'], ['dreaded', 'the', 'dark', 'dreaded', 'shadows', 'dreaded', 'black', 'things', 'darkness'], ['to', 'her', 'was', 'the', 'one', 'thing', 'dreadful', 'it', 'was', 'a', 'singularly', 'passionate'], ['emotion', 'and', 'it', 'set', 'me', 'thinking', 'and', 'observing', 'i', 'discovered', 'then'], ['among', 'other', 'things', 'that', 'these', 'little', 'people', 'gathered', 'into', 'the', 'great'], ['houses', 'after', 'dark', 'and', 'slept', 'in', 'droves', 'to', 'enter', 'upon', 'them', 'without', 'a'], ['light', 'was', 'to', 'put', 'them', 'into', 'a', 'tumult', 'of', 'apprehension', 'i', 'never', 'found'], ['one', 'out', 'of', 'doors', 'or', 'one', 'sleeping', 'alone', 'within', 'doors', 'after', 'dark'], ['yet', 'i', 'was', 'still', 'such', 'a', 'blockhead', 'that', 'i', 'missed', 'the', 'lesson', 'of', 'that'], ['fear', 'and', 'in', 'spite', 'of', 'weena', 's', 'distress', 'i', 'insisted', 'upon', 'sleeping', 'away'], ['from', 'these', 'slumbering', 'multitudes'], [], ['it', 'troubled', 'her', 'greatly', 'but', 'in', 'the', 'end', 'her', 'odd', 'affection', 'for', 'me'], ['triumphed', 'and', 'for', 'five', 'of', 'the', 'nights', 'of', 'our', 'acquaintance', 'including'], ['the', 'last', 'night', 'of', 'all', 'she', 'slept', 'with', 'her', 'head', 'pillowed', 'on', 'my', 'arm'], ['but', 'my', 'story', 'slips', 'away', 'from', 'me', 'as', 'i', 'speak', 'of', 'her', 'it', 'must', 'have', 'been'], ['the', 'night', 'before', 'her', 'rescue', 'that', 'i', 'was', 'awakened', 'about', 'dawn', 'i', 'had'], ['been', 'restless', 'dreaming', 'most', 'disagreeably', 'that', 'i', 'was', 'drowned', 'and'], ['that', 'sea', 'anemones', 'were', 'feeling', 'over', 'my', 'face', 'with', 'their', 'soft', 'palps'], ['i', 'woke', 'with', 'a', 'start', 'and', 'with', 'an', 'odd', 'fancy', 'that', 'some', 'greyish', 'animal'], ['had', 'just', 'rushed', 'out', 'of', 'the', 'chamber', 'i', 'tried', 'to', 'get', 'to', 'sleep', 'again'], ['but', 'i', 'felt', 'restless', 'and', 'uncomfortable', 'it', 'was', 'that', 'dim', 'grey', 'hour'], ['when', 'things', 'are', 'just', 'creeping', 'out', 'of', 'darkness', 'when', 'everything', 'is'], ['colourless', 'and', 'clear', 'cut', 'and', 'yet', 'unreal', 'i', 'got', 'up', 'and', 'went', 'down'], ['into', 'the', 'great', 'hall', 'and', 'so', 'out', 'upon', 'the', 'flagstones', 'in', 'front', 'of', 'the'], ['palace', 'i', 'thought', 'i', 'would', 'make', 'a', 'virtue', 'of', 'necessity', 'and', 'see', 'the'], ['sunrise'], [], ['the', 'moon', 'was', 'setting', 'and', 'the', 'dying', 'moonlight', 'and', 'the', 'first', 'pallor'], ['of', 'dawn', 'were', 'mingled', 'in', 'a', 'ghastly', 'half', 'light', 'the', 'bushes', 'were', 'inky'], ['black', 'the', 'ground', 'a', 'sombre', 'grey', 'the', 'sky', 'colourless', 'and', 'cheerless'], ['and', 'up', 'the', 'hill', 'i', 'thought', 'i', 'could', 'see', 'ghosts', 'there', 'several', 'times'], ['as', 'i', 'scanned', 'the', 'slope', 'i', 'saw', 'white', 'figures', 'twice', 'i', 'fancied', 'i', 'saw'], ['a', 'solitary', 'white', 'ape', 'like', 'creature', 'running', 'rather', 'quickly', 'up', 'the'], ['hill', 'and', 'once', 'near', 'the', 'ruins', 'i', 'saw', 'a', 'leash', 'of', 'them', 'carrying', 'some'], ['dark', 'body', 'they', 'moved', 'hastily', 'i', 'did', 'not', 'see', 'what', 'became', 'of', 'them'], ['it', 'seemed', 'that', 'they', 'vanished', 'among', 'the', 'bushes', 'the', 'dawn', 'was', 'still'], ['indistinct', 'you', 'must', 'understand', 'i', 'was', 'feeling', 'that', 'chill'], ['uncertain', 'early', 'morning', 'feeling', 'you', 'may', 'have', 'known', 'i', 'doubted'], ['my', 'eyes'], [], ['as', 'the', 'eastern', 'sky', 'grew', 'brighter', 'and', 'the', 'light', 'of', 'the', 'day', 'came', 'on'], ['and', 'its', 'vivid', 'colouring', 'returned', 'upon', 'the', 'world', 'once', 'more', 'i', 'scanned'], ['the', 'view', 'keenly', 'but', 'i', 'saw', 'no', 'vestige', 'of', 'my', 'white', 'figures', 'they', 'were'], ['mere', 'creatures', 'of', 'the', 'half', 'light', 'they', 'must', 'have', 'been', 'ghosts', 'i'], ['said', 'i', 'wonder', 'whence', 'they', 'dated', 'for', 'a', 'queer', 'notion', 'of', 'grant'], ['allen', 's', 'came', 'into', 'my', 'head', 'and', 'amused', 'me', 'if', 'each', 'generation', 'die', 'and'], ['leave', 'ghosts', 'he', 'argued', 'the', 'world', 'at', 'last', 'will', 'get', 'overcrowded', 'with'], ['them', 'on', 'that', 'theory', 'they', 'would', 'have', 'grown', 'innumerable', 'some', 'eight'], ['hundred', 'thousand', 'years', 'hence', 'and', 'it', 'was', 'no', 'great', 'wonder', 'to', 'see', 'four'], ['at', 'once', 'but', 'the', 'jest', 'was', 'unsatisfying', 'and', 'i', 'was', 'thinking', 'of', 'these'], ['figures', 'all', 'the', 'morning', 'until', 'weena', 's', 'rescue', 'drove', 'them', 'out', 'of', 'my'], ['head', 'i', 'associated', 'them', 'in', 'some', 'indefinite', 'way', 'with', 'the', 'white', 'animal'], ['i', 'had', 'startled', 'in', 'my', 'first', 'passionate', 'search', 'for', 'the', 'time', 'machine'], ['but', 'weena', 'was', 'a', 'pleasant', 'substitute', 'yet', 'all', 'the', 'same', 'they', 'were'], ['soon', 'destined', 'to', 'take', 'far', 'deadlier', 'possession', 'of', 'my', 'mind'], [], ['i', 'think', 'i', 'have', 'said', 'how', 'much', 'hotter', 'than', 'our', 'own', 'was', 'the', 'weather'], ['of', 'this', 'golden', 'age', 'i', 'cannot', 'account', 'for', 'it', 'it', 'may', 'be', 'that', 'the', 'sun'], ['was', 'hotter', 'or', 'the', 'earth', 'nearer', 'the', 'sun', 'it', 'is', 'usual', 'to', 'assume', 'that'], ['the', 'sun', 'will', 'go', 'on', 'cooling', 'steadily', 'in', 'the', 'future', 'but', 'people'], ['unfamiliar', 'with', 'such', 'speculations', 'as', 'those', 'of', 'the', 'younger', 'darwin'], ['forget', 'that', 'the', 'planets', 'must', 'ultimately', 'fall', 'back', 'one', 'by', 'one', 'into'], ['the', 'parent', 'body', 'as', 'these', 'catastrophes', 'occur', 'the', 'sun', 'will', 'blaze'], ['with', 'renewed', 'energy', 'and', 'it', 'may', 'be', 'that', 'some', 'inner', 'planet', 'had'], ['suffered', 'this', 'fate', 'whatever', 'the', 'reason', 'the', 'fact', 'remains', 'that', 'the'], ['sun', 'was', 'very', 'much', 'hotter', 'than', 'we', 'know', 'it'], [], ['well', 'one', 'very', 'hot', 'morning', 'my', 'fourth', 'i', 'think', 'as', 'i', 'was', 'seeking'], ['shelter', 'from', 'the', 'heat', 'and', 'glare', 'in', 'a', 'colossal', 'ruin', 'near', 'the', 'great'], ['house', 'where', 'i', 'slept', 'and', 'fed', 'there', 'happened', 'this', 'strange', 'thing'], ['clambering', 'among', 'these', 'heaps', 'of', 'masonry', 'i', 'found', 'a', 'narrow', 'gallery'], ['whose', 'end', 'and', 'side', 'windows', 'were', 'blocked', 'by', 'fallen', 'masses', 'of', 'stone'], ['by', 'contrast', 'with', 'the', 'brilliancy', 'outside', 'it', 'seemed', 'at', 'first'], ['impenetrably', 'dark', 'to', 'me', 'i', 'entered', 'it', 'groping', 'for', 'the', 'change', 'from'], ['light', 'to', 'blackness', 'made', 'spots', 'of', 'colour', 'swim', 'before', 'me', 'suddenly', 'i'], ['halted', 'spellbound', 'a', 'pair', 'of', 'eyes', 'luminous', 'by', 'reflection', 'against'], ['the', 'daylight', 'without', 'was', 'watching', 'me', 'out', 'of', 'the', 'darkness'], [], ['the', 'old', 'instinctive', 'dread', 'of', 'wild', 'beasts', 'came', 'upon', 'me', 'i', 'clenched'], ['my', 'hands', 'and', 'steadfastly', 'looked', 'into', 'the', 'glaring', 'eyeballs', 'i', 'was'], ['afraid', 'to', 'turn', 'then', 'the', 'thought', 'of', 'the', 'absolute', 'security', 'in', 'which'], ['humanity', 'appeared', 'to', 'be', 'living', 'came', 'to', 'my', 'mind', 'and', 'then', 'i'], ['remembered', 'that', 'strange', 'terror', 'of', 'the', 'dark', 'overcoming', 'my', 'fear', 'to'], ['some', 'extent', 'i', 'advanced', 'a', 'step', 'and', 'spoke', 'i', 'will', 'admit', 'that', 'my'], ['voice', 'was', 'harsh', 'and', 'ill', 'controlled', 'i', 'put', 'out', 'my', 'hand', 'and', 'touched'], ['something', 'soft', 'at', 'once', 'the', 'eyes', 'darted', 'sideways', 'and', 'something'], ['white', 'ran', 'past', 'me', 'i', 'turned', 'with', 'my', 'heart', 'in', 'my', 'mouth', 'and', 'saw', 'a'], ['queer', 'little', 'ape', 'like', 'figure', 'its', 'head', 'held', 'down', 'in', 'a', 'peculiar'], ['manner', 'running', 'across', 'the', 'sunlit', 'space', 'behind', 'me', 'it', 'blundered'], ['against', 'a', 'block', 'of', 'granite', 'staggered', 'aside', 'and', 'in', 'a', 'moment', 'was'], ['hidden', 'in', 'a', 'black', 'shadow', 'beneath', 'another', 'pile', 'of', 'ruined', 'masonry'], [], ['my', 'impression', 'of', 'it', 'is', 'of', 'course', 'imperfect', 'but', 'i', 'know', 'it', 'was', 'a'], ['dull', 'white', 'and', 'had', 'strange', 'large', 'greyish', 'red', 'eyes', 'also', 'that', 'there'], ['was', 'flaxen', 'hair', 'on', 'its', 'head', 'and', 'down', 'its', 'back', 'but', 'as', 'i', 'say', 'it'], ['went', 'too', 'fast', 'for', 'me', 'to', 'see', 'distinctly', 'i', 'cannot', 'even', 'say', 'whether', 'it'], ['ran', 'on', 'all', 'fours', 'or', 'only', 'with', 'its', 'forearms', 'held', 'very', 'low', 'after', 'an'], ['instant', 's', 'pause', 'i', 'followed', 'it', 'into', 'the', 'second', 'heap', 'of', 'ruins', 'i', 'could'], ['not', 'find', 'it', 'at', 'first', 'but', 'after', 'a', 'time', 'in', 'the', 'profound', 'obscurity', 'i'], ['came', 'upon', 'one', 'of', 'those', 'round', 'well', 'like', 'openings', 'of', 'which', 'i', 'have', 'told'], ['you', 'half', 'closed', 'by', 'a', 'fallen', 'pillar', 'a', 'sudden', 'thought', 'came', 'to', 'me'], ['could', 'this', 'thing', 'have', 'vanished', 'down', 'the', 'shaft', 'i', 'lit', 'a', 'match', 'and'], ['looking', 'down', 'i', 'saw', 'a', 'small', 'white', 'moving', 'creature', 'with', 'large'], ['bright', 'eyes', 'which', 'regarded', 'me', 'steadfastly', 'as', 'it', 'retreated', 'it', 'made'], ['me', 'shudder', 'it', 'was', 'so', 'like', 'a', 'human', 'spider', 'it', 'was', 'clambering', 'down'], ['the', 'wall', 'and', 'now', 'i', 'saw', 'for', 'the', 'first', 'time', 'a', 'number', 'of', 'metal', 'foot'], ['and', 'hand', 'rests', 'forming', 'a', 'kind', 'of', 'ladder', 'down', 'the', 'shaft', 'then', 'the'], ['light', 'burned', 'my', 'fingers', 'and', 'fell', 'out', 'of', 'my', 'hand', 'going', 'out', 'as', 'it'], ['dropped', 'and', 'when', 'i', 'had', 'lit', 'another', 'the', 'little', 'monster', 'had'], ['disappeared'], [], ['i', 'do', 'not', 'know', 'how', 'long', 'i', 'sat', 'peering', 'down', 'that', 'well', 'it', 'was', 'not', 'for'], ['some', 'time', 'that', 'i', 'could', 'succeed', 'in', 'persuading', 'myself', 'that', 'the', 'thing', 'i'], ['had', 'seen', 'was', 'human', 'but', 'gradually', 'the', 'truth', 'dawned', 'on', 'me', 'that'], ['man', 'had', 'not', 'remained', 'one', 'species', 'but', 'had', 'differentiated', 'into', 'two'], ['distinct', 'animals', 'that', 'my', 'graceful', 'children', 'of', 'the', 'upper', 'world', 'were'], ['not', 'the', 'sole', 'descendants', 'of', 'our', 'generation', 'but', 'that', 'this', 'bleached'], ['obscene', 'nocturnal', 'thing', 'which', 'had', 'flashed', 'before', 'me', 'was', 'also', 'heir'], ['to', 'all', 'the', 'ages'], [], ['i', 'thought', 'of', 'the', 'flickering', 'pillars', 'and', 'of', 'my', 'theory', 'of', 'an'], ['underground', 'ventilation', 'i', 'began', 'to', 'suspect', 'their', 'true', 'import', 'and'], ['what', 'i', 'wondered', 'was', 'this', 'lemur', 'doing', 'in', 'my', 'scheme', 'of', 'a', 'perfectly'], ['balanced', 'organization', 'how', 'was', 'it', 'related', 'to', 'the', 'indolent', 'serenity'], ['of', 'the', 'beautiful', 'upper', 'worlders', 'and', 'what', 'was', 'hidden', 'down', 'there'], ['at', 'the', 'foot', 'of', 'that', 'shaft', 'i', 'sat', 'upon', 'the', 'edge', 'of', 'the', 'well', 'telling'], ['myself', 'that', 'at', 'any', 'rate', 'there', 'was', 'nothing', 'to', 'fear', 'and', 'that', 'there'], ['i', 'must', 'descend', 'for', 'the', 'solution', 'of', 'my', 'difficulties', 'and', 'withal', 'i'], ['was', 'absolutely', 'afraid', 'to', 'go', 'as', 'i', 'hesitated', 'two', 'of', 'the', 'beautiful'], ['upper', 'world', 'people', 'came', 'running', 'in', 'their', 'amorous', 'sport', 'across', 'the'], ['daylight', 'in', 'the', 'shadow', 'the', 'male', 'pursued', 'the', 'female', 'flinging'], ['flowers', 'at', 'her', 'as', 'he', 'ran'], [], ['they', 'seemed', 'distressed', 'to', 'find', 'me', 'my', 'arm', 'against', 'the', 'overturned'], ['pillar', 'peering', 'down', 'the', 'well', 'apparently', 'it', 'was', 'considered', 'bad', 'form'], ['to', 'remark', 'these', 'apertures', 'for', 'when', 'i', 'pointed', 'to', 'this', 'one', 'and', 'tried'], ['to', 'frame', 'a', 'question', 'about', 'it', 'in', 'their', 'tongue', 'they', 'were', 'still', 'more'], ['visibly', 'distressed', 'and', 'turned', 'away', 'but', 'they', 'were', 'interested', 'by', 'my'], ['matches', 'and', 'i', 'struck', 'some', 'to', 'amuse', 'them', 'i', 'tried', 'them', 'again', 'about'], ['the', 'well', 'and', 'again', 'i', 'failed', 'so', 'presently', 'i', 'left', 'them', 'meaning', 'to'], ['go', 'back', 'to', 'weena', 'and', 'see', 'what', 'i', 'could', 'get', 'from', 'her', 'but', 'my', 'mind', 'was'], ['already', 'in', 'revolution', 'my', 'guesses', 'and', 'impressions', 'were', 'slipping', 'and'], ['sliding', 'to', 'a', 'new', 'adjustment', 'i', 'had', 'now', 'a', 'clue', 'to', 'the', 'import', 'of', 'these'], ['wells', 'to', 'the', 'ventilating', 'towers', 'to', 'the', 'mystery', 'of', 'the', 'ghosts', 'to'], ['say', 'nothing', 'of', 'a', 'hint', 'at', 'the', 'meaning', 'of', 'the', 'bronze', 'gates', 'and', 'the'], ['fate', 'of', 'the', 'time', 'machine', 'and', 'very', 'vaguely', 'there', 'came', 'a', 'suggestion'], ['towards', 'the', 'solution', 'of', 'the', 'economic', 'problem', 'that', 'had', 'puzzled', 'me'], [], ['here', 'was', 'the', 'new', 'view', 'plainly', 'this', 'second', 'species', 'of', 'man', 'was'], ['subterranean', 'there', 'were', 'three', 'circumstances', 'in', 'particular', 'which'], ['made', 'me', 'think', 'that', 'its', 'rare', 'emergence', 'above', 'ground', 'was', 'the', 'outcome'], ['of', 'a', 'long', 'continued', 'underground', 'habit', 'in', 'the', 'first', 'place', 'there', 'was'], ['the', 'bleached', 'look', 'common', 'in', 'most', 'animals', 'that', 'live', 'largely', 'in', 'the'], ['dark', 'the', 'white', 'fish', 'of', 'the', 'kentucky', 'caves', 'for', 'instance', 'then'], ['those', 'large', 'eyes', 'with', 'that', 'capacity', 'for', 'reflecting', 'light', 'are'], ['common', 'features', 'of', 'nocturnal', 'things', 'witness', 'the', 'owl', 'and', 'the', 'cat'], ['and', 'last', 'of', 'all', 'that', 'evident', 'confusion', 'in', 'the', 'sunshine', 'that', 'hasty'], ['yet', 'fumbling', 'awkward', 'flight', 'towards', 'dark', 'shadow', 'and', 'that', 'peculiar'], ['carriage', 'of', 'the', 'head', 'while', 'in', 'the', 'light', 'all', 'reinforced', 'the', 'theory'], ['of', 'an', 'extreme', 'sensitiveness', 'of', 'the', 'retina'], [], ['beneath', 'my', 'feet', 'then', 'the', 'earth', 'must', 'be', 'tunnelled', 'enormously', 'and'], ['these', 'tunnellings', 'were', 'the', 'habitat', 'of', 'the', 'new', 'race', 'the', 'presence', 'of'], ['ventilating', 'shafts', 'and', 'wells', 'along', 'the', 'hill', 'slopes', 'everywhere', 'in'], ['fact', 'except', 'along', 'the', 'river', 'valley', 'showed', 'how', 'universal', 'were', 'its'], ['ramifications', 'what', 'so', 'natural', 'then', 'as', 'to', 'assume', 'that', 'it', 'was', 'in'], ['this', 'artificial', 'underworld', 'that', 'such', 'work', 'as', 'was', 'necessary', 'to', 'the'], ['comfort', 'of', 'the', 'daylight', 'race', 'was', 'done', 'the', 'notion', 'was', 'so', 'plausible'], ['that', 'i', 'at', 'once', 'accepted', 'it', 'and', 'went', 'on', 'to', 'assume', 'the', 'how', 'of', 'this'], ['splitting', 'of', 'the', 'human', 'species', 'i', 'dare', 'say', 'you', 'will', 'anticipate', 'the'], ['shape', 'of', 'my', 'theory', 'though', 'for', 'myself', 'i', 'very', 'soon', 'felt', 'that', 'it'], ['fell', 'far', 'short', 'of', 'the', 'truth'], [], ['at', 'first', 'proceeding', 'from', 'the', 'problems', 'of', 'our', 'own', 'age', 'it', 'seemed'], ['clear', 'as', 'daylight', 'to', 'me', 'that', 'the', 'gradual', 'widening', 'of', 'the', 'present'], ['merely', 'temporary', 'and', 'social', 'difference', 'between', 'the', 'capitalist', 'and'], ['the', 'labourer', 'was', 'the', 'key', 'to', 'the', 'whole', 'position', 'no', 'doubt', 'it', 'will'], ['seem', 'grotesque', 'enough', 'to', 'you', 'and', 'wildly', 'incredible', 'and', 'yet', 'even'], ['now', 'there', 'are', 'existing', 'circumstances', 'to', 'point', 'that', 'way', 'there', 'is'], ['a', 'tendency', 'to', 'utilize', 'underground', 'space', 'for', 'the', 'less', 'ornamental'], ['purposes', 'of', 'civilization', 'there', 'is', 'the', 'metropolitan', 'railway', 'in'], ['london', 'for', 'instance', 'there', 'are', 'new', 'electric', 'railways', 'there', 'are'], ['subways', 'there', 'are', 'underground', 'workrooms', 'and', 'restaurants', 'and', 'they'], ['increase', 'and', 'multiply', 'evidently', 'i', 'thought', 'this', 'tendency', 'had'], ['increased', 'till', 'industry', 'had', 'gradually', 'lost', 'its', 'birthright', 'in', 'the'], ['sky', 'i', 'mean', 'that', 'it', 'had', 'gone', 'deeper', 'and', 'deeper', 'into', 'larger', 'and', 'ever'], ['larger', 'underground', 'factories', 'spending', 'a', 'still', 'increasing', 'amount', 'of'], ['its', 'time', 'therein', 'till', 'in', 'the', 'end', 'even', 'now', 'does', 'not', 'an', 'east', 'end'], ['worker', 'live', 'in', 'such', 'artificial', 'conditions', 'as', 'practically', 'to', 'be', 'cut'], ['off', 'from', 'the', 'natural', 'surface', 'of', 'the', 'earth'], [], ['again', 'the', 'exclusive', 'tendency', 'of', 'richer', 'people', 'due', 'no', 'doubt', 'to'], ['the', 'increasing', 'refinement', 'of', 'their', 'education', 'and', 'the', 'widening', 'gulf'], ['between', 'them', 'and', 'the', 'rude', 'violence', 'of', 'the', 'poor', 'is', 'already', 'leading'], ['to', 'the', 'closing', 'in', 'their', 'interest', 'of', 'considerable', 'portions', 'of', 'the'], ['surface', 'of', 'the', 'land', 'about', 'london', 'for', 'instance', 'perhaps', 'half', 'the'], ['prettier', 'country', 'is', 'shut', 'in', 'against', 'intrusion', 'and', 'this', 'same'], ['widening', 'gulf', 'which', 'is', 'due', 'to', 'the', 'length', 'and', 'expense', 'of', 'the', 'higher'], ['educational', 'process', 'and', 'the', 'increased', 'facilities', 'for', 'and', 'temptations'], ['towards', 'refined', 'habits', 'on', 'the', 'part', 'of', 'the', 'rich', 'will', 'make', 'that'], ['exchange', 'between', 'class', 'and', 'class', 'that', 'promotion', 'by', 'intermarriage'], ['which', 'at', 'present', 'retards', 'the', 'splitting', 'of', 'our', 'species', 'along', 'lines'], ['of', 'social', 'stratification', 'less', 'and', 'less', 'frequent', 'so', 'in', 'the', 'end'], ['above', 'ground', 'you', 'must', 'have', 'the', 'haves', 'pursuing', 'pleasure', 'and', 'comfort'], ['and', 'beauty', 'and', 'below', 'ground', 'the', 'have', 'nots', 'the', 'workers', 'getting'], ['continually', 'adapted', 'to', 'the', 'conditions', 'of', 'their', 'labour', 'once', 'they'], ['were', 'there', 'they', 'would', 'no', 'doubt', 'have', 'to', 'pay', 'rent', 'and', 'not', 'a', 'little'], ['of', 'it', 'for', 'the', 'ventilation', 'of', 'their', 'caverns', 'and', 'if', 'they', 'refused'], ['they', 'would', 'starve', 'or', 'be', 'suffocated', 'for', 'arrears', 'such', 'of', 'them', 'as', 'were'], ['so', 'constituted', 'as', 'to', 'be', 'miserable', 'and', 'rebellious', 'would', 'die', 'and', 'in'], ['the', 'end', 'the', 'balance', 'being', 'permanent', 'the', 'survivors', 'would', 'become', 'as'], ['well', 'adapted', 'to', 'the', 'conditions', 'of', 'underground', 'life', 'and', 'as', 'happy', 'in'], ['their', 'way', 'as', 'the', 'upper', 'world', 'people', 'were', 'to', 'theirs', 'as', 'it', 'seemed', 'to'], ['me', 'the', 'refined', 'beauty', 'and', 'the', 'etiolated', 'pallor', 'followed', 'naturally'], ['enough'], [], ['the', 'great', 'triumph', 'of', 'humanity', 'i', 'had', 'dreamed', 'of', 'took', 'a', 'different'], ['shape', 'in', 'my', 'mind', 'it', 'had', 'been', 'no', 'such', 'triumph', 'of', 'moral', 'education', 'and'], ['general', 'co', 'operation', 'as', 'i', 'had', 'imagined', 'instead', 'i', 'saw', 'a', 'real'], ['aristocracy', 'armed', 'with', 'a', 'perfected', 'science', 'and', 'working', 'to', 'a', 'logical'], ['conclusion', 'the', 'industrial', 'system', 'of', 'to', 'day', 'its', 'triumph', 'had', 'not', 'been'], ['simply', 'a', 'triumph', 'over', 'nature', 'but', 'a', 'triumph', 'over', 'nature', 'and', 'the'], ['fellow', 'man', 'this', 'i', 'must', 'warn', 'you', 'was', 'my', 'theory', 'at', 'the', 'time', 'i', 'had'], ['no', 'convenient', 'cicerone', 'in', 'the', 'pattern', 'of', 'the', 'utopian', 'books', 'my'], ['explanation', 'may', 'be', 'absolutely', 'wrong', 'i', 'still', 'think', 'it', 'is', 'the'], ['most', 'plausible', 'one', 'but', 'even', 'on', 'this', 'supposition', 'the', 'balanced'], ['civilization', 'that', 'was', 'at', 'last', 'attained', 'must', 'have', 'long', 'since', 'passed'], ['its', 'zenith', 'and', 'was', 'now', 'far', 'fallen', 'into', 'decay', 'the', 'too', 'perfect'], ['security', 'of', 'the', 'upper', 'worlders', 'had', 'led', 'them', 'to', 'a', 'slow', 'movement', 'of'], ['degeneration', 'to', 'a', 'general', 'dwindling', 'in', 'size', 'strength', 'and'], ['intelligence', 'that', 'i', 'could', 'see', 'clearly', 'enough', 'already', 'what', 'had'], ['happened', 'to', 'the', 'under', 'grounders', 'i', 'did', 'not', 'yet', 'suspect', 'but', 'from', 'what'], ['i', 'had', 'seen', 'of', 'the', 'morlocks', 'that', 'by', 'the', 'by', 'was', 'the', 'name', 'by', 'which'], ['these', 'creatures', 'were', 'called', 'i', 'could', 'imagine', 'that', 'the', 'modification'], ['of', 'the', 'human', 'type', 'was', 'even', 'far', 'more', 'profound', 'than', 'among', 'the', 'eloi'], ['the', 'beautiful', 'race', 'that', 'i', 'already', 'knew'], [], ['then', 'came', 'troublesome', 'doubts', 'why', 'had', 'the', 'morlocks', 'taken', 'my', 'time'], ['machine', 'for', 'i', 'felt', 'sure', 'it', 'was', 'they', 'who', 'had', 'taken', 'it', 'why', 'too', 'if'], ['the', 'eloi', 'were', 'masters', 'could', 'they', 'not', 'restore', 'the', 'machine', 'to', 'me', 'and'], ['why', 'were', 'they', 'so', 'terribly', 'afraid', 'of', 'the', 'dark', 'i', 'proceeded', 'as', 'i', 'have'], ['said', 'to', 'question', 'weena', 'about', 'this', 'under', 'world', 'but', 'here', 'again', 'i', 'was'], ['disappointed', 'at', 'first', 'she', 'would', 'not', 'understand', 'my', 'questions', 'and'], ['presently', 'she', 'refused', 'to', 'answer', 'them', 'she', 'shivered', 'as', 'though', 'the'], ['topic', 'was', 'unendurable', 'and', 'when', 'i', 'pressed', 'her', 'perhaps', 'a', 'little'], ['harshly', 'she', 'burst', 'into', 'tears', 'they', 'were', 'the', 'only', 'tears', 'except', 'my'], ['own', 'i', 'ever', 'saw', 'in', 'that', 'golden', 'age', 'when', 'i', 'saw', 'them', 'i', 'ceased'], ['abruptly', 'to', 'trouble', 'about', 'the', 'morlocks', 'and', 'was', 'only', 'concerned', 'in'], ['banishing', 'these', 'signs', 'of', 'the', 'human', 'inheritance', 'from', 'weena', 's', 'eyes'], ['and', 'very', 'soon', 'she', 'was', 'smiling', 'and', 'clapping', 'her', 'hands', 'while', 'i'], ['solemnly', 'burned', 'a', 'match'], [], [], [], [], ['vi'], [], [], ['it', 'may', 'seem', 'odd', 'to', 'you', 'but', 'it', 'was', 'two', 'days', 'before', 'i', 'could', 'follow'], ['up', 'the', 'new', 'found', 'clue', 'in', 'what', 'was', 'manifestly', 'the', 'proper', 'way', 'i', 'felt'], ['a', 'peculiar', 'shrinking', 'from', 'those', 'pallid', 'bodies', 'they', 'were', 'just', 'the'], ['half', 'bleached', 'colour', 'of', 'the', 'worms', 'and', 'things', 'one', 'sees', 'preserved', 'in'], ['spirit', 'in', 'a', 'zoological', 'museum', 'and', 'they', 'were', 'filthily', 'cold', 'to', 'the'], ['touch', 'probably', 'my', 'shrinking', 'was', 'largely', 'due', 'to', 'the', 'sympathetic'], ['influence', 'of', 'the', 'eloi', 'whose', 'disgust', 'of', 'the', 'morlocks', 'i', 'now', 'began'], ['to', 'appreciate'], [], ['the', 'next', 'night', 'i', 'did', 'not', 'sleep', 'well', 'probably', 'my', 'health', 'was', 'a'], ['little', 'disordered', 'i', 'was', 'oppressed', 'with', 'perplexity', 'and', 'doubt', 'once'], ['or', 'twice', 'i', 'had', 'a', 'feeling', 'of', 'intense', 'fear', 'for', 'which', 'i', 'could', 'perceive'], ['no', 'definite', 'reason', 'i', 'remember', 'creeping', 'noiselessly', 'into', 'the', 'great'], ['hall', 'where', 'the', 'little', 'people', 'were', 'sleeping', 'in', 'the', 'moonlight', 'that'], ['night', 'weena', 'was', 'among', 'them', 'and', 'feeling', 'reassured', 'by', 'their', 'presence'], ['it', 'occurred', 'to', 'me', 'even', 'then', 'that', 'in', 'the', 'course', 'of', 'a', 'few', 'days', 'the'], ['moon', 'must', 'pass', 'through', 'its', 'last', 'quarter', 'and', 'the', 'nights', 'grow', 'dark'], ['when', 'the', 'appearances', 'of', 'these', 'unpleasant', 'creatures', 'from', 'below', 'these'], ['whitened', 'lemurs', 'this', 'new', 'vermin', 'that', 'had', 'replaced', 'the', 'old', 'might', 'be'], ['more', 'abundant', 'and', 'on', 'both', 'these', 'days', 'i', 'had', 'the', 'restless', 'feeling', 'of'], ['one', 'who', 'shirks', 'an', 'inevitable', 'duty', 'i', 'felt', 'assured', 'that', 'the', 'time'], ['machine', 'was', 'only', 'to', 'be', 'recovered', 'by', 'boldly', 'penetrating', 'these'], ['underground', 'mysteries', 'yet', 'i', 'could', 'not', 'face', 'the', 'mystery', 'if', 'only', 'i'], ['had', 'had', 'a', 'companion', 'it', 'would', 'have', 'been', 'different', 'but', 'i', 'was', 'so'], ['horribly', 'alone', 'and', 'even', 'to', 'clamber', 'down', 'into', 'the', 'darkness', 'of', 'the'], ['well', 'appalled', 'me', 'i', 'don', 't', 'know', 'if', 'you', 'will', 'understand', 'my', 'feeling'], ['but', 'i', 'never', 'felt', 'quite', 'safe', 'at', 'my', 'back'], [], ['it', 'was', 'this', 'restlessness', 'this', 'insecurity', 'perhaps', 'that', 'drove', 'me'], ['further', 'and', 'further', 'afield', 'in', 'my', 'exploring', 'expeditions', 'going', 'to', 'the'], ['south', 'westward', 'towards', 'the', 'rising', 'country', 'that', 'is', 'now', 'called', 'combe'], ['wood', 'i', 'observed', 'far', 'off', 'in', 'the', 'direction', 'of', 'nineteenth', 'century'], ['banstead', 'a', 'vast', 'green', 'structure', 'different', 'in', 'character', 'from', 'any'], ['i', 'had', 'hitherto', 'seen', 'it', 'was', 'larger', 'than', 'the', 'largest', 'of', 'the', 'palaces'], ['or', 'ruins', 'i', 'knew', 'and', 'the', 'facade', 'had', 'an', 'oriental', 'look', 'the', 'face'], ['of', 'it', 'having', 'the', 'lustre', 'as', 'well', 'as', 'the', 'pale', 'green', 'tint', 'a', 'kind'], ['of', 'bluish', 'green', 'of', 'a', 'certain', 'type', 'of', 'chinese', 'porcelain', 'this'], ['difference', 'in', 'aspect', 'suggested', 'a', 'difference', 'in', 'use', 'and', 'i', 'was', 'minded'], ['to', 'push', 'on', 'and', 'explore', 'but', 'the', 'day', 'was', 'growing', 'late', 'and', 'i', 'had', 'come'], ['upon', 'the', 'sight', 'of', 'the', 'place', 'after', 'a', 'long', 'and', 'tiring', 'circuit', 'so', 'i'], ['resolved', 'to', 'hold', 'over', 'the', 'adventure', 'for', 'the', 'following', 'day', 'and', 'i'], ['returned', 'to', 'the', 'welcome', 'and', 'the', 'caresses', 'of', 'little', 'weena', 'but', 'next'], ['morning', 'i', 'perceived', 'clearly', 'enough', 'that', 'my', 'curiosity', 'regarding', 'the'], ['palace', 'of', 'green', 'porcelain', 'was', 'a', 'piece', 'of', 'self', 'deception', 'to', 'enable'], ['me', 'to', 'shirk', 'by', 'another', 'day', 'an', 'experience', 'i', 'dreaded', 'i', 'resolved', 'i'], ['would', 'make', 'the', 'descent', 'without', 'further', 'waste', 'of', 'time', 'and', 'started'], ['out', 'in', 'the', 'early', 'morning', 'towards', 'a', 'well', 'near', 'the', 'ruins', 'of', 'granite'], ['and', 'aluminium'], [], ['little', 'weena', 'ran', 'with', 'me', 'she', 'danced', 'beside', 'me', 'to', 'the', 'well', 'but'], ['when', 'she', 'saw', 'me', 'lean', 'over', 'the', 'mouth', 'and', 'look', 'downward', 'she', 'seemed'], ['strangely', 'disconcerted', 'good', 'bye', 'little', 'weena', 'i', 'said', 'kissing'], ['her', 'and', 'then', 'putting', 'her', 'down', 'i', 'began', 'to', 'feel', 'over', 'the', 'parapet'], ['for', 'the', 'climbing', 'hooks', 'rather', 'hastily', 'i', 'may', 'as', 'well', 'confess', 'for'], ['i', 'feared', 'my', 'courage', 'might', 'leak', 'away', 'at', 'first', 'she', 'watched', 'me', 'in'], ['amazement', 'then', 'she', 'gave', 'a', 'most', 'piteous', 'cry', 'and', 'running', 'to', 'me', 'she'], ['began', 'to', 'pull', 'at', 'me', 'with', 'her', 'little', 'hands', 'i', 'think', 'her', 'opposition'], ['nerved', 'me', 'rather', 'to', 'proceed', 'i', 'shook', 'her', 'off', 'perhaps', 'a', 'little'], ['roughly', 'and', 'in', 'another', 'moment', 'i', 'was', 'in', 'the', 'throat', 'of', 'the', 'well', 'i'], ['saw', 'her', 'agonized', 'face', 'over', 'the', 'parapet', 'and', 'smiled', 'to', 'reassure', 'her'], ['then', 'i', 'had', 'to', 'look', 'down', 'at', 'the', 'unstable', 'hooks', 'to', 'which', 'i', 'clung'], [], ['i', 'had', 'to', 'clamber', 'down', 'a', 'shaft', 'of', 'perhaps', 'two', 'hundred', 'yards', 'the'], ['descent', 'was', 'effected', 'by', 'means', 'of', 'metallic', 'bars', 'projecting', 'from'], ['the', 'sides', 'of', 'the', 'well', 'and', 'these', 'being', 'adapted', 'to', 'the', 'needs', 'of'], ['a', 'creature', 'much', 'smaller', 'and', 'lighter', 'than', 'myself', 'i', 'was', 'speedily'], ['cramped', 'and', 'fatigued', 'by', 'the', 'descent', 'and', 'not', 'simply', 'fatigued', 'one', 'of'], ['the', 'bars', 'bent', 'suddenly', 'under', 'my', 'weight', 'and', 'almost', 'swung', 'me', 'off', 'into'], ['the', 'blackness', 'beneath', 'for', 'a', 'moment', 'i', 'hung', 'by', 'one', 'hand', 'and', 'after'], ['that', 'experience', 'i', 'did', 'not', 'dare', 'to', 'rest', 'again', 'though', 'my', 'arms', 'and'], ['back', 'were', 'presently', 'acutely', 'painful', 'i', 'went', 'on', 'clambering', 'down', 'the'], ['sheer', 'descent', 'with', 'as', 'quick', 'a', 'motion', 'as', 'possible', 'glancing', 'upward'], ['i', 'saw', 'the', 'aperture', 'a', 'small', 'blue', 'disk', 'in', 'which', 'a', 'star', 'was', 'visible'], ['while', 'little', 'weena', 's', 'head', 'showed', 'as', 'a', 'round', 'black', 'projection', 'the'], ['thudding', 'sound', 'of', 'a', 'machine', 'below', 'grew', 'louder', 'and', 'more', 'oppressive'], ['everything', 'save', 'that', 'little', 'disk', 'above', 'was', 'profoundly', 'dark', 'and', 'when'], ['i', 'looked', 'up', 'again', 'weena', 'had', 'disappeared'], [], ['i', 'was', 'in', 'an', 'agony', 'of', 'discomfort', 'i', 'had', 'some', 'thought', 'of', 'trying', 'to', 'go'], ['up', 'the', 'shaft', 'again', 'and', 'leave', 'the', 'under', 'world', 'alone', 'but', 'even', 'while'], ['i', 'turned', 'this', 'over', 'in', 'my', 'mind', 'i', 'continued', 'to', 'descend', 'at', 'last', 'with'], ['intense', 'relief', 'i', 'saw', 'dimly', 'coming', 'up', 'a', 'foot', 'to', 'the', 'right', 'of', 'me', 'a'], ['slender', 'loophole', 'in', 'the', 'wall', 'swinging', 'myself', 'in', 'i', 'found', 'it', 'was', 'the'], ['aperture', 'of', 'a', 'narrow', 'horizontal', 'tunnel', 'in', 'which', 'i', 'could', 'lie', 'down', 'and'], ['rest', 'it', 'was', 'not', 'too', 'soon', 'my', 'arms', 'ached', 'my', 'back', 'was', 'cramped', 'and', 'i'], ['was', 'trembling', 'with', 'the', 'prolonged', 'terror', 'of', 'a', 'fall', 'besides', 'this', 'the'], ['unbroken', 'darkness', 'had', 'had', 'a', 'distressing', 'effect', 'upon', 'my', 'eyes', 'the', 'air'], ['was', 'full', 'of', 'the', 'throb', 'and', 'hum', 'of', 'machinery', 'pumping', 'air', 'down', 'the'], ['shaft'], [], ['i', 'do', 'not', 'know', 'how', 'long', 'i', 'lay', 'i', 'was', 'roused', 'by', 'a', 'soft', 'hand', 'touching'], ['my', 'face', 'starting', 'up', 'in', 'the', 'darkness', 'i', 'snatched', 'at', 'my', 'matches', 'and'], ['hastily', 'striking', 'one', 'i', 'saw', 'three', 'stooping', 'white', 'creatures', 'similar'], ['to', 'the', 'one', 'i', 'had', 'seen', 'above', 'ground', 'in', 'the', 'ruin', 'hastily', 'retreating'], ['before', 'the', 'light', 'living', 'as', 'they', 'did', 'in', 'what', 'appeared', 'to', 'me'], ['impenetrable', 'darkness', 'their', 'eyes', 'were', 'abnormally', 'large', 'and'], ['sensitive', 'just', 'as', 'are', 'the', 'pupils', 'of', 'the', 'abysmal', 'fishes', 'and', 'they'], ['reflected', 'the', 'light', 'in', 'the', 'same', 'way', 'i', 'have', 'no', 'doubt', 'they', 'could', 'see'], ['me', 'in', 'that', 'rayless', 'obscurity', 'and', 'they', 'did', 'not', 'seem', 'to', 'have', 'any', 'fear'], ['of', 'me', 'apart', 'from', 'the', 'light', 'but', 'so', 'soon', 'as', 'i', 'struck', 'a', 'match', 'in'], ['order', 'to', 'see', 'them', 'they', 'fled', 'incontinently', 'vanishing', 'into', 'dark'], ['gutters', 'and', 'tunnels', 'from', 'which', 'their', 'eyes', 'glared', 'at', 'me', 'in', 'the'], ['strangest', 'fashion'], [], ['i', 'tried', 'to', 'call', 'to', 'them', 'but', 'the', 'language', 'they', 'had', 'was', 'apparently'], ['different', 'from', 'that', 'of', 'the', 'over', 'world', 'people', 'so', 'that', 'i', 'was', 'needs'], ['left', 'to', 'my', 'own', 'unaided', 'efforts', 'and', 'the', 'thought', 'of', 'flight', 'before'], ['exploration', 'was', 'even', 'then', 'in', 'my', 'mind', 'but', 'i', 'said', 'to', 'myself', 'you', 'are'], ['in', 'for', 'it', 'now', 'and', 'feeling', 'my', 'way', 'along', 'the', 'tunnel', 'i', 'found', 'the'], ['noise', 'of', 'machinery', 'grow', 'louder', 'presently', 'the', 'walls', 'fell', 'away', 'from'], ['me', 'and', 'i', 'came', 'to', 'a', 'large', 'open', 'space', 'and', 'striking', 'another', 'match'], ['saw', 'that', 'i', 'had', 'entered', 'a', 'vast', 'arched', 'cavern', 'which', 'stretched', 'into'], ['utter', 'darkness', 'beyond', 'the', 'range', 'of', 'my', 'light', 'the', 'view', 'i', 'had', 'of', 'it'], ['was', 'as', 'much', 'as', 'one', 'could', 'see', 'in', 'the', 'burning', 'of', 'a', 'match'], [], ['necessarily', 'my', 'memory', 'is', 'vague', 'great', 'shapes', 'like', 'big', 'machines', 'rose'], ['out', 'of', 'the', 'dimness', 'and', 'cast', 'grotesque', 'black', 'shadows', 'in', 'which', 'dim'], ['spectral', 'morlocks', 'sheltered', 'from', 'the', 'glare', 'the', 'place', 'by', 'the', 'by'], ['was', 'very', 'stuffy', 'and', 'oppressive', 'and', 'the', 'faint', 'halitus', 'of', 'freshly'], ['shed', 'blood', 'was', 'in', 'the', 'air', 'some', 'way', 'down', 'the', 'central', 'vista', 'was', 'a'], ['little', 'table', 'of', 'white', 'metal', 'laid', 'with', 'what', 'seemed', 'a', 'meal', 'the'], ['morlocks', 'at', 'any', 'rate', 'were', 'carnivorous', 'even', 'at', 'the', 'time', 'i', 'remember'], ['wondering', 'what', 'large', 'animal', 'could', 'have', 'survived', 'to', 'furnish', 'the', 'red'], ['joint', 'i', 'saw', 'it', 'was', 'all', 'very', 'indistinct', 'the', 'heavy', 'smell', 'the', 'big'], ['unmeaning', 'shapes', 'the', 'obscene', 'figures', 'lurking', 'in', 'the', 'shadows', 'and'], ['only', 'waiting', 'for', 'the', 'darkness', 'to', 'come', 'at', 'me', 'again', 'then', 'the', 'match'], ['burned', 'down', 'and', 'stung', 'my', 'fingers', 'and', 'fell', 'a', 'wriggling', 'red', 'spot'], ['in', 'the', 'blackness'], [], ['i', 'have', 'thought', 'since', 'how', 'particularly', 'ill', 'equipped', 'i', 'was', 'for', 'such'], ['an', 'experience', 'when', 'i', 'had', 'started', 'with', 'the', 'time', 'machine', 'i', 'had'], ['started', 'with', 'the', 'absurd', 'assumption', 'that', 'the', 'men', 'of', 'the', 'future', 'would'], ['certainly', 'be', 'infinitely', 'ahead', 'of', 'ourselves', 'in', 'all', 'their', 'appliances'], ['i', 'had', 'come', 'without', 'arms', 'without', 'medicine', 'without', 'anything', 'to'], ['smoke', 'at', 'times', 'i', 'missed', 'tobacco', 'frightfully', 'even', 'without', 'enough'], ['matches', 'if', 'only', 'i', 'had', 'thought', 'of', 'a', 'kodak', 'i', 'could', 'have', 'flashed', 'that'], ['glimpse', 'of', 'the', 'underworld', 'in', 'a', 'second', 'and', 'examined', 'it', 'at', 'leisure'], ['but', 'as', 'it', 'was', 'i', 'stood', 'there', 'with', 'only', 'the', 'weapons', 'and', 'the', 'powers'], ['that', 'nature', 'had', 'endowed', 'me', 'with', 'hands', 'feet', 'and', 'teeth', 'these', 'and'], ['four', 'safety', 'matches', 'that', 'still', 'remained', 'to', 'me'], [], ['i', 'was', 'afraid', 'to', 'push', 'my', 'way', 'in', 'among', 'all', 'this', 'machinery', 'in', 'the'], ['dark', 'and', 'it', 'was', 'only', 'with', 'my', 'last', 'glimpse', 'of', 'light', 'i', 'discovered'], ['that', 'my', 'store', 'of', 'matches', 'had', 'run', 'low', 'it', 'had', 'never', 'occurred', 'to', 'me'], ['until', 'that', 'moment', 'that', 'there', 'was', 'any', 'need', 'to', 'economize', 'them', 'and', 'i'], ['had', 'wasted', 'almost', 'half', 'the', 'box', 'in', 'astonishing', 'the', 'upper', 'worlders', 'to'], ['whom', 'fire', 'was', 'a', 'novelty', 'now', 'as', 'i', 'say', 'i', 'had', 'four', 'left', 'and', 'while', 'i'], ['stood', 'in', 'the', 'dark', 'a', 'hand', 'touched', 'mine', 'lank', 'fingers', 'came', 'feeling'], ['over', 'my', 'face', 'and', 'i', 'was', 'sensible', 'of', 'a', 'peculiar', 'unpleasant', 'odour', 'i'], ['fancied', 'i', 'heard', 'the', 'breathing', 'of', 'a', 'crowd', 'of', 'those', 'dreadful', 'little'], ['beings', 'about', 'me', 'i', 'felt', 'the', 'box', 'of', 'matches', 'in', 'my', 'hand', 'being', 'gently'], ['disengaged', 'and', 'other', 'hands', 'behind', 'me', 'plucking', 'at', 'my', 'clothing', 'the'], ['sense', 'of', 'these', 'unseen', 'creatures', 'examining', 'me', 'was', 'indescribably'], ['unpleasant', 'the', 'sudden', 'realization', 'of', 'my', 'ignorance', 'of', 'their', 'ways', 'of'], ['thinking', 'and', 'doing', 'came', 'home', 'to', 'me', 'very', 'vividly', 'in', 'the', 'darkness', 'i'], ['shouted', 'at', 'them', 'as', 'loudly', 'as', 'i', 'could', 'they', 'started', 'away', 'and', 'then'], ['i', 'could', 'feel', 'them', 'approaching', 'me', 'again', 'they', 'clutched', 'at', 'me', 'more'], ['boldly', 'whispering', 'odd', 'sounds', 'to', 'each', 'other', 'i', 'shivered', 'violently'], ['and', 'shouted', 'again', 'rather', 'discordantly', 'this', 'time', 'they', 'were', 'not', 'so'], ['seriously', 'alarmed', 'and', 'they', 'made', 'a', 'queer', 'laughing', 'noise', 'as', 'they', 'came'], ['back', 'at', 'me', 'i', 'will', 'confess', 'i', 'was', 'horribly', 'frightened', 'i', 'determined'], ['to', 'strike', 'another', 'match', 'and', 'escape', 'under', 'the', 'protection', 'of', 'its'], ['glare', 'i', 'did', 'so', 'and', 'eking', 'out', 'the', 'flicker', 'with', 'a', 'scrap', 'of', 'paper'], ['from', 'my', 'pocket', 'i', 'made', 'good', 'my', 'retreat', 'to', 'the', 'narrow', 'tunnel', 'but', 'i'], ['had', 'scarce', 'entered', 'this', 'when', 'my', 'light', 'was', 'blown', 'out', 'and', 'in', 'the'], ['blackness', 'i', 'could', 'hear', 'the', 'morlocks', 'rustling', 'like', 'wind', 'among', 'leaves'], ['and', 'pattering', 'like', 'the', 'rain', 'as', 'they', 'hurried', 'after', 'me'], [], ['in', 'a', 'moment', 'i', 'was', 'clutched', 'by', 'several', 'hands', 'and', 'there', 'was', 'no'], ['mistaking', 'that', 'they', 'were', 'trying', 'to', 'haul', 'me', 'back', 'i', 'struck', 'another'], ['light', 'and', 'waved', 'it', 'in', 'their', 'dazzled', 'faces', 'you', 'can', 'scarce', 'imagine'], ['how', 'nauseatingly', 'inhuman', 'they', 'looked', 'those', 'pale', 'chinless', 'faces'], ['and', 'great', 'lidless', 'pinkish', 'grey', 'eyes', 'as', 'they', 'stared', 'in', 'their'], ['blindness', 'and', 'bewilderment', 'but', 'i', 'did', 'not', 'stay', 'to', 'look', 'i', 'promise'], ['you', 'i', 'retreated', 'again', 'and', 'when', 'my', 'second', 'match', 'had', 'ended', 'i', 'struck'], ['my', 'third', 'it', 'had', 'almost', 'burned', 'through', 'when', 'i', 'reached', 'the', 'opening'], ['into', 'the', 'shaft', 'i', 'lay', 'down', 'on', 'the', 'edge', 'for', 'the', 'throb', 'of', 'the', 'great'], ['pump', 'below', 'made', 'me', 'giddy', 'then', 'i', 'felt', 'sideways', 'for', 'the', 'projecting'], ['hooks', 'and', 'as', 'i', 'did', 'so', 'my', 'feet', 'were', 'grasped', 'from', 'behind', 'and', 'i'], ['was', 'violently', 'tugged', 'backward', 'i', 'lit', 'my', 'last', 'match', 'and', 'it'], ['incontinently', 'went', 'out', 'but', 'i', 'had', 'my', 'hand', 'on', 'the', 'climbing', 'bars', 'now'], ['and', 'kicking', 'violently', 'i', 'disengaged', 'myself', 'from', 'the', 'clutches', 'of', 'the'], ['morlocks', 'and', 'was', 'speedily', 'clambering', 'up', 'the', 'shaft', 'while', 'they', 'stayed'], ['peering', 'and', 'blinking', 'up', 'at', 'me', 'all', 'but', 'one', 'little', 'wretch', 'who'], ['followed', 'me', 'for', 'some', 'way', 'and', 'well', 'nigh', 'secured', 'my', 'boot', 'as', 'a', 'trophy'], [], ['that', 'climb', 'seemed', 'interminable', 'to', 'me', 'with', 'the', 'last', 'twenty', 'or'], ['thirty', 'feet', 'of', 'it', 'a', 'deadly', 'nausea', 'came', 'upon', 'me', 'i', 'had', 'the', 'greatest'], ['difficulty', 'in', 'keeping', 'my', 'hold', 'the', 'last', 'few', 'yards', 'was', 'a', 'frightful'], ['struggle', 'against', 'this', 'faintness', 'several', 'times', 'my', 'head', 'swam', 'and', 'i'], ['felt', 'all', 'the', 'sensations', 'of', 'falling', 'at', 'last', 'however', 'i', 'got', 'over', 'the'], ['well', 'mouth', 'somehow', 'and', 'staggered', 'out', 'of', 'the', 'ruin', 'into', 'the', 'blinding'], ['sunlight', 'i', 'fell', 'upon', 'my', 'face', 'even', 'the', 'soil', 'smelt', 'sweet', 'and', 'clean'], ['then', 'i', 'remember', 'weena', 'kissing', 'my', 'hands', 'and', 'ears', 'and', 'the', 'voices', 'of'], ['others', 'among', 'the', 'eloi', 'then', 'for', 'a', 'time', 'i', 'was', 'insensible'], [], [], [], [], ['vii'], [], [], ['now', 'indeed', 'i', 'seemed', 'in', 'a', 'worse', 'case', 'than', 'before', 'hitherto'], ['except', 'during', 'my', 'night', 's', 'anguish', 'at', 'the', 'loss', 'of', 'the', 'time', 'machine'], ['i', 'had', 'felt', 'a', 'sustaining', 'hope', 'of', 'ultimate', 'escape', 'but', 'that', 'hope', 'was'], ['staggered', 'by', 'these', 'new', 'discoveries', 'hitherto', 'i', 'had', 'merely', 'thought'], ['myself', 'impeded', 'by', 'the', 'childish', 'simplicity', 'of', 'the', 'little', 'people', 'and'], ['by', 'some', 'unknown', 'forces', 'which', 'i', 'had', 'only', 'to', 'understand', 'to', 'overcome'], ['but', 'there', 'was', 'an', 'altogether', 'new', 'element', 'in', 'the', 'sickening', 'quality', 'of'], ['the', 'morlocks', 'a', 'something', 'inhuman', 'and', 'malign', 'instinctively', 'i'], ['loathed', 'them', 'before', 'i', 'had', 'felt', 'as', 'a', 'man', 'might', 'feel', 'who', 'had', 'fallen'], ['into', 'a', 'pit', 'my', 'concern', 'was', 'with', 'the', 'pit', 'and', 'how', 'to', 'get', 'out', 'of', 'it'], ['now', 'i', 'felt', 'like', 'a', 'beast', 'in', 'a', 'trap', 'whose', 'enemy', 'would', 'come', 'upon', 'him'], ['soon'], [], ['the', 'enemy', 'i', 'dreaded', 'may', 'surprise', 'you', 'it', 'was', 'the', 'darkness', 'of', 'the'], ['new', 'moon', 'weena', 'had', 'put', 'this', 'into', 'my', 'head', 'by', 'some', 'at', 'first'], ['incomprehensible', 'remarks', 'about', 'the', 'dark', 'nights', 'it', 'was', 'not', 'now'], ['such', 'a', 'very', 'difficult', 'problem', 'to', 'guess', 'what', 'the', 'coming', 'dark', 'nights'], ['might', 'mean', 'the', 'moon', 'was', 'on', 'the', 'wane', 'each', 'night', 'there', 'was', 'a', 'longer'], ['interval', 'of', 'darkness', 'and', 'i', 'now', 'understood', 'to', 'some', 'slight', 'degree', 'at'], ['least', 'the', 'reason', 'of', 'the', 'fear', 'of', 'the', 'little', 'upper', 'world', 'people', 'for'], ['the', 'dark', 'i', 'wondered', 'vaguely', 'what', 'foul', 'villainy', 'it', 'might', 'be', 'that'], ['the', 'morlocks', 'did', 'under', 'the', 'new', 'moon', 'i', 'felt', 'pretty', 'sure', 'now', 'that'], ['my', 'second', 'hypothesis', 'was', 'all', 'wrong', 'the', 'upper', 'world', 'people', 'might'], ['once', 'have', 'been', 'the', 'favoured', 'aristocracy', 'and', 'the', 'morlocks', 'their'], ['mechanical', 'servants', 'but', 'that', 'had', 'long', 'since', 'passed', 'away', 'the', 'two'], ['species', 'that', 'had', 'resulted', 'from', 'the', 'evolution', 'of', 'man', 'were', 'sliding'], ['down', 'towards', 'or', 'had', 'already', 'arrived', 'at', 'an', 'altogether', 'new'], ['relationship', 'the', 'eloi', 'like', 'the', 'carolingian', 'kings', 'had', 'decayed'], ['to', 'a', 'mere', 'beautiful', 'futility', 'they', 'still', 'possessed', 'the', 'earth', 'on'], ['sufferance', 'since', 'the', 'morlocks', 'subterranean', 'for', 'innumerable'], ['generations', 'had', 'come', 'at', 'last', 'to', 'find', 'the', 'daylit', 'surface'], ['intolerable', 'and', 'the', 'morlocks', 'made', 'their', 'garments', 'i', 'inferred', 'and'], ['maintained', 'them', 'in', 'their', 'habitual', 'needs', 'perhaps', 'through', 'the'], ['survival', 'of', 'an', 'old', 'habit', 'of', 'service', 'they', 'did', 'it', 'as', 'a', 'standing', 'horse'], ['paws', 'with', 'his', 'foot', 'or', 'as', 'a', 'man', 'enjoys', 'killing', 'animals', 'in', 'sport'], ['because', 'ancient', 'and', 'departed', 'necessities', 'had', 'impressed', 'it', 'on', 'the'], ['organism', 'but', 'clearly', 'the', 'old', 'order', 'was', 'already', 'in', 'part', 'reversed'], ['the', 'nemesis', 'of', 'the', 'delicate', 'ones', 'was', 'creeping', 'on', 'apace', 'ages', 'ago'], ['thousands', 'of', 'generations', 'ago', 'man', 'had', 'thrust', 'his', 'brother', 'man', 'out', 'of'], ['the', 'ease', 'and', 'the', 'sunshine', 'and', 'now', 'that', 'brother', 'was', 'coming', 'back'], ['changed', 'already', 'the', 'eloi', 'had', 'begun', 'to', 'learn', 'one', 'old', 'lesson', 'anew'], ['they', 'were', 'becoming', 'reacquainted', 'with', 'fear', 'and', 'suddenly', 'there', 'came'], ['into', 'my', 'head', 'the', 'memory', 'of', 'the', 'meat', 'i', 'had', 'seen', 'in', 'the', 'under', 'world'], ['it', 'seemed', 'odd', 'how', 'it', 'floated', 'into', 'my', 'mind', 'not', 'stirred', 'up', 'as', 'it'], ['were', 'by', 'the', 'current', 'of', 'my', 'meditations', 'but', 'coming', 'in', 'almost', 'like', 'a'], ['question', 'from', 'outside', 'i', 'tried', 'to', 'recall', 'the', 'form', 'of', 'it', 'i', 'had', 'a'], ['vague', 'sense', 'of', 'something', 'familiar', 'but', 'i', 'could', 'not', 'tell', 'what', 'it', 'was'], ['at', 'the', 'time'], [], ['still', 'however', 'helpless', 'the', 'little', 'people', 'in', 'the', 'presence', 'of', 'their'], ['mysterious', 'fear', 'i', 'was', 'differently', 'constituted', 'i', 'came', 'out', 'of', 'this'], ['age', 'of', 'ours', 'this', 'ripe', 'prime', 'of', 'the', 'human', 'race', 'when', 'fear', 'does', 'not'], ['paralyse', 'and', 'mystery', 'has', 'lost', 'its', 'terrors', 'i', 'at', 'least', 'would', 'defend'], ['myself', 'without', 'further', 'delay', 'i', 'determined', 'to', 'make', 'myself', 'arms', 'and', 'a'], ['fastness', 'where', 'i', 'might', 'sleep', 'with', 'that', 'refuge', 'as', 'a', 'base', 'i', 'could'], ['face', 'this', 'strange', 'world', 'with', 'some', 'of', 'that', 'confidence', 'i', 'had', 'lost', 'in'], ['realizing', 'to', 'what', 'creatures', 'night', 'by', 'night', 'i', 'lay', 'exposed', 'i', 'felt'], ['i', 'could', 'never', 'sleep', 'again', 'until', 'my', 'bed', 'was', 'secure', 'from', 'them', 'i'], ['shuddered', 'with', 'horror', 'to', 'think', 'how', 'they', 'must', 'already', 'have', 'examined'], ['me'], [], ['i', 'wandered', 'during', 'the', 'afternoon', 'along', 'the', 'valley', 'of', 'the', 'thames', 'but'], ['found', 'nothing', 'that', 'commended', 'itself', 'to', 'my', 'mind', 'as', 'inaccessible', 'all'], ['the', 'buildings', 'and', 'trees', 'seemed', 'easily', 'practicable', 'to', 'such', 'dexterous'], ['climbers', 'as', 'the', 'morlocks', 'to', 'judge', 'by', 'their', 'wells', 'must', 'be', 'then', 'the'], ['tall', 'pinnacles', 'of', 'the', 'palace', 'of', 'green', 'porcelain', 'and', 'the', 'polished'], ['gleam', 'of', 'its', 'walls', 'came', 'back', 'to', 'my', 'memory', 'and', 'in', 'the', 'evening'], ['taking', 'weena', 'like', 'a', 'child', 'upon', 'my', 'shoulder', 'i', 'went', 'up', 'the', 'hills'], ['towards', 'the', 'south', 'west', 'the', 'distance', 'i', 'had', 'reckoned', 'was', 'seven', 'or'], ['eight', 'miles', 'but', 'it', 'must', 'have', 'been', 'nearer', 'eighteen', 'i', 'had', 'first', 'seen'], ['the', 'place', 'on', 'a', 'moist', 'afternoon', 'when', 'distances', 'are', 'deceptively'], ['diminished', 'in', 'addition', 'the', 'heel', 'of', 'one', 'of', 'my', 'shoes', 'was', 'loose', 'and'], ['a', 'nail', 'was', 'working', 'through', 'the', 'sole', 'they', 'were', 'comfortable', 'old', 'shoes'], ['i', 'wore', 'about', 'indoors', 'so', 'that', 'i', 'was', 'lame', 'and', 'it', 'was', 'already', 'long'], ['past', 'sunset', 'when', 'i', 'came', 'in', 'sight', 'of', 'the', 'palace', 'silhouetted', 'black'], ['against', 'the', 'pale', 'yellow', 'of', 'the', 'sky'], [], ['weena', 'had', 'been', 'hugely', 'delighted', 'when', 'i', 'began', 'to', 'carry', 'her', 'but'], ['after', 'a', 'while', 'she', 'desired', 'me', 'to', 'let', 'her', 'down', 'and', 'ran', 'along', 'by', 'the'], ['side', 'of', 'me', 'occasionally', 'darting', 'off', 'on', 'either', 'hand', 'to', 'pick', 'flowers'], ['to', 'stick', 'in', 'my', 'pockets', 'my', 'pockets', 'had', 'always', 'puzzled', 'weena', 'but', 'at'], ['the', 'last', 'she', 'had', 'concluded', 'that', 'they', 'were', 'an', 'eccentric', 'kind', 'of', 'vase'], ['for', 'floral', 'decoration', 'at', 'least', 'she', 'utilized', 'them', 'for', 'that', 'purpose'], ['and', 'that', 'reminds', 'me', 'in', 'changing', 'my', 'jacket', 'i', 'found'], [], ['the', 'time', 'traveller', 'paused', 'put', 'his', 'hand', 'into', 'his', 'pocket', 'and'], ['silently', 'placed', 'two', 'withered', 'flowers', 'not', 'unlike', 'very', 'large', 'white'], ['mallows', 'upon', 'the', 'little', 'table', 'then', 'he', 'resumed', 'his', 'narrative'], [], ['as', 'the', 'hush', 'of', 'evening', 'crept', 'over', 'the', 'world', 'and', 'we', 'proceeded', 'over'], ['the', 'hill', 'crest', 'towards', 'wimbledon', 'weena', 'grew', 'tired', 'and', 'wanted', 'to'], ['return', 'to', 'the', 'house', 'of', 'grey', 'stone', 'but', 'i', 'pointed', 'out', 'the', 'distant'], ['pinnacles', 'of', 'the', 'palace', 'of', 'green', 'porcelain', 'to', 'her', 'and', 'contrived', 'to'], ['make', 'her', 'understand', 'that', 'we', 'were', 'seeking', 'a', 'refuge', 'there', 'from', 'her'], ['fear', 'you', 'know', 'that', 'great', 'pause', 'that', 'comes', 'upon', 'things', 'before', 'the'], ['dusk', 'even', 'the', 'breeze', 'stops', 'in', 'the', 'trees', 'to', 'me', 'there', 'is', 'always', 'an'], ['air', 'of', 'expectation', 'about', 'that', 'evening', 'stillness', 'the', 'sky', 'was', 'clear'], ['remote', 'and', 'empty', 'save', 'for', 'a', 'few', 'horizontal', 'bars', 'far', 'down', 'in', 'the'], ['sunset', 'well', 'that', 'night', 'the', 'expectation', 'took', 'the', 'colour', 'of', 'my'], ['fears', 'in', 'that', 'darkling', 'calm', 'my', 'senses', 'seemed', 'preternaturally'], ['sharpened', 'i', 'fancied', 'i', 'could', 'even', 'feel', 'the', 'hollowness', 'of', 'the', 'ground'], ['beneath', 'my', 'feet', 'could', 'indeed', 'almost', 'see', 'through', 'it', 'the', 'morlocks'], ['on', 'their', 'ant', 'hill', 'going', 'hither', 'and', 'thither', 'and', 'waiting', 'for', 'the', 'dark'], ['in', 'my', 'excitement', 'i', 'fancied', 'that', 'they', 'would', 'receive', 'my', 'invasion', 'of'], ['their', 'burrows', 'as', 'a', 'declaration', 'of', 'war', 'and', 'why', 'had', 'they', 'taken', 'my'], ['time', 'machine'], [], ['so', 'we', 'went', 'on', 'in', 'the', 'quiet', 'and', 'the', 'twilight', 'deepened', 'into', 'night'], ['the', 'clear', 'blue', 'of', 'the', 'distance', 'faded', 'and', 'one', 'star', 'after', 'another'], ['came', 'out', 'the', 'ground', 'grew', 'dim', 'and', 'the', 'trees', 'black', 'weena', 's', 'fears', 'and'], ['her', 'fatigue', 'grew', 'upon', 'her', 'i', 'took', 'her', 'in', 'my', 'arms', 'and', 'talked', 'to', 'her'], ['and', 'caressed', 'her', 'then', 'as', 'the', 'darkness', 'grew', 'deeper', 'she', 'put', 'her'], ['arms', 'round', 'my', 'neck', 'and', 'closing', 'her', 'eyes', 'tightly', 'pressed', 'her', 'face'], ['against', 'my', 'shoulder', 'so', 'we', 'went', 'down', 'a', 'long', 'slope', 'into', 'a', 'valley', 'and'], ['there', 'in', 'the', 'dimness', 'i', 'almost', 'walked', 'into', 'a', 'little', 'river', 'this', 'i'], ['waded', 'and', 'went', 'up', 'the', 'opposite', 'side', 'of', 'the', 'valley', 'past', 'a', 'number'], ['of', 'sleeping', 'houses', 'and', 'by', 'a', 'statue', 'a', 'faun', 'or', 'some', 'such', 'figure'], ['minus', 'the', 'head', 'here', 'too', 'were', 'acacias', 'so', 'far', 'i', 'had', 'seen', 'nothing', 'of'], ['the', 'morlocks', 'but', 'it', 'was', 'yet', 'early', 'in', 'the', 'night', 'and', 'the', 'darker', 'hours'], ['before', 'the', 'old', 'moon', 'rose', 'were', 'still', 'to', 'come'], [], ['from', 'the', 'brow', 'of', 'the', 'next', 'hill', 'i', 'saw', 'a', 'thick', 'wood', 'spreading', 'wide'], ['and', 'black', 'before', 'me', 'i', 'hesitated', 'at', 'this', 'i', 'could', 'see', 'no', 'end', 'to'], ['it', 'either', 'to', 'the', 'right', 'or', 'the', 'left', 'feeling', 'tired', 'my', 'feet', 'in'], ['particular', 'were', 'very', 'sore', 'i', 'carefully', 'lowered', 'weena', 'from', 'my'], ['shoulder', 'as', 'i', 'halted', 'and', 'sat', 'down', 'upon', 'the', 'turf', 'i', 'could', 'no'], ['longer', 'see', 'the', 'palace', 'of', 'green', 'porcelain', 'and', 'i', 'was', 'in', 'doubt', 'of', 'my'], ['direction', 'i', 'looked', 'into', 'the', 'thickness', 'of', 'the', 'wood', 'and', 'thought', 'of'], ['what', 'it', 'might', 'hide', 'under', 'that', 'dense', 'tangle', 'of', 'branches', 'one', 'would'], ['be', 'out', 'of', 'sight', 'of', 'the', 'stars', 'even', 'were', 'there', 'no', 'other', 'lurking'], ['danger', 'a', 'danger', 'i', 'did', 'not', 'care', 'to', 'let', 'my', 'imagination', 'loose'], ['upon', 'there', 'would', 'still', 'be', 'all', 'the', 'roots', 'to', 'stumble', 'over', 'and', 'the'], ['tree', 'boles', 'to', 'strike', 'against'], [], ['i', 'was', 'very', 'tired', 'too', 'after', 'the', 'excitements', 'of', 'the', 'day', 'so', 'i'], ['decided', 'that', 'i', 'would', 'not', 'face', 'it', 'but', 'would', 'pass', 'the', 'night', 'upon', 'the'], ['open', 'hill'], [], ['weena', 'i', 'was', 'glad', 'to', 'find', 'was', 'fast', 'asleep', 'i', 'carefully', 'wrapped', 'her'], ['in', 'my', 'jacket', 'and', 'sat', 'down', 'beside', 'her', 'to', 'wait', 'for', 'the', 'moonrise', 'the'], ['hill', 'side', 'was', 'quiet', 'and', 'deserted', 'but', 'from', 'the', 'black', 'of', 'the', 'wood'], ['there', 'came', 'now', 'and', 'then', 'a', 'stir', 'of', 'living', 'things', 'above', 'me', 'shone', 'the'], ['stars', 'for', 'the', 'night', 'was', 'very', 'clear', 'i', 'felt', 'a', 'certain', 'sense', 'of'], ['friendly', 'comfort', 'in', 'their', 'twinkling', 'all', 'the', 'old', 'constellations'], ['had', 'gone', 'from', 'the', 'sky', 'however', 'that', 'slow', 'movement', 'which', 'is'], ['imperceptible', 'in', 'a', 'hundred', 'human', 'lifetimes', 'had', 'long', 'since'], ['rearranged', 'them', 'in', 'unfamiliar', 'groupings', 'but', 'the', 'milky', 'way', 'it'], ['seemed', 'to', 'me', 'was', 'still', 'the', 'same', 'tattered', 'streamer', 'of', 'star', 'dust', 'as'], ['of', 'yore', 'southward', 'as', 'i', 'judged', 'it', 'was', 'a', 'very', 'bright', 'red', 'star', 'that'], ['was', 'new', 'to', 'me', 'it', 'was', 'even', 'more', 'splendid', 'than', 'our', 'own', 'green', 'sirius'], ['and', 'amid', 'all', 'these', 'scintillating', 'points', 'of', 'light', 'one', 'bright', 'planet'], ['shone', 'kindly', 'and', 'steadily', 'like', 'the', 'face', 'of', 'an', 'old', 'friend'], [], ['looking', 'at', 'these', 'stars', 'suddenly', 'dwarfed', 'my', 'own', 'troubles', 'and', 'all'], ['the', 'gravities', 'of', 'terrestrial', 'life', 'i', 'thought', 'of', 'their', 'unfathomable'], ['distance', 'and', 'the', 'slow', 'inevitable', 'drift', 'of', 'their', 'movements', 'out', 'of'], ['the', 'unknown', 'past', 'into', 'the', 'unknown', 'future', 'i', 'thought', 'of', 'the', 'great'], ['precessional', 'cycle', 'that', 'the', 'pole', 'of', 'the', 'earth', 'describes', 'only', 'forty'], ['times', 'had', 'that', 'silent', 'revolution', 'occurred', 'during', 'all', 'the', 'years', 'that'], ['i', 'had', 'traversed', 'and', 'during', 'these', 'few', 'revolutions', 'all', 'the', 'activity'], ['all', 'the', 'traditions', 'the', 'complex', 'organizations', 'the', 'nations'], ['languages', 'literatures', 'aspirations', 'even', 'the', 'mere', 'memory', 'of', 'man', 'as'], ['i', 'knew', 'him', 'had', 'been', 'swept', 'out', 'of', 'existence', 'instead', 'were', 'these'], ['frail', 'creatures', 'who', 'had', 'forgotten', 'their', 'high', 'ancestry', 'and', 'the', 'white'], ['things', 'of', 'which', 'i', 'went', 'in', 'terror', 'then', 'i', 'thought', 'of', 'the', 'great', 'fear'], ['that', 'was', 'between', 'the', 'two', 'species', 'and', 'for', 'the', 'first', 'time', 'with', 'a'], ['sudden', 'shiver', 'came', 'the', 'clear', 'knowledge', 'of', 'what', 'the', 'meat', 'i', 'had', 'seen'], ['might', 'be', 'yet', 'it', 'was', 'too', 'horrible', 'i', 'looked', 'at', 'little', 'weena', 'sleeping'], ['beside', 'me', 'her', 'face', 'white', 'and', 'starlike', 'under', 'the', 'stars', 'and'], ['forthwith', 'dismissed', 'the', 'thought'], [], ['through', 'that', 'long', 'night', 'i', 'held', 'my', 'mind', 'off', 'the', 'morlocks', 'as', 'well', 'as'], ['i', 'could', 'and', 'whiled', 'away', 'the', 'time', 'by', 'trying', 'to', 'fancy', 'i', 'could', 'find'], ['signs', 'of', 'the', 'old', 'constellations', 'in', 'the', 'new', 'confusion', 'the', 'sky', 'kept'], ['very', 'clear', 'except', 'for', 'a', 'hazy', 'cloud', 'or', 'so', 'no', 'doubt', 'i', 'dozed', 'at'], ['times', 'then', 'as', 'my', 'vigil', 'wore', 'on', 'came', 'a', 'faintness', 'in', 'the', 'eastward'], ['sky', 'like', 'the', 'reflection', 'of', 'some', 'colourless', 'fire', 'and', 'the', 'old', 'moon'], ['rose', 'thin', 'and', 'peaked', 'and', 'white', 'and', 'close', 'behind', 'and', 'overtaking'], ['it', 'and', 'overflowing', 'it', 'the', 'dawn', 'came', 'pale', 'at', 'first', 'and', 'then'], ['growing', 'pink', 'and', 'warm', 'no', 'morlocks', 'had', 'approached', 'us', 'indeed', 'i', 'had'], ['seen', 'none', 'upon', 'the', 'hill', 'that', 'night', 'and', 'in', 'the', 'confidence', 'of', 'renewed'], ['day', 'it', 'almost', 'seemed', 'to', 'me', 'that', 'my', 'fear', 'had', 'been', 'unreasonable', 'i'], ['stood', 'up', 'and', 'found', 'my', 'foot', 'with', 'the', 'loose', 'heel', 'swollen', 'at', 'the', 'ankle'], ['and', 'painful', 'under', 'the', 'heel', 'so', 'i', 'sat', 'down', 'again', 'took', 'off', 'my', 'shoes'], ['and', 'flung', 'them', 'away'], [], ['i', 'awakened', 'weena', 'and', 'we', 'went', 'down', 'into', 'the', 'wood', 'now', 'green', 'and'], ['pleasant', 'instead', 'of', 'black', 'and', 'forbidding', 'we', 'found', 'some', 'fruit'], ['wherewith', 'to', 'break', 'our', 'fast', 'we', 'soon', 'met', 'others', 'of', 'the', 'dainty', 'ones'], ['laughing', 'and', 'dancing', 'in', 'the', 'sunlight', 'as', 'though', 'there', 'was', 'no', 'such'], ['thing', 'in', 'nature', 'as', 'the', 'night', 'and', 'then', 'i', 'thought', 'once', 'more', 'of', 'the'], ['meat', 'that', 'i', 'had', 'seen', 'i', 'felt', 'assured', 'now', 'of', 'what', 'it', 'was', 'and', 'from'], ['the', 'bottom', 'of', 'my', 'heart', 'i', 'pitied', 'this', 'last', 'feeble', 'rill', 'from', 'the', 'great'], ['flood', 'of', 'humanity', 'clearly', 'at', 'some', 'time', 'in', 'the', 'long', 'ago', 'of', 'human'], ['decay', 'the', 'morlocks', 'food', 'had', 'run', 'short', 'possibly', 'they', 'had', 'lived', 'on'], ['rats', 'and', 'such', 'like', 'vermin', 'even', 'now', 'man', 'is', 'far', 'less', 'discriminating'], ['and', 'exclusive', 'in', 'his', 'food', 'than', 'he', 'was', 'far', 'less', 'than', 'any', 'monkey', 'his'], ['prejudice', 'against', 'human', 'flesh', 'is', 'no', 'deep', 'seated', 'instinct', 'and', 'so'], ['these', 'inhuman', 'sons', 'of', 'men', 'i', 'tried', 'to', 'look', 'at', 'the', 'thing', 'in', 'a'], ['scientific', 'spirit', 'after', 'all', 'they', 'were', 'less', 'human', 'and', 'more', 'remote'], ['than', 'our', 'cannibal', 'ancestors', 'of', 'three', 'or', 'four', 'thousand', 'years', 'ago'], ['and', 'the', 'intelligence', 'that', 'would', 'have', 'made', 'this', 'state', 'of', 'things', 'a'], ['torment', 'had', 'gone', 'why', 'should', 'i', 'trouble', 'myself', 'these', 'eloi', 'were', 'mere'], ['fatted', 'cattle', 'which', 'the', 'ant', 'like', 'morlocks', 'preserved', 'and', 'preyed'], ['upon', 'probably', 'saw', 'to', 'the', 'breeding', 'of', 'and', 'there', 'was', 'weena', 'dancing'], ['at', 'my', 'side'], [], ['then', 'i', 'tried', 'to', 'preserve', 'myself', 'from', 'the', 'horror', 'that', 'was', 'coming'], ['upon', 'me', 'by', 'regarding', 'it', 'as', 'a', 'rigorous', 'punishment', 'of', 'human'], ['selfishness', 'man', 'had', 'been', 'content', 'to', 'live', 'in', 'ease', 'and', 'delight', 'upon'], ['the', 'labours', 'of', 'his', 'fellow', 'man', 'had', 'taken', 'necessity', 'as', 'his', 'watchword'], ['and', 'excuse', 'and', 'in', 'the', 'fullness', 'of', 'time', 'necessity', 'had', 'come', 'home', 'to'], ['him', 'i', 'even', 'tried', 'a', 'carlyle', 'like', 'scorn', 'of', 'this', 'wretched', 'aristocracy'], ['in', 'decay', 'but', 'this', 'attitude', 'of', 'mind', 'was', 'impossible', 'however', 'great'], ['their', 'intellectual', 'degradation', 'the', 'eloi', 'had', 'kept', 'too', 'much', 'of', 'the'], ['human', 'form', 'not', 'to', 'claim', 'my', 'sympathy', 'and', 'to', 'make', 'me', 'perforce', 'a'], ['sharer', 'in', 'their', 'degradation', 'and', 'their', 'fear'], [], ['i', 'had', 'at', 'that', 'time', 'very', 'vague', 'ideas', 'as', 'to', 'the', 'course', 'i', 'should'], ['pursue', 'my', 'first', 'was', 'to', 'secure', 'some', 'safe', 'place', 'of', 'refuge', 'and', 'to'], ['make', 'myself', 'such', 'arms', 'of', 'metal', 'or', 'stone', 'as', 'i', 'could', 'contrive', 'that'], ['necessity', 'was', 'immediate', 'in', 'the', 'next', 'place', 'i', 'hoped', 'to', 'procure', 'some'], ['means', 'of', 'fire', 'so', 'that', 'i', 'should', 'have', 'the', 'weapon', 'of', 'a', 'torch', 'at', 'hand'], ['for', 'nothing', 'i', 'knew', 'would', 'be', 'more', 'efficient', 'against', 'these', 'morlocks'], ['then', 'i', 'wanted', 'to', 'arrange', 'some', 'contrivance', 'to', 'break', 'open', 'the', 'doors', 'of'], ['bronze', 'under', 'the', 'white', 'sphinx', 'i', 'had', 'in', 'mind', 'a', 'battering', 'ram', 'i', 'had'], ['a', 'persuasion', 'that', 'if', 'i', 'could', 'enter', 'those', 'doors', 'and', 'carry', 'a', 'blaze', 'of'], ['light', 'before', 'me', 'i', 'should', 'discover', 'the', 'time', 'machine', 'and', 'escape', 'i'], ['could', 'not', 'imagine', 'the', 'morlocks', 'were', 'strong', 'enough', 'to', 'move', 'it', 'far'], ['away', 'weena', 'i', 'had', 'resolved', 'to', 'bring', 'with', 'me', 'to', 'our', 'own', 'time', 'and'], ['turning', 'such', 'schemes', 'over', 'in', 'my', 'mind', 'i', 'pursued', 'our', 'way', 'towards', 'the'], ['building', 'which', 'my', 'fancy', 'had', 'chosen', 'as', 'our', 'dwelling'], [], [], [], [], ['viii'], [], [], ['i', 'found', 'the', 'palace', 'of', 'green', 'porcelain', 'when', 'we', 'approached', 'it', 'about'], ['noon', 'deserted', 'and', 'falling', 'into', 'ruin', 'only', 'ragged', 'vestiges', 'of', 'glass'], ['remained', 'in', 'its', 'windows', 'and', 'great', 'sheets', 'of', 'the', 'green', 'facing', 'had'], ['fallen', 'away', 'from', 'the', 'corroded', 'metallic', 'framework', 'it', 'lay', 'very', 'high'], ['upon', 'a', 'turfy', 'down', 'and', 'looking', 'north', 'eastward', 'before', 'i', 'entered', 'it', 'i'], ['was', 'surprised', 'to', 'see', 'a', 'large', 'estuary', 'or', 'even', 'creek', 'where', 'i', 'judged'], ['wandsworth', 'and', 'battersea', 'must', 'once', 'have', 'been', 'i', 'thought', 'then', 'though'], ['i', 'never', 'followed', 'up', 'the', 'thought', 'of', 'what', 'might', 'have', 'happened', 'or'], ['might', 'be', 'happening', 'to', 'the', 'living', 'things', 'in', 'the', 'sea'], [], ['the', 'material', 'of', 'the', 'palace', 'proved', 'on', 'examination', 'to', 'be', 'indeed'], ['porcelain', 'and', 'along', 'the', 'face', 'of', 'it', 'i', 'saw', 'an', 'inscription', 'in', 'some'], ['unknown', 'character', 'i', 'thought', 'rather', 'foolishly', 'that', 'weena', 'might'], ['help', 'me', 'to', 'interpret', 'this', 'but', 'i', 'only', 'learned', 'that', 'the', 'bare', 'idea', 'of'], ['writing', 'had', 'never', 'entered', 'her', 'head', 'she', 'always', 'seemed', 'to', 'me', 'i'], ['fancy', 'more', 'human', 'than', 'she', 'was', 'perhaps', 'because', 'her', 'affection', 'was', 'so'], ['human'], [], ['within', 'the', 'big', 'valves', 'of', 'the', 'door', 'which', 'were', 'open', 'and', 'broken', 'we'], ['found', 'instead', 'of', 'the', 'customary', 'hall', 'a', 'long', 'gallery', 'lit', 'by', 'many'], ['side', 'windows', 'at', 'the', 'first', 'glance', 'i', 'was', 'reminded', 'of', 'a', 'museum'], ['the', 'tiled', 'floor', 'was', 'thick', 'with', 'dust', 'and', 'a', 'remarkable', 'array', 'of'], ['miscellaneous', 'objects', 'was', 'shrouded', 'in', 'the', 'same', 'grey', 'covering', 'then'], ['i', 'perceived', 'standing', 'strange', 'and', 'gaunt', 'in', 'the', 'centre', 'of', 'the', 'hall'], ['what', 'was', 'clearly', 'the', 'lower', 'part', 'of', 'a', 'huge', 'skeleton', 'i', 'recognized'], ['by', 'the', 'oblique', 'feet', 'that', 'it', 'was', 'some', 'extinct', 'creature', 'after', 'the'], ['fashion', 'of', 'the', 'megatherium', 'the', 'skull', 'and', 'the', 'upper', 'bones', 'lay'], ['beside', 'it', 'in', 'the', 'thick', 'dust', 'and', 'in', 'one', 'place', 'where', 'rain', 'water', 'had'], ['dropped', 'through', 'a', 'leak', 'in', 'the', 'roof', 'the', 'thing', 'itself', 'had', 'been', 'worn'], ['away', 'further', 'in', 'the', 'gallery', 'was', 'the', 'huge', 'skeleton', 'barrel', 'of', 'a'], ['brontosaurus', 'my', 'museum', 'hypothesis', 'was', 'confirmed', 'going', 'towards', 'the'], ['side', 'i', 'found', 'what', 'appeared', 'to', 'be', 'sloping', 'shelves', 'and', 'clearing', 'away'], ['the', 'thick', 'dust', 'i', 'found', 'the', 'old', 'familiar', 'glass', 'cases', 'of', 'our', 'own'], ['time', 'but', 'they', 'must', 'have', 'been', 'air', 'tight', 'to', 'judge', 'from', 'the', 'fair'], ['preservation', 'of', 'some', 'of', 'their', 'contents'], [], ['clearly', 'we', 'stood', 'among', 'the', 'ruins', 'of', 'some', 'latter', 'day', 'south'], ['kensington', 'here', 'apparently', 'was', 'the', 'palaeontological', 'section'], ['and', 'a', 'very', 'splendid', 'array', 'of', 'fossils', 'it', 'must', 'have', 'been', 'though', 'the'], ['inevitable', 'process', 'of', 'decay', 'that', 'had', 'been', 'staved', 'off', 'for', 'a', 'time', 'and'], ['had', 'through', 'the', 'extinction', 'of', 'bacteria', 'and', 'fungi', 'lost', 'ninety', 'nine'], ['hundredths', 'of', 'its', 'force', 'was', 'nevertheless', 'with', 'extreme', 'sureness', 'if'], ['with', 'extreme', 'slowness', 'at', 'work', 'again', 'upon', 'all', 'its', 'treasures', 'here', 'and'], ['there', 'i', 'found', 'traces', 'of', 'the', 'little', 'people', 'in', 'the', 'shape', 'of', 'rare'], ['fossils', 'broken', 'to', 'pieces', 'or', 'threaded', 'in', 'strings', 'upon', 'reeds', 'and', 'the'], ['cases', 'had', 'in', 'some', 'instances', 'been', 'bodily', 'removed', 'by', 'the', 'morlocks', 'as'], ['i', 'judged', 'the', 'place', 'was', 'very', 'silent', 'the', 'thick', 'dust', 'deadened', 'our'], ['footsteps', 'weena', 'who', 'had', 'been', 'rolling', 'a', 'sea', 'urchin', 'down', 'the', 'sloping'], ['glass', 'of', 'a', 'case', 'presently', 'came', 'as', 'i', 'stared', 'about', 'me', 'and', 'very'], ['quietly', 'took', 'my', 'hand', 'and', 'stood', 'beside', 'me'], [], ['and', 'at', 'first', 'i', 'was', 'so', 'much', 'surprised', 'by', 'this', 'ancient', 'monument', 'of', 'an'], ['intellectual', 'age', 'that', 'i', 'gave', 'no', 'thought', 'to', 'the', 'possibilities', 'it'], ['presented', 'even', 'my', 'preoccupation', 'about', 'the', 'time', 'machine', 'receded', 'a'], ['little', 'from', 'my', 'mind'], [], ['to', 'judge', 'from', 'the', 'size', 'of', 'the', 'place', 'this', 'palace', 'of', 'green', 'porcelain'], ['had', 'a', 'great', 'deal', 'more', 'in', 'it', 'than', 'a', 'gallery', 'of', 'palaeontology'], ['possibly', 'historical', 'galleries', 'it', 'might', 'be', 'even', 'a', 'library', 'to', 'me'], ['at', 'least', 'in', 'my', 'present', 'circumstances', 'these', 'would', 'be', 'vastly', 'more'], ['interesting', 'than', 'this', 'spectacle', 'of', 'oldtime', 'geology', 'in', 'decay'], ['exploring', 'i', 'found', 'another', 'short', 'gallery', 'running', 'transversely', 'to', 'the'], ['first', 'this', 'appeared', 'to', 'be', 'devoted', 'to', 'minerals', 'and', 'the', 'sight', 'of', 'a'], ['block', 'of', 'sulphur', 'set', 'my', 'mind', 'running', 'on', 'gunpowder', 'but', 'i', 'could', 'find'], ['no', 'saltpeter', 'indeed', 'no', 'nitrates', 'of', 'any', 'kind', 'doubtless', 'they', 'had'], ['deliquesced', 'ages', 'ago', 'yet', 'the', 'sulphur', 'hung', 'in', 'my', 'mind', 'and', 'set', 'up', 'a'], ['train', 'of', 'thinking', 'as', 'for', 'the', 'rest', 'of', 'the', 'contents', 'of', 'that', 'gallery'], ['though', 'on', 'the', 'whole', 'they', 'were', 'the', 'best', 'preserved', 'of', 'all', 'i', 'saw', 'i', 'had'], ['little', 'interest', 'i', 'am', 'no', 'specialist', 'in', 'mineralogy', 'and', 'i', 'went', 'on'], ['down', 'a', 'very', 'ruinous', 'aisle', 'running', 'parallel', 'to', 'the', 'first', 'hall', 'i', 'had'], ['entered', 'apparently', 'this', 'section', 'had', 'been', 'devoted', 'to', 'natural'], ['history', 'but', 'everything', 'had', 'long', 'since', 'passed', 'out', 'of', 'recognition', 'a'], ['few', 'shrivelled', 'and', 'blackened', 'vestiges', 'of', 'what', 'had', 'once', 'been', 'stuffed'], ['animals', 'desiccated', 'mummies', 'in', 'jars', 'that', 'had', 'once', 'held', 'spirit', 'a'], ['brown', 'dust', 'of', 'departed', 'plants', 'that', 'was', 'all', 'i', 'was', 'sorry', 'for', 'that'], ['because', 'i', 'should', 'have', 'been', 'glad', 'to', 'trace', 'the', 'patent', 'readjustments', 'by'], ['which', 'the', 'conquest', 'of', 'animated', 'nature', 'had', 'been', 'attained', 'then', 'we'], ['came', 'to', 'a', 'gallery', 'of', 'simply', 'colossal', 'proportions', 'but', 'singularly'], ['ill', 'lit', 'the', 'floor', 'of', 'it', 'running', 'downward', 'at', 'a', 'slight', 'angle', 'from', 'the'], ['end', 'at', 'which', 'i', 'entered', 'at', 'intervals', 'white', 'globes', 'hung', 'from', 'the'], ['ceiling', 'many', 'of', 'them', 'cracked', 'and', 'smashed', 'which', 'suggested', 'that'], ['originally', 'the', 'place', 'had', 'been', 'artificially', 'lit', 'here', 'i', 'was', 'more', 'in'], ['my', 'element', 'for', 'rising', 'on', 'either', 'side', 'of', 'me', 'were', 'the', 'huge', 'bulks', 'of'], ['big', 'machines', 'all', 'greatly', 'corroded', 'and', 'many', 'broken', 'down', 'but', 'some'], ['still', 'fairly', 'complete', 'you', 'know', 'i', 'have', 'a', 'certain', 'weakness', 'for'], ['mechanism', 'and', 'i', 'was', 'inclined', 'to', 'linger', 'among', 'these', 'the', 'more', 'so', 'as'], ['for', 'the', 'most', 'part', 'they', 'had', 'the', 'interest', 'of', 'puzzles', 'and', 'i', 'could', 'make'], ['only', 'the', 'vaguest', 'guesses', 'at', 'what', 'they', 'were', 'for', 'i', 'fancied', 'that', 'if'], ['i', 'could', 'solve', 'their', 'puzzles', 'i', 'should', 'find', 'myself', 'in', 'possession', 'of'], ['powers', 'that', 'might', 'be', 'of', 'use', 'against', 'the', 'morlocks'], [], ['suddenly', 'weena', 'came', 'very', 'close', 'to', 'my', 'side', 'so', 'suddenly', 'that', 'she'], ['startled', 'me', 'had', 'it', 'not', 'been', 'for', 'her', 'i', 'do', 'not', 'think', 'i', 'should', 'have'], ['noticed', 'that', 'the', 'floor', 'of', 'the', 'gallery', 'sloped', 'at', 'all', 'footnote', 'it'], ['may', 'be', 'of', 'course', 'that', 'the', 'floor', 'did', 'not', 'slope', 'but', 'that', 'the', 'museum'], ['was', 'built', 'into', 'the', 'side', 'of', 'a', 'hill', 'ed', 'the', 'end', 'i', 'had', 'come', 'in', 'at'], ['was', 'quite', 'above', 'ground', 'and', 'was', 'lit', 'by', 'rare', 'slit', 'like', 'windows', 'as'], ['you', 'went', 'down', 'the', 'length', 'the', 'ground', 'came', 'up', 'against', 'these', 'windows'], ['until', 'at', 'last', 'there', 'was', 'a', 'pit', 'like', 'the', 'area', 'of', 'a', 'london', 'house'], ['before', 'each', 'and', 'only', 'a', 'narrow', 'line', 'of', 'daylight', 'at', 'the', 'top', 'i', 'went'], ['slowly', 'along', 'puzzling', 'about', 'the', 'machines', 'and', 'had', 'been', 'too', 'intent'], ['upon', 'them', 'to', 'notice', 'the', 'gradual', 'diminution', 'of', 'the', 'light', 'until'], ['weena', 's', 'increasing', 'apprehensions', 'drew', 'my', 'attention', 'then', 'i', 'saw', 'that'], ['the', 'gallery', 'ran', 'down', 'at', 'last', 'into', 'a', 'thick', 'darkness', 'i', 'hesitated', 'and'], ['then', 'as', 'i', 'looked', 'round', 'me', 'i', 'saw', 'that', 'the', 'dust', 'was', 'less', 'abundant'], ['and', 'its', 'surface', 'less', 'even', 'further', 'away', 'towards', 'the', 'dimness', 'it'], ['appeared', 'to', 'be', 'broken', 'by', 'a', 'number', 'of', 'small', 'narrow', 'footprints', 'my'], ['sense', 'of', 'the', 'immediate', 'presence', 'of', 'the', 'morlocks', 'revived', 'at', 'that'], ['i', 'felt', 'that', 'i', 'was', 'wasting', 'my', 'time', 'in', 'the', 'academic', 'examination', 'of'], ['machinery', 'i', 'called', 'to', 'mind', 'that', 'it', 'was', 'already', 'far', 'advanced', 'in', 'the'], ['afternoon', 'and', 'that', 'i', 'had', 'still', 'no', 'weapon', 'no', 'refuge', 'and', 'no', 'means'], ['of', 'making', 'a', 'fire', 'and', 'then', 'down', 'in', 'the', 'remote', 'blackness', 'of', 'the'], ['gallery', 'i', 'heard', 'a', 'peculiar', 'pattering', 'and', 'the', 'same', 'odd', 'noises', 'i', 'had'], ['heard', 'down', 'the', 'well'], [], ['i', 'took', 'weena', 's', 'hand', 'then', 'struck', 'with', 'a', 'sudden', 'idea', 'i', 'left', 'her'], ['and', 'turned', 'to', 'a', 'machine', 'from', 'which', 'projected', 'a', 'lever', 'not', 'unlike'], ['those', 'in', 'a', 'signal', 'box', 'clambering', 'upon', 'the', 'stand', 'and', 'grasping', 'this'], ['lever', 'in', 'my', 'hands', 'i', 'put', 'all', 'my', 'weight', 'upon', 'it', 'sideways', 'suddenly'], ['weena', 'deserted', 'in', 'the', 'central', 'aisle', 'began', 'to', 'whimper', 'i', 'had', 'judged'], ['the', 'strength', 'of', 'the', 'lever', 'pretty', 'correctly', 'for', 'it', 'snapped', 'after', 'a'], ['minute', 's', 'strain', 'and', 'i', 'rejoined', 'her', 'with', 'a', 'mace', 'in', 'my', 'hand', 'more', 'than'], ['sufficient', 'i', 'judged', 'for', 'any', 'morlock', 'skull', 'i', 'might', 'encounter', 'and', 'i'], ['longed', 'very', 'much', 'to', 'kill', 'a', 'morlock', 'or', 'so', 'very', 'inhuman', 'you', 'may'], ['think', 'to', 'want', 'to', 'go', 'killing', 'one', 's', 'own', 'descendants', 'but', 'it', 'was'], ['impossible', 'somehow', 'to', 'feel', 'any', 'humanity', 'in', 'the', 'things', 'only', 'my'], ['disinclination', 'to', 'leave', 'weena', 'and', 'a', 'persuasion', 'that', 'if', 'i', 'began', 'to'], ['slake', 'my', 'thirst', 'for', 'murder', 'my', 'time', 'machine', 'might', 'suffer', 'restrained'], ['me', 'from', 'going', 'straight', 'down', 'the', 'gallery', 'and', 'killing', 'the', 'brutes', 'i'], ['heard'], [], ['well', 'mace', 'in', 'one', 'hand', 'and', 'weena', 'in', 'the', 'other', 'i', 'went', 'out', 'of', 'that'], ['gallery', 'and', 'into', 'another', 'and', 'still', 'larger', 'one', 'which', 'at', 'the', 'first'], ['glance', 'reminded', 'me', 'of', 'a', 'military', 'chapel', 'hung', 'with', 'tattered', 'flags'], ['the', 'brown', 'and', 'charred', 'rags', 'that', 'hung', 'from', 'the', 'sides', 'of', 'it', 'i'], ['presently', 'recognized', 'as', 'the', 'decaying', 'vestiges', 'of', 'books', 'they', 'had'], ['long', 'since', 'dropped', 'to', 'pieces', 'and', 'every', 'semblance', 'of', 'print', 'had', 'left'], ['them', 'but', 'here', 'and', 'there', 'were', 'warped', 'boards', 'and', 'cracked', 'metallic'], ['clasps', 'that', 'told', 'the', 'tale', 'well', 'enough', 'had', 'i', 'been', 'a', 'literary', 'man', 'i'], ['might', 'perhaps', 'have', 'moralized', 'upon', 'the', 'futility', 'of', 'all', 'ambition'], ['but', 'as', 'it', 'was', 'the', 'thing', 'that', 'struck', 'me', 'with', 'keenest', 'force', 'was', 'the'], ['enormous', 'waste', 'of', 'labour', 'to', 'which', 'this', 'sombre', 'wilderness', 'of', 'rotting'], ['paper', 'testified', 'at', 'the', 'time', 'i', 'will', 'confess', 'that', 'i', 'thought', 'chiefly'], ['of', 'the', 'philosophical', 'transactions', 'and', 'my', 'own', 'seventeen', 'papers', 'upon'], ['physical', 'optics'], [], ['then', 'going', 'up', 'a', 'broad', 'staircase', 'we', 'came', 'to', 'what', 'may', 'once', 'have'], ['been', 'a', 'gallery', 'of', 'technical', 'chemistry', 'and', 'here', 'i', 'had', 'not', 'a', 'little'], ['hope', 'of', 'useful', 'discoveries', 'except', 'at', 'one', 'end', 'where', 'the', 'roof', 'had'], ['collapsed', 'this', 'gallery', 'was', 'well', 'preserved', 'i', 'went', 'eagerly', 'to', 'every'], ['unbroken', 'case', 'and', 'at', 'last', 'in', 'one', 'of', 'the', 'really', 'air', 'tight', 'cases'], ['i', 'found', 'a', 'box', 'of', 'matches', 'very', 'eagerly', 'i', 'tried', 'them', 'they', 'were'], ['perfectly', 'good', 'they', 'were', 'not', 'even', 'damp', 'i', 'turned', 'to', 'weena', 'dance'], ['i', 'cried', 'to', 'her', 'in', 'her', 'own', 'tongue', 'for', 'now', 'i', 'had', 'a', 'weapon', 'indeed'], ['against', 'the', 'horrible', 'creatures', 'we', 'feared', 'and', 'so', 'in', 'that', 'derelict'], ['museum', 'upon', 'the', 'thick', 'soft', 'carpeting', 'of', 'dust', 'to', 'weena', 's', 'huge'], ['delight', 'i', 'solemnly', 'performed', 'a', 'kind', 'of', 'composite', 'dance', 'whistling'], ['the', 'land', 'of', 'the', 'leal', 'as', 'cheerfully', 'as', 'i', 'could', 'in', 'part', 'it', 'was', 'a'], ['modest', 'cancan', 'in', 'part', 'a', 'step', 'dance', 'in', 'part', 'a', 'skirt', 'dance', 'so', 'far'], ['as', 'my', 'tail', 'coat', 'permitted', 'and', 'in', 'part', 'original', 'for', 'i', 'am', 'naturally'], ['inventive', 'as', 'you', 'know'], [], ['now', 'i', 'still', 'think', 'that', 'for', 'this', 'box', 'of', 'matches', 'to', 'have', 'escaped'], ['the', 'wear', 'of', 'time', 'for', 'immemorial', 'years', 'was', 'a', 'most', 'strange', 'as', 'for'], ['me', 'it', 'was', 'a', 'most', 'fortunate', 'thing', 'yet', 'oddly', 'enough', 'i', 'found', 'a', 'far'], ['unlikelier', 'substance', 'and', 'that', 'was', 'camphor', 'i', 'found', 'it', 'in', 'a', 'sealed'], ['jar', 'that', 'by', 'chance', 'i', 'suppose', 'had', 'been', 'really', 'hermetically', 'sealed'], ['i', 'fancied', 'at', 'first', 'that', 'it', 'was', 'paraffin', 'wax', 'and', 'smashed', 'the', 'glass'], ['accordingly', 'but', 'the', 'odour', 'of', 'camphor', 'was', 'unmistakable', 'in', 'the'], ['universal', 'decay', 'this', 'volatile', 'substance', 'had', 'chanced', 'to', 'survive'], ['perhaps', 'through', 'many', 'thousands', 'of', 'centuries', 'it', 'reminded', 'me', 'of', 'a'], ['sepia', 'painting', 'i', 'had', 'once', 'seen', 'done', 'from', 'the', 'ink', 'of', 'a', 'fossil'], ['belemnite', 'that', 'must', 'have', 'perished', 'and', 'become', 'fossilized', 'millions'], ['of', 'years', 'ago', 'i', 'was', 'about', 'to', 'throw', 'it', 'away', 'but', 'i', 'remembered', 'that'], ['it', 'was', 'inflammable', 'and', 'burned', 'with', 'a', 'good', 'bright', 'flame', 'was', 'in'], ['fact', 'an', 'excellent', 'candle', 'and', 'i', 'put', 'it', 'in', 'my', 'pocket', 'i', 'found', 'no'], ['explosives', 'however', 'nor', 'any', 'means', 'of', 'breaking', 'down', 'the', 'bronze'], ['doors', 'as', 'yet', 'my', 'iron', 'crowbar', 'was', 'the', 'most', 'helpful', 'thing', 'i', 'had'], ['chanced', 'upon', 'nevertheless', 'i', 'left', 'that', 'gallery', 'greatly', 'elated'], [], ['i', 'cannot', 'tell', 'you', 'all', 'the', 'story', 'of', 'that', 'long', 'afternoon', 'it', 'would'], ['require', 'a', 'great', 'effort', 'of', 'memory', 'to', 'recall', 'my', 'explorations', 'in', 'at', 'all'], ['the', 'proper', 'order', 'i', 'remember', 'a', 'long', 'gallery', 'of', 'rusting', 'stands', 'of'], ['arms', 'and', 'how', 'i', 'hesitated', 'between', 'my', 'crowbar', 'and', 'a', 'hatchet', 'or', 'a'], ['sword', 'i', 'could', 'not', 'carry', 'both', 'however', 'and', 'my', 'bar', 'of', 'iron', 'promised'], ['best', 'against', 'the', 'bronze', 'gates', 'there', 'were', 'numbers', 'of', 'guns', 'pistols'], ['and', 'rifles', 'the', 'most', 'were', 'masses', 'of', 'rust', 'but', 'many', 'were', 'of', 'some'], ['new', 'metal', 'and', 'still', 'fairly', 'sound', 'but', 'any', 'cartridges', 'or', 'powder'], ['there', 'may', 'once', 'have', 'been', 'had', 'rotted', 'into', 'dust', 'one', 'corner', 'i', 'saw', 'was'], ['charred', 'and', 'shattered', 'perhaps', 'i', 'thought', 'by', 'an', 'explosion', 'among', 'the'], ['specimens', 'in', 'another', 'place', 'was', 'a', 'vast', 'array', 'of', 'idols', 'polynesian'], ['mexican', 'grecian', 'phoenician', 'every', 'country', 'on', 'earth', 'i', 'should', 'think'], ['and', 'here', 'yielding', 'to', 'an', 'irresistible', 'impulse', 'i', 'wrote', 'my', 'name', 'upon'], ['the', 'nose', 'of', 'a', 'steatite', 'monster', 'from', 'south', 'america', 'that', 'particularly'], ['took', 'my', 'fancy'], [], ['as', 'the', 'evening', 'drew', 'on', 'my', 'interest', 'waned', 'i', 'went', 'through', 'gallery'], ['after', 'gallery', 'dusty', 'silent', 'often', 'ruinous', 'the', 'exhibits', 'sometimes'], ['mere', 'heaps', 'of', 'rust', 'and', 'lignite', 'sometimes', 'fresher', 'in', 'one', 'place', 'i'], ['suddenly', 'found', 'myself', 'near', 'the', 'model', 'of', 'a', 'tin', 'mine', 'and', 'then', 'by', 'the'], ['merest', 'accident', 'i', 'discovered', 'in', 'an', 'air', 'tight', 'case', 'two', 'dynamite'], ['cartridges', 'i', 'shouted', 'eureka', 'and', 'smashed', 'the', 'case', 'with', 'joy', 'then'], ['came', 'a', 'doubt', 'i', 'hesitated', 'then', 'selecting', 'a', 'little', 'side', 'gallery'], ['i', 'made', 'my', 'essay', 'i', 'never', 'felt', 'such', 'a', 'disappointment', 'as', 'i', 'did', 'in'], ['waiting', 'five', 'ten', 'fifteen', 'minutes', 'for', 'an', 'explosion', 'that', 'never', 'came'], ['of', 'course', 'the', 'things', 'were', 'dummies', 'as', 'i', 'might', 'have', 'guessed', 'from'], ['their', 'presence', 'i', 'really', 'believe', 'that', 'had', 'they', 'not', 'been', 'so', 'i', 'should'], ['have', 'rushed', 'off', 'incontinently', 'and', 'blown', 'sphinx', 'bronze', 'doors', 'and'], ['as', 'it', 'proved', 'my', 'chances', 'of', 'finding', 'the', 'time', 'machine', 'all', 'together'], ['into', 'non', 'existence'], [], ['it', 'was', 'after', 'that', 'i', 'think', 'that', 'we', 'came', 'to', 'a', 'little', 'open', 'court'], ['within', 'the', 'palace', 'it', 'was', 'turfed', 'and', 'had', 'three', 'fruit', 'trees', 'so', 'we'], ['rested', 'and', 'refreshed', 'ourselves', 'towards', 'sunset', 'i', 'began', 'to', 'consider'], ['our', 'position', 'night', 'was', 'creeping', 'upon', 'us', 'and', 'my', 'inaccessible'], ['hiding', 'place', 'had', 'still', 'to', 'be', 'found', 'but', 'that', 'troubled', 'me', 'very', 'little'], ['now', 'i', 'had', 'in', 'my', 'possession', 'a', 'thing', 'that', 'was', 'perhaps', 'the', 'best', 'of'], ['all', 'defences', 'against', 'the', 'morlocks', 'i', 'had', 'matches', 'i', 'had', 'the', 'camphor'], ['in', 'my', 'pocket', 'too', 'if', 'a', 'blaze', 'were', 'needed', 'it', 'seemed', 'to', 'me', 'that'], ['the', 'best', 'thing', 'we', 'could', 'do', 'would', 'be', 'to', 'pass', 'the', 'night', 'in', 'the', 'open'], ['protected', 'by', 'a', 'fire', 'in', 'the', 'morning', 'there', 'was', 'the', 'getting', 'of', 'the'], ['time', 'machine', 'towards', 'that', 'as', 'yet', 'i', 'had', 'only', 'my', 'iron', 'mace', 'but'], ['now', 'with', 'my', 'growing', 'knowledge', 'i', 'felt', 'very', 'differently', 'towards'], ['those', 'bronze', 'doors', 'up', 'to', 'this', 'i', 'had', 'refrained', 'from', 'forcing', 'them'], ['largely', 'because', 'of', 'the', 'mystery', 'on', 'the', 'other', 'side', 'they', 'had', 'never'], ['impressed', 'me', 'as', 'being', 'very', 'strong', 'and', 'i', 'hoped', 'to', 'find', 'my', 'bar', 'of'], ['iron', 'not', 'altogether', 'inadequate', 'for', 'the', 'work'], [], [], [], [], ['ix'], [], [], ['we', 'emerged', 'from', 'the', 'palace', 'while', 'the', 'sun', 'was', 'still', 'in', 'part', 'above'], ['the', 'horizon', 'i', 'was', 'determined', 'to', 'reach', 'the', 'white', 'sphinx', 'early', 'the'], ['next', 'morning', 'and', 'ere', 'the', 'dusk', 'i', 'purposed', 'pushing', 'through', 'the', 'woods'], ['that', 'had', 'stopped', 'me', 'on', 'the', 'previous', 'journey', 'my', 'plan', 'was', 'to', 'go', 'as'], ['far', 'as', 'possible', 'that', 'night', 'and', 'then', 'building', 'a', 'fire', 'to', 'sleep'], ['in', 'the', 'protection', 'of', 'its', 'glare', 'accordingly', 'as', 'we', 'went', 'along', 'i'], ['gathered', 'any', 'sticks', 'or', 'dried', 'grass', 'i', 'saw', 'and', 'presently', 'had', 'my', 'arms'], ['full', 'of', 'such', 'litter', 'thus', 'loaded', 'our', 'progress', 'was', 'slower', 'than', 'i', 'had'], ['anticipated', 'and', 'besides', 'weena', 'was', 'tired', 'and', 'i', 'began', 'to', 'suffer', 'from'], ['sleepiness', 'too', 'so', 'that', 'it', 'was', 'full', 'night', 'before', 'we', 'reached', 'the'], ['wood', 'upon', 'the', 'shrubby', 'hill', 'of', 'its', 'edge', 'weena', 'would', 'have', 'stopped'], ['fearing', 'the', 'darkness', 'before', 'us', 'but', 'a', 'singular', 'sense', 'of', 'impending'], ['calamity', 'that', 'should', 'indeed', 'have', 'served', 'me', 'as', 'a', 'warning', 'drove', 'me'], ['onward', 'i', 'had', 'been', 'without', 'sleep', 'for', 'a', 'night', 'and', 'two', 'days', 'and', 'i', 'was'], ['feverish', 'and', 'irritable', 'i', 'felt', 'sleep', 'coming', 'upon', 'me', 'and', 'the'], ['morlocks', 'with', 'it'], [], ['while', 'we', 'hesitated', 'among', 'the', 'black', 'bushes', 'behind', 'us', 'and', 'dim'], ['against', 'their', 'blackness', 'i', 'saw', 'three', 'crouching', 'figures', 'there', 'was'], ['scrub', 'and', 'long', 'grass', 'all', 'about', 'us', 'and', 'i', 'did', 'not', 'feel', 'safe', 'from'], ['their', 'insidious', 'approach', 'the', 'forest', 'i', 'calculated', 'was', 'rather'], ['less', 'than', 'a', 'mile', 'across', 'if', 'we', 'could', 'get', 'through', 'it', 'to', 'the', 'bare'], ['hill', 'side', 'there', 'as', 'it', 'seemed', 'to', 'me', 'was', 'an', 'altogether', 'safer'], ['resting', 'place', 'i', 'thought', 'that', 'with', 'my', 'matches', 'and', 'my', 'camphor', 'i', 'could'], ['contrive', 'to', 'keep', 'my', 'path', 'illuminated', 'through', 'the', 'woods', 'yet', 'it', 'was'], ['evident', 'that', 'if', 'i', 'was', 'to', 'flourish', 'matches', 'with', 'my', 'hands', 'i', 'should'], ['have', 'to', 'abandon', 'my', 'firewood', 'so', 'rather', 'reluctantly', 'i', 'put', 'it', 'down'], ['and', 'then', 'it', 'came', 'into', 'my', 'head', 'that', 'i', 'would', 'amaze', 'our', 'friends', 'behind'], ['by', 'lighting', 'it', 'i', 'was', 'to', 'discover', 'the', 'atrocious', 'folly', 'of', 'this'], ['proceeding', 'but', 'it', 'came', 'to', 'my', 'mind', 'as', 'an', 'ingenious', 'move', 'for', 'covering'], ['our', 'retreat'], [], ['i', 'don', 't', 'know', 'if', 'you', 'have', 'ever', 'thought', 'what', 'a', 'rare', 'thing', 'flame', 'must'], ['be', 'in', 'the', 'absence', 'of', 'man', 'and', 'in', 'a', 'temperate', 'climate', 'the', 'sun', 's'], ['heat', 'is', 'rarely', 'strong', 'enough', 'to', 'burn', 'even', 'when', 'it', 'is', 'focused', 'by'], ['dewdrops', 'as', 'is', 'sometimes', 'the', 'case', 'in', 'more', 'tropical', 'districts'], ['lightning', 'may', 'blast', 'and', 'blacken', 'but', 'it', 'rarely', 'gives', 'rise', 'to'], ['widespread', 'fire', 'decaying', 'vegetation', 'may', 'occasionally', 'smoulder', 'with'], ['the', 'heat', 'of', 'its', 'fermentation', 'but', 'this', 'rarely', 'results', 'in', 'flame', 'in'], ['this', 'decadence', 'too', 'the', 'art', 'of', 'fire', 'making', 'had', 'been', 'forgotten', 'on'], ['the', 'earth', 'the', 'red', 'tongues', 'that', 'went', 'licking', 'up', 'my', 'heap', 'of', 'wood', 'were'], ['an', 'altogether', 'new', 'and', 'strange', 'thing', 'to', 'weena'], [], ['she', 'wanted', 'to', 'run', 'to', 'it', 'and', 'play', 'with', 'it', 'i', 'believe', 'she', 'would', 'have'], ['cast', 'herself', 'into', 'it', 'had', 'i', 'not', 'restrained', 'her', 'but', 'i', 'caught', 'her', 'up'], ['and', 'in', 'spite', 'of', 'her', 'struggles', 'plunged', 'boldly', 'before', 'me', 'into', 'the'], ['wood', 'for', 'a', 'little', 'way', 'the', 'glare', 'of', 'my', 'fire', 'lit', 'the', 'path', 'looking'], ['back', 'presently', 'i', 'could', 'see', 'through', 'the', 'crowded', 'stems', 'that', 'from', 'my'], ['heap', 'of', 'sticks', 'the', 'blaze', 'had', 'spread', 'to', 'some', 'bushes', 'adjacent', 'and', 'a'], ['curved', 'line', 'of', 'fire', 'was', 'creeping', 'up', 'the', 'grass', 'of', 'the', 'hill', 'i', 'laughed'], ['at', 'that', 'and', 'turned', 'again', 'to', 'the', 'dark', 'trees', 'before', 'me', 'it', 'was', 'very'], ['black', 'and', 'weena', 'clung', 'to', 'me', 'convulsively', 'but', 'there', 'was', 'still', 'as'], ['my', 'eyes', 'grew', 'accustomed', 'to', 'the', 'darkness', 'sufficient', 'light', 'for', 'me', 'to'], ['avoid', 'the', 'stems', 'overhead', 'it', 'was', 'simply', 'black', 'except', 'where', 'a', 'gap', 'of'], ['remote', 'blue', 'sky', 'shone', 'down', 'upon', 'us', 'here', 'and', 'there', 'i', 'struck', 'none', 'of'], ['my', 'matches', 'because', 'i', 'had', 'no', 'hand', 'free', 'upon', 'my', 'left', 'arm', 'i', 'carried', 'my'], ['little', 'one', 'in', 'my', 'right', 'hand', 'i', 'had', 'my', 'iron', 'bar'], [], ['for', 'some', 'way', 'i', 'heard', 'nothing', 'but', 'the', 'crackling', 'twigs', 'under', 'my', 'feet'], ['the', 'faint', 'rustle', 'of', 'the', 'breeze', 'above', 'and', 'my', 'own', 'breathing', 'and', 'the'], ['throb', 'of', 'the', 'blood', 'vessels', 'in', 'my', 'ears', 'then', 'i', 'seemed', 'to', 'know', 'of', 'a'], ['pattering', 'about', 'me', 'i', 'pushed', 'on', 'grimly', 'the', 'pattering', 'grew', 'more'], ['distinct', 'and', 'then', 'i', 'caught', 'the', 'same', 'queer', 'sound', 'and', 'voices', 'i', 'had'], ['heard', 'in', 'the', 'under', 'world', 'there', 'were', 'evidently', 'several', 'of', 'the'], ['morlocks', 'and', 'they', 'were', 'closing', 'in', 'upon', 'me', 'indeed', 'in', 'another'], ['minute', 'i', 'felt', 'a', 'tug', 'at', 'my', 'coat', 'then', 'something', 'at', 'my', 'arm', 'and', 'weena'], ['shivered', 'violently', 'and', 'became', 'quite', 'still'], [], ['it', 'was', 'time', 'for', 'a', 'match', 'but', 'to', 'get', 'one', 'i', 'must', 'put', 'her', 'down', 'i', 'did'], ['so', 'and', 'as', 'i', 'fumbled', 'with', 'my', 'pocket', 'a', 'struggle', 'began', 'in', 'the'], ['darkness', 'about', 'my', 'knees', 'perfectly', 'silent', 'on', 'her', 'part', 'and', 'with', 'the'], ['same', 'peculiar', 'cooing', 'sounds', 'from', 'the', 'morlocks', 'soft', 'little', 'hands'], ['too', 'were', 'creeping', 'over', 'my', 'coat', 'and', 'back', 'touching', 'even', 'my', 'neck'], ['then', 'the', 'match', 'scratched', 'and', 'fizzed', 'i', 'held', 'it', 'flaring', 'and', 'saw', 'the'], ['white', 'backs', 'of', 'the', 'morlocks', 'in', 'flight', 'amid', 'the', 'trees', 'i', 'hastily', 'took'], ['a', 'lump', 'of', 'camphor', 'from', 'my', 'pocket', 'and', 'prepared', 'to', 'light', 'it', 'as', 'soon'], ['as', 'the', 'match', 'should', 'wane', 'then', 'i', 'looked', 'at', 'weena', 'she', 'was', 'lying'], ['clutching', 'my', 'feet', 'and', 'quite', 'motionless', 'with', 'her', 'face', 'to', 'the', 'ground'], ['with', 'a', 'sudden', 'fright', 'i', 'stooped', 'to', 'her', 'she', 'seemed', 'scarcely', 'to'], ['breathe', 'i', 'lit', 'the', 'block', 'of', 'camphor', 'and', 'flung', 'it', 'to', 'the', 'ground'], ['and', 'as', 'it', 'split', 'and', 'flared', 'up', 'and', 'drove', 'back', 'the', 'morlocks', 'and', 'the'], ['shadows', 'i', 'knelt', 'down', 'and', 'lifted', 'her', 'the', 'wood', 'behind', 'seemed', 'full', 'of'], ['the', 'stir', 'and', 'murmur', 'of', 'a', 'great', 'company'], [], ['she', 'seemed', 'to', 'have', 'fainted', 'i', 'put', 'her', 'carefully', 'upon', 'my', 'shoulder'], ['and', 'rose', 'to', 'push', 'on', 'and', 'then', 'there', 'came', 'a', 'horrible', 'realization', 'in'], ['manoeuvring', 'with', 'my', 'matches', 'and', 'weena', 'i', 'had', 'turned', 'myself', 'about'], ['several', 'times', 'and', 'now', 'i', 'had', 'not', 'the', 'faintest', 'idea', 'in', 'what', 'direction'], ['lay', 'my', 'path', 'for', 'all', 'i', 'knew', 'i', 'might', 'be', 'facing', 'back', 'towards', 'the'], ['palace', 'of', 'green', 'porcelain', 'i', 'found', 'myself', 'in', 'a', 'cold', 'sweat', 'i', 'had', 'to'], ['think', 'rapidly', 'what', 'to', 'do', 'i', 'determined', 'to', 'build', 'a', 'fire', 'and', 'encamp'], ['where', 'we', 'were', 'i', 'put', 'weena', 'still', 'motionless', 'down', 'upon', 'a', 'turfy'], ['bole', 'and', 'very', 'hastily', 'as', 'my', 'first', 'lump', 'of', 'camphor', 'waned', 'i', 'began'], ['collecting', 'sticks', 'and', 'leaves', 'here', 'and', 'there', 'out', 'of', 'the', 'darkness'], ['round', 'me', 'the', 'morlocks', 'eyes', 'shone', 'like', 'carbuncles'], [], ['the', 'camphor', 'flickered', 'and', 'went', 'out', 'i', 'lit', 'a', 'match', 'and', 'as', 'i', 'did', 'so'], ['two', 'white', 'forms', 'that', 'had', 'been', 'approaching', 'weena', 'dashed', 'hastily', 'away'], ['one', 'was', 'so', 'blinded', 'by', 'the', 'light', 'that', 'he', 'came', 'straight', 'for', 'me', 'and', 'i'], ['felt', 'his', 'bones', 'grind', 'under', 'the', 'blow', 'of', 'my', 'fist', 'he', 'gave', 'a', 'whoop', 'of'], ['dismay', 'staggered', 'a', 'little', 'way', 'and', 'fell', 'down', 'i', 'lit', 'another', 'piece'], ['of', 'camphor', 'and', 'went', 'on', 'gathering', 'my', 'bonfire', 'presently', 'i', 'noticed'], ['how', 'dry', 'was', 'some', 'of', 'the', 'foliage', 'above', 'me', 'for', 'since', 'my', 'arrival'], ['on', 'the', 'time', 'machine', 'a', 'matter', 'of', 'a', 'week', 'no', 'rain', 'had', 'fallen', 'so'], ['instead', 'of', 'casting', 'about', 'among', 'the', 'trees', 'for', 'fallen', 'twigs', 'i', 'began'], ['leaping', 'up', 'and', 'dragging', 'down', 'branches', 'very', 'soon', 'i', 'had', 'a', 'choking'], ['smoky', 'fire', 'of', 'green', 'wood', 'and', 'dry', 'sticks', 'and', 'could', 'economize', 'my'], ['camphor', 'then', 'i', 'turned', 'to', 'where', 'weena', 'lay', 'beside', 'my', 'iron', 'mace', 'i'], ['tried', 'what', 'i', 'could', 'to', 'revive', 'her', 'but', 'she', 'lay', 'like', 'one', 'dead', 'i', 'could'], ['not', 'even', 'satisfy', 'myself', 'whether', 'or', 'not', 'she', 'breathed'], [], ['now', 'the', 'smoke', 'of', 'the', 'fire', 'beat', 'over', 'towards', 'me', 'and', 'it', 'must', 'have'], ['made', 'me', 'heavy', 'of', 'a', 'sudden', 'moreover', 'the', 'vapour', 'of', 'camphor', 'was', 'in'], ['the', 'air', 'my', 'fire', 'would', 'not', 'need', 'replenishing', 'for', 'an', 'hour', 'or', 'so', 'i'], ['felt', 'very', 'weary', 'after', 'my', 'exertion', 'and', 'sat', 'down', 'the', 'wood', 'too', 'was'], ['full', 'of', 'a', 'slumbrous', 'murmur', 'that', 'i', 'did', 'not', 'understand', 'i', 'seemed', 'just'], ['to', 'nod', 'and', 'open', 'my', 'eyes', 'but', 'all', 'was', 'dark', 'and', 'the', 'morlocks', 'had'], ['their', 'hands', 'upon', 'me', 'flinging', 'off', 'their', 'clinging', 'fingers', 'i', 'hastily'], ['felt', 'in', 'my', 'pocket', 'for', 'the', 'match', 'box', 'and', 'it', 'had', 'gone', 'then', 'they'], ['gripped', 'and', 'closed', 'with', 'me', 'again', 'in', 'a', 'moment', 'i', 'knew', 'what', 'had'], ['happened', 'i', 'had', 'slept', 'and', 'my', 'fire', 'had', 'gone', 'out', 'and', 'the', 'bitterness'], ['of', 'death', 'came', 'over', 'my', 'soul', 'the', 'forest', 'seemed', 'full', 'of', 'the', 'smell', 'of'], ['burning', 'wood', 'i', 'was', 'caught', 'by', 'the', 'neck', 'by', 'the', 'hair', 'by', 'the', 'arms'], ['and', 'pulled', 'down', 'it', 'was', 'indescribably', 'horrible', 'in', 'the', 'darkness', 'to'], ['feel', 'all', 'these', 'soft', 'creatures', 'heaped', 'upon', 'me', 'i', 'felt', 'as', 'if', 'i', 'was', 'in'], ['a', 'monstrous', 'spider', 's', 'web', 'i', 'was', 'overpowered', 'and', 'went', 'down', 'i', 'felt'], ['little', 'teeth', 'nipping', 'at', 'my', 'neck', 'i', 'rolled', 'over', 'and', 'as', 'i', 'did', 'so', 'my'], ['hand', 'came', 'against', 'my', 'iron', 'lever', 'it', 'gave', 'me', 'strength', 'i', 'struggled'], ['up', 'shaking', 'the', 'human', 'rats', 'from', 'me', 'and', 'holding', 'the', 'bar', 'short'], ['i', 'thrust', 'where', 'i', 'judged', 'their', 'faces', 'might', 'be', 'i', 'could', 'feel', 'the'], ['succulent', 'giving', 'of', 'flesh', 'and', 'bone', 'under', 'my', 'blows', 'and', 'for', 'a', 'moment'], ['i', 'was', 'free'], [], ['the', 'strange', 'exultation', 'that', 'so', 'often', 'seems', 'to', 'accompany', 'hard'], ['fighting', 'came', 'upon', 'me', 'i', 'knew', 'that', 'both', 'i', 'and', 'weena', 'were', 'lost', 'but', 'i'], ['determined', 'to', 'make', 'the', 'morlocks', 'pay', 'for', 'their', 'meat', 'i', 'stood', 'with', 'my'], ['back', 'to', 'a', 'tree', 'swinging', 'the', 'iron', 'bar', 'before', 'me', 'the', 'whole', 'wood', 'was'], ['full', 'of', 'the', 'stir', 'and', 'cries', 'of', 'them', 'a', 'minute', 'passed', 'their', 'voices'], ['seemed', 'to', 'rise', 'to', 'a', 'higher', 'pitch', 'of', 'excitement', 'and', 'their', 'movements'], ['grew', 'faster', 'yet', 'none', 'came', 'within', 'reach', 'i', 'stood', 'glaring', 'at', 'the'], ['blackness', 'then', 'suddenly', 'came', 'hope', 'what', 'if', 'the', 'morlocks', 'were'], ['afraid', 'and', 'close', 'on', 'the', 'heels', 'of', 'that', 'came', 'a', 'strange', 'thing', 'the'], ['darkness', 'seemed', 'to', 'grow', 'luminous', 'very', 'dimly', 'i', 'began', 'to', 'see', 'the'], ['morlocks', 'about', 'me', 'three', 'battered', 'at', 'my', 'feet', 'and', 'then', 'i', 'recognized'], ['with', 'incredulous', 'surprise', 'that', 'the', 'others', 'were', 'running', 'in', 'an'], ['incessant', 'stream', 'as', 'it', 'seemed', 'from', 'behind', 'me', 'and', 'away', 'through', 'the'], ['wood', 'in', 'front', 'and', 'their', 'backs', 'seemed', 'no', 'longer', 'white', 'but', 'reddish'], ['as', 'i', 'stood', 'agape', 'i', 'saw', 'a', 'little', 'red', 'spark', 'go', 'drifting', 'across', 'a', 'gap'], ['of', 'starlight', 'between', 'the', 'branches', 'and', 'vanish', 'and', 'at', 'that', 'i'], ['understood', 'the', 'smell', 'of', 'burning', 'wood', 'the', 'slumbrous', 'murmur', 'that', 'was'], ['growing', 'now', 'into', 'a', 'gusty', 'roar', 'the', 'red', 'glow', 'and', 'the', 'morlocks'], ['flight'], [], ['stepping', 'out', 'from', 'behind', 'my', 'tree', 'and', 'looking', 'back', 'i', 'saw', 'through'], ['the', 'black', 'pillars', 'of', 'the', 'nearer', 'trees', 'the', 'flames', 'of', 'the', 'burning'], ['forest', 'it', 'was', 'my', 'first', 'fire', 'coming', 'after', 'me', 'with', 'that', 'i', 'looked', 'for'], ['weena', 'but', 'she', 'was', 'gone', 'the', 'hissing', 'and', 'crackling', 'behind', 'me', 'the'], ['explosive', 'thud', 'as', 'each', 'fresh', 'tree', 'burst', 'into', 'flame', 'left', 'little'], ['time', 'for', 'reflection', 'my', 'iron', 'bar', 'still', 'gripped', 'i', 'followed', 'in', 'the'], ['morlocks', 'path', 'it', 'was', 'a', 'close', 'race', 'once', 'the', 'flames', 'crept', 'forward'], ['so', 'swiftly', 'on', 'my', 'right', 'as', 'i', 'ran', 'that', 'i', 'was', 'outflanked', 'and', 'had', 'to'], ['strike', 'off', 'to', 'the', 'left', 'but', 'at', 'last', 'i', 'emerged', 'upon', 'a', 'small', 'open'], ['space', 'and', 'as', 'i', 'did', 'so', 'a', 'morlock', 'came', 'blundering', 'towards', 'me', 'and'], ['past', 'me', 'and', 'went', 'on', 'straight', 'into', 'the', 'fire'], [], ['and', 'now', 'i', 'was', 'to', 'see', 'the', 'most', 'weird', 'and', 'horrible', 'thing', 'i', 'think', 'of'], ['all', 'that', 'i', 'beheld', 'in', 'that', 'future', 'age', 'this', 'whole', 'space', 'was', 'as', 'bright'], ['as', 'day', 'with', 'the', 'reflection', 'of', 'the', 'fire', 'in', 'the', 'centre', 'was', 'a', 'hillock'], ['or', 'tumulus', 'surmounted', 'by', 'a', 'scorched', 'hawthorn', 'beyond', 'this', 'was'], ['another', 'arm', 'of', 'the', 'burning', 'forest', 'with', 'yellow', 'tongues', 'already'], ['writhing', 'from', 'it', 'completely', 'encircling', 'the', 'space', 'with', 'a', 'fence', 'of'], ['fire', 'upon', 'the', 'hill', 'side', 'were', 'some', 'thirty', 'or', 'forty', 'morlocks', 'dazzled'], ['by', 'the', 'light', 'and', 'heat', 'and', 'blundering', 'hither', 'and', 'thither', 'against'], ['each', 'other', 'in', 'their', 'bewilderment', 'at', 'first', 'i', 'did', 'not', 'realize', 'their'], ['blindness', 'and', 'struck', 'furiously', 'at', 'them', 'with', 'my', 'bar', 'in', 'a', 'frenzy', 'of'], ['fear', 'as', 'they', 'approached', 'me', 'killing', 'one', 'and', 'crippling', 'several', 'more'], ['but', 'when', 'i', 'had', 'watched', 'the', 'gestures', 'of', 'one', 'of', 'them', 'groping', 'under', 'the'], ['hawthorn', 'against', 'the', 'red', 'sky', 'and', 'heard', 'their', 'moans', 'i', 'was', 'assured'], ['of', 'their', 'absolute', 'helplessness', 'and', 'misery', 'in', 'the', 'glare', 'and', 'i', 'struck'], ['no', 'more', 'of', 'them'], [], ['yet', 'every', 'now', 'and', 'then', 'one', 'would', 'come', 'straight', 'towards', 'me', 'setting'], ['loose', 'a', 'quivering', 'horror', 'that', 'made', 'me', 'quick', 'to', 'elude', 'him', 'at', 'one'], ['time', 'the', 'flames', 'died', 'down', 'somewhat', 'and', 'i', 'feared', 'the', 'foul', 'creatures'], ['would', 'presently', 'be', 'able', 'to', 'see', 'me', 'i', 'was', 'thinking', 'of', 'beginning', 'the'], ['fight', 'by', 'killing', 'some', 'of', 'them', 'before', 'this', 'should', 'happen', 'but', 'the'], ['fire', 'burst', 'out', 'again', 'brightly', 'and', 'i', 'stayed', 'my', 'hand', 'i', 'walked', 'about'], ['the', 'hill', 'among', 'them', 'and', 'avoided', 'them', 'looking', 'for', 'some', 'trace', 'of'], ['weena', 'but', 'weena', 'was', 'gone'], [], ['at', 'last', 'i', 'sat', 'down', 'on', 'the', 'summit', 'of', 'the', 'hillock', 'and', 'watched', 'this'], ['strange', 'incredible', 'company', 'of', 'blind', 'things', 'groping', 'to', 'and', 'fro', 'and'], ['making', 'uncanny', 'noises', 'to', 'each', 'other', 'as', 'the', 'glare', 'of', 'the', 'fire', 'beat'], ['on', 'them', 'the', 'coiling', 'uprush', 'of', 'smoke', 'streamed', 'across', 'the', 'sky', 'and'], ['through', 'the', 'rare', 'tatters', 'of', 'that', 'red', 'canopy', 'remote', 'as', 'though', 'they'], ['belonged', 'to', 'another', 'universe', 'shone', 'the', 'little', 'stars', 'two', 'or', 'three'], ['morlocks', 'came', 'blundering', 'into', 'me', 'and', 'i', 'drove', 'them', 'off', 'with', 'blows'], ['of', 'my', 'fists', 'trembling', 'as', 'i', 'did', 'so'], [], ['for', 'the', 'most', 'part', 'of', 'that', 'night', 'i', 'was', 'persuaded', 'it', 'was', 'a', 'nightmare'], ['i', 'bit', 'myself', 'and', 'screamed', 'in', 'a', 'passionate', 'desire', 'to', 'awake', 'i', 'beat'], ['the', 'ground', 'with', 'my', 'hands', 'and', 'got', 'up', 'and', 'sat', 'down', 'again', 'and'], ['wandered', 'here', 'and', 'there', 'and', 'again', 'sat', 'down', 'then', 'i', 'would', 'fall', 'to'], ['rubbing', 'my', 'eyes', 'and', 'calling', 'upon', 'god', 'to', 'let', 'me', 'awake', 'thrice', 'i', 'saw'], ['morlocks', 'put', 'their', 'heads', 'down', 'in', 'a', 'kind', 'of', 'agony', 'and', 'rush', 'into', 'the'], ['flames', 'but', 'at', 'last', 'above', 'the', 'subsiding', 'red', 'of', 'the', 'fire', 'above', 'the'], ['streaming', 'masses', 'of', 'black', 'smoke', 'and', 'the', 'whitening', 'and', 'blackening'], ['tree', 'stumps', 'and', 'the', 'diminishing', 'numbers', 'of', 'these', 'dim', 'creatures'], ['came', 'the', 'white', 'light', 'of', 'the', 'day'], [], ['i', 'searched', 'again', 'for', 'traces', 'of', 'weena', 'but', 'there', 'were', 'none', 'it', 'was'], ['plain', 'that', 'they', 'had', 'left', 'her', 'poor', 'little', 'body', 'in', 'the', 'forest', 'i'], ['cannot', 'describe', 'how', 'it', 'relieved', 'me', 'to', 'think', 'that', 'it', 'had', 'escaped', 'the'], ['awful', 'fate', 'to', 'which', 'it', 'seemed', 'destined', 'as', 'i', 'thought', 'of', 'that', 'i', 'was'], ['almost', 'moved', 'to', 'begin', 'a', 'massacre', 'of', 'the', 'helpless', 'abominations', 'about'], ['me', 'but', 'i', 'contained', 'myself', 'the', 'hillock', 'as', 'i', 'have', 'said', 'was', 'a', 'kind'], ['of', 'island', 'in', 'the', 'forest', 'from', 'its', 'summit', 'i', 'could', 'now', 'make', 'out'], ['through', 'a', 'haze', 'of', 'smoke', 'the', 'palace', 'of', 'green', 'porcelain', 'and', 'from', 'that'], ['i', 'could', 'get', 'my', 'bearings', 'for', 'the', 'white', 'sphinx', 'and', 'so', 'leaving', 'the'], ['remnant', 'of', 'these', 'damned', 'souls', 'still', 'going', 'hither', 'and', 'thither', 'and'], ['moaning', 'as', 'the', 'day', 'grew', 'clearer', 'i', 'tied', 'some', 'grass', 'about', 'my', 'feet'], ['and', 'limped', 'on', 'across', 'smoking', 'ashes', 'and', 'among', 'black', 'stems', 'that', 'still'], ['pulsated', 'internally', 'with', 'fire', 'towards', 'the', 'hiding', 'place', 'of', 'the', 'time'], ['machine', 'i', 'walked', 'slowly', 'for', 'i', 'was', 'almost', 'exhausted', 'as', 'well', 'as'], ['lame', 'and', 'i', 'felt', 'the', 'intensest', 'wretchedness', 'for', 'the', 'horrible', 'death'], ['of', 'little', 'weena', 'it', 'seemed', 'an', 'overwhelming', 'calamity', 'now', 'in', 'this'], ['old', 'familiar', 'room', 'it', 'is', 'more', 'like', 'the', 'sorrow', 'of', 'a', 'dream', 'than', 'an'], ['actual', 'loss', 'but', 'that', 'morning', 'it', 'left', 'me', 'absolutely', 'lonely'], ['again', 'terribly', 'alone', 'i', 'began', 'to', 'think', 'of', 'this', 'house', 'of', 'mine', 'of'], ['this', 'fireside', 'of', 'some', 'of', 'you', 'and', 'with', 'such', 'thoughts', 'came', 'a', 'longing'], ['that', 'was', 'pain'], [], ['but', 'as', 'i', 'walked', 'over', 'the', 'smoking', 'ashes', 'under', 'the', 'bright', 'morning'], ['sky', 'i', 'made', 'a', 'discovery', 'in', 'my', 'trouser', 'pocket', 'were', 'still', 'some', 'loose'], ['matches', 'the', 'box', 'must', 'have', 'leaked', 'before', 'it', 'was', 'lost'], [], [], [], [], ['x'], [], [], ['about', 'eight', 'or', 'nine', 'in', 'the', 'morning', 'i', 'came', 'to', 'the', 'same', 'seat', 'of'], ['yellow', 'metal', 'from', 'which', 'i', 'had', 'viewed', 'the', 'world', 'upon', 'the', 'evening', 'of'], ['my', 'arrival', 'i', 'thought', 'of', 'my', 'hasty', 'conclusions', 'upon', 'that', 'evening', 'and'], ['could', 'not', 'refrain', 'from', 'laughing', 'bitterly', 'at', 'my', 'confidence', 'here'], ['was', 'the', 'same', 'beautiful', 'scene', 'the', 'same', 'abundant', 'foliage', 'the', 'same'], ['splendid', 'palaces', 'and', 'magnificent', 'ruins', 'the', 'same', 'silver', 'river'], ['running', 'between', 'its', 'fertile', 'banks', 'the', 'gay', 'robes', 'of', 'the', 'beautiful'], ['people', 'moved', 'hither', 'and', 'thither', 'among', 'the', 'trees', 'some', 'were', 'bathing'], ['in', 'exactly', 'the', 'place', 'where', 'i', 'had', 'saved', 'weena', 'and', 'that', 'suddenly', 'gave'], ['me', 'a', 'keen', 'stab', 'of', 'pain', 'and', 'like', 'blots', 'upon', 'the', 'landscape', 'rose', 'the'], ['cupolas', 'above', 'the', 'ways', 'to', 'the', 'under', 'world', 'i', 'understood', 'now', 'what', 'all'], ['the', 'beauty', 'of', 'the', 'over', 'world', 'people', 'covered', 'very', 'pleasant', 'was', 'their'], ['day', 'as', 'pleasant', 'as', 'the', 'day', 'of', 'the', 'cattle', 'in', 'the', 'field', 'like', 'the'], ['cattle', 'they', 'knew', 'of', 'no', 'enemies', 'and', 'provided', 'against', 'no', 'needs', 'and'], ['their', 'end', 'was', 'the', 'same'], [], ['i', 'grieved', 'to', 'think', 'how', 'brief', 'the', 'dream', 'of', 'the', 'human', 'intellect', 'had'], ['been', 'it', 'had', 'committed', 'suicide', 'it', 'had', 'set', 'itself', 'steadfastly'], ['towards', 'comfort', 'and', 'ease', 'a', 'balanced', 'society', 'with', 'security', 'and'], ['permanency', 'as', 'its', 'watchword', 'it', 'had', 'attained', 'its', 'hopes', 'to', 'come'], ['to', 'this', 'at', 'last', 'once', 'life', 'and', 'property', 'must', 'have', 'reached', 'almost'], ['absolute', 'safety', 'the', 'rich', 'had', 'been', 'assured', 'of', 'his', 'wealth', 'and'], ['comfort', 'the', 'toiler', 'assured', 'of', 'his', 'life', 'and', 'work', 'no', 'doubt', 'in', 'that'], ['perfect', 'world', 'there', 'had', 'been', 'no', 'unemployed', 'problem', 'no', 'social'], ['question', 'left', 'unsolved', 'and', 'a', 'great', 'quiet', 'had', 'followed'], [], ['it', 'is', 'a', 'law', 'of', 'nature', 'we', 'overlook', 'that', 'intellectual', 'versatility'], ['is', 'the', 'compensation', 'for', 'change', 'danger', 'and', 'trouble', 'an', 'animal'], ['perfectly', 'in', 'harmony', 'with', 'its', 'environment', 'is', 'a', 'perfect', 'mechanism'], ['nature', 'never', 'appeals', 'to', 'intelligence', 'until', 'habit', 'and', 'instinct', 'are'], ['useless', 'there', 'is', 'no', 'intelligence', 'where', 'there', 'is', 'no', 'change', 'and', 'no'], ['need', 'of', 'change', 'only', 'those', 'animals', 'partake', 'of', 'intelligence', 'that', 'have'], ['to', 'meet', 'a', 'huge', 'variety', 'of', 'needs', 'and', 'dangers'], [], ['so', 'as', 'i', 'see', 'it', 'the', 'upper', 'world', 'man', 'had', 'drifted', 'towards', 'his'], ['feeble', 'prettiness', 'and', 'the', 'under', 'world', 'to', 'mere', 'mechanical', 'industry'], ['but', 'that', 'perfect', 'state', 'had', 'lacked', 'one', 'thing', 'even', 'for', 'mechanical'], ['perfection', 'absolute', 'permanency', 'apparently', 'as', 'time', 'went', 'on', 'the'], ['feeding', 'of', 'the', 'under', 'world', 'however', 'it', 'was', 'effected', 'had', 'become'], ['disjointed', 'mother', 'necessity', 'who', 'had', 'been', 'staved', 'off', 'for', 'a'], ['few', 'thousand', 'years', 'came', 'back', 'again', 'and', 'she', 'began', 'below', 'the'], ['under', 'world', 'being', 'in', 'contact', 'with', 'machinery', 'which', 'however', 'perfect'], ['still', 'needs', 'some', 'little', 'thought', 'outside', 'habit', 'had', 'probably', 'retained'], ['perforce', 'rather', 'more', 'initiative', 'if', 'less', 'of', 'every', 'other', 'human'], ['character', 'than', 'the', 'upper', 'and', 'when', 'other', 'meat', 'failed', 'them', 'they'], ['turned', 'to', 'what', 'old', 'habit', 'had', 'hitherto', 'forbidden', 'so', 'i', 'say', 'i', 'saw', 'it'], ['in', 'my', 'last', 'view', 'of', 'the', 'world', 'of', 'eight', 'hundred', 'and', 'two', 'thousand', 'seven'], ['hundred', 'and', 'one', 'it', 'may', 'be', 'as', 'wrong', 'an', 'explanation', 'as', 'mortal', 'wit'], ['could', 'invent', 'it', 'is', 'how', 'the', 'thing', 'shaped', 'itself', 'to', 'me', 'and', 'as', 'that', 'i'], ['give', 'it', 'to', 'you'], [], ['after', 'the', 'fatigues', 'excitements', 'and', 'terrors', 'of', 'the', 'past', 'days', 'and'], ['in', 'spite', 'of', 'my', 'grief', 'this', 'seat', 'and', 'the', 'tranquil', 'view', 'and', 'the', 'warm'], ['sunlight', 'were', 'very', 'pleasant', 'i', 'was', 'very', 'tired', 'and', 'sleepy', 'and', 'soon'], ['my', 'theorizing', 'passed', 'into', 'dozing', 'catching', 'myself', 'at', 'that', 'i', 'took', 'my'], ['own', 'hint', 'and', 'spreading', 'myself', 'out', 'upon', 'the', 'turf', 'i', 'had', 'a', 'long', 'and'], ['refreshing', 'sleep'], [], ['i', 'awoke', 'a', 'little', 'before', 'sunsetting', 'i', 'now', 'felt', 'safe', 'against', 'being'], ['caught', 'napping', 'by', 'the', 'morlocks', 'and', 'stretching', 'myself', 'i', 'came', 'on'], ['down', 'the', 'hill', 'towards', 'the', 'white', 'sphinx', 'i', 'had', 'my', 'crowbar', 'in', 'one'], ['hand', 'and', 'the', 'other', 'hand', 'played', 'with', 'the', 'matches', 'in', 'my', 'pocket'], [], ['and', 'now', 'came', 'a', 'most', 'unexpected', 'thing', 'as', 'i', 'approached', 'the', 'pedestal'], ['of', 'the', 'sphinx', 'i', 'found', 'the', 'bronze', 'valves', 'were', 'open', 'they', 'had', 'slid'], ['down', 'into', 'grooves'], [], ['at', 'that', 'i', 'stopped', 'short', 'before', 'them', 'hesitating', 'to', 'enter'], [], ['within', 'was', 'a', 'small', 'apartment', 'and', 'on', 'a', 'raised', 'place', 'in', 'the', 'corner'], ['of', 'this', 'was', 'the', 'time', 'machine', 'i', 'had', 'the', 'small', 'levers', 'in', 'my', 'pocket'], ['so', 'here', 'after', 'all', 'my', 'elaborate', 'preparations', 'for', 'the', 'siege', 'of', 'the'], ['white', 'sphinx', 'was', 'a', 'meek', 'surrender', 'i', 'threw', 'my', 'iron', 'bar', 'away', 'almost'], ['sorry', 'not', 'to', 'use', 'it'], [], ['a', 'sudden', 'thought', 'came', 'into', 'my', 'head', 'as', 'i', 'stooped', 'towards', 'the', 'portal'], ['for', 'once', 'at', 'least', 'i', 'grasped', 'the', 'mental', 'operations', 'of', 'the', 'morlocks'], ['suppressing', 'a', 'strong', 'inclination', 'to', 'laugh', 'i', 'stepped', 'through', 'the'], ['bronze', 'frame', 'and', 'up', 'to', 'the', 'time', 'machine', 'i', 'was', 'surprised', 'to', 'find', 'it'], ['had', 'been', 'carefully', 'oiled', 'and', 'cleaned', 'i', 'have', 'suspected', 'since', 'that'], ['the', 'morlocks', 'had', 'even', 'partially', 'taken', 'it', 'to', 'pieces', 'while', 'trying', 'in'], ['their', 'dim', 'way', 'to', 'grasp', 'its', 'purpose'], [], ['now', 'as', 'i', 'stood', 'and', 'examined', 'it', 'finding', 'a', 'pleasure', 'in', 'the', 'mere'], ['touch', 'of', 'the', 'contrivance', 'the', 'thing', 'i', 'had', 'expected', 'happened', 'the'], ['bronze', 'panels', 'suddenly', 'slid', 'up', 'and', 'struck', 'the', 'frame', 'with', 'a', 'clang'], ['i', 'was', 'in', 'the', 'dark', 'trapped', 'so', 'the', 'morlocks', 'thought', 'at', 'that', 'i'], ['chuckled', 'gleefully'], [], ['i', 'could', 'already', 'hear', 'their', 'murmuring', 'laughter', 'as', 'they', 'came', 'towards'], ['me', 'very', 'calmly', 'i', 'tried', 'to', 'strike', 'the', 'match', 'i', 'had', 'only', 'to', 'fix', 'on'], ['the', 'levers', 'and', 'depart', 'then', 'like', 'a', 'ghost', 'but', 'i', 'had', 'overlooked', 'one'], ['little', 'thing', 'the', 'matches', 'were', 'of', 'that', 'abominable', 'kind', 'that', 'light'], ['only', 'on', 'the', 'box'], [], ['you', 'may', 'imagine', 'how', 'all', 'my', 'calm', 'vanished', 'the', 'little', 'brutes', 'were'], ['close', 'upon', 'me', 'one', 'touched', 'me', 'i', 'made', 'a', 'sweeping', 'blow', 'in', 'the', 'dark', 'at'], ['them', 'with', 'the', 'levers', 'and', 'began', 'to', 'scramble', 'into', 'the', 'saddle', 'of', 'the'], ['machine', 'then', 'came', 'one', 'hand', 'upon', 'me', 'and', 'then', 'another', 'then', 'i', 'had'], ['simply', 'to', 'fight', 'against', 'their', 'persistent', 'fingers', 'for', 'my', 'levers', 'and'], ['at', 'the', 'same', 'time', 'feel', 'for', 'the', 'studs', 'over', 'which', 'these', 'fitted', 'one'], ['indeed', 'they', 'almost', 'got', 'away', 'from', 'me', 'as', 'it', 'slipped', 'from', 'my', 'hand'], ['i', 'had', 'to', 'butt', 'in', 'the', 'dark', 'with', 'my', 'head', 'i', 'could', 'hear', 'the', 'morlock', 's'], ['skull', 'ring', 'to', 'recover', 'it', 'it', 'was', 'a', 'nearer', 'thing', 'than', 'the', 'fight', 'in'], ['the', 'forest', 'i', 'think', 'this', 'last', 'scramble'], [], ['but', 'at', 'last', 'the', 'lever', 'was', 'fitted', 'and', 'pulled', 'over', 'the', 'clinging'], ['hands', 'slipped', 'from', 'me', 'the', 'darkness', 'presently', 'fell', 'from', 'my', 'eyes'], ['i', 'found', 'myself', 'in', 'the', 'same', 'grey', 'light', 'and', 'tumult', 'i', 'have', 'already'], ['described'], [], [], [], [], ['xi'], [], [], ['i', 'have', 'already', 'told', 'you', 'of', 'the', 'sickness', 'and', 'confusion', 'that', 'comes'], ['with', 'time', 'travelling', 'and', 'this', 'time', 'i', 'was', 'not', 'seated', 'properly', 'in', 'the'], ['saddle', 'but', 'sideways', 'and', 'in', 'an', 'unstable', 'fashion', 'for', 'an', 'indefinite'], ['time', 'i', 'clung', 'to', 'the', 'machine', 'as', 'it', 'swayed', 'and', 'vibrated', 'quite'], ['unheeding', 'how', 'i', 'went', 'and', 'when', 'i', 'brought', 'myself', 'to', 'look', 'at', 'the', 'dials'], ['again', 'i', 'was', 'amazed', 'to', 'find', 'where', 'i', 'had', 'arrived', 'one', 'dial', 'records'], ['days', 'and', 'another', 'thousands', 'of', 'days', 'another', 'millions', 'of', 'days', 'and'], ['another', 'thousands', 'of', 'millions', 'now', 'instead', 'of', 'reversing', 'the', 'levers'], ['i', 'had', 'pulled', 'them', 'over', 'so', 'as', 'to', 'go', 'forward', 'with', 'them', 'and', 'when', 'i'], ['came', 'to', 'look', 'at', 'these', 'indicators', 'i', 'found', 'that', 'the', 'thousands', 'hand', 'was'], ['sweeping', 'round', 'as', 'fast', 'as', 'the', 'seconds', 'hand', 'of', 'a', 'watch', 'into'], ['futurity'], [], ['as', 'i', 'drove', 'on', 'a', 'peculiar', 'change', 'crept', 'over', 'the', 'appearance', 'of'], ['things', 'the', 'palpitating', 'greyness', 'grew', 'darker', 'then', 'though', 'i', 'was'], ['still', 'travelling', 'with', 'prodigious', 'velocity', 'the', 'blinking', 'succession'], ['of', 'day', 'and', 'night', 'which', 'was', 'usually', 'indicative', 'of', 'a', 'slower', 'pace'], ['returned', 'and', 'grew', 'more', 'and', 'more', 'marked', 'this', 'puzzled', 'me', 'very', 'much'], ['at', 'first', 'the', 'alternations', 'of', 'night', 'and', 'day', 'grew', 'slower', 'and', 'slower'], ['and', 'so', 'did', 'the', 'passage', 'of', 'the', 'sun', 'across', 'the', 'sky', 'until', 'they', 'seemed'], ['to', 'stretch', 'through', 'centuries', 'at', 'last', 'a', 'steady', 'twilight', 'brooded', 'over'], ['the', 'earth', 'a', 'twilight', 'only', 'broken', 'now', 'and', 'then', 'when', 'a', 'comet', 'glared'], ['across', 'the', 'darkling', 'sky', 'the', 'band', 'of', 'light', 'that', 'had', 'indicated', 'the'], ['sun', 'had', 'long', 'since', 'disappeared', 'for', 'the', 'sun', 'had', 'ceased', 'to', 'set', 'it'], ['simply', 'rose', 'and', 'fell', 'in', 'the', 'west', 'and', 'grew', 'ever', 'broader', 'and', 'more'], ['red', 'all', 'trace', 'of', 'the', 'moon', 'had', 'vanished', 'the', 'circling', 'of', 'the', 'stars'], ['growing', 'slower', 'and', 'slower', 'had', 'given', 'place', 'to', 'creeping', 'points', 'of'], ['light', 'at', 'last', 'some', 'time', 'before', 'i', 'stopped', 'the', 'sun', 'red', 'and', 'very'], ['large', 'halted', 'motionless', 'upon', 'the', 'horizon', 'a', 'vast', 'dome', 'glowing', 'with'], ['a', 'dull', 'heat', 'and', 'now', 'and', 'then', 'suffering', 'a', 'momentary', 'extinction', 'at'], ['one', 'time', 'it', 'had', 'for', 'a', 'little', 'while', 'glowed', 'more', 'brilliantly', 'again'], ['but', 'it', 'speedily', 'reverted', 'to', 'its', 'sullen', 'red', 'heat', 'i', 'perceived', 'by', 'this'], ['slowing', 'down', 'of', 'its', 'rising', 'and', 'setting', 'that', 'the', 'work', 'of', 'the', 'tidal'], ['drag', 'was', 'done', 'the', 'earth', 'had', 'come', 'to', 'rest', 'with', 'one', 'face', 'to', 'the', 'sun'], ['even', 'as', 'in', 'our', 'own', 'time', 'the', 'moon', 'faces', 'the', 'earth', 'very', 'cautiously'], ['for', 'i', 'remembered', 'my', 'former', 'headlong', 'fall', 'i', 'began', 'to', 'reverse'], ['my', 'motion', 'slower', 'and', 'slower', 'went', 'the', 'circling', 'hands', 'until', 'the'], ['thousands', 'one', 'seemed', 'motionless', 'and', 'the', 'daily', 'one', 'was', 'no', 'longer', 'a'], ['mere', 'mist', 'upon', 'its', 'scale', 'still', 'slower', 'until', 'the', 'dim', 'outlines', 'of', 'a'], ['desolate', 'beach', 'grew', 'visible'], [], ['i', 'stopped', 'very', 'gently', 'and', 'sat', 'upon', 'the', 'time', 'machine', 'looking', 'round'], ['the', 'sky', 'was', 'no', 'longer', 'blue', 'north', 'eastward', 'it', 'was', 'inky', 'black'], ['and', 'out', 'of', 'the', 'blackness', 'shone', 'brightly', 'and', 'steadily', 'the', 'pale'], ['white', 'stars', 'overhead', 'it', 'was', 'a', 'deep', 'indian', 'red', 'and', 'starless', 'and'], ['south', 'eastward', 'it', 'grew', 'brighter', 'to', 'a', 'glowing', 'scarlet', 'where', 'cut', 'by'], ['the', 'horizon', 'lay', 'the', 'huge', 'hull', 'of', 'the', 'sun', 'red', 'and', 'motionless', 'the'], ['rocks', 'about', 'me', 'were', 'of', 'a', 'harsh', 'reddish', 'colour', 'and', 'all', 'the', 'trace', 'of'], ['life', 'that', 'i', 'could', 'see', 'at', 'first', 'was', 'the', 'intensely', 'green', 'vegetation'], ['that', 'covered', 'every', 'projecting', 'point', 'on', 'their', 'south', 'eastern', 'face', 'it'], ['was', 'the', 'same', 'rich', 'green', 'that', 'one', 'sees', 'on', 'forest', 'moss', 'or', 'on', 'the'], ['lichen', 'in', 'caves', 'plants', 'which', 'like', 'these', 'grow', 'in', 'a', 'perpetual'], ['twilight'], [], ['the', 'machine', 'was', 'standing', 'on', 'a', 'sloping', 'beach', 'the', 'sea', 'stretched', 'away'], ['to', 'the', 'south', 'west', 'to', 'rise', 'into', 'a', 'sharp', 'bright', 'horizon', 'against', 'the'], ['wan', 'sky', 'there', 'were', 'no', 'breakers', 'and', 'no', 'waves', 'for', 'not', 'a', 'breath', 'of'], ['wind', 'was', 'stirring', 'only', 'a', 'slight', 'oily', 'swell', 'rose', 'and', 'fell', 'like', 'a'], ['gentle', 'breathing', 'and', 'showed', 'that', 'the', 'eternal', 'sea', 'was', 'still', 'moving'], ['and', 'living', 'and', 'along', 'the', 'margin', 'where', 'the', 'water', 'sometimes', 'broke', 'was'], ['a', 'thick', 'incrustation', 'of', 'salt', 'pink', 'under', 'the', 'lurid', 'sky', 'there', 'was', 'a'], ['sense', 'of', 'oppression', 'in', 'my', 'head', 'and', 'i', 'noticed', 'that', 'i', 'was', 'breathing'], ['very', 'fast', 'the', 'sensation', 'reminded', 'me', 'of', 'my', 'only', 'experience', 'of'], ['mountaineering', 'and', 'from', 'that', 'i', 'judged', 'the', 'air', 'to', 'be', 'more', 'rarefied'], ['than', 'it', 'is', 'now'], [], ['far', 'away', 'up', 'the', 'desolate', 'slope', 'i', 'heard', 'a', 'harsh', 'scream', 'and', 'saw', 'a'], ['thing', 'like', 'a', 'huge', 'white', 'butterfly', 'go', 'slanting', 'and', 'fluttering', 'up', 'into'], ['the', 'sky', 'and', 'circling', 'disappear', 'over', 'some', 'low', 'hillocks', 'beyond', 'the'], ['sound', 'of', 'its', 'voice', 'was', 'so', 'dismal', 'that', 'i', 'shivered', 'and', 'seated', 'myself'], ['more', 'firmly', 'upon', 'the', 'machine', 'looking', 'round', 'me', 'again', 'i', 'saw', 'that'], ['quite', 'near', 'what', 'i', 'had', 'taken', 'to', 'be', 'a', 'reddish', 'mass', 'of', 'rock', 'was', 'moving'], ['slowly', 'towards', 'me', 'then', 'i', 'saw', 'the', 'thing', 'was', 'really', 'a', 'monstrous'], ['crab', 'like', 'creature', 'can', 'you', 'imagine', 'a', 'crab', 'as', 'large', 'as', 'yonder', 'table'], ['with', 'its', 'many', 'legs', 'moving', 'slowly', 'and', 'uncertainly', 'its', 'big', 'claws'], ['swaying', 'its', 'long', 'antennae', 'like', 'carters', 'whips', 'waving', 'and', 'feeling'], ['and', 'its', 'stalked', 'eyes', 'gleaming', 'at', 'you', 'on', 'either', 'side', 'of', 'its', 'metallic'], ['front', 'its', 'back', 'was', 'corrugated', 'and', 'ornamented', 'with', 'ungainly', 'bosses'], ['and', 'a', 'greenish', 'incrustation', 'blotched', 'it', 'here', 'and', 'there', 'i', 'could', 'see'], ['the', 'many', 'palps', 'of', 'its', 'complicated', 'mouth', 'flickering', 'and', 'feeling', 'as', 'it'], ['moved'], [], ['as', 'i', 'stared', 'at', 'this', 'sinister', 'apparition', 'crawling', 'towards', 'me', 'i', 'felt'], ['a', 'tickling', 'on', 'my', 'cheek', 'as', 'though', 'a', 'fly', 'had', 'lighted', 'there', 'i', 'tried', 'to'], ['brush', 'it', 'away', 'with', 'my', 'hand', 'but', 'in', 'a', 'moment', 'it', 'returned', 'and', 'almost'], ['immediately', 'came', 'another', 'by', 'my', 'ear', 'i', 'struck', 'at', 'this', 'and', 'caught'], ['something', 'threadlike', 'it', 'was', 'drawn', 'swiftly', 'out', 'of', 'my', 'hand', 'with', 'a'], ['frightful', 'qualm', 'i', 'turned', 'and', 'i', 'saw', 'that', 'i', 'had', 'grasped', 'the', 'antenna'], ['of', 'another', 'monster', 'crab', 'that', 'stood', 'just', 'behind', 'me', 'its', 'evil', 'eyes'], ['were', 'wriggling', 'on', 'their', 'stalks', 'its', 'mouth', 'was', 'all', 'alive', 'with'], ['appetite', 'and', 'its', 'vast', 'ungainly', 'claws', 'smeared', 'with', 'an', 'algal', 'slime'], ['were', 'descending', 'upon', 'me', 'in', 'a', 'moment', 'my', 'hand', 'was', 'on', 'the', 'lever', 'and'], ['i', 'had', 'placed', 'a', 'month', 'between', 'myself', 'and', 'these', 'monsters', 'but', 'i', 'was'], ['still', 'on', 'the', 'same', 'beach', 'and', 'i', 'saw', 'them', 'distinctly', 'now', 'as', 'soon', 'as', 'i'], ['stopped', 'dozens', 'of', 'them', 'seemed', 'to', 'be', 'crawling', 'here', 'and', 'there', 'in', 'the'], ['sombre', 'light', 'among', 'the', 'foliated', 'sheets', 'of', 'intense', 'green'], [], ['i', 'cannot', 'convey', 'the', 'sense', 'of', 'abominable', 'desolation', 'that', 'hung', 'over'], ['the', 'world', 'the', 'red', 'eastern', 'sky', 'the', 'northward', 'blackness', 'the', 'salt'], ['dead', 'sea', 'the', 'stony', 'beach', 'crawling', 'with', 'these', 'foul', 'slow', 'stirring'], ['monsters', 'the', 'uniform', 'poisonous', 'looking', 'green', 'of', 'the', 'lichenous'], ['plants', 'the', 'thin', 'air', 'that', 'hurts', 'one', 's', 'lungs', 'all', 'contributed', 'to', 'an'], ['appalling', 'effect', 'i', 'moved', 'on', 'a', 'hundred', 'years', 'and', 'there', 'was', 'the', 'same'], ['red', 'sun', 'a', 'little', 'larger', 'a', 'little', 'duller', 'the', 'same', 'dying', 'sea', 'the'], ['same', 'chill', 'air', 'and', 'the', 'same', 'crowd', 'of', 'earthy', 'crustacea', 'creeping', 'in'], ['and', 'out', 'among', 'the', 'green', 'weed', 'and', 'the', 'red', 'rocks', 'and', 'in', 'the', 'westward'], ['sky', 'i', 'saw', 'a', 'curved', 'pale', 'line', 'like', 'a', 'vast', 'new', 'moon'], [], ['so', 'i', 'travelled', 'stopping', 'ever', 'and', 'again', 'in', 'great', 'strides', 'of', 'a'], ['thousand', 'years', 'or', 'more', 'drawn', 'on', 'by', 'the', 'mystery', 'of', 'the', 'earth', 's', 'fate'], ['watching', 'with', 'a', 'strange', 'fascination', 'the', 'sun', 'grow', 'larger', 'and', 'duller'], ['in', 'the', 'westward', 'sky', 'and', 'the', 'life', 'of', 'the', 'old', 'earth', 'ebb', 'away', 'at'], ['last', 'more', 'than', 'thirty', 'million', 'years', 'hence', 'the', 'huge', 'red', 'hot', 'dome', 'of'], ['the', 'sun', 'had', 'come', 'to', 'obscure', 'nearly', 'a', 'tenth', 'part', 'of', 'the', 'darkling'], ['heavens', 'then', 'i', 'stopped', 'once', 'more', 'for', 'the', 'crawling', 'multitude', 'of'], ['crabs', 'had', 'disappeared', 'and', 'the', 'red', 'beach', 'save', 'for', 'its', 'livid', 'green'], ['liverworts', 'and', 'lichens', 'seemed', 'lifeless', 'and', 'now', 'it', 'was', 'flecked', 'with'], ['white', 'a', 'bitter', 'cold', 'assailed', 'me', 'rare', 'white', 'flakes', 'ever', 'and', 'again'], ['came', 'eddying', 'down', 'to', 'the', 'north', 'eastward', 'the', 'glare', 'of', 'snow', 'lay'], ['under', 'the', 'starlight', 'of', 'the', 'sable', 'sky', 'and', 'i', 'could', 'see', 'an', 'undulating'], ['crest', 'of', 'hillocks', 'pinkish', 'white', 'there', 'were', 'fringes', 'of', 'ice', 'along', 'the'], ['sea', 'margin', 'with', 'drifting', 'masses', 'further', 'out', 'but', 'the', 'main', 'expanse'], ['of', 'that', 'salt', 'ocean', 'all', 'bloody', 'under', 'the', 'eternal', 'sunset', 'was', 'still'], ['unfrozen'], [], ['i', 'looked', 'about', 'me', 'to', 'see', 'if', 'any', 'traces', 'of', 'animal', 'life', 'remained', 'a'], ['certain', 'indefinable', 'apprehension', 'still', 'kept', 'me', 'in', 'the', 'saddle', 'of', 'the'], ['machine', 'but', 'i', 'saw', 'nothing', 'moving', 'in', 'earth', 'or', 'sky', 'or', 'sea', 'the', 'green'], ['slime', 'on', 'the', 'rocks', 'alone', 'testified', 'that', 'life', 'was', 'not', 'extinct', 'a'], ['shallow', 'sandbank', 'had', 'appeared', 'in', 'the', 'sea', 'and', 'the', 'water', 'had', 'receded'], ['from', 'the', 'beach', 'i', 'fancied', 'i', 'saw', 'some', 'black', 'object', 'flopping', 'about'], ['upon', 'this', 'bank', 'but', 'it', 'became', 'motionless', 'as', 'i', 'looked', 'at', 'it', 'and', 'i'], ['judged', 'that', 'my', 'eye', 'had', 'been', 'deceived', 'and', 'that', 'the', 'black', 'object', 'was'], ['merely', 'a', 'rock', 'the', 'stars', 'in', 'the', 'sky', 'were', 'intensely', 'bright', 'and', 'seemed'], ['to', 'me', 'to', 'twinkle', 'very', 'little'], [], ['suddenly', 'i', 'noticed', 'that', 'the', 'circular', 'westward', 'outline', 'of', 'the', 'sun'], ['had', 'changed', 'that', 'a', 'concavity', 'a', 'bay', 'had', 'appeared', 'in', 'the', 'curve', 'i'], ['saw', 'this', 'grow', 'larger', 'for', 'a', 'minute', 'perhaps', 'i', 'stared', 'aghast', 'at', 'this'], ['blackness', 'that', 'was', 'creeping', 'over', 'the', 'day', 'and', 'then', 'i', 'realized', 'that'], ['an', 'eclipse', 'was', 'beginning', 'either', 'the', 'moon', 'or', 'the', 'planet', 'mercury', 'was'], ['passing', 'across', 'the', 'sun', 's', 'disk', 'naturally', 'at', 'first', 'i', 'took', 'it', 'to', 'be'], ['the', 'moon', 'but', 'there', 'is', 'much', 'to', 'incline', 'me', 'to', 'believe', 'that', 'what', 'i'], ['really', 'saw', 'was', 'the', 'transit', 'of', 'an', 'inner', 'planet', 'passing', 'very', 'near', 'to'], ['the', 'earth'], [], ['the', 'darkness', 'grew', 'apace', 'a', 'cold', 'wind', 'began', 'to', 'blow', 'in', 'freshening'], ['gusts', 'from', 'the', 'east', 'and', 'the', 'showering', 'white', 'flakes', 'in', 'the', 'air'], ['increased', 'in', 'number', 'from', 'the', 'edge', 'of', 'the', 'sea', 'came', 'a', 'ripple', 'and'], ['whisper', 'beyond', 'these', 'lifeless', 'sounds', 'the', 'world', 'was', 'silent', 'silent'], ['it', 'would', 'be', 'hard', 'to', 'convey', 'the', 'stillness', 'of', 'it', 'all', 'the', 'sounds', 'of'], ['man', 'the', 'bleating', 'of', 'sheep', 'the', 'cries', 'of', 'birds', 'the', 'hum', 'of', 'insects'], ['the', 'stir', 'that', 'makes', 'the', 'background', 'of', 'our', 'lives', 'all', 'that', 'was', 'over'], ['as', 'the', 'darkness', 'thickened', 'the', 'eddying', 'flakes', 'grew', 'more', 'abundant'], ['dancing', 'before', 'my', 'eyes', 'and', 'the', 'cold', 'of', 'the', 'air', 'more', 'intense', 'at'], ['last', 'one', 'by', 'one', 'swiftly', 'one', 'after', 'the', 'other', 'the', 'white', 'peaks', 'of'], ['the', 'distant', 'hills', 'vanished', 'into', 'blackness', 'the', 'breeze', 'rose', 'to', 'a'], ['moaning', 'wind', 'i', 'saw', 'the', 'black', 'central', 'shadow', 'of', 'the', 'eclipse', 'sweeping'], ['towards', 'me', 'in', 'another', 'moment', 'the', 'pale', 'stars', 'alone', 'were', 'visible', 'all'], ['else', 'was', 'rayless', 'obscurity', 'the', 'sky', 'was', 'absolutely', 'black'], [], ['a', 'horror', 'of', 'this', 'great', 'darkness', 'came', 'on', 'me', 'the', 'cold', 'that', 'smote'], ['to', 'my', 'marrow', 'and', 'the', 'pain', 'i', 'felt', 'in', 'breathing', 'overcame', 'me', 'i'], ['shivered', 'and', 'a', 'deadly', 'nausea', 'seized', 'me', 'then', 'like', 'a', 'red', 'hot', 'bow'], ['in', 'the', 'sky', 'appeared', 'the', 'edge', 'of', 'the', 'sun', 'i', 'got', 'off', 'the', 'machine', 'to'], ['recover', 'myself', 'i', 'felt', 'giddy', 'and', 'incapable', 'of', 'facing', 'the', 'return'], ['journey', 'as', 'i', 'stood', 'sick', 'and', 'confused', 'i', 'saw', 'again', 'the', 'moving', 'thing'], ['upon', 'the', 'shoal', 'there', 'was', 'no', 'mistake', 'now', 'that', 'it', 'was', 'a', 'moving'], ['thing', 'against', 'the', 'red', 'water', 'of', 'the', 'sea', 'it', 'was', 'a', 'round', 'thing', 'the'], ['size', 'of', 'a', 'football', 'perhaps', 'or', 'it', 'may', 'be', 'bigger', 'and', 'tentacles'], ['trailed', 'down', 'from', 'it', 'it', 'seemed', 'black', 'against', 'the', 'weltering'], ['blood', 'red', 'water', 'and', 'it', 'was', 'hopping', 'fitfully', 'about', 'then', 'i', 'felt', 'i'], ['was', 'fainting', 'but', 'a', 'terrible', 'dread', 'of', 'lying', 'helpless', 'in', 'that', 'remote'], ['and', 'awful', 'twilight', 'sustained', 'me', 'while', 'i', 'clambered', 'upon', 'the', 'saddle'], [], [], [], [], ['xii'], [], [], ['so', 'i', 'came', 'back', 'for', 'a', 'long', 'time', 'i', 'must', 'have', 'been', 'insensible', 'upon'], ['the', 'machine', 'the', 'blinking', 'succession', 'of', 'the', 'days', 'and', 'nights', 'was'], ['resumed', 'the', 'sun', 'got', 'golden', 'again', 'the', 'sky', 'blue', 'i', 'breathed', 'with'], ['greater', 'freedom', 'the', 'fluctuating', 'contours', 'of', 'the', 'land', 'ebbed', 'and'], ['flowed', 'the', 'hands', 'spun', 'backward', 'upon', 'the', 'dials', 'at', 'last', 'i', 'saw', 'again'], ['the', 'dim', 'shadows', 'of', 'houses', 'the', 'evidences', 'of', 'decadent', 'humanity'], ['these', 'too', 'changed', 'and', 'passed', 'and', 'others', 'came', 'presently', 'when', 'the'], ['million', 'dial', 'was', 'at', 'zero', 'i', 'slackened', 'speed', 'i', 'began', 'to', 'recognize'], ['our', 'own', 'pretty', 'and', 'familiar', 'architecture', 'the', 'thousands', 'hand', 'ran', 'back'], ['to', 'the', 'starting', 'point', 'the', 'night', 'and', 'day', 'flapped', 'slower', 'and', 'slower'], ['then', 'the', 'old', 'walls', 'of', 'the', 'laboratory', 'came', 'round', 'me', 'very', 'gently'], ['now', 'i', 'slowed', 'the', 'mechanism', 'down'], [], ['i', 'saw', 'one', 'little', 'thing', 'that', 'seemed', 'odd', 'to', 'me', 'i', 'think', 'i', 'have', 'told'], ['you', 'that', 'when', 'i', 'set', 'out', 'before', 'my', 'velocity', 'became', 'very', 'high', 'mrs'], ['watchett', 'had', 'walked', 'across', 'the', 'room', 'travelling', 'as', 'it', 'seemed', 'to', 'me'], ['like', 'a', 'rocket', 'as', 'i', 'returned', 'i', 'passed', 'again', 'across', 'that', 'minute', 'when'], ['she', 'traversed', 'the', 'laboratory', 'but', 'now', 'her', 'every', 'motion', 'appeared', 'to'], ['be', 'the', 'exact', 'inversion', 'of', 'her', 'previous', 'ones', 'the', 'door', 'at', 'the', 'lower'], ['end', 'opened', 'and', 'she', 'glided', 'quietly', 'up', 'the', 'laboratory', 'back', 'foremost'], ['and', 'disappeared', 'behind', 'the', 'door', 'by', 'which', 'she', 'had', 'previously', 'entered'], ['just', 'before', 'that', 'i', 'seemed', 'to', 'see', 'hillyer', 'for', 'a', 'moment', 'but', 'he', 'passed'], ['like', 'a', 'flash'], [], ['then', 'i', 'stopped', 'the', 'machine', 'and', 'saw', 'about', 'me', 'again', 'the', 'old', 'familiar'], ['laboratory', 'my', 'tools', 'my', 'appliances', 'just', 'as', 'i', 'had', 'left', 'them', 'i', 'got'], ['off', 'the', 'thing', 'very', 'shakily', 'and', 'sat', 'down', 'upon', 'my', 'bench', 'for', 'several'], ['minutes', 'i', 'trembled', 'violently', 'then', 'i', 'became', 'calmer', 'around', 'me', 'was'], ['my', 'old', 'workshop', 'again', 'exactly', 'as', 'it', 'had', 'been', 'i', 'might', 'have', 'slept'], ['there', 'and', 'the', 'whole', 'thing', 'have', 'been', 'a', 'dream'], [], ['and', 'yet', 'not', 'exactly', 'the', 'thing', 'had', 'started', 'from', 'the', 'south', 'east'], ['corner', 'of', 'the', 'laboratory', 'it', 'had', 'come', 'to', 'rest', 'again', 'in', 'the'], ['north', 'west', 'against', 'the', 'wall', 'where', 'you', 'saw', 'it', 'that', 'gives', 'you', 'the'], ['exact', 'distance', 'from', 'my', 'little', 'lawn', 'to', 'the', 'pedestal', 'of', 'the', 'white'], ['sphinx', 'into', 'which', 'the', 'morlocks', 'had', 'carried', 'my', 'machine'], [], ['for', 'a', 'time', 'my', 'brain', 'went', 'stagnant', 'presently', 'i', 'got', 'up', 'and', 'came'], ['through', 'the', 'passage', 'here', 'limping', 'because', 'my', 'heel', 'was', 'still'], ['painful', 'and', 'feeling', 'sorely', 'begrimed', 'i', 'saw', 'the', 'pall', 'mall', 'gazette'], ['on', 'the', 'table', 'by', 'the', 'door', 'i', 'found', 'the', 'date', 'was', 'indeed', 'to', 'day', 'and'], ['looking', 'at', 'the', 'timepiece', 'saw', 'the', 'hour', 'was', 'almost', 'eight', 'o', 'clock', 'i'], ['heard', 'your', 'voices', 'and', 'the', 'clatter', 'of', 'plates', 'i', 'hesitated', 'i', 'felt', 'so'], ['sick', 'and', 'weak', 'then', 'i', 'sniffed', 'good', 'wholesome', 'meat', 'and', 'opened', 'the'], ['door', 'on', 'you', 'you', 'know', 'the', 'rest', 'i', 'washed', 'and', 'dined', 'and', 'now', 'i', 'am'], ['telling', 'you', 'the', 'story'], [], ['i', 'know', 'he', 'said', 'after', 'a', 'pause', 'that', 'all', 'this', 'will', 'be', 'absolutely'], ['incredible', 'to', 'you', 'to', 'me', 'the', 'one', 'incredible', 'thing', 'is', 'that', 'i', 'am', 'here'], ['to', 'night', 'in', 'this', 'old', 'familiar', 'room', 'looking', 'into', 'your', 'friendly', 'faces'], ['and', 'telling', 'you', 'these', 'strange', 'adventures'], [], ['he', 'looked', 'at', 'the', 'medical', 'man', 'no', 'i', 'cannot', 'expect', 'you', 'to', 'believe'], ['it', 'take', 'it', 'as', 'a', 'lie', 'or', 'a', 'prophecy', 'say', 'i', 'dreamed', 'it', 'in', 'the'], ['workshop', 'consider', 'i', 'have', 'been', 'speculating', 'upon', 'the', 'destinies', 'of', 'our'], ['race', 'until', 'i', 'have', 'hatched', 'this', 'fiction', 'treat', 'my', 'assertion', 'of', 'its'], ['truth', 'as', 'a', 'mere', 'stroke', 'of', 'art', 'to', 'enhance', 'its', 'interest', 'and', 'taking'], ['it', 'as', 'a', 'story', 'what', 'do', 'you', 'think', 'of', 'it'], [], ['he', 'took', 'up', 'his', 'pipe', 'and', 'began', 'in', 'his', 'old', 'accustomed', 'manner', 'to', 'tap'], ['with', 'it', 'nervously', 'upon', 'the', 'bars', 'of', 'the', 'grate', 'there', 'was', 'a', 'momentary'], ['stillness', 'then', 'chairs', 'began', 'to', 'creak', 'and', 'shoes', 'to', 'scrape', 'upon', 'the'], ['carpet', 'i', 'took', 'my', 'eyes', 'off', 'the', 'time', 'traveller', 's', 'face', 'and', 'looked'], ['round', 'at', 'his', 'audience', 'they', 'were', 'in', 'the', 'dark', 'and', 'little', 'spots', 'of'], ['colour', 'swam', 'before', 'them', 'the', 'medical', 'man', 'seemed', 'absorbed', 'in', 'the'], ['contemplation', 'of', 'our', 'host', 'the', 'editor', 'was', 'looking', 'hard', 'at', 'the', 'end'], ['of', 'his', 'cigar', 'the', 'sixth', 'the', 'journalist', 'fumbled', 'for', 'his', 'watch', 'the'], ['others', 'as', 'far', 'as', 'i', 'remember', 'were', 'motionless'], [], ['the', 'editor', 'stood', 'up', 'with', 'a', 'sigh', 'what', 'a', 'pity', 'it', 'is', 'you', 're', 'not'], ['a', 'writer', 'of', 'stories', 'he', 'said', 'putting', 'his', 'hand', 'on', 'the', 'time'], ['traveller', 's', 'shoulder'], [], ['you', 'don', 't', 'believe', 'it'], [], ['well'], [], ['i', 'thought', 'not'], [], ['the', 'time', 'traveller', 'turned', 'to', 'us', 'where', 'are', 'the', 'matches', 'he', 'said'], ['he', 'lit', 'one', 'and', 'spoke', 'over', 'his', 'pipe', 'puffing', 'to', 'tell', 'you', 'the', 'truth'], ['i', 'hardly', 'believe', 'it', 'myself', 'and', 'yet'], [], ['his', 'eye', 'fell', 'with', 'a', 'mute', 'inquiry', 'upon', 'the', 'withered', 'white', 'flowers'], ['upon', 'the', 'little', 'table', 'then', 'he', 'turned', 'over', 'the', 'hand', 'holding', 'his'], ['pipe', 'and', 'i', 'saw', 'he', 'was', 'looking', 'at', 'some', 'half', 'healed', 'scars', 'on', 'his'], ['knuckles'], [], ['the', 'medical', 'man', 'rose', 'came', 'to', 'the', 'lamp', 'and', 'examined', 'the', 'flowers'], ['the', 'gynaeceum', 's', 'odd', 'he', 'said', 'the', 'psychologist', 'leant', 'forward', 'to'], ['see', 'holding', 'out', 'his', 'hand', 'for', 'a', 'specimen'], [], ['i', 'm', 'hanged', 'if', 'it', 'isn', 't', 'a', 'quarter', 'to', 'one', 'said', 'the', 'journalist'], ['how', 'shall', 'we', 'get', 'home'], [], ['plenty', 'of', 'cabs', 'at', 'the', 'station', 'said', 'the', 'psychologist'], [], ['it', 's', 'a', 'curious', 'thing', 'said', 'the', 'medical', 'man', 'but', 'i', 'certainly', 'don', 't'], ['know', 'the', 'natural', 'order', 'of', 'these', 'flowers', 'may', 'i', 'have', 'them'], [], ['the', 'time', 'traveller', 'hesitated', 'then', 'suddenly', 'certainly', 'not'], [], ['where', 'did', 'you', 'really', 'get', 'them', 'said', 'the', 'medical', 'man'], [], ['the', 'time', 'traveller', 'put', 'his', 'hand', 'to', 'his', 'head', 'he', 'spoke', 'like', 'one', 'who'], ['was', 'trying', 'to', 'keep', 'hold', 'of', 'an', 'idea', 'that', 'eluded', 'him', 'they', 'were', 'put'], ['into', 'my', 'pocket', 'by', 'weena', 'when', 'i', 'travelled', 'into', 'time', 'he', 'stared'], ['round', 'the', 'room', 'i', 'm', 'damned', 'if', 'it', 'isn', 't', 'all', 'going', 'this', 'room', 'and', 'you'], ['and', 'the', 'atmosphere', 'of', 'every', 'day', 'is', 'too', 'much', 'for', 'my', 'memory', 'did', 'i'], ['ever', 'make', 'a', 'time', 'machine', 'or', 'a', 'model', 'of', 'a', 'time', 'machine', 'or', 'is', 'it', 'all'], ['only', 'a', 'dream', 'they', 'say', 'life', 'is', 'a', 'dream', 'a', 'precious', 'poor', 'dream', 'at'], ['times', 'but', 'i', 'can', 't', 'stand', 'another', 'that', 'won', 't', 'fit', 'it', 's', 'madness', 'and'], ['where', 'did', 'the', 'dream', 'come', 'from', 'i', 'must', 'look', 'at', 'that', 'machine', 'if'], ['there', 'is', 'one'], [], ['he', 'caught', 'up', 'the', 'lamp', 'swiftly', 'and', 'carried', 'it', 'flaring', 'red', 'through'], ['the', 'door', 'into', 'the', 'corridor', 'we', 'followed', 'him', 'there', 'in', 'the', 'flickering'], ['light', 'of', 'the', 'lamp', 'was', 'the', 'machine', 'sure', 'enough', 'squat', 'ugly', 'and'], ['askew', 'a', 'thing', 'of', 'brass', 'ebony', 'ivory', 'and', 'translucent', 'glimmering'], ['quartz', 'solid', 'to', 'the', 'touch', 'for', 'i', 'put', 'out', 'my', 'hand', 'and', 'felt', 'the', 'rail'], ['of', 'it', 'and', 'with', 'brown', 'spots', 'and', 'smears', 'upon', 'the', 'ivory', 'and', 'bits', 'of'], ['grass', 'and', 'moss', 'upon', 'the', 'lower', 'parts', 'and', 'one', 'rail', 'bent', 'awry'], [], ['the', 'time', 'traveller', 'put', 'the', 'lamp', 'down', 'on', 'the', 'bench', 'and', 'ran', 'his', 'hand'], ['along', 'the', 'damaged', 'rail', 'it', 's', 'all', 'right', 'now', 'he', 'said', 'the', 'story', 'i'], ['told', 'you', 'was', 'true', 'i', 'm', 'sorry', 'to', 'have', 'brought', 'you', 'out', 'here', 'in', 'the'], ['cold', 'he', 'took', 'up', 'the', 'lamp', 'and', 'in', 'an', 'absolute', 'silence', 'we'], ['returned', 'to', 'the', 'smoking', 'room'], [], ['he', 'came', 'into', 'the', 'hall', 'with', 'us', 'and', 'helped', 'the', 'editor', 'on', 'with', 'his'], ['coat', 'the', 'medical', 'man', 'looked', 'into', 'his', 'face', 'and', 'with', 'a', 'certain'], ['hesitation', 'told', 'him', 'he', 'was', 'suffering', 'from', 'overwork', 'at', 'which', 'he'], ['laughed', 'hugely', 'i', 'remember', 'him', 'standing', 'in', 'the', 'open', 'doorway', 'bawling'], ['good', 'night'], [], ['i', 'shared', 'a', 'cab', 'with', 'the', 'editor', 'he', 'thought', 'the', 'tale', 'a', 'gaudy', 'lie'], ['for', 'my', 'own', 'part', 'i', 'was', 'unable', 'to', 'come', 'to', 'a', 'conclusion', 'the', 'story', 'was'], ['so', 'fantastic', 'and', 'incredible', 'the', 'telling', 'so', 'credible', 'and', 'sober', 'i'], ['lay', 'awake', 'most', 'of', 'the', 'night', 'thinking', 'about', 'it', 'i', 'determined', 'to', 'go'], ['next', 'day', 'and', 'see', 'the', 'time', 'traveller', 'again', 'i', 'was', 'told', 'he', 'was', 'in', 'the'], ['laboratory', 'and', 'being', 'on', 'easy', 'terms', 'in', 'the', 'house', 'i', 'went', 'up', 'to', 'him'], ['the', 'laboratory', 'however', 'was', 'empty', 'i', 'stared', 'for', 'a', 'minute', 'at', 'the'], ['time', 'machine', 'and', 'put', 'out', 'my', 'hand', 'and', 'touched', 'the', 'lever', 'at', 'that', 'the'], ['squat', 'substantial', 'looking', 'mass', 'swayed', 'like', 'a', 'bough', 'shaken', 'by', 'the'], ['wind', 'its', 'instability', 'startled', 'me', 'extremely', 'and', 'i', 'had', 'a', 'queer'], ['reminiscence', 'of', 'the', 'childish', 'days', 'when', 'i', 'used', 'to', 'be', 'forbidden', 'to'], ['meddle', 'i', 'came', 'back', 'through', 'the', 'corridor', 'the', 'time', 'traveller', 'met', 'me'], ['in', 'the', 'smoking', 'room', 'he', 'was', 'coming', 'from', 'the', 'house', 'he', 'had', 'a', 'small'], ['camera', 'under', 'one', 'arm', 'and', 'a', 'knapsack', 'under', 'the', 'other', 'he', 'laughed', 'when'], ['he', 'saw', 'me', 'and', 'gave', 'me', 'an', 'elbow', 'to', 'shake', 'i', 'm', 'frightfully', 'busy'], ['said', 'he', 'with', 'that', 'thing', 'in', 'there'], [], ['but', 'is', 'it', 'not', 'some', 'hoax', 'i', 'said', 'do', 'you', 'really', 'travel', 'through'], ['time'], [], ['really', 'and', 'truly', 'i', 'do', 'and', 'he', 'looked', 'frankly', 'into', 'my', 'eyes', 'he'], ['hesitated', 'his', 'eye', 'wandered', 'about', 'the', 'room', 'i', 'only', 'want', 'half', 'an'], ['hour', 'he', 'said', 'i', 'know', 'why', 'you', 'came', 'and', 'it', 's', 'awfully', 'good', 'of', 'you'], ['there', 's', 'some', 'magazines', 'here', 'if', 'you', 'll', 'stop', 'to', 'lunch', 'i', 'll', 'prove', 'you'], ['this', 'time', 'travelling', 'up', 'to', 'the', 'hilt', 'specimen', 'and', 'all', 'if', 'you', 'll'], ['forgive', 'my', 'leaving', 'you', 'now'], [], ['i', 'consented', 'hardly', 'comprehending', 'then', 'the', 'full', 'import', 'of', 'his', 'words'], ['and', 'he', 'nodded', 'and', 'went', 'on', 'down', 'the', 'corridor', 'i', 'heard', 'the', 'door', 'of'], ['the', 'laboratory', 'slam', 'seated', 'myself', 'in', 'a', 'chair', 'and', 'took', 'up', 'a', 'daily'], ['paper', 'what', 'was', 'he', 'going', 'to', 'do', 'before', 'lunch', 'time', 'then', 'suddenly'], ['i', 'was', 'reminded', 'by', 'an', 'advertisement', 'that', 'i', 'had', 'promised', 'to', 'meet'], ['richardson', 'the', 'publisher', 'at', 'two', 'i', 'looked', 'at', 'my', 'watch', 'and', 'saw'], ['that', 'i', 'could', 'barely', 'save', 'that', 'engagement', 'i', 'got', 'up', 'and', 'went', 'down', 'the'], ['passage', 'to', 'tell', 'the', 'time', 'traveller'], [], ['as', 'i', 'took', 'hold', 'of', 'the', 'handle', 'of', 'the', 'door', 'i', 'heard', 'an', 'exclamation'], ['oddly', 'truncated', 'at', 'the', 'end', 'and', 'a', 'click', 'and', 'a', 'thud', 'a', 'gust', 'of', 'air'], ['whirled', 'round', 'me', 'as', 'i', 'opened', 'the', 'door', 'and', 'from', 'within', 'came', 'the'], ['sound', 'of', 'broken', 'glass', 'falling', 'on', 'the', 'floor', 'the', 'time', 'traveller', 'was'], ['not', 'there', 'i', 'seemed', 'to', 'see', 'a', 'ghostly', 'indistinct', 'figure', 'sitting', 'in'], ['a', 'whirling', 'mass', 'of', 'black', 'and', 'brass', 'for', 'a', 'moment', 'a', 'figure', 'so'], ['transparent', 'that', 'the', 'bench', 'behind', 'with', 'its', 'sheets', 'of', 'drawings', 'was'], ['absolutely', 'distinct', 'but', 'this', 'phantasm', 'vanished', 'as', 'i', 'rubbed', 'my', 'eyes'], ['the', 'time', 'machine', 'had', 'gone', 'save', 'for', 'a', 'subsiding', 'stir', 'of', 'dust', 'the'], ['further', 'end', 'of', 'the', 'laboratory', 'was', 'empty', 'a', 'pane', 'of', 'the', 'skylight', 'had'], ['apparently', 'just', 'been', 'blown', 'in'], [], ['i', 'felt', 'an', 'unreasonable', 'amazement', 'i', 'knew', 'that', 'something', 'strange', 'had'], ['happened', 'and', 'for', 'the', 'moment', 'could', 'not', 'distinguish', 'what', 'the', 'strange'], ['thing', 'might', 'be', 'as', 'i', 'stood', 'staring', 'the', 'door', 'into', 'the', 'garden', 'opened'], ['and', 'the', 'man', 'servant', 'appeared'], [], ['we', 'looked', 'at', 'each', 'other', 'then', 'ideas', 'began', 'to', 'come', 'has', 'mr'], ['gone', 'out', 'that', 'way', 'said', 'i'], [], ['no', 'sir', 'no', 'one', 'has', 'come', 'out', 'this', 'way', 'i', 'was', 'expecting', 'to', 'find', 'him'], ['here'], [], ['at', 'that', 'i', 'understood', 'at', 'the', 'risk', 'of', 'disappointing', 'richardson', 'i'], ['stayed', 'on', 'waiting', 'for', 'the', 'time', 'traveller', 'waiting', 'for', 'the', 'second'], ['perhaps', 'still', 'stranger', 'story', 'and', 'the', 'specimens', 'and', 'photographs', 'he'], ['would', 'bring', 'with', 'him', 'but', 'i', 'am', 'beginning', 'now', 'to', 'fear', 'that', 'i', 'must'], ['wait', 'a', 'lifetime', 'the', 'time', 'traveller', 'vanished', 'three', 'years', 'ago', 'and'], ['as', 'everybody', 'knows', 'now', 'he', 'has', 'never', 'returned'], [], [], [], [], ['epilogue'], [], [], ['one', 'cannot', 'choose', 'but', 'wonder', 'will', 'he', 'ever', 'return', 'it', 'may', 'be', 'that', 'he'], ['swept', 'back', 'into', 'the', 'past', 'and', 'fell', 'among', 'the', 'blood', 'drinking', 'hairy'], ['savages', 'of', 'the', 'age', 'of', 'unpolished', 'stone', 'into', 'the', 'abysses', 'of', 'the'], ['cretaceous', 'sea', 'or', 'among', 'the', 'grotesque', 'saurians', 'the', 'huge', 'reptilian'], ['brutes', 'of', 'the', 'jurassic', 'times', 'he', 'may', 'even', 'now', 'if', 'i', 'may', 'use', 'the'], ['phrase', 'be', 'wandering', 'on', 'some', 'plesiosaurus', 'haunted', 'oolitic', 'coral'], ['reef', 'or', 'beside', 'the', 'lonely', 'saline', 'lakes', 'of', 'the', 'triassic', 'age', 'or', 'did'], ['he', 'go', 'forward', 'into', 'one', 'of', 'the', 'nearer', 'ages', 'in', 'which', 'men', 'are', 'still'], ['men', 'but', 'with', 'the', 'riddles', 'of', 'our', 'own', 'time', 'answered', 'and', 'its', 'wearisome'], ['problems', 'solved', 'into', 'the', 'manhood', 'of', 'the', 'race', 'for', 'i', 'for', 'my', 'own'], ['part', 'cannot', 'think', 'that', 'these', 'latter', 'days', 'of', 'weak', 'experiment'], ['fragmentary', 'theory', 'and', 'mutual', 'discord', 'are', 'indeed', 'man', 's', 'culminating'], ['time', 'i', 'say', 'for', 'my', 'own', 'part', 'he', 'i', 'know', 'for', 'the', 'question', 'had', 'been'], ['discussed', 'among', 'us', 'long', 'before', 'the', 'time', 'machine', 'was', 'made', 'thought'], ['but', 'cheerlessly', 'of', 'the', 'advancement', 'of', 'mankind', 'and', 'saw', 'in', 'the'], ['growing', 'pile', 'of', 'civilization', 'only', 'a', 'foolish', 'heaping', 'that', 'must'], ['inevitably', 'fall', 'back', 'upon', 'and', 'destroy', 'its', 'makers', 'in', 'the', 'end', 'if', 'that'], ['is', 'so', 'it', 'remains', 'for', 'us', 'to', 'live', 'as', 'though', 'it', 'were', 'not', 'so', 'but', 'to', 'me'], ['the', 'future', 'is', 'still', 'black', 'and', 'blank', 'is', 'a', 'vast', 'ignorance', 'lit', 'at', 'a'], ['few', 'casual', 'places', 'by', 'the', 'memory', 'of', 'his', 'story', 'and', 'i', 'have', 'by', 'me', 'for'], ['my', 'comfort', 'two', 'strange', 'white', 'flowers', 'shrivelled', 'now', 'and', 'brown', 'and'], ['flat', 'and', 'brittle', 'to', 'witness', 'that', 'even', 'when', 'mind', 'and', 'strength', 'had'], ['gone', 'gratitude', 'and', 'a', 'mutual', 'tenderness', 'still', 'lived', 'on', 'in', 'the', 'heart'], ['of', 'man']]\n",
      "0: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "1: []\n",
      "2: []\n",
      "3: []\n",
      "4: []\n",
      "5: ['i']\n",
      "6: []\n",
      "7: []\n",
      "8: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "9: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "10: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "11: ['fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent']\n",
      "12: ['lights', 'in', 'the', 'lilies', 'of', 'silver', 'caught', 'the', 'bubbles', 'that', 'flashed', 'and']\n",
      "13: ['passed', 'in', 'our', 'glasses', 'our', 'chairs', 'being', 'his', 'patents', 'embraced', 'and']\n",
      "14: ['caressed', 'us', 'rather', 'than', 'submitted', 'to', 'be', 'sat', 'upon', 'and', 'there', 'was', 'that']\n",
      "15: ['luxurious', 'after', 'dinner', 'atmosphere', 'when', 'thought', 'roams', 'gracefully']\n",
      "16: ['free', 'of', 'the', 'trammels', 'of', 'precision', 'and', 'he', 'put', 'it', 'to', 'us', 'in', 'this']\n",
      "17: ['way', 'marking', 'the', 'points', 'with', 'a', 'lean', 'forefinger', 'as', 'we', 'sat', 'and', 'lazily']\n",
      "18: ['admired', 'his', 'earnestness', 'over', 'this', 'new', 'paradox', 'as', 'we', 'thought', 'it']\n",
      "19: ['and', 'his', 'fecundity']\n",
      "20: []\n",
      "21: ['you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two']\n",
      "22: ['ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for']\n",
      "23: ['instance', 'they', 'taught', 'you', 'at', 'school', 'is', 'founded', 'on', 'a', 'misconception']\n",
      "24: []\n",
      "25: ['is', 'not', 'that', 'rather', 'a', 'large', 'thing', 'to', 'expect', 'us', 'to', 'begin', 'upon']\n"
     ]
    }
   ],
   "source": [
    "# 按照word\n",
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "\n",
    "# [['The', 'machine', 'is', 'haha'], [], [], ...]\n",
    "tokens = tokenize(lines, token='word')\n",
    "# tokens = tokenize(lines, token='char')\n",
    "\n",
    "print('tokens:', tokens)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f'{i}: {token}')\n",
    "    if i == 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.3. <a id='toc11_2_3_3_'></a>[词表（Vocabulary）](#toc0_)\n",
    "* 构建(token：索引)查询元组\n",
    "* 并将文本的token替换成索引\n",
    "\n",
    "    |token|indice|annotation|\n",
    "    |---|---|---|\n",
    "    |unk|0|unknown|\n",
    "    |PAD|1|padding|\n",
    "    |SOS|2|start of sentence|\n",
    "    |EOS|3|end of sentence| \n",
    "    |...|...|...|\n",
    "\n",
    "\n",
    "* id_to_token：索引到token的映射，列表格式\n",
    "* token_to_id：token到索引的映射，字典格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab type: <class '__main__.Vocab'>\n",
      "vocab size: 4580\n",
      "vocab[0:5]:\n",
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4)]\n",
      "====================================================================================================\n",
      "文本: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "====================================================================================================\n",
      "文本: ['fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the', 'incandescent']\n",
      "索引: [148, 588, 825, 3, 1, 244, 2187, 4, 1, 2188]\n"
     ]
    }
   ],
   "source": [
    "class Vocab():  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    \n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)                                                      # 统计词频\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)       # 词频从高到低排序\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens                                     # 列表格式 ['<unk>', ...]\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}     # 字典格式 {token: idx}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:                                         # 如果词语的频率低于 min_freq，则停止添加。\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)                                 # 字典追加\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1           # 字典更新\n",
    "\n",
    "    def __len__(self):\n",
    "        '''魔法函数，返回词表的长度'''\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        '''魔法函数，返回词表的索引'''\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        '''\n",
    "        单个索引：如果传入的是单个词语（不是列表或元组），则返回对应的索引。如果词语未在词汇表中出现，则返回 <unk> 的索引（默认为 0）。\n",
    "        批量词语查询：如果传入的是词语列表或元组，则返回对应的索引列表。\n",
    "        '''\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        '''属性方法，返回未知词元的索引'''\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        '''属性方法，返回词频'''\n",
    "        return self._token_freqs\n",
    "\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "vocab = Vocab(tokens)\n",
    "\n",
    "print(f'vocab type: {type(vocab)}')\n",
    "print(f'vocab size: {len(vocab)}')\n",
    "print('vocab[0:5]:', list(vocab.token_to_idx.items())[:5], sep='\\n')\n",
    "\n",
    "for i in [0, 11]:\n",
    "    print(\"=\"*100)\n",
    "    print('文本:', tokens[i])\n",
    "    print('索引:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.4. <a id='toc11_2_3_4_'></a>[整合所有功能](#toc0_)\n",
    "* 读取数据\n",
    "* 分割成token\n",
    "* 并构建(token, indice)查询表\n",
    "* 替换token成indice，从而构成corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照char进行词元化 \n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    # 读取数据，返回 ['The machine is haha', '', '', ...]\n",
    "    lines = read_time_machine()\n",
    "\n",
    "    # 分词数据，返回 [['The', 'machine', 'is', 'haha'], [], [], ...]\n",
    "    tokens = tokenize(lines, 'char')      # char\n",
    "    # tokens = tokenize(lines, token='word')  # word\n",
    "\n",
    "    # 构建词表 vocab.__getitem__(token_to_idx.get(token)) 在字典中查找\n",
    "    # 列表：idx_to_token = ['<unk>', reserved_tokens, 'the', 'i', 'and', ...]\n",
    "    # 字典：token_to_idx = {'<unk>': indice, reserved_tokens: indice, 'the': indice, 'i': indice, 'and': indice, ...}\n",
    "    vocab = Vocab(tokens)\n",
    "\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中，构成语料库 (corpus) \n",
    "    # 展开为一维列表：corpus是[indice, indice, indice, ...]\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "\n",
    "    if max_tokens > 0:  # 如果max_tokens大于0，则截断corpus\n",
    "        corpus = corpus[:max_tokens]\n",
    "\n",
    "    return corpus, vocab\n",
    "\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "\n",
    "# 语料库长度，词表长度\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3.5. <a id='toc11_2_3_5_'></a>[文本编码与向量化](#toc0_)\n",
    "文本编码的目的\n",
    "文本编码是将文本数据转换为数值形式，以便机器学习模型能够处理。常见的编码方法包括：\n",
    "  - 独热编码（One-Hot Encoding）\n",
    "  - 词袋模型（Bag-of-Words）\n",
    "  - TF-IDF（Term Frequency-Inverse Document Frequency）\n",
    "  - 词嵌入（Word Embeddings）： 词嵌入通过将词汇映射到高维连续向量空间，捕捉词汇之间的语义关系。常见的词嵌入方法包括Word2Vec、GloVe和FastText。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.2.3.5.1. <a id='toc11_2_3_5_1_'></a>[word2vec](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.4. <a id='toc11_2_4_'></a>[语言模型数据集](#toc0_)\n",
    "#### 11.2.4.1. <a id='toc11_2_4_1_'></a>[顺序采样 (Sequential Sampling)](#toc0_)\n",
    "\n",
    "- 顺序采样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch \n",
    "\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列，起始点。\n",
    "    # 生成一个随机偏移量 offset，范围在 0 到 num_steps 之间。这样做的目的是为了随机化数据的起始位置，避免模型过于依赖数据的开始部分，提高泛化能力。\n",
    "    offset = random.randint(0, num_steps)   \n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size  # 计算可以形成完整批次数(取整) * batch_size = 可以形成完整批次的所有token\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])  # 提取 X 序列\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])  # 提取 Y 序列，只是X右移一位\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)  # 重塑形状\n",
    "    num_batches = Xs.shape[1] // num_steps  # 计算批次数\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps] # batch_size, num_steps\n",
    "        Y = Ys[:, i: i + num_steps] # batch_size, num_steps\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " tensor([[ 1,  3,  5, 13,  2],\n",
      "        [ 4,  3,  1,  3,  9]])\n",
      "Y: \n",
      " tensor([[ 3,  5, 13,  2,  1],\n",
      "        [ 3,  1,  3,  9,  5]])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in seq_data_iter_sequential(corpus=corpus, batch_size=2, num_steps=5):\n",
    "    print('X: \\n', X)\n",
    "    print('Y: \\n', Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.2. <a id='toc11_2_4_2_'></a>[随机采样 (Random Sampling)](#toc0_)\n",
    "\n",
    "- 随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import torch \n",
    "\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " tensor([[ 9,  2,  1,  3,  5],\n",
      "        [ 3,  9,  2,  1, 17]])\n",
      "Y: \n",
      " tensor([[ 2,  1,  3,  5, 13],\n",
      "        [ 9,  2,  1, 17,  4]])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in seq_data_iter_random(corpus=corpus, batch_size=2, num_steps=5):\n",
    "    print('X: \\n', X)\n",
    "    print('Y: \\n', Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.3. <a id='toc11_2_4_3_'></a>[PyTorch分装的顺序或随机采样](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils import data \n",
    "\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    def __init__(self, corpus:list, num_steps:int):\n",
    "        self.datas = self._get_data_list(corpus, num_steps)\n",
    "\n",
    "    def _get_data_list(self, corpus:list, num_steps:int):\n",
    "        datas = []\n",
    "        # 在corpus中随机选择一个起始位置，范围在0到num_steps之间\n",
    "        start_position = torch.randint(low=0, high=num_steps, size=(1,))\n",
    "        corpus = corpus[start_position:]\n",
    "        num_subseqs = len(corpus) // num_steps \n",
    "        for num in range(0, (num_subseqs-1) * num_steps, num_steps):\n",
    "            x = torch.tensor(corpus[num: num + num_steps])\n",
    "            y = torch.tensor(corpus[num + 1: num + 1 + num_steps])\n",
    "            datas.append((x, y))\n",
    "        return datas \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx]\n",
    "    \n",
    "\n",
    "# datasets\n",
    "datasets = TextDataset(corpus=corpus, num_steps=5)\n",
    "\n",
    "# 顺序采样, 不shuffle\n",
    "sequential_train_loader = data.DataLoader(datasets, batch_size=2, shuffle=False)\n",
    "\n",
    "# 随机采样, shuffle\n",
    "random_train_loader = data.DataLoader(datasets, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  1,  3,  5, 13]), tensor([ 1,  3,  5, 13,  2]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  1,  3,  5, 13],\n",
      "        [ 2,  1, 13,  4, 15]])\n",
      "tensor([[ 1,  3,  5, 13,  2],\n",
      "        [ 1, 13,  4, 15,  9]])\n"
     ]
    }
   ],
   "source": [
    "for X, y in sequential_train_loader:\n",
    "    print(X, y, sep='\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  1,  7, 16,  1],\n",
      "        [ 9,  4, 22,  2,  1]])\n",
      "tensor([[ 1,  7, 16,  1, 15],\n",
      "        [ 4, 22,  2,  1, 21]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x, y in random_train_loader:\n",
    "    print(x, y, sep='\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.4. <a id='toc11_2_4_4_'></a>[总结](#toc0_)\n",
    "\n",
    "| 特性 | 顺序采样 | 随机采样 |\n",
    "| -------------- | --------------------------------- | --------------------------------- |\n",
    "| 采样方式 | 按照数据顺序选择 | 随机选择 |\n",
    "| 代表性 | 可能不具备良好代表性 | 通常具有较好代表性 |\n",
    "| 适用场景 | 时间序列数据、在线学习 | 大部分机器学习任务、模型评估 |\n",
    "| 优点 | 保留数据顺序、实现简单 | 减少偏差、适用于更多算法 |\n",
    "| 缺点 | 可能存在偏差、无法处理Non-IID数据 | 可能打破时间序列信息、实现复杂度较高 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4.5. <a id='toc11_2_4_5_'></a>[包装](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            # self.data_iter_fn = d2l.seq_data_iter_random\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            # self.data_iter_fn = d2l.seq_data_iter_sequential\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "\n",
    "        # self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"\n",
    "    返回时光机器数据集的迭代器和词表\n",
    "    \"\"\"\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens) \n",
    "       \n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3. <a id='toc11_3_'></a>[RNN](#toc0_)\n",
    "可以处理有顺序的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1. <a id='toc11_3_1_'></a>[RNN-循环神经网络原理](#toc0_)\n",
    "* 结构：\n",
    "    * 有一层（或多层）隐藏结构；\n",
    "    * 当前隐藏结构由上一侧隐藏结构和当前输入决定\n",
    "    * 依次类推\n",
    "\n",
    "<!-- <img src=\"./Pytorch_Pictures/RNN//Simple-RNN.jpg\" width = \"500\" height = \"300\" alt=\"图片名称\" align=center /> -->\n",
    "![Simple-RNN](./Pytorch_Pictures/RNN/base-RNN.jpg)\n",
    "\n",
    "更新隐藏状态：      \n",
    "$\\mathbf{h}_t=\\phi(\\mathbf{W}_{hh}\\mathbf{h}_{t-1}+\\mathbf{W}_{hx}\\mathbf{x}_{t}+\\mathbf{b}_h)$  \n",
    "输出：             \n",
    "$\\mathbf{o}_t=\\phi(\\mathbf{W}_\\textit{ho}\\mathbf{h}_t+\\mathbf{b}_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.1. <a id='toc11_3_1_1_'></a>[从头实现网络](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 初始化模型    \n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):                        \n",
    "    # 初始化第一个隐变量的值, (num_layers, batch_size, num_hiddens)，此时num_layers=1, num_layers=1\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小), (num_steps, batch_size, vocab_size)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(batch_size, vocab_size))\n",
    "    for X in inputs:    # 依次在num_steps中遍历，X的形状：(batch_size, vocab_size),依序列顺序展开\n",
    "        # X: (batch_size, vocab_size)\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)         # 隐藏变量: (batch_size, num_hiddens)\n",
    "        # H: (batch_size, num_hiddens)\n",
    "\n",
    "        Y = torch.mm(H, W_hq) + b_q                                         # 输出: (batch_size, num_outputs) 此时num_outputs=vocab_size\n",
    "        # Y: (batch_size, num_outputs/vocab_size)\n",
    "\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)  # 返回所有时间步的输出，以及最终的隐藏状态, (num_steps, batch_size, num_outputs)\n",
    "\n",
    "\n",
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32) # (num_steps, batch_size, vocab_size)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.Size([5, 2]), torch.Size([5, 2, 28]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "batch_size, num_steps = 2, 5\n",
    "\n",
    "X = torch.arange(10).reshape((batch_size, num_steps))\n",
    "\n",
    "X.shape, X.T.shape, F.one_hot(X.T, len(vocab)).shape    # 此时，vocab_size=len(vocab) = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "net = RNNModelScratch(\n",
    "    vocab_size=len(vocab), \n",
    "    num_hiddens=num_hiddens, \n",
    "    device=d2l.try_gpu(), \n",
    "    get_params=get_params,\n",
    "    init_state=init_rnn_state, \n",
    "    forward_fn=rnn\n",
    ")\n",
    "\n",
    "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "\n",
    "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.2. <a id='toc11_3_1_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape: torch.Size([5, 2, 128])\n",
      "state_new.shape: torch.Size([1, 2, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(28, 128)\n",
       "  (linear): Linear(in_features=128, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), num_classes=self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态,hiddens=(h, c)\n",
    "            return (torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, num_steps, num_hiddens = 2, 5, 128\n",
    "\n",
    "# 用PyTorch直接实现\n",
    "rnn_layer = nn.RNN(\n",
    "    input_size=len(vocab),          # 输入特征的维度, vocab_size\n",
    "    hidden_size=num_hiddens,         # 隐藏层大小\n",
    "    num_layers=1,                    # 深层神经网络，默认是1层\n",
    "    bidirectional=False,            # 双向神经网络，默认是单向\n",
    "    batch_first=False\n",
    ")\n",
    "\n",
    "# 我们(**使用张量来初始化隐状态**)，它的形状是（隐藏层数，批量大小，隐藏单元数），\n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "\n",
    "# [**通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。**]\n",
    "# 需要强调的是，`rnn_layer`的“输出”（`Y`）不涉及输出层的计算：\n",
    "# 它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。\n",
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))    # (num_steps, batch_size, vocab_size)\n",
    "\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "# Y: (num_steps, batch_size, num_hiddens)\n",
    "# state_new: (num_layers, batch_size, num_hiddens)\n",
    "print(f'Y.shape: {Y.shape}')\n",
    "print(f'state_new.shape: {state_new.shape}')\n",
    "\n",
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab)).to(device)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.3. <a id='toc11_3_1_3_'></a>[训练和预测](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.4. <a id='toc11_3_1_4_'></a>[warm-up 预热期](#toc0_)\n",
    "* 预热期：在预测之前，先输入一些字符，让模型逐渐进入状态\n",
    "* 预热期长度：num_steps\n",
    "* 预测长度：num_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    # print(f'prefix: {prefix}')\n",
    "    # print(f'prefix[0]: {prefix[0]}')\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    # print(f'outputs: {outputs}')\n",
    "    # print(f'outputs[-1]: {outputs[-1]}')\n",
    "\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape(shape=(1, 1))  # (batch_size, num_steps)\n",
    "\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        # print(f'y: {y}')\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "        # print(f'outputs: {[vocab.idx_to_token[i] for i in outputs]}')\n",
    "\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "\n",
    "\n",
    "# 测试以下\n",
    "predict_ch8('time traveller ', 50, net, vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度剪裁（Gradient Clipping）\n",
    "梯度剪裁（Gradient Clipping）是深度学习中用于控制梯度爆炸（Gradient Explosion）的一种技术。通过限制梯度的大小，梯度剪裁帮助稳定模型的训练过程，特别是在处理深层网络或复杂模型时。\n",
    "\n",
    "```python\n",
    "def train(model, optimizer, criterion, data, targets, clip_value=None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    if clip_value is not None:\n",
    "        # 基于梯度范数的剪裁\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        # 或者基于梯度值的剪裁\n",
    "        # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    " # 防止梯度爆炸\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度剪裁\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 5.2, 478202.3 词元/秒 cuda:0\n",
      "time traveller soun and her the time traveller and thisk d an th\n",
      "travellerso he hat he thithe thimenstore whon this thes sou\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.61745pt\" viewBox=\"0 0 262.1875 183.61745\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-06T17:00:51.967896</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.61745 \n",
       "L 262.1875 183.61745 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 146.0612 \n",
       "L 245.44375 146.0612 \n",
       "L 245.44375 7.4612 \n",
       "L 50.14375 7.4612 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 86.015179 146.0612 \n",
       "L 86.015179 7.4612 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mf4d0899589\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf4d0899589\" x=\"86.015179\" y=\"146.0612\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(76.471429 160.659638) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 125.872321 146.0612 \n",
       "L 125.872321 7.4612 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf4d0899589\" x=\"125.872321\" y=\"146.0612\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(116.328571 160.659638) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 165.729464 146.0612 \n",
       "L 165.729464 7.4612 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf4d0899589\" x=\"165.729464\" y=\"146.0612\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(156.185714 160.659638) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 205.586607 146.0612 \n",
       "L 205.586607 7.4612 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf4d0899589\" x=\"205.586607\" y=\"146.0612\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(196.042857 160.659638) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 245.44375 146.0612 \n",
       "L 245.44375 7.4612 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf4d0899589\" x=\"245.44375\" y=\"146.0612\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(235.9 160.659638) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.337763) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 50.14375 142.121112 \n",
       "L 245.44375 142.121112 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"mf10a972430\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf10a972430\" x=\"50.14375\" y=\"142.121112\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 5.0 -->\n",
       "      <g transform=\"translate(27.240625 145.92033) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 115.896733 \n",
       "L 245.44375 115.896733 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf10a972430\" x=\"50.14375\" y=\"115.896733\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 7.5 -->\n",
       "      <g transform=\"translate(27.240625 119.695952) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 50.14375 89.672355 \n",
       "L 245.44375 89.672355 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf10a972430\" x=\"50.14375\" y=\"89.672355\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 10.0 -->\n",
       "      <g transform=\"translate(20.878125 93.471573) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 50.14375 63.447976 \n",
       "L 245.44375 63.447976 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf10a972430\" x=\"50.14375\" y=\"63.447976\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 12.5 -->\n",
       "      <g transform=\"translate(20.878125 67.247195) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 50.14375 37.223597 \n",
       "L 245.44375 37.223597 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf10a972430\" x=\"50.14375\" y=\"37.223597\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 15.0 -->\n",
       "      <g transform=\"translate(20.878125 41.022816) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 50.14375 10.999219 \n",
       "L 245.44375 10.999219 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf10a972430\" x=\"50.14375\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 17.5 -->\n",
       "      <g transform=\"translate(20.878125 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- perplexity -->\n",
       "     <g transform=\"translate(14.798437 101.887763) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"166.113281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"229.589844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"257.373047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"317.146484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"376.326172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"404.109375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"443.318359\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 50.14375 13.7612 \n",
       "L 54.129464 18.467618 \n",
       "L 58.115179 24.884673 \n",
       "L 62.100893 34.88225 \n",
       "L 66.086607 45.536211 \n",
       "L 70.072321 55.009733 \n",
       "L 74.058036 63.370682 \n",
       "L 78.04375 69.15354 \n",
       "L 82.029464 74.916733 \n",
       "L 86.015179 79.350534 \n",
       "L 90.000893 82.773584 \n",
       "L 93.986607 85.66081 \n",
       "L 97.972321 88.890177 \n",
       "L 101.958036 90.739493 \n",
       "L 105.94375 93.061336 \n",
       "L 109.929464 94.791318 \n",
       "L 113.915179 96.915394 \n",
       "L 117.900893 98.08836 \n",
       "L 121.886607 100.140139 \n",
       "L 125.872321 101.078346 \n",
       "L 129.858036 102.087415 \n",
       "L 133.84375 103.995898 \n",
       "L 137.829464 104.64709 \n",
       "L 141.815179 106.041312 \n",
       "L 145.800893 107.002881 \n",
       "L 149.786607 109.04261 \n",
       "L 153.772321 109.701807 \n",
       "L 157.758036 110.669838 \n",
       "L 161.74375 111.572872 \n",
       "L 165.729464 112.728382 \n",
       "L 169.715179 114.328082 \n",
       "L 173.700893 115.132851 \n",
       "L 177.686607 116.978913 \n",
       "L 181.672321 117.614098 \n",
       "L 185.658036 119.159134 \n",
       "L 189.64375 120.577285 \n",
       "L 193.629464 121.090796 \n",
       "L 197.615179 123.183784 \n",
       "L 201.600893 124.342304 \n",
       "L 205.586607 125.496211 \n",
       "L 209.572321 126.427624 \n",
       "L 213.558036 128.636546 \n",
       "L 217.54375 129.114664 \n",
       "L 221.529464 130.681251 \n",
       "L 225.515179 132.346025 \n",
       "L 229.500893 133.569204 \n",
       "L 233.486607 135.277565 \n",
       "L 237.472321 136.653373 \n",
       "L 241.458036 138.434066 \n",
       "L 245.44375 139.7612 \n",
       "\" clip-path=\"url(#p7b82475789)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 146.0612 \n",
       "L 50.14375 7.4612 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 146.0612 \n",
       "L 245.44375 7.4612 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 146.0612 \n",
       "L 245.44375 146.0612 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.4612 \n",
       "L 245.44375 7.4612 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 183.16875 30.139325 \n",
       "L 238.44375 30.139325 \n",
       "Q 240.44375 30.139325 240.44375 28.139325 \n",
       "L 240.44375 14.4612 \n",
       "Q 240.44375 12.4612 238.44375 12.4612 \n",
       "L 183.16875 12.4612 \n",
       "Q 181.16875 12.4612 181.16875 14.4612 \n",
       "L 181.16875 28.139325 \n",
       "Q 181.16875 30.139325 183.16875 30.139325 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 185.16875 20.559638 \n",
       "L 195.16875 20.559638 \n",
       "L 205.16875 20.559638 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- train -->\n",
       "     <g transform=\"translate(213.16875 24.059638) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p7b82475789\">\n",
       "   <rect x=\"50.14375\" y=\"7.4612\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练\n",
    "import math \n",
    "\n",
    "\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量,\n",
    "                # 对于nn.LSTM, state是个元组, state=(h, c)\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "            \n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
    "\n",
    "\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "batch_size, num_steps = 32, 35\n",
    "# train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "num_epochs, lr = 500, 0.1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.5. <a id='toc11_3_1_5_'></a>[深层RNN](#toc0_)\n",
    "* 有多个隐藏层\n",
    "* 深层神经网络，默认是1层\n",
    "* 所以hiddens的形状是(num_layers, batch_size, num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 512]), torch.Size([2, 2, 512]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512\n",
    "\n",
    "\n",
    "rnn_layer = nn.RNN(\n",
    "    input_size=vocab_size,            # 输入特征的维度\n",
    "    hidden_size=num_hiddens,           # 隐藏层大小\n",
    "    bidirectional=False,     # 双向神经网络，默认是单向\n",
    "    num_layers=num_layers,             # 深层神经网络，默认是1层\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "\n",
    "# dir(rnn_layer)      # 查看属性\n",
    "# help(rnn_layer)   # 查看方法\n",
    "\n",
    "# 输入\n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.randn(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state \n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# new_state: (num_layers, batch_size, num_hiddens)\n",
    "y, new_state = rnn_layer(input, state)\n",
    "\n",
    "y.shape, new_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1.6. <a id='toc11_3_1_6_'></a>[双向RNN](#toc0_)\n",
    "* 双向（其实就是将输入倒过来再输入）\n",
    "* 不能用双向循环神经网络来预测未来，因为从一开始就透露未来的信息。\n",
    "* 那实际引用场景是什么？\n",
    "    * 翻译\n",
    "    * 文本句子分类\n",
    "* 双向神经网络，所以是2倍\n",
    "* 所以hiddens的形状是(num_layers*2, batch_size, num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 1024]), torch.Size([4, 2, 512]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512\n",
    "\n",
    "rnn_layer = nn.RNN(\n",
    "    input_size = vocab_size,            # 输入特征维度\n",
    "    hidden_size = num_hiddens,           # 隐藏层大小\n",
    "    bidirectional = True,     # 双向神经网络，默认是单向\n",
    "    num_layers = num_layers,             # 深层神经网络，默认是1层 \n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "# dir(rnn_layer)      # 查看属性\n",
    "# help(rnn_layer)   # 查看方法\n",
    "\n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.zeros(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state\n",
    "# 双向神经网络，所以是2倍\n",
    "# (num_layers*2, batch_size, num_hiddens)\n",
    "state = torch.zeros(size=(num_layers*2, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens*2)\n",
    "# new_state: (num_layers*2, batch_size, num_hiddens)\n",
    "y, new_state = rnn_layer(input, state)\n",
    "\n",
    "y.shape, new_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2. <a id='toc11_3_2_'></a>[GRU](#toc0_)\n",
    "* GRU实际晚于LSTM，但是作用效果相当而更容易理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.1. <a id='toc11_3_2_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2.2. <a id='toc11_3_2_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 512]), torch.Size([2, 2, 512]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512 \n",
    "\n",
    "\n",
    "gru_layer = nn.GRU(\n",
    "    input_size = vocab_size, \n",
    "    hidden_size = num_hiddens, \n",
    "    num_layers =  num_layers, \n",
    "    bidirectional = False,\n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "# input     \n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.zeros(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state \n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# new_state: (num_layers, batch_size, num_hiddens)\n",
    "y, new_state = gru_layer(input, state)\n",
    "\n",
    "y.shape, new_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3. <a id='toc11_3_3_'></a>[LSTM](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.3.1. <a id='toc11_3_3_1_'></a>[从头实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.3.2. <a id='toc11_3_3_2_'></a>[简洁实现](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 512]), torch.Size([2, 2, 512]), torch.Size([2, 2, 512]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "batch_size, num_steps, vocab_size = 2, 5, 1\n",
    "\n",
    "num_layers, num_hiddens = 2, 512\n",
    "\n",
    "\n",
    "lstm_layer = nn.LSTM(\n",
    "    input_size = vocab_size,  # 输入特征维度  \n",
    "    hidden_size = num_hiddens, \n",
    "    num_layers = num_layers, \n",
    "    bidirectional = False,\n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "\n",
    "# input\n",
    "# input: (batch_size, num_steps, vocab_size)\n",
    "input = torch.zeros(size=(batch_size, num_steps, vocab_size))\n",
    "\n",
    "# Initial state \n",
    "# hidden_state 和 cell_state \n",
    "# (num_layers, batch_size, num_hiddens)\n",
    "hidden_state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "cell_state = torch.zeros(size=(num_layers, batch_size, num_hiddens))\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# new_state: (num_layers, batch_size, num_hiddens)\n",
    "y, new_state = lstm_layer(input, (hidden_state, cell_state))\n",
    "\n",
    "y.shape, new_state[0].shape, new_state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.4. <a id='toc11_3_4_'></a>[Encoder-Decoder框架](#toc0_)\n",
    "Encoder-Decoder 架构是一种用于序列到序列（Seq2Seq）任务的常用结构，广泛应用于机器翻译、文本摘要、图像标注等任务。\n",
    "\n",
    "Encoder: 编码器将输入序列 𝑋=(𝑥1,𝑥2,...,𝑥𝑛) 转换为固定长度的上下文向量 𝐶 或一系列隐状态。\n",
    "\n",
    "Decoder: 解码器接收上下文向量 𝐶 和自身的历史输出，生成目标序列 𝑌=(𝑦1,𝑦2,...,𝑦𝑚)。\n",
    "\n",
    "一般来说，Encoder 和 Decoder 都基于 RNN、GRU、LSTM 或 Transformer。\n",
    "\n",
    "```shell\n",
    "输入-Encoder-中间状态-Decoder-输出\n",
    "                       输入\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.4.1. <a id='toc11_3_4_1_'></a>[Encoder部分](#toc0_)\n",
    "编码器（Encoder）：将输入序列转换为一个固定长度的上下文向量（或一系列上下文向量）。\n",
    "```shell\n",
    "可变长度的输入，固定长度的输出中间状态\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "#@save\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.4.2. <a id='toc11_3_4_2_'></a>[Decoder部分](#toc0_)\n",
    "解码器（Decoder）：根据上下文向量生成目标序列。\n",
    "```shell\n",
    "固定长度中间状态\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.4.3. <a id='toc11_3_4_3_'></a>[Encoder-Decoder（合并编码器和解码器）](#toc0_)\n",
    "```shell\n",
    "Encoder-Decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4. <a id='toc11_4_'></a>[seq2seq (Sequence to sequence learning)](#toc0_)\n",
    "Seq2Seq 模型最早由 Google 提出，用于机器翻译任务。其核心思想是使用两个递归神经网络（RNN）组成的架构：一个编码器（Encoder）将输入序列编码成上下文向量，另一个解码器（Decoder）根据该上下文向量生成输出序列。近年来，随着注意力机制（Attention）的引入，Seq2Seq 模型在各类序列转换任务中表现出了更强的性能。\n",
    "```shell\n",
    "基于RNN的编码器-解码器框架(Encoder-Decoder)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1. <a id='toc11_4_1_'></a>[机器翻译与数据集](#toc0_)\n",
    "机器翻译的数据集是由源语言和目标语言的文本序列对组成的，不是单个文本序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.1. <a id='toc11_4_1_1_'></a>[下载和预处理数据集](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Go.\tVa !\n",
      "1 : Hi.\tSalut !\n",
      "2 : Run!\tCours !\n",
      "3 : Run!\tCourez !\n",
      "4 : Who?\tQui ?\n",
      "5 : Wow!\tÇa alors !\n",
      "6 : Fire!\tAu feu !\n",
      "7 : Help!\tÀ l'aide !\n",
      "8 : Jump.\tSaute.\n",
      "9 : Stop!\tÇa suffit !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip', '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
    "\n",
    "#@save\n",
    "def read_data_nmt():    \n",
    "    \"\"\"载入“英语－法语”数据集\"\"\"\n",
    "    data_dir = d2l.download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "# raw_text[:100]\n",
    "for line_num, content in enumerate(raw_text.split('\\n')):\n",
    "    if line_num < 10:\n",
    "        print(f'{line_num} : {content}')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : go .\tva !\n",
      "1 : hi .\tsalut !\n",
      "2 : run !\tcours !\n",
      "3 : run !\tcourez !\n",
      "4 : who ?\tqui ?\n",
      "5 : wow !\tça alors !\n",
      "6 : fire !\tau feu !\n",
      "7 : help !\tà l'aide !\n",
      "8 : jump .\tsaute .\n",
      "9 : stop !\tça suffit !\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "下载数据集后，原始文本数据需要经过几个预处理步骤。\n",
    "例如，我们用空格代替不间断空格（non‐breaking space），\n",
    "使用小写字母替换大写字母，并在单词和标点符号之间插入空格。\n",
    "'''\n",
    "#@tab all\n",
    "#@save\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # Replace non-breaking space with space, and convert uppercase letters to\n",
    "    # lowercase ones\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # Insert space between words and punctuation marks\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "\n",
    "for line_num, content in enumerate(text.split('\\n')):\n",
    "    if line_num < 10:\n",
    "        print(f'{line_num} : {content}')\n",
    "    else:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.2. <a id='toc11_4_1_2_'></a>[词元化](#toc0_)\n",
    "在机器翻译中，我们更喜欢单词级词元化（最先进的模型可能使用更高级的词元化技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ['go', '.'], ['va', '!']\n",
      "1 : ['hi', '.'], ['salut', '!']\n",
      "2 : ['run', '!'], ['cours', '!']\n",
      "3 : ['run', '!'], ['courez', '!']\n",
      "4 : ['who', '?'], ['qui', '?']\n",
      "5 : ['wow', '!'], ['ça', 'alors', '!']\n",
      "6 : ['fire', '!'], ['au', 'feu', '!']\n",
      "7 : ['help', '!'], ['à', \"l'aide\", '!']\n",
      "8 : ['jump', '.'], ['saute', '.']\n",
      "9 : ['stop', '!'], ['ça', 'suffit', '!']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "此函数返回两个词元列表,source和target：\n",
    "source[i]是源语言（这里是英语）第i个文本序列的词元列表，\n",
    "target[i]是目标语言（这里是法语）第i个文本序列的词元列表。\n",
    "'''\n",
    "#@tab all\n",
    "#@save\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"Tokenize the English-French dataset.\"\"\"\n",
    "    \n",
    "    # 源语言和目标语言的词元列表\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "# source[:6], target[:6]\n",
    "for line_num, (src, tgt) in enumerate(zip(source, target)):\n",
    "    if line_num < 10:\n",
    "        print(f'{line_num} : {src}, {tgt}')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.3. <a id='toc11_4_1_3_'></a>[词表](#toc0_)\n",
    "为`source`和`target`分别构建词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10012\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "print(len(src_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab['unk'], src_vocab['<pad>'], src_vocab['<bos>'], src_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<pad>': 1,\n",
       " '<bos>': 2,\n",
       " '<eos>': 3,\n",
       " '.': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'to': 7,\n",
       " 'the': 8,\n",
       " '?': 9,\n",
       " 'a': 10,\n",
       " 'is': 11,\n",
       " 'tom': 12,\n",
       " 'that': 13,\n",
       " 'he': 14,\n",
       " 'do': 15,\n",
       " 'of': 16,\n",
       " 'it': 17,\n",
       " 'this': 18,\n",
       " 'in': 19,\n",
       " 'me': 20,\n",
       " 'have': 21,\n",
       " \"don't\": 22,\n",
       " ',': 23,\n",
       " 'was': 24,\n",
       " 'my': 25,\n",
       " 'are': 26,\n",
       " 'for': 27,\n",
       " 'your': 28,\n",
       " 'what': 29,\n",
       " \"i'm\": 30,\n",
       " 'we': 31,\n",
       " 'be': 32,\n",
       " 'want': 33,\n",
       " 'she': 34,\n",
       " 'not': 35,\n",
       " 'know': 36,\n",
       " 'like': 37,\n",
       " 'on': 38,\n",
       " 'with': 39,\n",
       " 'can': 40,\n",
       " 'his': 41,\n",
       " 'all': 42,\n",
       " 'did': 43,\n",
       " 'at': 44,\n",
       " \"you're\": 45,\n",
       " 'how': 46,\n",
       " 'go': 47,\n",
       " 'they': 48,\n",
       " 'him': 49,\n",
       " 'think': 50,\n",
       " 'and': 51,\n",
       " \"it's\": 52,\n",
       " 'about': 53,\n",
       " 'time': 54,\n",
       " \"can't\": 55,\n",
       " 'here': 56,\n",
       " 'very': 57,\n",
       " \"didn't\": 58,\n",
       " 'get': 59,\n",
       " 'there': 60,\n",
       " 'her': 61,\n",
       " 'were': 62,\n",
       " 'as': 63,\n",
       " 'will': 64,\n",
       " 'had': 65,\n",
       " 'if': 66,\n",
       " 'why': 67,\n",
       " 'just': 68,\n",
       " 'up': 69,\n",
       " 'out': 70,\n",
       " 'no': 71,\n",
       " 'has': 72,\n",
       " 'one': 73,\n",
       " 'going': 74,\n",
       " 'would': 75,\n",
       " 'so': 76,\n",
       " 'good': 77,\n",
       " 'need': 78,\n",
       " 'tell': 79,\n",
       " 'an': 80,\n",
       " 'see': 81,\n",
       " \"i'll\": 82,\n",
       " 'come': 83,\n",
       " 'when': 84,\n",
       " 'from': 85,\n",
       " 'by': 86,\n",
       " 'really': 87,\n",
       " 'mary': 88,\n",
       " 'help': 89,\n",
       " 'who': 90,\n",
       " 'please': 91,\n",
       " 'us': 92,\n",
       " \"that's\": 93,\n",
       " 'should': 94,\n",
       " 'could': 95,\n",
       " 'been': 96,\n",
       " \"i've\": 97,\n",
       " 'never': 98,\n",
       " 'more': 99,\n",
       " 'now': 100,\n",
       " 'where': 101,\n",
       " 'take': 102,\n",
       " 'something': 103,\n",
       " 'got': 104,\n",
       " 'too': 105,\n",
       " 'than': 106,\n",
       " 'much': 107,\n",
       " 'make': 108,\n",
       " 'some': 109,\n",
       " \"i'd\": 110,\n",
       " \"we're\": 111,\n",
       " 'right': 112,\n",
       " 'but': 113,\n",
       " 'work': 114,\n",
       " 'am': 115,\n",
       " 'money': 116,\n",
       " 'any': 117,\n",
       " 'home': 118,\n",
       " 'last': 119,\n",
       " 'thought': 120,\n",
       " 'say': 121,\n",
       " 'sure': 122,\n",
       " 'anything': 123,\n",
       " 'look': 124,\n",
       " 'back': 125,\n",
       " '!': 126,\n",
       " 'day': 127,\n",
       " \"doesn't\": 128,\n",
       " 'give': 129,\n",
       " 'car': 130,\n",
       " 'told': 131,\n",
       " 'talk': 132,\n",
       " 'people': 133,\n",
       " 'made': 134,\n",
       " 'lot': 135,\n",
       " 'let': 136,\n",
       " 'way': 137,\n",
       " 'our': 138,\n",
       " 'must': 139,\n",
       " 'many': 140,\n",
       " 'said': 141,\n",
       " \"he's\": 142,\n",
       " 'love': 143,\n",
       " 'long': 144,\n",
       " 'went': 145,\n",
       " 'still': 146,\n",
       " 'feel': 147,\n",
       " 'only': 148,\n",
       " 'eat': 149,\n",
       " 'always': 150,\n",
       " 'better': 151,\n",
       " 'happy': 152,\n",
       " 'doing': 153,\n",
       " 'today': 154,\n",
       " 'french': 155,\n",
       " 'house': 156,\n",
       " \"isn't\": 157,\n",
       " \"let's\": 158,\n",
       " 'does': 159,\n",
       " 'new': 160,\n",
       " 'believe': 161,\n",
       " 'before': 162,\n",
       " 'leave': 163,\n",
       " \"what's\": 164,\n",
       " 'book': 165,\n",
       " 'again': 166,\n",
       " 'them': 167,\n",
       " 'room': 168,\n",
       " 'job': 169,\n",
       " 'off': 170,\n",
       " 'school': 171,\n",
       " 'night': 172,\n",
       " 'little': 173,\n",
       " 'well': 174,\n",
       " \"won't\": 175,\n",
       " 'may': 176,\n",
       " 'old': 177,\n",
       " 'down': 178,\n",
       " 'wanted': 179,\n",
       " 'everything': 180,\n",
       " 'yesterday': 181,\n",
       " 'alone': 182,\n",
       " 'happened': 183,\n",
       " 'tomorrow': 184,\n",
       " 'father': 185,\n",
       " 'stay': 186,\n",
       " 'two': 187,\n",
       " 'put': 188,\n",
       " 'left': 189,\n",
       " 'over': 190,\n",
       " 'enough': 191,\n",
       " 'every': 192,\n",
       " 'asked': 193,\n",
       " 'three': 194,\n",
       " 'speak': 195,\n",
       " 'find': 196,\n",
       " 'stop': 197,\n",
       " 'these': 198,\n",
       " 'saw': 199,\n",
       " 'man': 200,\n",
       " 'into': 201,\n",
       " 'done': 202,\n",
       " 'try': 203,\n",
       " 'understand': 204,\n",
       " 'ask': 205,\n",
       " 'or': 206,\n",
       " 'ever': 207,\n",
       " 'keep': 208,\n",
       " 'friends': 209,\n",
       " 'problem': 210,\n",
       " 'sorry': 211,\n",
       " 'next': 212,\n",
       " 'nothing': 213,\n",
       " \"there's\": 214,\n",
       " 'dog': 215,\n",
       " 'after': 216,\n",
       " 'call': 217,\n",
       " 'buy': 218,\n",
       " 'hard': 219,\n",
       " \"you've\": 220,\n",
       " 'hope': 221,\n",
       " 'busy': 222,\n",
       " 'read': 223,\n",
       " 'away': 224,\n",
       " 'live': 225,\n",
       " 'friend': 226,\n",
       " 'wrong': 227,\n",
       " 'late': 228,\n",
       " 'first': 229,\n",
       " 'things': 230,\n",
       " 'door': 231,\n",
       " 'hear': 232,\n",
       " \"tom's\": 233,\n",
       " 'life': 234,\n",
       " \"they're\": 235,\n",
       " 'thing': 236,\n",
       " 'other': 237,\n",
       " 'remember': 238,\n",
       " 'idea': 239,\n",
       " \"wasn't\": 240,\n",
       " 'boston': 241,\n",
       " 'anyone': 242,\n",
       " 'mother': 243,\n",
       " 'years': 244,\n",
       " 'took': 245,\n",
       " 'gave': 246,\n",
       " 'without': 247,\n",
       " 'being': 248,\n",
       " 'their': 249,\n",
       " 'everyone': 250,\n",
       " \"couldn't\": 251,\n",
       " 'mind': 252,\n",
       " 'came': 253,\n",
       " 'children': 254,\n",
       " 'yet': 255,\n",
       " 'knew': 256,\n",
       " 'already': 257,\n",
       " \"you'd\": 258,\n",
       " 'used': 259,\n",
       " 'name': 260,\n",
       " 'kind': 261,\n",
       " 'drink': 262,\n",
       " 'tired': 263,\n",
       " 'looking': 264,\n",
       " 'morning': 265,\n",
       " 'heard': 266,\n",
       " 'seen': 267,\n",
       " 'best': 268,\n",
       " 'bad': 269,\n",
       " \"you'll\": 270,\n",
       " 'lost': 271,\n",
       " 'teacher': 272,\n",
       " 'found': 273,\n",
       " 'even': 274,\n",
       " 'play': 275,\n",
       " 'water': 276,\n",
       " \"haven't\": 277,\n",
       " 'same': 278,\n",
       " 'care': 279,\n",
       " 'often': 280,\n",
       " 'week': 281,\n",
       " 'english': 282,\n",
       " \"aren't\": 283,\n",
       " 'use': 284,\n",
       " 'soon': 285,\n",
       " 'wait': 286,\n",
       " 'afraid': 287,\n",
       " 'ready': 288,\n",
       " 'wish': 289,\n",
       " 'answer': 290,\n",
       " 'big': 291,\n",
       " 'yourself': 292,\n",
       " 'bed': 293,\n",
       " 'party': 294,\n",
       " 'someone': 295,\n",
       " 'while': 296,\n",
       " 'few': 297,\n",
       " 'happen': 298,\n",
       " 'talking': 299,\n",
       " 'else': 300,\n",
       " 'parents': 301,\n",
       " 'wants': 302,\n",
       " 'cold': 303,\n",
       " 'train': 304,\n",
       " 'myself': 305,\n",
       " 'open': 306,\n",
       " 'around': 307,\n",
       " 'show': 308,\n",
       " 'might': 309,\n",
       " 'bought': 310,\n",
       " 'nice': 311,\n",
       " 'glad': 312,\n",
       " 'both': 313,\n",
       " 'getting': 314,\n",
       " 'married': 315,\n",
       " 'another': 316,\n",
       " \"we'll\": 317,\n",
       " 'place': 318,\n",
       " 'watch': 319,\n",
       " 'great': 320,\n",
       " 'turn': 321,\n",
       " 'year': 322,\n",
       " 'true': 323,\n",
       " 'looks': 324,\n",
       " 'early': 325,\n",
       " 'because': 326,\n",
       " 'such': 327,\n",
       " 'knows': 328,\n",
       " 'beautiful': 329,\n",
       " 'sleep': 330,\n",
       " 'write': 331,\n",
       " 'plan': 332,\n",
       " 'hurt': 333,\n",
       " \"wouldn't\": 334,\n",
       " 'almost': 335,\n",
       " 'able': 336,\n",
       " \"she's\": 337,\n",
       " 'those': 338,\n",
       " 'walk': 339,\n",
       " 'once': 340,\n",
       " 'which': 341,\n",
       " 'tried': 342,\n",
       " 'pay': 343,\n",
       " 'matter': 344,\n",
       " 'question': 345,\n",
       " 'food': 346,\n",
       " 'fun': 347,\n",
       " 'meet': 348,\n",
       " 'dinner': 349,\n",
       " 'days': 350,\n",
       " 'bus': 351,\n",
       " 'coffee': 352,\n",
       " 'brother': 353,\n",
       " 'letter': 354,\n",
       " 'truth': 355,\n",
       " 'met': 356,\n",
       " 'coming': 357,\n",
       " 'anymore': 358,\n",
       " 'everybody': 359,\n",
       " 'young': 360,\n",
       " 'meeting': 361,\n",
       " 'most': 362,\n",
       " 'person': 363,\n",
       " 'mine': 364,\n",
       " 'books': 365,\n",
       " \"shouldn't\": 366,\n",
       " 'careful': 367,\n",
       " 'sister': 368,\n",
       " 'each': 369,\n",
       " 'own': 370,\n",
       " 'family': 371,\n",
       " 'together': 372,\n",
       " 'hate': 373,\n",
       " 'felt': 374,\n",
       " 'seems': 375,\n",
       " 'tonight': 376,\n",
       " 'doctor': 377,\n",
       " 'change': 378,\n",
       " 'learn': 379,\n",
       " 'seem': 380,\n",
       " 'since': 381,\n",
       " 'likes': 382,\n",
       " 'study': 383,\n",
       " 'word': 384,\n",
       " 'waiting': 385,\n",
       " 'forget': 386,\n",
       " 'easy': 387,\n",
       " 'pretty': 388,\n",
       " 'died': 389,\n",
       " 'trying': 390,\n",
       " 'girl': 391,\n",
       " 'phone': 392,\n",
       " 'world': 393,\n",
       " 'far': 394,\n",
       " 'gone': 395,\n",
       " 'working': 396,\n",
       " \"we've\": 397,\n",
       " 'hand': 398,\n",
       " 'started': 399,\n",
       " 'child': 400,\n",
       " 'accident': 401,\n",
       " 'station': 402,\n",
       " 'mean': 403,\n",
       " 'nobody': 404,\n",
       " 'finished': 405,\n",
       " 'longer': 406,\n",
       " 'difficult': 407,\n",
       " 'sick': 408,\n",
       " 'boy': 409,\n",
       " 'surprised': 410,\n",
       " 'important': 411,\n",
       " 'start': 412,\n",
       " 'rain': 413,\n",
       " 'looked': 414,\n",
       " 'quite': 415,\n",
       " 'lunch': 416,\n",
       " 'cat': 417,\n",
       " 'drive': 418,\n",
       " 'wife': 419,\n",
       " 'questions': 420,\n",
       " 'weather': 421,\n",
       " 'anybody': 422,\n",
       " 'reading': 423,\n",
       " 'movie': 424,\n",
       " 'having': 425,\n",
       " 'yours': 426,\n",
       " 'makes': 427,\n",
       " 'mistake': 428,\n",
       " 'supposed': 429,\n",
       " 'thank': 430,\n",
       " 'office': 431,\n",
       " 'story': 432,\n",
       " 'until': 433,\n",
       " 'ten': 434,\n",
       " 'run': 435,\n",
       " 'swim': 436,\n",
       " 'small': 437,\n",
       " 'tv': 438,\n",
       " 'times': 439,\n",
       " 'close': 440,\n",
       " 'himself': 441,\n",
       " 'bit': 442,\n",
       " 'playing': 443,\n",
       " 'spend': 444,\n",
       " 'angry': 445,\n",
       " 'eyes': 446,\n",
       " 'trust': 447,\n",
       " 'stupid': 448,\n",
       " 'called': 449,\n",
       " 'ago': 450,\n",
       " 'guess': 451,\n",
       " 'advice': 452,\n",
       " 'japan': 453,\n",
       " 'hurry': 454,\n",
       " 'picture': 455,\n",
       " 'hours': 456,\n",
       " '.\"': 457,\n",
       " 'broke': 458,\n",
       " 'music': 459,\n",
       " 'exactly': 460,\n",
       " 'says': 461,\n",
       " 'caught': 462,\n",
       " 'students': 463,\n",
       " 'wonder': 464,\n",
       " 'son': 465,\n",
       " 'lives': 466,\n",
       " 'fire': 467,\n",
       " 'afternoon': 468,\n",
       " 'window': 469,\n",
       " 'eating': 470,\n",
       " 'turned': 471,\n",
       " 'police': 472,\n",
       " 'bicycle': 473,\n",
       " 'thinking': 474,\n",
       " 'fell': 475,\n",
       " 'ran': 476,\n",
       " 'decided': 477,\n",
       " 'table': 478,\n",
       " 'fast': 479,\n",
       " 'usually': 480,\n",
       " 'probably': 481,\n",
       " \"who's\": 482,\n",
       " 'minutes': 483,\n",
       " 'japanese': 484,\n",
       " 'interested': 485,\n",
       " 'interesting': 486,\n",
       " 'trouble': 487,\n",
       " 'free': 488,\n",
       " 'hair': 489,\n",
       " 'arrived': 490,\n",
       " 'hot': 491,\n",
       " 'bring': 492,\n",
       " \"weren't\": 493,\n",
       " 'light': 494,\n",
       " 'town': 495,\n",
       " 'homework': 496,\n",
       " 'advised': 497,\n",
       " 'worry': 498,\n",
       " 'number': 499,\n",
       " 'park': 500,\n",
       " 'game': 501,\n",
       " 'under': 502,\n",
       " 'safe': 503,\n",
       " 'listen': 504,\n",
       " 'forgot': 505,\n",
       " 'stand': 506,\n",
       " 'enjoy': 507,\n",
       " 'country': 508,\n",
       " 'living': 509,\n",
       " 'news': 510,\n",
       " 'through': 511,\n",
       " 'against': 512,\n",
       " 'appreciate': 513,\n",
       " 'agree': 514,\n",
       " 'age': 515,\n",
       " 'miss': 516,\n",
       " 'sit': 517,\n",
       " 'hands': 518,\n",
       " 'quit': 519,\n",
       " 'promise': 520,\n",
       " 'proud': 521,\n",
       " 'sing': 522,\n",
       " 'chance': 523,\n",
       " 'saying': 524,\n",
       " 'possible': 525,\n",
       " 'visit': 526,\n",
       " 'finish': 527,\n",
       " 'different': 528,\n",
       " 'maybe': 529,\n",
       " 'later': 530,\n",
       " 'high': 531,\n",
       " 'reason': 532,\n",
       " 'wine': 533,\n",
       " 'then': 534,\n",
       " 'outside': 535,\n",
       " 'summer': 536,\n",
       " 'kept': 537,\n",
       " 'taking': 538,\n",
       " 'catch': 539,\n",
       " 'hungry': 540,\n",
       " 'needs': 541,\n",
       " 'born': 542,\n",
       " 'lie': 543,\n",
       " 'making': 544,\n",
       " \"should've\": 545,\n",
       " 'cut': 546,\n",
       " 'business': 547,\n",
       " 'moment': 548,\n",
       " 'trip': 549,\n",
       " 'favorite': 550,\n",
       " 'dead': 551,\n",
       " 'end': 552,\n",
       " 'ok': 553,\n",
       " 'shoes': 554,\n",
       " 'older': 555,\n",
       " 'become': 556,\n",
       " 'win': 557,\n",
       " 'tree': 558,\n",
       " 'behind': 559,\n",
       " 'five': 560,\n",
       " 'box': 561,\n",
       " 'near': 562,\n",
       " 'works': 563,\n",
       " 'red': 564,\n",
       " 'girlfriend': 565,\n",
       " 'breakfast': 566,\n",
       " 'die': 567,\n",
       " 'class': 568,\n",
       " 'song': 569,\n",
       " 'tea': 570,\n",
       " 'eaten': 571,\n",
       " 'city': 572,\n",
       " 'dress': 573,\n",
       " 'student': 574,\n",
       " 'baby': 575,\n",
       " \"mary's\": 576,\n",
       " 'rich': 577,\n",
       " 'needed': 578,\n",
       " 'guy': 579,\n",
       " 'face': 580,\n",
       " 'funny': 581,\n",
       " 'secret': 582,\n",
       " 'team': 583,\n",
       " 'month': 584,\n",
       " 'company': 585,\n",
       " 'full': 586,\n",
       " 'quickly': 587,\n",
       " 'comes': 588,\n",
       " 'paid': 589,\n",
       " 'stayed': 590,\n",
       " \"where's\": 591,\n",
       " 'crazy': 592,\n",
       " 'fish': 593,\n",
       " 'rest': 594,\n",
       " 'ate': 595,\n",
       " 'lose': 596,\n",
       " 'woman': 597,\n",
       " 'point': 598,\n",
       " 'watching': 599,\n",
       " 'tennis': 600,\n",
       " 'beer': 601,\n",
       " 'explain': 602,\n",
       " 'part': 603,\n",
       " 'invited': 604,\n",
       " 'serious': 605,\n",
       " 'cannot': 606,\n",
       " 'break': 607,\n",
       " 'large': 608,\n",
       " 'clothes': 609,\n",
       " 'daughter': 610,\n",
       " 'smoking': 611,\n",
       " 'hotel': 612,\n",
       " 'kids': 613,\n",
       " 'key': 614,\n",
       " 'choice': 615,\n",
       " 'asleep': 616,\n",
       " 'hat': 617,\n",
       " 'feeling': 618,\n",
       " 'sound': 619,\n",
       " 'death': 620,\n",
       " 'cost': 621,\n",
       " 'somebody': 622,\n",
       " 'clean': 623,\n",
       " 'australia': 624,\n",
       " 'spent': 625,\n",
       " 'worried': 626,\n",
       " 'began': 627,\n",
       " 'words': 628,\n",
       " 'real': 629,\n",
       " 'lived': 630,\n",
       " 'wearing': 631,\n",
       " '?\"': 632,\n",
       " 'studying': 633,\n",
       " 'handle': 634,\n",
       " 'expensive': 635,\n",
       " 'sometimes': 636,\n",
       " 'goes': 637,\n",
       " 'hour': 638,\n",
       " 'became': 639,\n",
       " 'street': 640,\n",
       " 'touch': 641,\n",
       " 'hit': 642,\n",
       " 'milk': 643,\n",
       " 'river': 644,\n",
       " 'killed': 645,\n",
       " 'store': 646,\n",
       " 'hospital': 647,\n",
       " 'changed': 648,\n",
       " 'short': 649,\n",
       " 'rather': 650,\n",
       " 'decision': 651,\n",
       " 'computer': 652,\n",
       " 'others': 653,\n",
       " 'telling': 654,\n",
       " 'drunk': 655,\n",
       " 'deal': 656,\n",
       " 'between': 657,\n",
       " 'paper': 658,\n",
       " 'hold': 659,\n",
       " 'lucky': 660,\n",
       " 'cake': 661,\n",
       " 'scared': 662,\n",
       " 'takes': 663,\n",
       " 'birthday': 664,\n",
       " 'snow': 665,\n",
       " 'language': 666,\n",
       " 'closed': 667,\n",
       " 'present': 668,\n",
       " 'helped': 669,\n",
       " 'dark': 670,\n",
       " 'minute': 671,\n",
       " 'stopped': 672,\n",
       " 'problems': 673,\n",
       " 'restaurant': 674,\n",
       " 'sat': 675,\n",
       " 'monday': 676,\n",
       " 'speaking': 677,\n",
       " 'expect': 678,\n",
       " 'front': 679,\n",
       " 'whole': 680,\n",
       " 'quiet': 681,\n",
       " 'war': 682,\n",
       " 'mistakes': 683,\n",
       " 'figured': 684,\n",
       " 'worked': 685,\n",
       " 'finally': 686,\n",
       " 'gets': 687,\n",
       " 'along': 688,\n",
       " 'head': 689,\n",
       " 'report': 690,\n",
       " 'wrote': 691,\n",
       " 'happening': 692,\n",
       " 'dogs': 693,\n",
       " 'coat': 694,\n",
       " 'sense': 695,\n",
       " 'cup': 696,\n",
       " 'talked': 697,\n",
       " 'liked': 698,\n",
       " 'strong': 699,\n",
       " 'upset': 700,\n",
       " 'kill': 701,\n",
       " 'speaks': 702,\n",
       " 'thanks': 703,\n",
       " 'missed': 704,\n",
       " 'forward': 705,\n",
       " 'strange': 706,\n",
       " 'expected': 707,\n",
       " 'check': 708,\n",
       " 'boss': 709,\n",
       " 'dream': 710,\n",
       " 'dangerous': 711,\n",
       " 'beach': 712,\n",
       " 'known': 713,\n",
       " 'health': 714,\n",
       " 'situation': 715,\n",
       " 'actually': 716,\n",
       " 'running': 717,\n",
       " 'brought': 718,\n",
       " 'whether': 719,\n",
       " 'six': 720,\n",
       " 'its': 721,\n",
       " 'whatever': 722,\n",
       " 'dictionary': 723,\n",
       " 'move': 724,\n",
       " 'seeing': 725,\n",
       " 'shut': 726,\n",
       " 'order': 727,\n",
       " 'swimming': 728,\n",
       " 'weekend': 729,\n",
       " 'tall': 730,\n",
       " 'men': 731,\n",
       " 'air': 732,\n",
       " 'evening': 733,\n",
       " 'walked': 734,\n",
       " 'plane': 735,\n",
       " 'allowed': 736,\n",
       " 'thirty': 737,\n",
       " 'loves': 738,\n",
       " 'case': 739,\n",
       " 'least': 740,\n",
       " 'building': 741,\n",
       " 'broken': 742,\n",
       " 'list': 743,\n",
       " 'worth': 744,\n",
       " 'happens': 745,\n",
       " 'heart': 746,\n",
       " 'disappointed': 747,\n",
       " 'follow': 748,\n",
       " 'famous': 749,\n",
       " 'prefer': 750,\n",
       " 'written': 751,\n",
       " 'smoke': 752,\n",
       " 'christmas': 753,\n",
       " 'perfect': 754,\n",
       " 'drinking': 755,\n",
       " \"o'clock\": 756,\n",
       " 'luck': 757,\n",
       " 'choose': 758,\n",
       " 'completely': 759,\n",
       " 'dollars': 760,\n",
       " 'pain': 761,\n",
       " 'future': 762,\n",
       " 'either': 763,\n",
       " 'mad': 764,\n",
       " 'seat': 765,\n",
       " 'crying': 766,\n",
       " 'dance': 767,\n",
       " 'second': 768,\n",
       " 'piano': 769,\n",
       " 'offer': 770,\n",
       " 'necessary': 771,\n",
       " 'fine': 772,\n",
       " 'half': 773,\n",
       " 'sent': 774,\n",
       " 'sunday': 775,\n",
       " \"hasn't\": 776,\n",
       " 'garden': 777,\n",
       " 'apologize': 778,\n",
       " 'rules': 779,\n",
       " 'road': 780,\n",
       " 'library': 781,\n",
       " 'leaving': 782,\n",
       " 'noise': 783,\n",
       " 'weight': 784,\n",
       " 'flowers': 785,\n",
       " 'wear': 786,\n",
       " 'opinion': 787,\n",
       " 'cook': 788,\n",
       " 'writing': 789,\n",
       " 'camera': 790,\n",
       " 'set': 791,\n",
       " 'taken': 792,\n",
       " 'glass': 793,\n",
       " 'learned': 794,\n",
       " 'address': 795,\n",
       " 'somewhere': 796,\n",
       " 'poor': 797,\n",
       " 'several': 798,\n",
       " \"it'll\": 799,\n",
       " 'white': 800,\n",
       " 'danger': 801,\n",
       " 'attention': 802,\n",
       " \"he'll\": 803,\n",
       " 'tokyo': 804,\n",
       " 'floor': 805,\n",
       " 'save': 806,\n",
       " 'alive': 807,\n",
       " 'accept': 808,\n",
       " 'information': 809,\n",
       " 'clear': 810,\n",
       " 'suppose': 811,\n",
       " 'opened': 812,\n",
       " 'during': 813,\n",
       " 'kiss': 814,\n",
       " 'loved': 815,\n",
       " 'nervous': 816,\n",
       " 'grow': 817,\n",
       " 'girls': 818,\n",
       " 'waste': 819,\n",
       " 'showed': 820,\n",
       " 'join': 821,\n",
       " 'women': 822,\n",
       " 'bag': 823,\n",
       " 'sleeping': 824,\n",
       " 'listening': 825,\n",
       " 'less': 826,\n",
       " 'solve': 827,\n",
       " 'sad': 828,\n",
       " 'won': 829,\n",
       " 'date': 830,\n",
       " 'shopping': 831,\n",
       " 'blame': 832,\n",
       " 'desk': 833,\n",
       " 'tie': 834,\n",
       " 'medicine': 835,\n",
       " 'vacation': 836,\n",
       " 'pass': 837,\n",
       " 'college': 838,\n",
       " 'side': 839,\n",
       " 'success': 840,\n",
       " 'teach': 841,\n",
       " 'keys': 842,\n",
       " 'arrive': 843,\n",
       " 'umbrella': 844,\n",
       " 'fix': 845,\n",
       " 'sounds': 846,\n",
       " 'ticket': 847,\n",
       " 'whose': 848,\n",
       " 'ship': 849,\n",
       " 'radio': 850,\n",
       " 'lying': 851,\n",
       " 'spoke': 852,\n",
       " 'smart': 853,\n",
       " 'means': 854,\n",
       " 'lawyer': 855,\n",
       " 'thinks': 856,\n",
       " 'telephone': 857,\n",
       " 'abroad': 858,\n",
       " 'promised': 859,\n",
       " 'glasses': 860,\n",
       " 'willing': 861,\n",
       " 'wake': 862,\n",
       " 'shot': 863,\n",
       " 'black': 864,\n",
       " 'owe': 865,\n",
       " 'husband': 866,\n",
       " 'speech': 867,\n",
       " 'cats': 868,\n",
       " 'plans': 869,\n",
       " 'horse': 870,\n",
       " 'borrow': 871,\n",
       " 'driving': 872,\n",
       " 'staying': 873,\n",
       " 'apple': 874,\n",
       " 'message': 875,\n",
       " 'immediately': 876,\n",
       " 'guitar': 877,\n",
       " 'piece': 878,\n",
       " 'discuss': 879,\n",
       " 'across': 880,\n",
       " 'excuse': 881,\n",
       " 'dressed': 882,\n",
       " 'uncle': 883,\n",
       " 'pictures': 884,\n",
       " 'given': 885,\n",
       " 'fat': 886,\n",
       " 'guys': 887,\n",
       " 'waited': 888,\n",
       " 'prepared': 889,\n",
       " 'fight': 890,\n",
       " 'bank': 891,\n",
       " 'asking': 892,\n",
       " 'involved': 893,\n",
       " 'american': 894,\n",
       " 'lend': 895,\n",
       " 'carefully': 896,\n",
       " 'harder': 897,\n",
       " 'none': 898,\n",
       " 'boyfriend': 899,\n",
       " 'concert': 900,\n",
       " 'sign': 901,\n",
       " 'boys': 902,\n",
       " 'ride': 903,\n",
       " 'traffic': 904,\n",
       " 'cry': 905,\n",
       " 'certain': 906,\n",
       " 'gun': 907,\n",
       " 'shower': 908,\n",
       " 'afford': 909,\n",
       " 'cute': 910,\n",
       " 'joke': 911,\n",
       " 'price': 912,\n",
       " 'couple': 913,\n",
       " 'inside': 914,\n",
       " 'easily': 915,\n",
       " 'arm': 916,\n",
       " 'lake': 917,\n",
       " 'novel': 918,\n",
       " 'shirt': 919,\n",
       " 'storm': 920,\n",
       " 'painting': 921,\n",
       " 'agreed': 922,\n",
       " 'send': 923,\n",
       " 'satisfied': 924,\n",
       " 'worse': 925,\n",
       " 'regret': 926,\n",
       " 'america': 927,\n",
       " 'fired': 928,\n",
       " 'begin': 929,\n",
       " 'kid': 930,\n",
       " 'twice': 931,\n",
       " 'raining': 932,\n",
       " 'succeed': 933,\n",
       " 'travel': 934,\n",
       " 'fault': 935,\n",
       " \"how's\": 936,\n",
       " 'count': 937,\n",
       " 'correct': 938,\n",
       " 'act': 939,\n",
       " \"they'll\": 940,\n",
       " 'passed': 941,\n",
       " 'chair': 942,\n",
       " \"we'd\": 943,\n",
       " 'difference': 944,\n",
       " 'ice': 945,\n",
       " 'test': 946,\n",
       " 'empty': 947,\n",
       " 'honest': 948,\n",
       " 'laugh': 949,\n",
       " 'meat': 950,\n",
       " 'movies': 951,\n",
       " 'television': 952,\n",
       " 'four': 953,\n",
       " 'pen': 954,\n",
       " 'earlier': 955,\n",
       " 'hardly': 956,\n",
       " 'impossible': 957,\n",
       " 'held': 958,\n",
       " 'color': 959,\n",
       " 'figure': 960,\n",
       " 'ahead': 961,\n",
       " 'wonderful': 962,\n",
       " 'blue': 963,\n",
       " 'enjoyed': 964,\n",
       " 'winter': 965,\n",
       " 'likely': 966,\n",
       " 'machine': 967,\n",
       " 'carry': 968,\n",
       " 'warm': 969,\n",
       " 'refused': 970,\n",
       " 'fall': 971,\n",
       " 'line': 972,\n",
       " 'bill': 973,\n",
       " 'played': 974,\n",
       " 'decide': 975,\n",
       " 'kissed': 976,\n",
       " 'terrible': 977,\n",
       " 'animals': 978,\n",
       " 'younger': 979,\n",
       " 'herself': 980,\n",
       " 'mountain': 981,\n",
       " 'bother': 982,\n",
       " 'laughed': 983,\n",
       " 'slept': 984,\n",
       " 'walking': 985,\n",
       " 'ideas': 986,\n",
       " 'past': 987,\n",
       " 'heavy': 988,\n",
       " 'shop': 989,\n",
       " 'anywhere': 990,\n",
       " 'foreign': 991,\n",
       " 'sooner': 992,\n",
       " 'liar': 993,\n",
       " 'cooking': 994,\n",
       " 'injured': 995,\n",
       " 'leaves': 996,\n",
       " 'newspaper': 997,\n",
       " 'pick': 998,\n",
       " 'smell': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.token_to_idx  # dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<bos>',\n",
       " '<eos>',\n",
       " '.',\n",
       " 'i',\n",
       " 'you',\n",
       " 'to',\n",
       " 'the',\n",
       " '?',\n",
       " 'a',\n",
       " 'is',\n",
       " 'tom',\n",
       " 'that',\n",
       " 'he',\n",
       " 'do',\n",
       " 'of',\n",
       " 'it',\n",
       " 'this',\n",
       " 'in',\n",
       " 'me',\n",
       " 'have',\n",
       " \"don't\",\n",
       " ',',\n",
       " 'was',\n",
       " 'my',\n",
       " 'are',\n",
       " 'for',\n",
       " 'your',\n",
       " 'what',\n",
       " \"i'm\",\n",
       " 'we',\n",
       " 'be',\n",
       " 'want',\n",
       " 'she',\n",
       " 'not',\n",
       " 'know',\n",
       " 'like',\n",
       " 'on',\n",
       " 'with',\n",
       " 'can',\n",
       " 'his',\n",
       " 'all',\n",
       " 'did',\n",
       " 'at',\n",
       " \"you're\",\n",
       " 'how',\n",
       " 'go',\n",
       " 'they',\n",
       " 'him',\n",
       " 'think',\n",
       " 'and',\n",
       " \"it's\",\n",
       " 'about',\n",
       " 'time',\n",
       " \"can't\",\n",
       " 'here',\n",
       " 'very',\n",
       " \"didn't\",\n",
       " 'get',\n",
       " 'there',\n",
       " 'her',\n",
       " 'were',\n",
       " 'as',\n",
       " 'will',\n",
       " 'had',\n",
       " 'if',\n",
       " 'why',\n",
       " 'just',\n",
       " 'up',\n",
       " 'out',\n",
       " 'no',\n",
       " 'has',\n",
       " 'one',\n",
       " 'going',\n",
       " 'would',\n",
       " 'so',\n",
       " 'good',\n",
       " 'need',\n",
       " 'tell',\n",
       " 'an',\n",
       " 'see',\n",
       " \"i'll\",\n",
       " 'come',\n",
       " 'when',\n",
       " 'from',\n",
       " 'by',\n",
       " 'really',\n",
       " 'mary',\n",
       " 'help',\n",
       " 'who',\n",
       " 'please',\n",
       " 'us',\n",
       " \"that's\",\n",
       " 'should',\n",
       " 'could',\n",
       " 'been',\n",
       " \"i've\",\n",
       " 'never',\n",
       " 'more',\n",
       " 'now',\n",
       " 'where',\n",
       " 'take',\n",
       " 'something',\n",
       " 'got',\n",
       " 'too',\n",
       " 'than',\n",
       " 'much',\n",
       " 'make',\n",
       " 'some',\n",
       " \"i'd\",\n",
       " \"we're\",\n",
       " 'right',\n",
       " 'but',\n",
       " 'work',\n",
       " 'am',\n",
       " 'money',\n",
       " 'any',\n",
       " 'home',\n",
       " 'last',\n",
       " 'thought',\n",
       " 'say',\n",
       " 'sure',\n",
       " 'anything',\n",
       " 'look',\n",
       " 'back',\n",
       " '!',\n",
       " 'day',\n",
       " \"doesn't\",\n",
       " 'give',\n",
       " 'car',\n",
       " 'told',\n",
       " 'talk',\n",
       " 'people',\n",
       " 'made',\n",
       " 'lot',\n",
       " 'let',\n",
       " 'way',\n",
       " 'our',\n",
       " 'must',\n",
       " 'many',\n",
       " 'said',\n",
       " \"he's\",\n",
       " 'love',\n",
       " 'long',\n",
       " 'went',\n",
       " 'still',\n",
       " 'feel',\n",
       " 'only',\n",
       " 'eat',\n",
       " 'always',\n",
       " 'better',\n",
       " 'happy',\n",
       " 'doing',\n",
       " 'today',\n",
       " 'french',\n",
       " 'house',\n",
       " \"isn't\",\n",
       " \"let's\",\n",
       " 'does',\n",
       " 'new',\n",
       " 'believe',\n",
       " 'before',\n",
       " 'leave',\n",
       " \"what's\",\n",
       " 'book',\n",
       " 'again',\n",
       " 'them',\n",
       " 'room',\n",
       " 'job',\n",
       " 'off',\n",
       " 'school',\n",
       " 'night',\n",
       " 'little',\n",
       " 'well',\n",
       " \"won't\",\n",
       " 'may',\n",
       " 'old',\n",
       " 'down',\n",
       " 'wanted',\n",
       " 'everything',\n",
       " 'yesterday',\n",
       " 'alone',\n",
       " 'happened',\n",
       " 'tomorrow',\n",
       " 'father',\n",
       " 'stay',\n",
       " 'two',\n",
       " 'put',\n",
       " 'left',\n",
       " 'over',\n",
       " 'enough',\n",
       " 'every',\n",
       " 'asked',\n",
       " 'three',\n",
       " 'speak',\n",
       " 'find',\n",
       " 'stop',\n",
       " 'these',\n",
       " 'saw',\n",
       " 'man',\n",
       " 'into',\n",
       " 'done',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'ask',\n",
       " 'or',\n",
       " 'ever',\n",
       " 'keep',\n",
       " 'friends',\n",
       " 'problem',\n",
       " 'sorry',\n",
       " 'next',\n",
       " 'nothing',\n",
       " \"there's\",\n",
       " 'dog',\n",
       " 'after',\n",
       " 'call',\n",
       " 'buy',\n",
       " 'hard',\n",
       " \"you've\",\n",
       " 'hope',\n",
       " 'busy',\n",
       " 'read',\n",
       " 'away',\n",
       " 'live',\n",
       " 'friend',\n",
       " 'wrong',\n",
       " 'late',\n",
       " 'first',\n",
       " 'things',\n",
       " 'door',\n",
       " 'hear',\n",
       " \"tom's\",\n",
       " 'life',\n",
       " \"they're\",\n",
       " 'thing',\n",
       " 'other',\n",
       " 'remember',\n",
       " 'idea',\n",
       " \"wasn't\",\n",
       " 'boston',\n",
       " 'anyone',\n",
       " 'mother',\n",
       " 'years',\n",
       " 'took',\n",
       " 'gave',\n",
       " 'without',\n",
       " 'being',\n",
       " 'their',\n",
       " 'everyone',\n",
       " \"couldn't\",\n",
       " 'mind',\n",
       " 'came',\n",
       " 'children',\n",
       " 'yet',\n",
       " 'knew',\n",
       " 'already',\n",
       " \"you'd\",\n",
       " 'used',\n",
       " 'name',\n",
       " 'kind',\n",
       " 'drink',\n",
       " 'tired',\n",
       " 'looking',\n",
       " 'morning',\n",
       " 'heard',\n",
       " 'seen',\n",
       " 'best',\n",
       " 'bad',\n",
       " \"you'll\",\n",
       " 'lost',\n",
       " 'teacher',\n",
       " 'found',\n",
       " 'even',\n",
       " 'play',\n",
       " 'water',\n",
       " \"haven't\",\n",
       " 'same',\n",
       " 'care',\n",
       " 'often',\n",
       " 'week',\n",
       " 'english',\n",
       " \"aren't\",\n",
       " 'use',\n",
       " 'soon',\n",
       " 'wait',\n",
       " 'afraid',\n",
       " 'ready',\n",
       " 'wish',\n",
       " 'answer',\n",
       " 'big',\n",
       " 'yourself',\n",
       " 'bed',\n",
       " 'party',\n",
       " 'someone',\n",
       " 'while',\n",
       " 'few',\n",
       " 'happen',\n",
       " 'talking',\n",
       " 'else',\n",
       " 'parents',\n",
       " 'wants',\n",
       " 'cold',\n",
       " 'train',\n",
       " 'myself',\n",
       " 'open',\n",
       " 'around',\n",
       " 'show',\n",
       " 'might',\n",
       " 'bought',\n",
       " 'nice',\n",
       " 'glad',\n",
       " 'both',\n",
       " 'getting',\n",
       " 'married',\n",
       " 'another',\n",
       " \"we'll\",\n",
       " 'place',\n",
       " 'watch',\n",
       " 'great',\n",
       " 'turn',\n",
       " 'year',\n",
       " 'true',\n",
       " 'looks',\n",
       " 'early',\n",
       " 'because',\n",
       " 'such',\n",
       " 'knows',\n",
       " 'beautiful',\n",
       " 'sleep',\n",
       " 'write',\n",
       " 'plan',\n",
       " 'hurt',\n",
       " \"wouldn't\",\n",
       " 'almost',\n",
       " 'able',\n",
       " \"she's\",\n",
       " 'those',\n",
       " 'walk',\n",
       " 'once',\n",
       " 'which',\n",
       " 'tried',\n",
       " 'pay',\n",
       " 'matter',\n",
       " 'question',\n",
       " 'food',\n",
       " 'fun',\n",
       " 'meet',\n",
       " 'dinner',\n",
       " 'days',\n",
       " 'bus',\n",
       " 'coffee',\n",
       " 'brother',\n",
       " 'letter',\n",
       " 'truth',\n",
       " 'met',\n",
       " 'coming',\n",
       " 'anymore',\n",
       " 'everybody',\n",
       " 'young',\n",
       " 'meeting',\n",
       " 'most',\n",
       " 'person',\n",
       " 'mine',\n",
       " 'books',\n",
       " \"shouldn't\",\n",
       " 'careful',\n",
       " 'sister',\n",
       " 'each',\n",
       " 'own',\n",
       " 'family',\n",
       " 'together',\n",
       " 'hate',\n",
       " 'felt',\n",
       " 'seems',\n",
       " 'tonight',\n",
       " 'doctor',\n",
       " 'change',\n",
       " 'learn',\n",
       " 'seem',\n",
       " 'since',\n",
       " 'likes',\n",
       " 'study',\n",
       " 'word',\n",
       " 'waiting',\n",
       " 'forget',\n",
       " 'easy',\n",
       " 'pretty',\n",
       " 'died',\n",
       " 'trying',\n",
       " 'girl',\n",
       " 'phone',\n",
       " 'world',\n",
       " 'far',\n",
       " 'gone',\n",
       " 'working',\n",
       " \"we've\",\n",
       " 'hand',\n",
       " 'started',\n",
       " 'child',\n",
       " 'accident',\n",
       " 'station',\n",
       " 'mean',\n",
       " 'nobody',\n",
       " 'finished',\n",
       " 'longer',\n",
       " 'difficult',\n",
       " 'sick',\n",
       " 'boy',\n",
       " 'surprised',\n",
       " 'important',\n",
       " 'start',\n",
       " 'rain',\n",
       " 'looked',\n",
       " 'quite',\n",
       " 'lunch',\n",
       " 'cat',\n",
       " 'drive',\n",
       " 'wife',\n",
       " 'questions',\n",
       " 'weather',\n",
       " 'anybody',\n",
       " 'reading',\n",
       " 'movie',\n",
       " 'having',\n",
       " 'yours',\n",
       " 'makes',\n",
       " 'mistake',\n",
       " 'supposed',\n",
       " 'thank',\n",
       " 'office',\n",
       " 'story',\n",
       " 'until',\n",
       " 'ten',\n",
       " 'run',\n",
       " 'swim',\n",
       " 'small',\n",
       " 'tv',\n",
       " 'times',\n",
       " 'close',\n",
       " 'himself',\n",
       " 'bit',\n",
       " 'playing',\n",
       " 'spend',\n",
       " 'angry',\n",
       " 'eyes',\n",
       " 'trust',\n",
       " 'stupid',\n",
       " 'called',\n",
       " 'ago',\n",
       " 'guess',\n",
       " 'advice',\n",
       " 'japan',\n",
       " 'hurry',\n",
       " 'picture',\n",
       " 'hours',\n",
       " '.\"',\n",
       " 'broke',\n",
       " 'music',\n",
       " 'exactly',\n",
       " 'says',\n",
       " 'caught',\n",
       " 'students',\n",
       " 'wonder',\n",
       " 'son',\n",
       " 'lives',\n",
       " 'fire',\n",
       " 'afternoon',\n",
       " 'window',\n",
       " 'eating',\n",
       " 'turned',\n",
       " 'police',\n",
       " 'bicycle',\n",
       " 'thinking',\n",
       " 'fell',\n",
       " 'ran',\n",
       " 'decided',\n",
       " 'table',\n",
       " 'fast',\n",
       " 'usually',\n",
       " 'probably',\n",
       " \"who's\",\n",
       " 'minutes',\n",
       " 'japanese',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'trouble',\n",
       " 'free',\n",
       " 'hair',\n",
       " 'arrived',\n",
       " 'hot',\n",
       " 'bring',\n",
       " \"weren't\",\n",
       " 'light',\n",
       " 'town',\n",
       " 'homework',\n",
       " 'advised',\n",
       " 'worry',\n",
       " 'number',\n",
       " 'park',\n",
       " 'game',\n",
       " 'under',\n",
       " 'safe',\n",
       " 'listen',\n",
       " 'forgot',\n",
       " 'stand',\n",
       " 'enjoy',\n",
       " 'country',\n",
       " 'living',\n",
       " 'news',\n",
       " 'through',\n",
       " 'against',\n",
       " 'appreciate',\n",
       " 'agree',\n",
       " 'age',\n",
       " 'miss',\n",
       " 'sit',\n",
       " 'hands',\n",
       " 'quit',\n",
       " 'promise',\n",
       " 'proud',\n",
       " 'sing',\n",
       " 'chance',\n",
       " 'saying',\n",
       " 'possible',\n",
       " 'visit',\n",
       " 'finish',\n",
       " 'different',\n",
       " 'maybe',\n",
       " 'later',\n",
       " 'high',\n",
       " 'reason',\n",
       " 'wine',\n",
       " 'then',\n",
       " 'outside',\n",
       " 'summer',\n",
       " 'kept',\n",
       " 'taking',\n",
       " 'catch',\n",
       " 'hungry',\n",
       " 'needs',\n",
       " 'born',\n",
       " 'lie',\n",
       " 'making',\n",
       " \"should've\",\n",
       " 'cut',\n",
       " 'business',\n",
       " 'moment',\n",
       " 'trip',\n",
       " 'favorite',\n",
       " 'dead',\n",
       " 'end',\n",
       " 'ok',\n",
       " 'shoes',\n",
       " 'older',\n",
       " 'become',\n",
       " 'win',\n",
       " 'tree',\n",
       " 'behind',\n",
       " 'five',\n",
       " 'box',\n",
       " 'near',\n",
       " 'works',\n",
       " 'red',\n",
       " 'girlfriend',\n",
       " 'breakfast',\n",
       " 'die',\n",
       " 'class',\n",
       " 'song',\n",
       " 'tea',\n",
       " 'eaten',\n",
       " 'city',\n",
       " 'dress',\n",
       " 'student',\n",
       " 'baby',\n",
       " \"mary's\",\n",
       " 'rich',\n",
       " 'needed',\n",
       " 'guy',\n",
       " 'face',\n",
       " 'funny',\n",
       " 'secret',\n",
       " 'team',\n",
       " 'month',\n",
       " 'company',\n",
       " 'full',\n",
       " 'quickly',\n",
       " 'comes',\n",
       " 'paid',\n",
       " 'stayed',\n",
       " \"where's\",\n",
       " 'crazy',\n",
       " 'fish',\n",
       " 'rest',\n",
       " 'ate',\n",
       " 'lose',\n",
       " 'woman',\n",
       " 'point',\n",
       " 'watching',\n",
       " 'tennis',\n",
       " 'beer',\n",
       " 'explain',\n",
       " 'part',\n",
       " 'invited',\n",
       " 'serious',\n",
       " 'cannot',\n",
       " 'break',\n",
       " 'large',\n",
       " 'clothes',\n",
       " 'daughter',\n",
       " 'smoking',\n",
       " 'hotel',\n",
       " 'kids',\n",
       " 'key',\n",
       " 'choice',\n",
       " 'asleep',\n",
       " 'hat',\n",
       " 'feeling',\n",
       " 'sound',\n",
       " 'death',\n",
       " 'cost',\n",
       " 'somebody',\n",
       " 'clean',\n",
       " 'australia',\n",
       " 'spent',\n",
       " 'worried',\n",
       " 'began',\n",
       " 'words',\n",
       " 'real',\n",
       " 'lived',\n",
       " 'wearing',\n",
       " '?\"',\n",
       " 'studying',\n",
       " 'handle',\n",
       " 'expensive',\n",
       " 'sometimes',\n",
       " 'goes',\n",
       " 'hour',\n",
       " 'became',\n",
       " 'street',\n",
       " 'touch',\n",
       " 'hit',\n",
       " 'milk',\n",
       " 'river',\n",
       " 'killed',\n",
       " 'store',\n",
       " 'hospital',\n",
       " 'changed',\n",
       " 'short',\n",
       " 'rather',\n",
       " 'decision',\n",
       " 'computer',\n",
       " 'others',\n",
       " 'telling',\n",
       " 'drunk',\n",
       " 'deal',\n",
       " 'between',\n",
       " 'paper',\n",
       " 'hold',\n",
       " 'lucky',\n",
       " 'cake',\n",
       " 'scared',\n",
       " 'takes',\n",
       " 'birthday',\n",
       " 'snow',\n",
       " 'language',\n",
       " 'closed',\n",
       " 'present',\n",
       " 'helped',\n",
       " 'dark',\n",
       " 'minute',\n",
       " 'stopped',\n",
       " 'problems',\n",
       " 'restaurant',\n",
       " 'sat',\n",
       " 'monday',\n",
       " 'speaking',\n",
       " 'expect',\n",
       " 'front',\n",
       " 'whole',\n",
       " 'quiet',\n",
       " 'war',\n",
       " 'mistakes',\n",
       " 'figured',\n",
       " 'worked',\n",
       " 'finally',\n",
       " 'gets',\n",
       " 'along',\n",
       " 'head',\n",
       " 'report',\n",
       " 'wrote',\n",
       " 'happening',\n",
       " 'dogs',\n",
       " 'coat',\n",
       " 'sense',\n",
       " 'cup',\n",
       " 'talked',\n",
       " 'liked',\n",
       " 'strong',\n",
       " 'upset',\n",
       " 'kill',\n",
       " 'speaks',\n",
       " 'thanks',\n",
       " 'missed',\n",
       " 'forward',\n",
       " 'strange',\n",
       " 'expected',\n",
       " 'check',\n",
       " 'boss',\n",
       " 'dream',\n",
       " 'dangerous',\n",
       " 'beach',\n",
       " 'known',\n",
       " 'health',\n",
       " 'situation',\n",
       " 'actually',\n",
       " 'running',\n",
       " 'brought',\n",
       " 'whether',\n",
       " 'six',\n",
       " 'its',\n",
       " 'whatever',\n",
       " 'dictionary',\n",
       " 'move',\n",
       " 'seeing',\n",
       " 'shut',\n",
       " 'order',\n",
       " 'swimming',\n",
       " 'weekend',\n",
       " 'tall',\n",
       " 'men',\n",
       " 'air',\n",
       " 'evening',\n",
       " 'walked',\n",
       " 'plane',\n",
       " 'allowed',\n",
       " 'thirty',\n",
       " 'loves',\n",
       " 'case',\n",
       " 'least',\n",
       " 'building',\n",
       " 'broken',\n",
       " 'list',\n",
       " 'worth',\n",
       " 'happens',\n",
       " 'heart',\n",
       " 'disappointed',\n",
       " 'follow',\n",
       " 'famous',\n",
       " 'prefer',\n",
       " 'written',\n",
       " 'smoke',\n",
       " 'christmas',\n",
       " 'perfect',\n",
       " 'drinking',\n",
       " \"o'clock\",\n",
       " 'luck',\n",
       " 'choose',\n",
       " 'completely',\n",
       " 'dollars',\n",
       " 'pain',\n",
       " 'future',\n",
       " 'either',\n",
       " 'mad',\n",
       " 'seat',\n",
       " 'crying',\n",
       " 'dance',\n",
       " 'second',\n",
       " 'piano',\n",
       " 'offer',\n",
       " 'necessary',\n",
       " 'fine',\n",
       " 'half',\n",
       " 'sent',\n",
       " 'sunday',\n",
       " \"hasn't\",\n",
       " 'garden',\n",
       " 'apologize',\n",
       " 'rules',\n",
       " 'road',\n",
       " 'library',\n",
       " 'leaving',\n",
       " 'noise',\n",
       " 'weight',\n",
       " 'flowers',\n",
       " 'wear',\n",
       " 'opinion',\n",
       " 'cook',\n",
       " 'writing',\n",
       " 'camera',\n",
       " 'set',\n",
       " 'taken',\n",
       " 'glass',\n",
       " 'learned',\n",
       " 'address',\n",
       " 'somewhere',\n",
       " 'poor',\n",
       " 'several',\n",
       " \"it'll\",\n",
       " 'white',\n",
       " 'danger',\n",
       " 'attention',\n",
       " \"he'll\",\n",
       " 'tokyo',\n",
       " 'floor',\n",
       " 'save',\n",
       " 'alive',\n",
       " 'accept',\n",
       " 'information',\n",
       " 'clear',\n",
       " 'suppose',\n",
       " 'opened',\n",
       " 'during',\n",
       " 'kiss',\n",
       " 'loved',\n",
       " 'nervous',\n",
       " 'grow',\n",
       " 'girls',\n",
       " 'waste',\n",
       " 'showed',\n",
       " 'join',\n",
       " 'women',\n",
       " 'bag',\n",
       " 'sleeping',\n",
       " 'listening',\n",
       " 'less',\n",
       " 'solve',\n",
       " 'sad',\n",
       " 'won',\n",
       " 'date',\n",
       " 'shopping',\n",
       " 'blame',\n",
       " 'desk',\n",
       " 'tie',\n",
       " 'medicine',\n",
       " 'vacation',\n",
       " 'pass',\n",
       " 'college',\n",
       " 'side',\n",
       " 'success',\n",
       " 'teach',\n",
       " 'keys',\n",
       " 'arrive',\n",
       " 'umbrella',\n",
       " 'fix',\n",
       " 'sounds',\n",
       " 'ticket',\n",
       " 'whose',\n",
       " 'ship',\n",
       " 'radio',\n",
       " 'lying',\n",
       " 'spoke',\n",
       " 'smart',\n",
       " 'means',\n",
       " 'lawyer',\n",
       " 'thinks',\n",
       " 'telephone',\n",
       " 'abroad',\n",
       " 'promised',\n",
       " 'glasses',\n",
       " 'willing',\n",
       " 'wake',\n",
       " 'shot',\n",
       " 'black',\n",
       " 'owe',\n",
       " 'husband',\n",
       " 'speech',\n",
       " 'cats',\n",
       " 'plans',\n",
       " 'horse',\n",
       " 'borrow',\n",
       " 'driving',\n",
       " 'staying',\n",
       " 'apple',\n",
       " 'message',\n",
       " 'immediately',\n",
       " 'guitar',\n",
       " 'piece',\n",
       " 'discuss',\n",
       " 'across',\n",
       " 'excuse',\n",
       " 'dressed',\n",
       " 'uncle',\n",
       " 'pictures',\n",
       " 'given',\n",
       " 'fat',\n",
       " 'guys',\n",
       " 'waited',\n",
       " 'prepared',\n",
       " 'fight',\n",
       " 'bank',\n",
       " 'asking',\n",
       " 'involved',\n",
       " 'american',\n",
       " 'lend',\n",
       " 'carefully',\n",
       " 'harder',\n",
       " 'none',\n",
       " 'boyfriend',\n",
       " 'concert',\n",
       " 'sign',\n",
       " 'boys',\n",
       " 'ride',\n",
       " 'traffic',\n",
       " 'cry',\n",
       " 'certain',\n",
       " 'gun',\n",
       " 'shower',\n",
       " 'afford',\n",
       " 'cute',\n",
       " 'joke',\n",
       " 'price',\n",
       " 'couple',\n",
       " 'inside',\n",
       " 'easily',\n",
       " 'arm',\n",
       " 'lake',\n",
       " 'novel',\n",
       " 'shirt',\n",
       " 'storm',\n",
       " 'painting',\n",
       " 'agreed',\n",
       " 'send',\n",
       " 'satisfied',\n",
       " 'worse',\n",
       " 'regret',\n",
       " 'america',\n",
       " 'fired',\n",
       " 'begin',\n",
       " 'kid',\n",
       " 'twice',\n",
       " 'raining',\n",
       " 'succeed',\n",
       " 'travel',\n",
       " 'fault',\n",
       " \"how's\",\n",
       " 'count',\n",
       " 'correct',\n",
       " 'act',\n",
       " \"they'll\",\n",
       " 'passed',\n",
       " 'chair',\n",
       " \"we'd\",\n",
       " 'difference',\n",
       " 'ice',\n",
       " 'test',\n",
       " 'empty',\n",
       " 'honest',\n",
       " 'laugh',\n",
       " 'meat',\n",
       " 'movies',\n",
       " 'television',\n",
       " 'four',\n",
       " 'pen',\n",
       " 'earlier',\n",
       " 'hardly',\n",
       " 'impossible',\n",
       " 'held',\n",
       " 'color',\n",
       " 'figure',\n",
       " 'ahead',\n",
       " 'wonderful',\n",
       " 'blue',\n",
       " 'enjoyed',\n",
       " 'winter',\n",
       " 'likely',\n",
       " 'machine',\n",
       " 'carry',\n",
       " 'warm',\n",
       " 'refused',\n",
       " 'fall',\n",
       " 'line',\n",
       " 'bill',\n",
       " 'played',\n",
       " 'decide',\n",
       " 'kissed',\n",
       " 'terrible',\n",
       " 'animals',\n",
       " 'younger',\n",
       " 'herself',\n",
       " 'mountain',\n",
       " 'bother',\n",
       " 'laughed',\n",
       " 'slept',\n",
       " 'walking',\n",
       " 'ideas',\n",
       " 'past',\n",
       " 'heavy',\n",
       " 'shop',\n",
       " 'anywhere',\n",
       " 'foreign',\n",
       " 'sooner',\n",
       " 'liar',\n",
       " 'cooking',\n",
       " 'injured',\n",
       " 'leaves',\n",
       " 'newspaper',\n",
       " 'pick',\n",
       " 'smell',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.idx_to_token  # list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.4. <a id='toc11_4_1_4_'></a>[截断和填充](#toc0_)\n",
    "truncation和padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]                                     # Truncate\n",
    "    else:\n",
    "        return line + [padding_token] * (num_steps - len(line))     # Pad\n",
    "\n",
    "truncate_pad(line=src_vocab[source[0]], num_steps=10, padding_token=src_vocab['<pad>'])      # return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "现在我们定义一个函数，可以将文本序列转换成小批量数据集用于训练。\n",
    "我们将特定的“<eos>”词元添加到所有序列的末尾，用于表示序列的结束。\n",
    "当模型通过一个词元接一个词元地生成序列进行预测时，生成的“<eos>”词元说明完成了序列输出工作。\n",
    "此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息。\n",
    "'''\n",
    "#@tab all\n",
    "#@save\n",
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
    "    \n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    # 计算每个序列中非填充 <pad> 标记的数量，得到每个序列的有效长度。\n",
    "    # array != vocab['<pad>']：创建一个布尔张量，标识哪些位置不是填充标记。\n",
    "    # d2l.astype(..., d2l.int32)：将布尔值转换为整数类型（1 和 0）。\n",
    "    # d2l.reduce_sum(..., 1)：沿着序列长度的维度求和，得到每个序列的有效长度。\n",
    "    valid_len = d2l.reduce_sum(d2l.astype(array != vocab['<pad>'], torch.int32), 1)\n",
    "    \n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1.5. <a id='toc11_4_1_5_'></a>[集合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 7, 84,  4,  3,  1,  1,  1,  1],\n",
      "        [90, 19,  4,  3,  1,  1,  1,  1]], dtype=torch.int32)\n",
      "valid lengths for X: tensor([4, 4])\n",
      "Y: tensor([[ 0, 16, 17,  4,  3,  1,  1,  1],\n",
      "        [ 0, 12,  5,  3,  1,  1,  1,  1]], dtype=torch.int32)\n",
      "valid lengths for Y: tensor([5, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#@tab all\n",
    "#@save\n",
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
    "\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "\n",
    "    src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = d2l.Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    \n",
    "    # 数据集顺序：源语言序列、源语言序列有效长度、目标语言序列、目标语言序列有效长度\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)      \n",
    "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    \n",
    "    return data_iter, src_vocab, tgt_vocab\n",
    "\n",
    "\n",
    "#@tab all\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)\n",
    "\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X:', d2l.astype(X, torch.int32))\n",
    "    print('valid lengths for X:', X_valid_len)\n",
    "    print('Y:', d2l.astype(Y, torch.int32))\n",
    "    print('valid lengths for Y:', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab['<pad>'], tgt_vocab['<pad>']  # 用1填充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2. <a id='toc11_4_2_'></a>[编码器-解码器架构](#toc0_)\n",
    "机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一个包含两个主要组件的架构：  \n",
    "  * 第一个组件是一个编码器（encoder）：它接受一个`长度可变的序列作为输入`，并将其转换为具有`固定形状的编码状态`。\n",
    "  * 第二个组件是解码器（decoder）：它将`固定形状的编码状态`映射到`长度可变的输出序列`。\n",
    "\n",
    "我们以英语到法语的机器翻译为例：给定一个英文的输入序列：“They”“are”“watching”“.”。首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”，然后对该状态进行解码，一个词元接着一个词元地生成翻译后的序列作为输出：“Ils”“regordent”“.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "编码器\n",
    "'''\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "解码器\n",
    "'''\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        '''我们新增一个init_state函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如：输入序列的有效长度'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "编码器-解码器架构\n",
    "'''\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder \n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.3. <a id='toc11_4_3_'></a>[序列到序列学习](#toc0_)\n",
    "我们将使用两个循环神经网络的编码器和解码器，并将其应用于序列到序列（sequencetosequence，seq2seq）类的学习任务.\n",
    "\n",
    "![序列到序列学习](./Pytorch_Pictures/seq2seq/seq2seq_learning.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7]), torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch\n",
    "\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    '''编码器'''\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(input_size=embed_size, hidden_size=num_hiddens, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X 的形状：(batch_size, num_steps, embed_size)\n",
    "        X = self.embedding(X)\n",
    "        # 在循环神经网络模型中，第一个时间步的输入需要使用形状为(num_layers, batch_size, num_hiddens)的初始化隐藏状态\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # 如果未提及初始化隐藏状态，则默认为0\n",
    "        output, state = self.rnn(X)\n",
    "        # output的形状:(num_steps, batch_size, num_hiddens)\n",
    "        # state的形状:(num_layers, batch_size, num_hiddens)，num_steps的最后一个时刻的hidden state\n",
    "        return output, state\n",
    "        \n",
    "\n",
    "# Test\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2, dropout=0)\n",
    "encoder.eval()\n",
    "\n",
    "batch_size = 4\n",
    "num_steps = 7\n",
    "\n",
    "# 输入：(batch_size, num_steps)\n",
    "X = torch.ones(size=(batch_size, num_steps), dtype=torch.long)\n",
    "\n",
    "# output: (num_steps, batch_size, num_hiddens)\n",
    "# state: (num_layers, batch_size, num_hiddens)\n",
    "output, state = encoder(X)\n",
    "\n",
    "X.shape, output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out state: torch.Size([2, 4, 16])\n",
      "encoder_out state[-1]: torch.Size([4, 16])\n",
      "encoder_out state[-1].repeat(X.shape[0], 1, 1): torch.Size([7, 4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7]), torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "\n",
    "\n",
    "class Seq2SeqDecoder(nn.Module):\n",
    "    '''解码器'''\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将enc_outputs的hidden state的最后一层与dec_x拼接起来\n",
    "        self.rnn = nn.GRU(input_size=embed_size + num_hiddens, hidden_size=num_hiddens, num_layers=num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(in_features=num_hiddens, out_features=vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输入的X形状：(batch_size, num_steps)\n",
    "        # 经过embedding后，X的形状：(batch_size, num_steps, embed_size)\n",
    "        # 输出'X'的形状：(num_steps, batch_size, embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "\n",
    "        # state: (num_layers, batch_size, num_hiddens)  \n",
    "        # state[-1]: hidden state最后一层的hidden state，形状：(batch_size, num_hiddens)\n",
    "        # state[-1].repeat(X.shape[0], 1, 1)，形状：(num_steps, batch_size, num_hiddens)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        # 将X和context在最后一维上连接\n",
    "        # X: (num_steps, batch_size, embed_size) + context: (num_steps, batch_size, num_hiddens) = (num_steps, batch_size, embed_size + num_hiddens)\n",
    "        X_and_context = torch.cat((X, context), dim=2)\n",
    "        # 通过时间步展开\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size, num_steps, vocab_size)\n",
    "        # state的形状:(num_layers, batch_size, num_hiddens)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2, dropout=0)\n",
    "decoder.eval()\n",
    "\n",
    "batch_size = 4\n",
    "num_steps = 7\n",
    "\n",
    "# 输入：(batch_size, num_steps)\n",
    "X = torch.zeros(size=(batch_size, num_steps), dtype=torch.long)\n",
    "\n",
    "# state: (num_layers, batch_size, num_hiddens)\n",
    "state = decoder.init_state(encoder(X))\n",
    "print(f'encoder_out state: {state.shape}')\n",
    "print(f'encoder_out state[-1]: {state[-1].shape}')\n",
    "print(f'encoder_out state[-1].repeat(X.shape[0], 1, 1): {state[-1].repeat(num_steps, 1, 1).shape}')\n",
    "\n",
    "# output: (batch_size, num_steps, num_hiddens), 经过permute转置后的结果\n",
    "# state: (num_layers, batch_size, num_hiddens)\n",
    "output, state = decoder(X, state)\n",
    "\n",
    "X.shape, output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.4. <a id='toc11_4_4_'></a>[损失函数](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.4.1. <a id='toc11_4_4_1_'></a>[掩码](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "torch.Size([3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[1, 2, 3]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# [:, None] 和 [None, :] 是用于在张量的维度上添加一个新的维度，以便进行广播操作。\n",
    "# 在二维张量中，[:, None] 表示在列维度上添加一个维度，[None, :] 表示在行维度上添加一个维度。\n",
    "x1 = x[:, None]\n",
    "x2 = x[None, :]\n",
    "\n",
    "print(x, x.shape, sep='\\n')\n",
    "print(x1, x1.shape, sep='\\n')\n",
    "print(x2, x2.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3, -1, -1],\n",
       "        [ 4,  5,  6,  7,  8],\n",
       "        [ 9, 10, -1, -1, -1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "X = [['a', 'b', 'c', 'pad', 'pad'],\n",
    "     ['d', 'e', 'f', 'g', 'h'], \n",
    "     ['i', 'j', 'pad', 'pad', 'pad']]\n",
    "\n",
    "# corpus\n",
    "X = [[1, 2, 3, 0, 0], \n",
    "     [4, 5, 6, 7, 8], \n",
    "     [9, 10, 0, 0, 0]]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "\n",
    "# mask\n",
    "mask = [[True, True, True, False, False], \n",
    "        [True, True, True, True, True], \n",
    "        [True, True, False, False, False]]\n",
    "\n",
    "mask = torch.tensor(mask)\n",
    "\n",
    "# ~表示取反操作\n",
    "X[~mask] = -1\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 0],\n",
       "         [4, 5, 0]]),\n",
       " tensor([[1, 0, 0],\n",
       "         [4, 5, 0]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    '''\n",
    "    为序列生成掩码，将无效/填充位置的值替换为指定值\n",
    "    '''\n",
    "\n",
    "    maxlen = X.size(1)\n",
    "\n",
    "    # 作用：生成一个布尔掩码，用于标识每个序列中有效的时间步。\n",
    "    # [None, :] 和 [:, None]：通过添加新的维度，将一维张量扩展为二维，以便进行广播操作\n",
    "    # < valid_len[:, None]：比较操作，将生成一个布尔张量，当位置索引小于 valid_len 时为 True，否则为 False。valid_len 是一个包含每个序列实际长度的张量。\n",
    "    mask = torch.arange(maxlen, dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    \n",
    "    # 将无效位置的值替换为指定值\n",
    "    # ~mask：取反，将无效位置的值替换为指定值\n",
    "    X[~mask] = value \n",
    "\n",
    "    return X \n",
    "\n",
    "\n",
    "# tokens with padding\n",
    "X = torch.tensor([[1, -1, -1], \n",
    "                  [4, 5, -1]])\n",
    "\n",
    "# tokens 有效长度\n",
    "valid_len = torch.tensor([1, 2])\n",
    "\n",
    "# 输出：(batch_size, num_steps)\n",
    "X, sequence_mask(X=X, valid_len=valid_len, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]]),\n",
       " tensor([[[1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens with padding\n",
    "X = torch.ones(size=(2, 3, 4))\n",
    "\n",
    "X, sequence_mask(X=X, valid_len=torch.tensor([1, 2]), value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.4.2. <a id='toc11_4_4_2_'></a>[带掩码的softmax交叉熵损失](#toc0_)\n",
    "我们可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "#@save\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        # 将损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测 (0乘以任何数为0，达到过滤的作用)   \n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss\n",
    "    \n",
    "\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "\n",
    "loss(\n",
    "    pred = torch.ones(size=(3, 4, 10)), \n",
    "    label = torch.ones(size=(3, 4), dtype=torch.long), \n",
    "    valid_len = torch.tensor([4, 2, 0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.5. <a id='toc11_4_5_'></a>[训练](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.019, 10052.5 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-06T20:27:06.543030</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 262.1875 183.35625 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 86.015179 145.8 \n",
       "L 86.015179 7.2 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mf859e8e277\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf859e8e277\" x=\"86.015179\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(76.471429 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 125.872321 145.8 \n",
       "L 125.872321 7.2 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf859e8e277\" x=\"125.872321\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(116.328571 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 165.729464 145.8 \n",
       "L 165.729464 7.2 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf859e8e277\" x=\"165.729464\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(156.185714 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 205.586607 145.8 \n",
       "L 205.586607 7.2 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf859e8e277\" x=\"205.586607\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(196.042857 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf859e8e277\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(235.9 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 50.14375 119.332497 \n",
       "L 245.44375 119.332497 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m344cc1b810\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m344cc1b810\" x=\"50.14375\" y=\"119.332497\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 123.131716) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 86.675132 \n",
       "L 245.44375 86.675132 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m344cc1b810\" x=\"50.14375\" y=\"86.675132\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 90.474351) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 50.14375 54.017767 \n",
       "L 245.44375 54.017767 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m344cc1b810\" x=\"50.14375\" y=\"54.017767\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 57.816986) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 50.14375 21.360402 \n",
       "L 245.44375 21.360402 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m344cc1b810\" x=\"50.14375\" y=\"21.360402\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 25.159621) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_19\">\n",
       "    <path d=\"M 50.14375 13.5 \n",
       "L 54.129464 48.523568 \n",
       "L 58.115179 68.312039 \n",
       "L 62.100893 81.781528 \n",
       "L 66.086607 92.283597 \n",
       "L 70.072321 100.630944 \n",
       "L 74.058036 107.037339 \n",
       "L 78.04375 112.203162 \n",
       "L 82.029464 117.515825 \n",
       "L 86.015179 121.371848 \n",
       "L 90.000893 123.900814 \n",
       "L 93.986607 126.368093 \n",
       "L 97.972321 128.215837 \n",
       "L 101.958036 129.516862 \n",
       "L 105.94375 130.428123 \n",
       "L 109.929464 131.497932 \n",
       "L 113.915179 132.132061 \n",
       "L 117.900893 133.50306 \n",
       "L 121.886607 134.258222 \n",
       "L 125.872321 134.398627 \n",
       "L 129.858036 135.534273 \n",
       "L 133.84375 135.581176 \n",
       "L 137.829464 136.292695 \n",
       "L 141.815179 136.960656 \n",
       "L 145.800893 136.75814 \n",
       "L 149.786607 136.538446 \n",
       "L 153.772321 136.960175 \n",
       "L 157.758036 137.589058 \n",
       "L 161.74375 138.01357 \n",
       "L 165.729464 137.525863 \n",
       "L 169.715179 137.853843 \n",
       "L 173.700893 137.214598 \n",
       "L 177.686607 137.92942 \n",
       "L 181.672321 138.219824 \n",
       "L 185.658036 138.411656 \n",
       "L 189.64375 138.47868 \n",
       "L 193.629464 138.498101 \n",
       "L 197.615179 138.583641 \n",
       "L 201.600893 139.277759 \n",
       "L 205.586607 138.457891 \n",
       "L 209.572321 138.451397 \n",
       "L 213.558036 138.790962 \n",
       "L 217.54375 138.827241 \n",
       "L 221.529464 138.456782 \n",
       "L 225.515179 138.867523 \n",
       "L 229.500893 139.200147 \n",
       "L 233.486607 139.336128 \n",
       "L 237.472321 138.649635 \n",
       "L 241.458036 139.311311 \n",
       "L 245.44375 139.5 \n",
       "\" clip-path=\"url(#p9d4386f907)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p9d4386f907\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)                       # 对于每个线性层 nn.Linear，应用 xavier_uniform_ 初始化。\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])   # 模型中的线性层和GRU层应用Xavier均匀初始化，以确保权重在训练开始时处于合适的范围，促进梯度流动。\n",
    "    \n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()                                    # 使用带掩码的Softmax交叉熵损失 MaskedSoftmaxCELoss，适用于序列到序列任务，可以处理不同长度的序列。\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            # 为每个目标序列添加开始符 <bos>，并去除原序列的最后一个标记，以实现教师强制（Teacher Forcing），即使用真实的目标标记作为下一个时间步的输入。\n",
    "            # Y: (batch_size, num_steps)\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "            dec_input = d2l.concat([bos, Y[:, :-1]], dim=1)  # Teacher forcing\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)   # X_valid_len没有被用上？\n",
    "            # Y_hat, _ = net(X, dec_input)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "          f'tokens/sec on {str(device)}')\n",
    "    \n",
    "\n",
    "#@tab all\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 500, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.6. <a id='toc11_4_6_'></a>[预测](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=False):\n",
    "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # Add the batch axis\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # Add the batch axis\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights (to be covered later)\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # Once the end-of-sequence token is predicted, the generation of the\n",
    "        # output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"Compute the BLEU.\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[''.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[''.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[''.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !, bleu 1.000\n",
      "i lost . => j'ai tom <unk> ., bleu 0.000\n",
      "he's calm . => il est riche ., bleu 0.658\n",
      "i'm home . => je suis en retard ., bleu 0.548\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5. <a id='toc11_5_'></a>[Attention](#toc0_)\n",
    "\n",
    "- 不是一个新的概念，很早之前就已经出现，只是在Google发表论文[Attention is all you need](https://arxiv.org/abs/1706.03762)后，越来越知名； \n",
    "- 如果非要找一个依据，从心理学上讲： \n",
    " \n",
    "    1. 之前学习的神经网络（CNN、RNN等）都是提取特征->全连接网络，属于“非随意识注意力”-即非主观，如一排黑色咖啡杯中有一个红色的就会很吸引人；  \n",
    "    2. Attention提出的是“随意识注意力”即主观的去注意那个物体，如喝完咖啡后想去找一本关于Attention方面的书去看。\n",
    "\n",
    "        Query:人主动去查询（注意）  \n",
    "        Key:  物体的属性  \n",
    "        Value:  物体的属性  \n",
    "\n",
    "- `说白了，注意力就是加权平均数，首先计算query与key的相似度，越相似就给越高的权重，最后用权重乘以value再求和，即得注意力值。`\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention//Attention_principle.jpg\" width = \"700\" height = \"300\" alt=\"图片名称\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.1. <a id='toc11_5_1_'></a>[实例数据](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99dad76930>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"217.220312pt\" height=\"155.63625pt\" viewBox=\"0 0 217.220312 155.63625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T12:21:47.423846</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 155.63625 \n",
       "L 217.220312 155.63625 \n",
       "L 217.220312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "L 42.620312 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"mec0ed91afe\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pdd575c3db8)\">\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"50.229403\" y=\"80.024525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"67.138494\" y=\"36.398732\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"84.047585\" y=\"12.24\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"100.956676\" y=\"24.590756\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"117.865767\" y=\"96.646056\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"134.774858\" y=\"113.04\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"151.683949\" y=\"67.314675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"168.59304\" y=\"32.236658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"185.502131\" y=\"31.318269\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mec0ed91afe\" x=\"202.411222\" y=\"51.618842\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m19db1e6d54\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m19db1e6d54\" x=\"50.229403\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(47.048153 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m19db1e6d54\" x=\"84.047585\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(80.866335 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m19db1e6d54\" x=\"117.865767\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(114.684517 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m19db1e6d54\" x=\"151.683949\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(148.502699 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m19db1e6d54\" x=\"185.502131\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(182.320881 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(123.360938 146.356562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m35cd566cc3\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m35cd566cc3\" x=\"42.620312\" y=\"112.891122\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(20.878125 116.690341) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m35cd566cc3\" x=\"42.620312\" y=\"69.938938\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 73.738157) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m35cd566cc3\" x=\"42.620312\" y=\"26.986754\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 30.785973) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.798437 71.261094) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_9\">\n",
       "    <path d=\"M 50.229403 69.938938 \n",
       "L 67.138494 33.795923 \n",
       "L 84.047585 30.882628 \n",
       "L 100.956676 63.877526 \n",
       "L 117.865767 102.445258 \n",
       "L 134.774858 111.126831 \n",
       "L 151.683949 81.940444 \n",
       "L 168.59304 41.719929 \n",
       "L 185.502131 27.443841 \n",
       "L 202.411222 52.237549 \n",
       "\" clip-path=\"url(#pdd575c3db8)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 42.620312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 7.2 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 49.620312 113.08 \n",
       "L 102.854687 113.08 \n",
       "Q 104.854687 113.08 104.854687 111.08 \n",
       "L 104.854687 97.401875 \n",
       "Q 104.854687 95.401875 102.854687 95.401875 \n",
       "L 49.620312 95.401875 \n",
       "Q 47.620312 95.401875 47.620312 97.401875 \n",
       "L 47.620312 111.08 \n",
       "Q 47.620312 113.08 49.620312 113.08 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_10\">\n",
       "     <path d=\"M 51.620312 103.500312 \n",
       "L 61.620312 103.500312 \n",
       "L 71.620312 103.500312 \n",
       "\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- True -->\n",
       "     <g transform=\"translate(79.620312 107.000312) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"87.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"150.826172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdd575c3db8\">\n",
       "   <rect x=\"42.620312\" y=\"7.2\" width=\"167.4\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import torch \n",
    "\n",
    "\n",
    "def f(x):\n",
    "    '''真实函数'''\n",
    "    return 2 * torch.sin(x)\n",
    "\n",
    "\n",
    "def f_noise(x):\n",
    "    '''添加噪声'''\n",
    "    return f(x) + torch.normal(mean=0, std=1, size=x.shape)\n",
    "\n",
    "\n",
    "# 生成数据\n",
    "x = torch.arange(start=0, end=10, step=1)\n",
    "y = f_noise(x)\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.2. <a id='toc11_5_2_'></a>[无注意力的方式-如平均汇聚](#toc0_)\n",
    "如直接就平均y值，得到的结果就是一条平滑的曲线，及所有数据的加权信息都一样。\n",
    "\n",
    "其实就是注意力汇聚的平均值，即所有数据都一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99dadaa7b0>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"217.220312pt\" height=\"155.63625pt\" viewBox=\"0 0 217.220312 155.63625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T12:21:56.485745</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 155.63625 \n",
       "L 217.220312 155.63625 \n",
       "L 217.220312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "L 42.620312 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"md3d74671e4\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pa98fd78c26)\">\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"50.229403\" y=\"80.024525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"67.138494\" y=\"36.398732\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"84.047585\" y=\"12.24\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"100.956676\" y=\"24.590756\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"117.865767\" y=\"96.646056\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"134.774858\" y=\"113.04\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"151.683949\" y=\"67.314675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"168.59304\" y=\"32.236658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"185.502131\" y=\"31.318269\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#md3d74671e4\" x=\"202.411222\" y=\"51.618842\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m0039f4fcf2\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0039f4fcf2\" x=\"50.229403\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(47.048153 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0039f4fcf2\" x=\"84.047585\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(80.866335 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0039f4fcf2\" x=\"117.865767\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(114.684517 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0039f4fcf2\" x=\"151.683949\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(148.502699 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0039f4fcf2\" x=\"185.502131\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(182.320881 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(123.360938 146.356562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mf36bda4616\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf36bda4616\" x=\"42.620312\" y=\"112.891122\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(20.878125 116.690341) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf36bda4616\" x=\"42.620312\" y=\"69.938938\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 73.738157) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf36bda4616\" x=\"42.620312\" y=\"26.986754\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 30.785973) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.798437 71.261094) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_9\">\n",
       "    <path d=\"M 50.229403 69.938938 \n",
       "L 67.138494 33.795923 \n",
       "L 84.047585 30.882628 \n",
       "L 100.956676 63.877526 \n",
       "L 117.865767 102.445258 \n",
       "L 134.774858 111.126831 \n",
       "L 151.683949 81.940444 \n",
       "L 168.59304 41.719929 \n",
       "L 185.502131 27.443841 \n",
       "L 202.411222 52.237549 \n",
       "\" clip-path=\"url(#pa98fd78c26)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_10\">\n",
       "    <path d=\"M 50.229403 54.542851 \n",
       "L 67.138494 54.542851 \n",
       "L 84.047585 54.542851 \n",
       "L 100.956676 54.542851 \n",
       "L 117.865767 54.542851 \n",
       "L 134.774858 54.542851 \n",
       "L 151.683949 54.542851 \n",
       "L 168.59304 54.542851 \n",
       "L 185.502131 54.542851 \n",
       "L 202.411222 54.542851 \n",
       "\" clip-path=\"url(#pa98fd78c26)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 42.620312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 7.2 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 89.7875 44.55625 \n",
       "L 162.853125 44.55625 \n",
       "Q 164.853125 44.55625 164.853125 42.55625 \n",
       "L 164.853125 14.2 \n",
       "Q 164.853125 12.2 162.853125 12.2 \n",
       "L 89.7875 12.2 \n",
       "Q 87.7875 12.2 87.7875 14.2 \n",
       "L 87.7875 42.55625 \n",
       "Q 87.7875 44.55625 89.7875 44.55625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_11\">\n",
       "     <path d=\"M 91.7875 20.298438 \n",
       "L 101.7875 20.298438 \n",
       "L 111.7875 20.298438 \n",
       "\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- True -->\n",
       "     <g transform=\"translate(119.7875 23.798438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"87.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"150.826172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_12\">\n",
       "     <path d=\"M 91.7875 34.976562 \n",
       "L 101.7875 34.976562 \n",
       "L 111.7875 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- Average -->\n",
       "     <g transform=\"translate(119.7875 38.476562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \n",
       "L 1331 1722 \n",
       "L 3047 1722 \n",
       "L 2188 4044 \n",
       "z\n",
       "M 1831 4666 \n",
       "L 2547 4666 \n",
       "L 4325 0 \n",
       "L 3669 0 \n",
       "L 3244 1197 \n",
       "L 1141 1197 \n",
       "L 716 0 \n",
       "L 50 0 \n",
       "L 1831 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-41\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"62.533203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"121.712891\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"183.236328\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"224.349609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"285.628906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"349.105469\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pa98fd78c26\">\n",
       "   <rect x=\"42.620312\" y=\"7.2\" width=\"167.4\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import torch \n",
    "\n",
    "\n",
    "def average_pooling(y):\n",
    "    '''平均汇聚后再复制len(y)次'''\n",
    "    # return torch.mean(y) * torch.ones_like(y)\n",
    "    return torch.repeat_interleave(torch.mean(y),  len(y)) # repeat_interleave 重复元素\n",
    "\n",
    "\n",
    "# 平均汇聚\n",
    "y_avg = average_pooling(y)\n",
    "\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "plt.plot(x, y_avg, color='green', linestyle='--', label='Average')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.3. <a id='toc11_5_3_'></a>[非参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc0_)\n",
    "在以前，统计学家计算机用的不是很溜。用统计模型进行预测，而不是利用计算机的计算资源进行迭代优化逼近真实分布。所得的结果就是只是利用统计模型进行预测的曲线会比较平滑但是准确性不高，可能随着数据量的增高可以提高准确性，但是，现实中能有那么多够用的数据吗？而利用计算迭代优化逼近的方法可以很准确的拟合现有的数据，虽然不是很平滑，优点是数据虽少但可以被充分利用。\n",
    "\n",
    "如Nadraya-Watson核回归，利用核函数计算x和x'的相似度，即权重，然后利用权重对y进行加权求和，得到最终的结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NW_PY: 0.03251147270202637 s, NW_PYT: 0.0012345314025878906 s, NW_PYT_B: 0.0010378360748291016 s\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"217.220312pt\" height=\"155.63625pt\" viewBox=\"0 0 217.220312 155.63625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T12:22:27.132273</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 155.63625 \n",
       "L 217.220312 155.63625 \n",
       "L 217.220312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "L 42.620312 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"mf024d02d2b\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pa999ffb7f2)\">\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"50.229403\" y=\"80.024523\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"67.138494\" y=\"36.398735\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"84.047585\" y=\"12.240005\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"100.956676\" y=\"24.59076\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"117.865767\" y=\"96.646052\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"134.774858\" y=\"113.039995\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"151.683949\" y=\"67.314674\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"168.59304\" y=\"32.236661\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"185.502131\" y=\"31.318272\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mf024d02d2b\" x=\"202.411222\" y=\"51.618844\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m9c1e658e3b\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9c1e658e3b\" x=\"50.229403\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(47.048153 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9c1e658e3b\" x=\"84.047585\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(80.866335 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9c1e658e3b\" x=\"117.865767\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(114.684517 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9c1e658e3b\" x=\"151.683949\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(148.502699 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9c1e658e3b\" x=\"185.502131\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(182.320881 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(123.360938 146.356562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m1b8afa2849\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b8afa2849\" x=\"42.620312\" y=\"112.891117\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(20.878125 116.690336) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b8afa2849\" x=\"42.620312\" y=\"69.938937\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 73.738156) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1b8afa2849\" x=\"42.620312\" y=\"26.986758\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 30.785976) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.798437 71.261094) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_9\">\n",
       "    <path d=\"M 50.229403 69.938937 \n",
       "L 67.138494 33.795926 \n",
       "L 84.047585 30.882632 \n",
       "L 100.956676 63.877526 \n",
       "L 117.865767 102.445254 \n",
       "L 134.774858 111.126826 \n",
       "L 151.683949 81.940442 \n",
       "L 168.59304 41.719931 \n",
       "L 185.502131 27.443844 \n",
       "L 202.411222 52.23755 \n",
       "\" clip-path=\"url(#pa999ffb7f2)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_10\">\n",
       "    <path d=\"M 50.229403 80.024522 \n",
       "L 67.138494 36.398735 \n",
       "L 84.047585 12.24 \n",
       "L 100.956676 24.590765 \n",
       "L 117.865767 96.646052 \n",
       "L 134.774858 113.039995 \n",
       "L 151.683949 67.314674 \n",
       "L 168.59304 32.236656 \n",
       "L 185.502131 31.318269 \n",
       "L 202.411222 51.618844 \n",
       "\" clip-path=\"url(#pa999ffb7f2)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_11\">\n",
       "    <path d=\"M 50.229403 80.024523 \n",
       "L 67.138494 36.398737 \n",
       "L 84.047585 12.240005 \n",
       "L 100.956676 24.59076 \n",
       "L 117.865767 96.646055 \n",
       "L 134.774858 113.04 \n",
       "L 151.683949 67.314674 \n",
       "L 168.59304 32.236661 \n",
       "L 185.502131 31.318277 \n",
       "L 202.411222 51.618841 \n",
       "\" clip-path=\"url(#pa999ffb7f2)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #800080; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 50.229403 80.024523 \n",
       "L 67.138494 36.398737 \n",
       "L 84.047585 12.240005 \n",
       "L 100.956676 24.59076 \n",
       "L 117.865767 96.646055 \n",
       "L 134.774858 113.04 \n",
       "L 151.683949 67.314674 \n",
       "L 168.59304 32.236661 \n",
       "L 185.502131 31.318277 \n",
       "L 202.411222 51.618841 \n",
       "\" clip-path=\"url(#pa999ffb7f2)\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #00ffff; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 42.620312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 7.2 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 49.620312 93.913438 \n",
       "L 133.870313 93.913438 \n",
       "Q 135.870313 93.913438 135.870313 91.913438 \n",
       "L 135.870313 33.366563 \n",
       "Q 135.870313 31.366563 133.870313 31.366563 \n",
       "L 49.620312 31.366563 \n",
       "Q 47.620312 31.366563 47.620312 33.366563 \n",
       "L 47.620312 91.913438 \n",
       "Q 47.620312 93.913438 49.620312 93.913438 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_13\">\n",
       "     <path d=\"M 51.620312 39.465 \n",
       "L 61.620312 39.465 \n",
       "L 71.620312 39.465 \n",
       "\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- True -->\n",
       "     <g transform=\"translate(79.620312 42.965) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"87.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"150.826172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_14\">\n",
       "     <path d=\"M 51.620312 54.143125 \n",
       "L 61.620312 54.143125 \n",
       "L 71.620312 54.143125 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- NW_PY -->\n",
       "     <g transform=\"translate(79.620312 57.643125) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \n",
       "L 1478 4666 \n",
       "L 3547 763 \n",
       "L 3547 4666 \n",
       "L 4159 4666 \n",
       "L 4159 0 \n",
       "L 3309 0 \n",
       "L 1241 3903 \n",
       "L 1241 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-57\" d=\"M 213 4666 \n",
       "L 850 4666 \n",
       "L 1831 722 \n",
       "L 2809 4666 \n",
       "L 3519 4666 \n",
       "L 4500 722 \n",
       "L 5478 4666 \n",
       "L 6119 4666 \n",
       "L 4947 0 \n",
       "L 4153 0 \n",
       "L 3169 4050 \n",
       "L 2175 0 \n",
       "L 1381 0 \n",
       "L 213 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-59\" d=\"M -13 4666 \n",
       "L 666 4666 \n",
       "L 1959 2747 \n",
       "L 3244 4666 \n",
       "L 3922 4666 \n",
       "L 2272 2222 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 2222 \n",
       "L -13 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-4e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-57\" x=\"74.804688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"173.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-50\" x=\"223.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-59\" x=\"281.734375\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\">\n",
       "     <path d=\"M 51.620312 69.099375 \n",
       "L 61.620312 69.099375 \n",
       "L 71.620312 69.099375 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #800080; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- NW_PYT -->\n",
       "     <g transform=\"translate(79.620312 72.599375) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-4e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-57\" x=\"74.804688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"173.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-50\" x=\"223.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-59\" x=\"281.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-54\" x=\"342.818359\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <path d=\"M 51.620312 84.055625 \n",
       "L 61.620312 84.055625 \n",
       "L 71.620312 84.055625 \n",
       "\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #00ffff; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- NW_PYT_B -->\n",
       "     <g transform=\"translate(79.620312 87.555625) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-42\" d=\"M 1259 2228 \n",
       "L 1259 519 \n",
       "L 2272 519 \n",
       "Q 2781 519 3026 730 \n",
       "Q 3272 941 3272 1375 \n",
       "Q 3272 1813 3026 2020 \n",
       "Q 2781 2228 2272 2228 \n",
       "L 1259 2228 \n",
       "z\n",
       "M 1259 4147 \n",
       "L 1259 2741 \n",
       "L 2194 2741 \n",
       "Q 2656 2741 2882 2914 \n",
       "Q 3109 3088 3109 3444 \n",
       "Q 3109 3797 2882 3972 \n",
       "Q 2656 4147 2194 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2241 4666 \n",
       "Q 2963 4666 3353 4366 \n",
       "Q 3744 4066 3744 3513 \n",
       "Q 3744 3084 3544 2831 \n",
       "Q 3344 2578 2956 2516 \n",
       "Q 3422 2416 3680 2098 \n",
       "Q 3938 1781 3938 1306 \n",
       "Q 3938 681 3513 340 \n",
       "Q 3088 0 2303 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-4e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-57\" x=\"74.804688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"173.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-50\" x=\"223.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-59\" x=\"281.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-54\" x=\"342.818359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"403.902344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-42\" x=\"453.902344\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pa999ffb7f2\">\n",
       "   <rect x=\"42.620312\" y=\"7.2\" width=\"167.4\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import torch \n",
    "import time\n",
    "\n",
    "\n",
    "def Nadraya_Watson_kernel(x, y):\n",
    "    '''用Python的for循环实现Nadraya-Watson核回归'''\n",
    "    y_hat = []\n",
    "    for x_i, y_i in zip(x, y):        \n",
    "        div = 0 \n",
    "        for i in x:\n",
    "            div_term = 0\n",
    "            for j in x:\n",
    "                div_term += torch.exp(-0.5 * (x_i - j)** 2)\n",
    "            div_up = torch.exp(-0.5 * (x_i - i) ** 2)\n",
    "            div += (div_up / div_term) * y_i\n",
    "        y_hat.append(div)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def Nadraya_Watson_kernel_matrix(x, y):\n",
    "    '''用Pytorch的矩阵（广播之后的）实现Nadraya-Watson核回归'''\n",
    "    sub = x.unsqueeze(dim=1).repeat(1, len(x)) - x.repeat(len(x), 1)\n",
    "    attention_scores = torch.exp(-0.5 * sub** 2)\n",
    "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "    attention_output = (attention_weights * y.unsqueeze(dim=1).repeat(1, len(x))).sum(dim=-1)\n",
    "    return attention_output\n",
    "\n",
    "\n",
    "def Nadraya_Watson_kernel_broadcast(x, y):\n",
    "    '''用Pytorch的广播实现Nadraya-Watson核回归'''\n",
    "    sub = x.unsqueeze(dim=1) - x.unsqueeze(dim=0)\n",
    "    attention_scores = torch.exp(-0.5 * sub**2)\n",
    "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "    attention_output = (attention_weights * y.unsqueeze(dim=1)).sum(dim=-1)\n",
    "    return attention_output\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "t1 = time.time()\n",
    "# 很慢\n",
    "plt.plot(x, Nadraya_Watson_kernel(x, y), color='green', linestyle='--', label='NW_PY')\n",
    "t2 = time.time()\n",
    "# 展开后，速度快\n",
    "plt.plot(x, Nadraya_Watson_kernel_matrix(x, y), color='purple', linestyle='-.', label='NW_PYT')\n",
    "t3 = time.time()\n",
    "# 广播后，速度更快\n",
    "plt.plot(x, Nadraya_Watson_kernel_broadcast(x, y), color='cyan', linestyle=':', label='NW_PYT_B')\n",
    "t4 = time.time()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "\n",
    "print(f'NW_PY: {t2 - t1} s, NW_PYT: {t3 - t2} s, NW_PYT_B: {t4 - t3} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.4. <a id='toc11_5_4_'></a>[参数注意力汇聚（Attention Pooling）-计算q和k相似度](#toc0_)\n",
    "加入可学习的参数w。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99dadd60f0>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"218.38125pt\" height=\"166.234687pt\" viewBox=\"0 0 218.38125 166.234687\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T12:22:37.789367</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 166.234687 \n",
       "L 218.38125 166.234687 \n",
       "L 218.38125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 128.678437 \n",
       "L 211.18125 128.678437 \n",
       "L 211.18125 17.798437 \n",
       "L 43.78125 17.798437 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m4e5ee7fb55\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4e5ee7fb55\" x=\"51.390341\" y=\"128.678437\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(48.209091 143.276875) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4e5ee7fb55\" x=\"89.435795\" y=\"128.678437\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(86.254545 143.276875) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4e5ee7fb55\" x=\"127.48125\" y=\"128.678437\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(124.3 143.276875) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4e5ee7fb55\" x=\"165.526705\" y=\"128.678437\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(162.345455 143.276875) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4e5ee7fb55\" x=\"203.572159\" y=\"128.678437\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(200.390909 143.276875) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(112.253125 156.955) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m8209c582ae\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8209c582ae\" x=\"43.78125\" y=\"128.476916\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 6.8 -->\n",
       "      <g transform=\"translate(20.878125 132.276135) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8209c582ae\" x=\"43.78125\" y=\"100.454519\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 7.0 -->\n",
       "      <g transform=\"translate(20.878125 104.253737) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8209c582ae\" x=\"43.78125\" y=\"72.432121\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 7.2 -->\n",
       "      <g transform=\"translate(20.878125 76.23134) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8209c582ae\" x=\"43.78125\" y=\"44.409723\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 7.4 -->\n",
       "      <g transform=\"translate(20.878125 48.208942) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 82.89625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- 1e−14 -->\n",
       "     <g transform=\"translate(43.78125 14.798437) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"63.623047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2212\" x=\"125.146484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" x=\"208.935547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-34\" x=\"272.558594\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_10\">\n",
       "    <path d=\"M 51.390341 73.238438 \n",
       "L 89.435795 73.238438 \n",
       "L 127.48125 73.238438 \n",
       "L 165.526705 73.238438 \n",
       "L 203.572159 73.238438 \n",
       "\" clip-path=\"url(#p5a36e9fa0c)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 128.678438 \n",
       "L 43.78125 17.798438 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 211.18125 128.678438 \n",
       "L 211.18125 17.798438 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 128.678437 \n",
       "L 211.18125 128.678437 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 17.798437 \n",
       "L 211.18125 17.798437 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 152.865625 40.476562 \n",
       "L 204.18125 40.476562 \n",
       "Q 206.18125 40.476562 206.18125 38.476562 \n",
       "L 206.18125 24.798437 \n",
       "Q 206.18125 22.798437 204.18125 22.798437 \n",
       "L 152.865625 22.798437 \n",
       "Q 150.865625 22.798437 150.865625 24.798437 \n",
       "L 150.865625 38.476562 \n",
       "Q 150.865625 40.476562 152.865625 40.476562 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_11\">\n",
       "     <path d=\"M 154.865625 30.896875 \n",
       "L 164.865625 30.896875 \n",
       "L 174.865625 30.896875 \n",
       "\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(182.865625 34.396875) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5a36e9fa0c\">\n",
       "   <rect x=\"43.78125\" y=\"17.798437\" width=\"167.4\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "class Nadraya_Watson_kernel_w(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.randn(size=(1,), requires_grad=True))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        sub = x.unsqueeze(dim=1) - x.unsqueeze(dim=0)\n",
    "        attention_scores = torch.exp(-0.5 * (sub * self.w)** 2)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_output = (attention_weights * y.unsqueeze(dim=1)).sum(dim=-1)\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "net = Nadraya_Watson_kernel_w()\n",
    "loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001) \n",
    "\n",
    "\n",
    "epochs = 5\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad()\n",
    "    y_hat = net(x, y)\n",
    "    loss = loss_fn(y_hat, y).sum()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    # print(f'epoch {epoch + 1}, loss {loss:.3f}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(loss_list, color='blue', linestyle='-', label='loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"217.220312pt\" height=\"155.63625pt\" viewBox=\"0 0 217.220312 155.63625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T12:22:47.985900</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 155.63625 \n",
       "L 217.220312 155.63625 \n",
       "L 217.220312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "L 42.620312 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"mc169d8bc7b\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p9a897dc54a)\">\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"50.229403\" y=\"80.024525\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"67.138494\" y=\"36.398732\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"84.047585\" y=\"12.24\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"100.956676\" y=\"24.590756\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"117.865767\" y=\"96.646056\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"134.774858\" y=\"113.04\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"151.683949\" y=\"67.314675\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"168.59304\" y=\"32.236658\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"185.502131\" y=\"31.318269\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc169d8bc7b\" x=\"202.411222\" y=\"51.618842\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mbf05ae8b15\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbf05ae8b15\" x=\"50.229403\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(47.048153 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbf05ae8b15\" x=\"84.047585\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(80.866335 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbf05ae8b15\" x=\"117.865767\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(114.684517 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbf05ae8b15\" x=\"151.683949\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(148.502699 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbf05ae8b15\" x=\"185.502131\" y=\"118.08\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(182.320881 132.678438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(123.360938 146.356562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"me2a878ab05\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#me2a878ab05\" x=\"42.620312\" y=\"112.891122\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(20.878125 116.690341) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me2a878ab05\" x=\"42.620312\" y=\"69.938938\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(29.257812 73.738157) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me2a878ab05\" x=\"42.620312\" y=\"26.986754\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(29.257812 30.785973) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.798437 71.261094) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_9\">\n",
       "    <path d=\"M 50.229403 69.938938 \n",
       "L 67.138494 33.795923 \n",
       "L 84.047585 30.882628 \n",
       "L 100.956676 63.877526 \n",
       "L 117.865767 102.445258 \n",
       "L 134.774858 111.126831 \n",
       "L 151.683949 81.940444 \n",
       "L 168.59304 41.719929 \n",
       "L 185.502131 27.443841 \n",
       "L 202.411222 52.237549 \n",
       "\" clip-path=\"url(#p9a897dc54a)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_10\">\n",
       "    <path d=\"M 50.229403 80.024525 \n",
       "L 67.138494 36.398732 \n",
       "L 84.047585 12.240005 \n",
       "L 100.956676 24.590756 \n",
       "L 117.865767 96.646059 \n",
       "L 134.774858 113.04 \n",
       "L 151.683949 67.314675 \n",
       "L 168.59304 32.236658 \n",
       "L 185.502131 31.318269 \n",
       "L 202.411222 51.618842 \n",
       "\" clip-path=\"url(#p9a897dc54a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 42.620312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 210.020313 118.08 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.620312 118.08 \n",
       "L 210.020313 118.08 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.620312 7.2 \n",
       "L 210.020313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 49.620312 78.957188 \n",
       "L 122.009375 78.957188 \n",
       "Q 124.009375 78.957188 124.009375 76.957188 \n",
       "L 124.009375 48.322812 \n",
       "Q 124.009375 46.322812 122.009375 46.322812 \n",
       "L 49.620312 46.322812 \n",
       "Q 47.620312 46.322812 47.620312 48.322812 \n",
       "L 47.620312 76.957188 \n",
       "Q 47.620312 78.957188 49.620312 78.957188 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_11\">\n",
       "     <path d=\"M 51.620312 54.42125 \n",
       "L 61.620312 54.42125 \n",
       "L 71.620312 54.42125 \n",
       "\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- True -->\n",
       "     <g transform=\"translate(79.620312 57.92125) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"87.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"150.826172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_12\">\n",
       "     <path d=\"M 51.620312 69.099375 \n",
       "L 61.620312 69.099375 \n",
       "L 71.620312 69.099375 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- NW_PYT -->\n",
       "     <g transform=\"translate(79.620312 72.599375) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \n",
       "L 1478 4666 \n",
       "L 3547 763 \n",
       "L 3547 4666 \n",
       "L 4159 4666 \n",
       "L 4159 0 \n",
       "L 3309 0 \n",
       "L 1241 3903 \n",
       "L 1241 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-57\" d=\"M 213 4666 \n",
       "L 850 4666 \n",
       "L 1831 722 \n",
       "L 2809 4666 \n",
       "L 3519 4666 \n",
       "L 4500 722 \n",
       "L 5478 4666 \n",
       "L 6119 4666 \n",
       "L 4947 0 \n",
       "L 4153 0 \n",
       "L 3169 4050 \n",
       "L 2175 0 \n",
       "L 1381 0 \n",
       "L 213 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-59\" d=\"M -13 4666 \n",
       "L 666 4666 \n",
       "L 1959 2747 \n",
       "L 3244 4666 \n",
       "L 3922 4666 \n",
       "L 2272 2222 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 2222 \n",
       "L -13 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-4e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-57\" x=\"74.804688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"173.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-50\" x=\"223.681641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-59\" x=\"281.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-54\" x=\"342.818359\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p9a897dc54a\">\n",
       "   <rect x=\"42.620312\" y=\"7.2\" width=\"167.4\" height=\"110.88\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, f(x), color='red', linestyle='-', label='True')\n",
    "    plt.plot(x, net(x, y), color='green', linestyle='-.', label='NW_PYT')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2220], requires_grad=True)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学习的参数w\n",
    "net.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.5. <a id='toc11_5_5_'></a>[注意力分数函数-计算q和k相似度](#toc0_)\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention/Attention_score.jpg\" width = \"500\" height = \"300\" alt=\"图片名称\" align=center />\n",
    "\n",
    "原理：\n",
    "- 本质上`Attention机制是Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数`；\n",
    "- 首先利用`注意力分数函数`计算`Query`和`Key`的`相似度 (注意力分数)`；\n",
    "- 利用`softmax`计算相似度 (注意力分数)后，得到`加权数值` (`注意力权重`，query和key越相似该权重越大，即获取的注意力越大)；\n",
    "- 利用注意力权重对value进行`加权求和`，即最终的`注意力值`。\n",
    "\n",
    "解释：\n",
    "  |注释|公式|\n",
    "  |:-|:-|\n",
    "  |注意力评分函数|$a(q, k)$|\n",
    "  |注意力权重|$softmax( a(q, k) )$|\n",
    "  |注意力|$softmax( a(q, k) ) * v$|\n",
    "\n",
    "注意力评分函数：\n",
    "  - `加性注意力 (Additive Attention)`\n",
    "  - `缩放点积注意力 (Scaled Dot-Product Attention)`\n",
    "  - 乘性注意力 (Multiplicative Attention)\n",
    "  - 位置注意力 (Location-based Attention)\n",
    "  - 线性注意力 (Linear Attention)\n",
    "  - 自适应注意力 (Adaptive Attention)\n",
    "  - 稀疏注意力 (Sparse Attention)\n",
    "\n",
    "总结: \n",
    " - 不同的注意力机制在计算注意力分数时采用了不同的方法，以适应不同的任务需求和计算资源。选择合适的注意力机制可以提高模型的性能和效率。随着研究的不断深入，新的注意力机制也在不断涌现，为各种应用场景提供了更多的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.5.5.1. <a id='toc11_5_5_1_'></a>[加性注意力 (Additive Attention)-计算q、k相似度](#toc0_)\n",
    "加性注意力最早由 Bahdanau 等人在 2014 年的论文中提出，通常用于序列到序列模型中。其核心思想是通过一个小型的前馈神经网络来计算注意力权重。\n",
    "计算过程:\n",
    "  - 对于给定的查询（query）和键（key），首先通过线性变换将它们`映射到相同的维度`。\n",
    "  - 将映射后的查询和键`相加`，并通过一个激活函数（如 tanh）进行`非线性变换`。\n",
    "  - 使用一个`可学习的参数向量`对变换后的结果进行`线性变换`，得到注意力得分。\n",
    "  - 对所有注意力得分进行 softmax 操作，得到注意力权重。\n",
    "\n",
    "公式：$\\mathrm{score}(q,k)=v^T\\cdot\\mathrm{tanh}(W_qq+W_kk)$，其中，$W_q$和$W_k$是可学习的线性变换矩阵，$v$是可学习的参数向量。  \n",
    "​\n",
    "优点：\n",
    "  - 能够处理不同维度的查询和键。\n",
    "  - 适用于较小的序列长度。\n",
    "\n",
    "缺点：\n",
    "  - 计算复杂度较高，尤其在序列长度较大时。\n",
    "\n",
    "---\n",
    "\n",
    "动手学深度学习：\n",
    "- 当`查询`和`键`是`不同长度`的`矢量`时，可以使用`加性注意力作为评分函数`。\n",
    "- 注意力评分函数：$a(\\mathbf{q},\\mathbf{k})=\\mathbf{w}_v^\\top\\tanh(\\mathbf{W}_q\\mathbf{q}+\\mathbf{W}_k\\mathbf{k})\\in\\mathbb{R}$\n",
    "- $\\mathbf{q}\\in\\mathbb{R}^q\\text{和 键}\\mathbf{k}\\in\\mathbb{R}^k,$\n",
    "- $\\mathbf{W}_q\\in\\mathbb{R}^{h\\times q}\\mathrm{、}\\mathbf{W}_k\\in\\mathbb{R}^{h\\times k}\\text{和 }\\mathbf{w}_v\\in\\mathbb{R}^h$, `投影`到`相同维度h`上\n",
    "- `有可学习的参数`，效果会好一些。\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention/Additive_attention.jpg\" width = \"800\" height = \"600\"/>\n",
    "\n",
    "\n",
    "- 使用:\n",
    "```python\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num, key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "            # query_size, key_size, value_size 可以不一样\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参考案例-李沐\n",
    "  - 带有掩码 (masked) 和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"\n",
    "    为序列生成掩码，将无效/填充位置的值替换为指定值\n",
    "    \n",
    "    参数:\n",
    "        X: 输入张量，形状为 (batch_size, seq_len, ...)\n",
    "        valid_len: 每个序列的有效长度，形状为 (batch_size,)\n",
    "        value: 用于替换无效位置的值，默认为0\n",
    "        \n",
    "    返回:\n",
    "        掩码后的张量，形状与输入X相同\n",
    "        \n",
    "    实现步骤:\n",
    "        1. 获取序列最大长度maxlen\n",
    "        2. 生成掩码矩阵:\n",
    "           - torch.arange生成[0,1,...,maxlen-1]\n",
    "           - [None,:]增加batch维度变为(1,maxlen) \n",
    "           - valid_len[:,None]将(batch_size,)变为(batch_size,1)\n",
    "           - 比较生成(batch_size,maxlen)的布尔掩码\n",
    "        3. 将~mask位置(无效位置)的值替换为value\n",
    "    \"\"\"\n",
    "    # 获取序列最大长度\n",
    "    maxlen = X.size(1)\n",
    "    \n",
    "    # 生成掩码矩阵: (batch_size, maxlen)\n",
    "    # 其中True表示有效位置,False表示无效位置\n",
    "\n",
    "    ## 实现方式一：\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    ## 实现方式二：\n",
    "    # mask = torch.arange(maxlen, dtype=torch.float32, device=X.device).reshape(1, -1) < valid_len.reshape(-1, 1)\n",
    "    \n",
    "    # 将无效位置(~mask)替换为value，取反\n",
    "    X[~mask] = value\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "#@save\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"\n",
    "    通过在最后一个轴上掩蔽元素来执行softmax操作     \n",
    "    参数:\n",
    "        X: 3D张量, shape为(batch_size, seq_len, feature_dim)\n",
    "        valid_lens: 1D或2D张量,指定每个序列的有效长度\n",
    "            - 1D时shape为(batch_size,),表示每个batch中所有序列的有效长度\n",
    "            - 2D时shape为(batch_size, seq_len),可以为每个序列的每个位置指定不同的有效长度\n",
    "    返回:\n",
    "        经过masked softmax后的张量,shape与输入X相同\n",
    "    \"\"\"\n",
    "    # 如果没有指定valid_lens,直接在最后一维上做softmax\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        # 如果valid_lens是1D\n",
    "        if valid_lens.dim() == 1:\n",
    "            ## 以dim=0方向按元素个数重复shape[1] (seq_len) 次, \n",
    "            ## e.g., [1, 2] -> [1, 1, 1, 2, 2, 2]\n",
    "            valid_lens = torch.repeat_interleave(input=valid_lens, repeats=shape[1])\n",
    "        else:\n",
    "            # 如果是2D,按顺序将其展平为1D\n",
    "            ## e.g., [[2, 3, 4], [3, 2, 1]] -> [2, 3, 4, 3, 2, 1]\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "            \n",
    "        # 使用sequence_mask生将超出有效长度的位置用一个很小的负值(-1e6)或者-inf替换,使其softmax后接近0\n",
    "        ## X重塑为:(batch_size * seq_len, feature_dim)  \n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "\n",
    "        # 重塑回原始形状并做softmax\n",
    "        ## X: (batch_size, seq_len, feature_dim)\n",
    "        output = nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "        return output \n",
    "\n",
    "\n",
    "#@save\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)         # (key_size, num_hiddens)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)       # (query_size, num_hiddens)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)                # (num_hiddens, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # queries:              (batch_size, num_query, num_hiddens)\n",
    "        # keys:                 (batch_size, k_v_pair_num, num_hiddens)\n",
    "\n",
    "        # 在维度扩展后，使用广播方式进行求和\n",
    "        # queries的形状：       (batch_size，num_query，        1，        num_hiddens)\n",
    "        # key的形状：           (batch_size，    1，    k_v_pair_num，  num_hiddens)\n",
    "        # (batch_size, num_query, 1, num_hiddens) + (batch_size, 1, k_v_pair_num, num_hiddens) \n",
    "        # = (batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # features的形状：(batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        \n",
    "        # self.w_v: (num_hiddens, 1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num, 1)\n",
    "        # 移除最后一个维度squeeze(-1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "\n",
    "        # 注意力权重\n",
    "        ## 使用masked_softmax计算注意力权重, 有效长度为valid_lens\n",
    "        ## attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        ## 使用dropout\n",
    "        self.attention_weights = self.dropout(self.attention_weights)\n",
    "\n",
    "        # 注意力输出值\n",
    "        ## values的形状：(batch_size，k_v_pair_num，value_size)\n",
    "        attention_output = torch.bmm(self.attention_weights, values)\n",
    "\n",
    "        return attention_output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 2\n",
    "num_query, query_size = 1, 20\n",
    "k_v_pair_num, key_size, value_size = 10, 2, 4\n",
    "\n",
    "\n",
    "# 查询的小批量 (batch_size, num_query, query_size)\n",
    "# queries = torch.normal(mean=0, std=1, size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "# 键的小批量 (batch_size, k_v_pair_num, key_size)\n",
    "keys = torch.ones(size=(batch_size, k_v_pair_num, key_size))\n",
    "# 值的小批量 (batch_size, k_v_pair_num, value_size)\n",
    "values = torch.randn(size=(batch_size, k_v_pair_num, value_size))\n",
    "\n",
    "# 每个batch中序列的有效长度 (batch_size,)\n",
    "valid_lens = torch.tensor([2, 6])   # 每个batch中序列的有效长度\n",
    "\n",
    "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=False)\n",
    "attention.eval()\n",
    "\n",
    "attention(queries=queries, keys=keys, values=values, valid_lens=valid_lens).shape\n",
    "\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"389.729474pt\" height=\"98.547142pt\" viewBox=\"0 0 389.729474 98.547142\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:32:50.154858</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 98.547142 \n",
       "L 389.729474 98.547142 \n",
       "L 389.729474 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "L 52.160938 32.049318 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p55b15169f7)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAu0lEQVR4nO3VMQ2AQADAQEAGMwJwgX8VqCBgoWHgQ3KnoFvn69zuiVeOdR+dAPCpZXQAAP9hGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFA9gCBmwPoec3/KwAAAABJRU5ErkJggg==\" id=\"image37c0b2e04e\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-31.818918\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mbc9d573739\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbc9d573739\" x=\"66.445738\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbc9d573739\" x=\"123.584938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbc9d573739\" x=\"180.724138\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbc9d573739\" x=\"237.863338\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbc9d573739\" x=\"295.002538\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 88.89548) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"md3a86a407d\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a86a407d\" x=\"52.160938\" y=\"32.049318\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 35.848536) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a86a407d\" x=\"52.160938\" y=\"46.334118\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 50.133336) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a86a407d\" x=\"52.160938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 64.418136) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 60.768493) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 52.160938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 32.049318 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 0 -->\n",
       "    <g transform=\"translate(172.344875 26.049318) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 355.712938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABtCAYAAABpyKvSAAAA3UlEQVR4nK2WUQ6DMAxD6yRtOdiOsPtfZBvln75KQR2flp8dCCropfcotysU9a6VkPssFjdwVsDZiZkRIBrgmPkAD2gfriQ+gjLRSZmDhmenbeKTthwpLWImzXnO23iCj3lFSxzEk7eZx0HM4w/EvdvM42V+Hn9on46QVTtnpnEqKmlcabwIRMRFOIruZ9Jp1B6Em4HolBnsBLH6D5wCJ2Y2AxydC/wLTm6Hok543cXBedgn6USxE96Ubhe2b+GNh4fMg4tomwXETq93ExzUXfCNq3CIRBV8yrvm/5ALjElbuhEjQ6YAAAAASUVORK5CYII=\" id=\"imagedfc6346656\" transform=\"scale(1 -1) translate(0 -78.48)\" x=\"355.68\" y=\"-6.48\" width=\"3.6\" height=\"78.48\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"madfbcb69cd\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#madfbcb69cd\" x=\"359.626349\" y=\"85.468235\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(366.626349 89.267454) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#madfbcb69cd\" x=\"359.626349\" y=\"54.160941\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(366.626349 57.96016) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#madfbcb69cd\" x=\"359.626349\" y=\"22.853647\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(366.626349 26.652866) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 357.669643 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 357.669643 7.2 \n",
       "L 355.712938 7.2 \n",
       "L 355.712938 85.468235 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p55b15169f7\">\n",
       "   <rect x=\"52.160938\" y=\"32.049318\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"389.729474pt\" height=\"98.547142pt\" viewBox=\"0 0 389.729474 98.547142\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:32:50.283805</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 98.547142 \n",
       "L 389.729474 98.547142 \n",
       "L 389.729474 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "L 52.160938 32.049318 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pa90465991b)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAuklEQVR4nO3VMQ2AQADAQEAGMwJwgX8VqCBgodvnw52Cbl2f+3gXYCrXfo5O4Ke20QEAzMM0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DsAxlQA+iiPoU4AAAAAElFTkSuQmCC\" id=\"image08a7bbf018\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-31.818918\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"me248a24dfc\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#me248a24dfc\" x=\"66.445738\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me248a24dfc\" x=\"123.584938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me248a24dfc\" x=\"180.724138\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me248a24dfc\" x=\"237.863338\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me248a24dfc\" x=\"295.002538\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 88.89548) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m647b1ec80e\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m647b1ec80e\" x=\"52.160938\" y=\"32.049318\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 35.848536) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m647b1ec80e\" x=\"52.160938\" y=\"46.334118\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 50.133336) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m647b1ec80e\" x=\"52.160938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 64.418136) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 60.768493) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 52.160938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 32.049318 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 1 -->\n",
       "    <g transform=\"translate(172.344875 26.049318) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 355.712938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABtCAYAAABpyKvSAAAA3UlEQVR4nK2WUQ6DMAxD6yRtOdiOsPtfZBvln75KQR2flp8dCCropfcotysU9a6VkPssFjdwVsDZiZkRIBrgmPkAD2gfriQ+gjLRSZmDhmenbeKTthwpLWImzXnO23iCj3lFSxzEk7eZx0HM4w/EvdvM42V+Hn9on46QVTtnpnEqKmlcabwIRMRFOIruZ9Jp1B6Em4HolBnsBLH6D5wCJ2Y2AxydC/wLTm6Hok543cXBedgn6USxE96Ubhe2b+GNh4fMg4tomwXETq93ExzUXfCNq3CIRBV8yrvm/5ALjElbuhEjQ6YAAAAASUVORK5CYII=\" id=\"image5d107e55a0\" transform=\"scale(1 -1) translate(0 -78.48)\" x=\"355.68\" y=\"-6.48\" width=\"3.6\" height=\"78.48\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m20e13787f6\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m20e13787f6\" x=\"359.626349\" y=\"85.468235\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(366.626349 89.267454) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m20e13787f6\" x=\"359.626349\" y=\"38.507296\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.1 -->\n",
       "      <g transform=\"translate(366.626349 42.306514) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 357.669643 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 357.669643 7.2 \n",
       "L 355.712938 7.2 \n",
       "L 355.712938 85.468235 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pa90465991b\">\n",
       "   <rect x=\"52.160938\" y=\"32.049318\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''valid_lens：[2, 6], 切query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "for batch in range(attention.attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention.attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (详细) 从头手写-逐步分析“加性注意力机制代码”\n",
    "  - 无掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries size:  torch.Size([2, 1, 20])\n",
      "keys size:  torch.Size([2, 10, 2])\n",
      "values size:  torch.Size([2, 10, 4])\n",
      "Q size:  torch.Size([2, 1, 1, 4])\n",
      "K size:  torch.Size([2, 1, 10, 4])\n",
      "features size:  torch.Size([2, 1, 10, 4])\n",
      "features size (tanh):  torch.Size([2, 1, 10, 4])\n",
      "scores size:  torch.Size([2, 1, 10, 1])\n",
      "scores size squeeze:  torch.Size([2, 1, 10])\n",
      "attention_weights:  torch.Size([2, 1, 10])\n",
      "attention:  torch.Size([2, 1, 4])\n",
      "attention_wights_droputed size:  torch.Size([2, 1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 测试数据\n",
    "# ------------------------------------------------------\n",
    "batch_size = 2\n",
    "num_query = 1\n",
    "query_size = 20             # 一个query的向量长度\n",
    "\n",
    "num_key = 10                # “键－值”对的个数\n",
    "key_size = 2                # 一个key的向量长度\n",
    "\n",
    "num_value = num_key         # “键－值”对的个数\n",
    "value_size = 4              # 一个value的向量长度\n",
    "\n",
    "# queries = torch.randn(size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "print('queries size: ', queries.size())\n",
    "# batch_size, num_query, query_size\n",
    "# 2, 1, 20\n",
    "\n",
    "keys = torch.ones(size=(batch_size, num_key, key_size))\n",
    "print('keys size: ', keys.size())\n",
    "# batch_size, kv_pair_num, key_size\n",
    "# 2, 10, 2\n",
    "\n",
    "values = torch.randn(size=(batch_size, num_value, value_size))\n",
    "print('values size: ', values.size())\n",
    "# batch_size, kv_pair_num, value_size\n",
    "# 2, 10, 4\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 投影 (可学习的参数W)\n",
    "# ------------------------------------------------------\n",
    "## 全部投影到value_size一致的维度，便于计算\n",
    "num_hiddens = value_size\n",
    "bias = False\n",
    "W_q = nn.Linear(query_size, num_hiddens, bias=bias)     # 20 , 4\n",
    "W_k = nn.Linear(key_size, num_hiddens, bias=bias)       # 2, 4\n",
    "w_v = nn.Linear(num_hiddens, 1, bias=bias)              # 4, 1\n",
    "\n",
    "Q = W_q(queries)                    # (batch_size，查询的个数，num_hidden) 3维\n",
    "# 2, 1, 20 * 20, 4 = 2, 1, 4\n",
    "Q = Q.unsqueeze(2)                  # (batch_size，查询的个数，1，num_hidden) 插入一个维度  (重要) 4维\n",
    "# 2, 1, (1), 4                        # 为什么要插入一个维度？便于后续做广播\n",
    "print('Q size: ', Q.size())\n",
    "\n",
    "K = W_k(keys)                       # (batch_size，“键－值”对的个数，num_hiddens)   3维\n",
    "# 2, 10, 2 * 2, 4 = 2, 10, 4\n",
    "K = K.unsqueeze(1)                  # (batch_size，1，“键－值”对的个数，num_hiddens) 插入一个维度 (重要) 4维度\n",
    "# 2, (1), 10, 4                       # 为什么要插入一个维度？便于后续做广播\n",
    "print('K size: ', K.size())\n",
    "\n",
    "\n",
    "features = Q + K                    # 自动做广播后做加法    (重要)                                  (2,1,(1),4) + (2,(1),10,4) = (2,1,10,4)\n",
    "# 2, 1, 10, 4                       # (batch_size，查询个数，“键－值”对的个数，num_hiddens) 广播后   (2,1,(10),4)+ (2,(1),10,4) = (2,1,10,4)\n",
    "print('features size: ', features.size())\n",
    "\n",
    "features = torch.tanh(features)\n",
    "# 2, 1, 10, 4\n",
    "print('features size (tanh): ', features.size())\n",
    "\n",
    "scores = w_v(features)              # 自动做广播后做乘法    (2,1,10,4) @ (    4,1) = (2,1,10,1)\n",
    "                                    #                      (2,1,10,4) @ (2,1,4,1) = (2,1,10,1)\n",
    "# 2, 1, 10, 1\n",
    "print('scores size: ', scores.size())\n",
    "\n",
    "# w_v仅有一个输出，因此从形状中移除最后那个维度\n",
    "# scores的形状：(batch_size，查询的个数，“键-值”对的个数)\n",
    "scores = scores.squeeze(-1) \n",
    "# 2, 1, 10\n",
    "print('scores size squeeze: ', scores.size())\n",
    "\n",
    "attention_weights = torch.softmax(scores, dim=-1)\n",
    "# 2, 1, 10\n",
    "print('attention_weights: ', attention_weights.size())\n",
    "# attention_weights\n",
    "\n",
    "attention = torch.bmm(attention_weights, values)    # (2,1,10) @ (  10,4) = (2,1,4)\n",
    "                                                    # (2,1,10) @ (2,10,4) = (2,1,4) 广播后\n",
    "print('attention: ', attention.size())\n",
    "# batch_size, num_query, value_size\n",
    "# 2, 1, 4\n",
    "\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "\n",
    "\n",
    "# 添加dropout\n",
    "dropout = nn.Dropout(p=0.1)     # 0.1的概率失活\n",
    "attention_weights_droputed = dropout(attention_weights)\n",
    "# attention_weights_droputed.shape = 2, 1, 10    dropout后不改变attention_weights的形状\n",
    "print('attention_wights_droputed size: ', attention_weights_droputed.size())\n",
    "\n",
    "attention_droputed = torch.bmm(attention_weights_droputed, values)\n",
    "\n",
    "attention_droputed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 注意力权重可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:30:55.436434</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 -0 \n",
       "L 0 -0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p15c71f91a2)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"image6b4d99c5b7\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m5823275551\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5823275551\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5823275551\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5823275551\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5823275551\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5823275551\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mb4e1f95684\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb4e1f95684\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb4e1f95684\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb4e1f95684\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 0 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"imagea59dd754bd\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m8744adab3a\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8744adab3a\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8744adab3a\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p15c71f91a2\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:30:55.575718</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p0fe4d806c1)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"imageebc24989cf\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m1f0eca7b6c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f0eca7b6c\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f0eca7b6c\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f0eca7b6c\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f0eca7b6c\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f0eca7b6c\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mb6131dd1e9\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb6131dd1e9\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb6131dd1e9\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb6131dd1e9\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 1 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"imagedd5cd7f14e\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m7fe9aad535\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7fe9aad535\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7fe9aad535\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p0fe4d806c1\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''没有使用valid_lens, 且query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.5.5.2. <a id='toc11_5_5_2_'></a>[缩放点积注意力 (Scaled Dot-Product Attention)-计算q、k相似度](#toc0_)\n",
    "缩放点积注意力是由 Vaswani 等人在 2017 年的 Transformer 论文中提出的。它通过点积计算注意力得分，并对得分进行缩放以提高数值稳定性。\n",
    "计算过程：\n",
    "  - 对于给定的查询和键，计算它们的`点积`。\n",
    "  - 将点积结果`除以键的维度的平方根`，以避免数值过大。\n",
    "  - 对缩放后的得分进行 `softmax` 操作，得到注意力权重。\n",
    "\n",
    "公式：$\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax} (\\frac{QK^T}{\\sqrt{d_k}}) V$, 其中，$d_k$是键的维度。\n",
    "\n",
    "优点：\n",
    "  - 计算效率高，适合并行化。\n",
    "  - 在大多数现代深度学习模型中广泛使用。\n",
    "\n",
    "缺点：\n",
    "  - 对于不同维度的查询和键，需要进行额外的线性变换。\n",
    "\n",
    "---\n",
    "\n",
    "- q和k的长度`一致`，为`d`\n",
    "- 注意力评分函数：$a(\\mathbf{q},\\mathbf{k})=\\mathbf{q}^\\top\\mathbf{k}/\\sqrt{d}$\n",
    "- 向量版本注意力权重：$\\mathrm{softmax}\\left(\\frac{\\mathrm{QK}^\\top}{\\sqrt{d}}\\right)\\mathbf{V}\\in\\mathbb{R}^{n\\times v}$\n",
    "- $\\text{查询}\\mathbf{Q}\\in\\mathbb{R}^{n\\times d}\\text{、键}\\mathbf{K}\\in\\mathbb{R}^{m\\times d}\\text{和 值}\\mathbf{V}\\in\\mathbb{R}^{m\\times v}$\n",
    "- `无可学习`参数\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention/scale-dot-product.png\" width = \"300\" height = \"300\" alt=\"图片名称\" align=center />\n",
    "\n",
    "- 使用：\n",
    "```python\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "            # query_size, key_size必须一样，最好是等于value_size\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch简洁实现  \n",
    "query_size = key_size = value_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "batch_size, num_query, query_size = 2, 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4\n",
    "\n",
    "# queries = torch.randn(size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "keys = torch.ones(size=(batch_size, k_v_pair_num, key_size))\n",
    "values = torch.randn(size=(batch_size, k_v_pair_num, value_size))\n",
    "\n",
    "# num_heads = 1\n",
    "att = nn.MultiheadAttention(embed_dim=value_size, num_heads=1, batch_first=True)\n",
    "out, weights = att(queries, keys, values)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:35:26.379175</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 -0 \n",
       "L 0 -0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pc4b5900356)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"image93f7f2c5b3\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m0ba6ae3ec9\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0ba6ae3ec9\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0ba6ae3ec9\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0ba6ae3ec9\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0ba6ae3ec9\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0ba6ae3ec9\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mcf451fa3e7\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcf451fa3e7\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcf451fa3e7\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcf451fa3e7\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 0 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"image17eb226e51\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m1c4f905f58\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1c4f905f58\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1c4f905f58\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pc4b5900356\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:35:26.609484</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p274bb3ef3d)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"imaged4c76d48fd\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m848a2f7057\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m848a2f7057\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m848a2f7057\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m848a2f7057\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m848a2f7057\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m848a2f7057\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mcaf4ab1f78\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcaf4ab1f78\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcaf4ab1f78\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcaf4ab1f78\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 1 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"imagec085c27840\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m5b298ccb3f\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5b298ccb3f\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5b298ccb3f\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p274bb3ef3d\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''没有使用valid_lens, 且query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "for batch in range(weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 案例-李沐\n",
    "  - 掩码 (masked) 和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import math\n",
    "\n",
    "\n",
    "#@save\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    def __init__(self, query_size, key_size, value_size, num_hiddens, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # queries的形状：(batch_size，查询的个数，d)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens) \n",
    "        # keys的形状：(batch_size，“键－值”对的个数，d)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens)\n",
    "        # values的形状：(batch_size，“键－值”对的个数，值的维度)\n",
    "        self.w_v = nn.Linear(value_size, num_hiddens)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        queries, keys, values = self.W_q(queries), self.W_k(keys), self.w_v(values)\n",
    "        # queries: (batch_size, num_query, num_hiddens)\n",
    "        # keys: (batch_size, k_v_pair_num, num_hiddens)\n",
    "        # values: (batch_size, k_v_pair_num, value_size)\n",
    "        d = queries.shape[-1]\n",
    "\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        # (batch_size, num_query, num_hiddens) @ (batch_size, num_hiddens, k_v_pair_num) = (batch_size, num_query, k_v_pair_num)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "\n",
    "        # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n",
    "        # 使用masked_softmax计算注意力权重\n",
    "        # attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\n",
    "        # (batch_size, num_query, k_v_pair_num) @ (batch_size, k_v_pair_num, value_size) = (batch_size, num_query, value_size)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "    \n",
    "\n",
    "# Test   \n",
    "batch_size, num_query, query_size = 2, 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4\n",
    "num_hiddens = 8\n",
    "dropout = 0.1\n",
    "\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "keys = torch.ones(size=(batch_size, k_v_pair_num, key_size))\n",
    "values = torch.randn(size=(batch_size, k_v_pair_num, value_size))\n",
    "\n",
    "attention = DotProductAttention(query_size=query_size, key_size=key_size, value_size=value_size, num_hiddens=num_hiddens, dropout=dropout)\n",
    "attention.eval()\n",
    "\n",
    "# batch_size, 1\n",
    "## 依次的每个批次中所有num_steps有效个数一致\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention(queries, keys, values, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"389.729474pt\" height=\"98.547142pt\" viewBox=\"0 0 389.729474 98.547142\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:44:45.618494</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 98.547142 \n",
       "L 389.729474 98.547142 \n",
       "L 389.729474 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "L 52.160938 32.049318 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p2135d8c8d2)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAu0lEQVR4nO3VMQ2AQADAQEAGMwJwgX8VqCBgoWHgQ3KnoFvn69zuiVeOdR+dAPCpZXQAAP9hGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFAZhoAZKYBQGYaAGSmAUBmGgBkpgFA9gCBmwPoec3/KwAAAABJRU5ErkJggg==\" id=\"image1b0f416b86\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-31.818918\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m900254aa10\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m900254aa10\" x=\"66.445738\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m900254aa10\" x=\"123.584938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m900254aa10\" x=\"180.724138\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m900254aa10\" x=\"237.863338\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m900254aa10\" x=\"295.002538\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 88.89548) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m25b6947436\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m25b6947436\" x=\"52.160938\" y=\"32.049318\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 35.848536) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m25b6947436\" x=\"52.160938\" y=\"46.334118\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 50.133336) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m25b6947436\" x=\"52.160938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 64.418136) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 60.768493) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 52.160938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 32.049318 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 0 -->\n",
       "    <g transform=\"translate(172.344875 26.049318) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 355.712938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABtCAYAAABpyKvSAAAA3UlEQVR4nK2WUQ6DMAxD6yRtOdiOsPtfZBvln75KQR2flp8dCCropfcotysU9a6VkPssFjdwVsDZiZkRIBrgmPkAD2gfriQ+gjLRSZmDhmenbeKTthwpLWImzXnO23iCj3lFSxzEk7eZx0HM4w/EvdvM42V+Hn9on46QVTtnpnEqKmlcabwIRMRFOIruZ9Jp1B6Em4HolBnsBLH6D5wCJ2Y2AxydC/wLTm6Hok543cXBedgn6USxE96Ubhe2b+GNh4fMg4tomwXETq93ExzUXfCNq3CIRBV8yrvm/5ALjElbuhEjQ6YAAAAASUVORK5CYII=\" id=\"image0adb780b15\" transform=\"scale(1 -1) translate(0 -78.48)\" x=\"355.68\" y=\"-6.48\" width=\"3.6\" height=\"78.48\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m6c3d6a8e36\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6c3d6a8e36\" x=\"359.626349\" y=\"85.468235\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(366.626349 89.267454) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6c3d6a8e36\" x=\"359.626349\" y=\"54.160941\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(366.626349 57.96016) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6c3d6a8e36\" x=\"359.626349\" y=\"22.853647\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(366.626349 26.652866) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 357.669643 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 357.669643 7.2 \n",
       "L 355.712938 7.2 \n",
       "L 355.712938 85.468235 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2135d8c8d2\">\n",
       "   <rect x=\"52.160938\" y=\"32.049318\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"389.729474pt\" height=\"98.547142pt\" viewBox=\"0 0 389.729474 98.547142\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:44:45.741760</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 98.547142 \n",
       "L 389.729474 98.547142 \n",
       "L 389.729474 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "L 52.160938 32.049318 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p10b47c1dce)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAuklEQVR4nO3VMQ2AQADAQEAGMwJwgX8VqCBgodvnw52Cbl2f+3gXYCrXfo5O4Ke20QEAzMM0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DsAxlQA+iiPoU4AAAAAElFTkSuQmCC\" id=\"imageaaa177b7be\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-31.818918\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m8582e72ff5\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8582e72ff5\" x=\"66.445738\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8582e72ff5\" x=\"123.584938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8582e72ff5\" x=\"180.724138\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8582e72ff5\" x=\"237.863338\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8582e72ff5\" x=\"295.002538\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 75.217355) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 88.89548) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mfdacbf44e2\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdacbf44e2\" x=\"52.160938\" y=\"32.049318\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 35.848536) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdacbf44e2\" x=\"52.160938\" y=\"46.334118\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 50.133336) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdacbf44e2\" x=\"52.160938\" y=\"60.618918\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 64.418136) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 60.768493) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 52.160938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 60.618918 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 60.618918 \n",
       "L 337.856938 60.618918 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 32.049318 \n",
       "L 337.856938 32.049318 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 1 -->\n",
       "    <g transform=\"translate(172.344875 26.049318) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 355.712938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABtCAYAAABpyKvSAAAA3UlEQVR4nK2WUQ6DMAxD6yRtOdiOsPtfZBvln75KQR2flp8dCCropfcotysU9a6VkPssFjdwVsDZiZkRIBrgmPkAD2gfriQ+gjLRSZmDhmenbeKTthwpLWImzXnO23iCj3lFSxzEk7eZx0HM4w/EvdvM42V+Hn9on46QVTtnpnEqKmlcabwIRMRFOIruZ9Jp1B6Em4HolBnsBLH6D5wCJ2Y2AxydC/wLTm6Hok543cXBedgn6USxE96Ubhe2b+GNh4fMg4tomwXETq93ExzUXfCNq3CIRBV8yrvm/5ALjElbuhEjQ6YAAAAASUVORK5CYII=\" id=\"image56d402bc9a\" transform=\"scale(1 -1) translate(0 -78.48)\" x=\"355.68\" y=\"-6.48\" width=\"3.6\" height=\"78.48\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m7839b15b04\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7839b15b04\" x=\"359.626349\" y=\"85.468235\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(366.626349 89.267454) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7839b15b04\" x=\"359.626349\" y=\"38.507296\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.1 -->\n",
       "      <g transform=\"translate(366.626349 42.306514) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 85.468235 \n",
       "L 357.669643 85.468235 \n",
       "L 359.626349 85.468235 \n",
       "L 359.626349 7.2 \n",
       "L 357.669643 7.2 \n",
       "L 355.712938 7.2 \n",
       "L 355.712938 85.468235 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p10b47c1dce\">\n",
       "   <rect x=\"52.160938\" y=\"32.049318\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''valid_lens：[2, 6], 切query和key的1是相似的，所有注意力权重都平等'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for batch in range(attention.attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention.attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (详细) 从头手写\n",
    "  - 无掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries size:  torch.Size([2, 1, 4])\n",
      "keys size:  torch.Size([2, 10, 4])\n",
      "values size:  torch.Size([2, 10, 4])\n",
      "features size: torch.Size([2, 1, 10])\n",
      "scores size: torch.Size([2, 1, 10])\n",
      "attention_weights size: torch.Size([2, 1, 10])\n",
      "attention size: torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "# 示例数据  \n",
    "batch_size = 2\n",
    "num_query = 1\n",
    "query_size = 4             # 一个query的向量长度，d\n",
    "\n",
    "num_key = 10                    # “键－值”对的个数，m\n",
    "key_size = 4                # 一个key的向量长度，d\n",
    "\n",
    "num_value = num_key             # “键－值”对的个数，m\n",
    "value_size = 4              # 一个value的向量长度，v\n",
    "\n",
    "# queries = torch.randn(size=(batch_size, num_query, query_size))\n",
    "queries = torch.ones(size=(batch_size, num_query, query_size))\n",
    "print('queries size: ', queries.size())\n",
    "# (batch_size, num_query, query_size)\n",
    "# (2, 1, 20)\n",
    "\n",
    "keys = torch.ones(size=(batch_size, num_key, key_size))\n",
    "print('keys size: ', keys.size())\n",
    "# (batch_size, num_key, key_size)\n",
    "# (2, 10, 20)\n",
    "\n",
    "values = torch.randn(size=(batch_size, num_value, value_size))\n",
    "print('values size: ', values.size())\n",
    "# (batch_size, num_value, value_size)\n",
    "# (2, 10, 4)\n",
    "\n",
    "features = (queries @ keys.transpose(1, 2)) \n",
    "# features = torch.bmm(queries, keys.transpose(1, 2))               # 同上，都可以\n",
    "# (2, 1, 20) @ (2, 20, 10) = (2, 1, 10)\n",
    "print(f'features size: {features.shape}')\n",
    "\n",
    "scores = features / torch.sqrt(torch.tensor(queries.shape[2]))\n",
    "# (2, 1, 10)\n",
    "print(f'scores size: {scores.shape}')\n",
    "\n",
    "attention_weights = torch.softmax(scores, dim=-1)\n",
    "# (2, 1, 10) / 标量 = (2, 1, 10)\n",
    "print(f'attention_weights size: {attention_weights.shape}')\n",
    "\n",
    "attention = torch.bmm(attention_weights, values)\n",
    "# (2, 1, 4)\n",
    "print(f'attention size: {attention.shape}')\n",
    "\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:48:37.618814</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 -0 \n",
       "L 0 -0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p7bdae17f51)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"image93f5d8b652\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m24052d2f56\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24052d2f56\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24052d2f56\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24052d2f56\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24052d2f56\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24052d2f56\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m0344db93dc\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0344db93dc\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0344db93dc\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0344db93dc\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 0 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"image00d8048ed0\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m8b75365bed\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8b75365bed\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8b75365bed\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p7bdae17f51\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T14:48:37.783269</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pcb97a5d7bf)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"imaged71df6fa05\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"md641518be2\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md641518be2\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md641518be2\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md641518be2\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md641518be2\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md641518be2\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"mf87e693002\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf87e693002\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf87e693002\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf87e693002\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 1 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"image34677b58d5\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m9361651875\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9361651875\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9361651875\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pcb97a5d7bf\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.6. <a id='toc11_5_6_'></a>[自注意力机制-q、k和v相同](#toc0_)\n",
    "\n",
    "自注意力机制：就是用同一个`X`分别于`W_q`、`W_k`和`W_v`矩阵相乘得到`Q`、`K`和`V` `向量/矩阵`。因为用的是同一个X同时作为q、k和v，所以得名为 `自注意力` 。\n",
    "\n",
    "- 使用：\n",
    "```python\n",
    "# self-attention:                       queries = keys = values\n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import time \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "dbs = './Pytorch_datasets/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(dataset=train_dataset, batch_size=128,  shuffle=True)\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构1:CNN\n",
    "class MLPMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(28*28, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 10), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.network(X)\n",
    "    \n",
    "    \n",
    "# 构建模型2:Additive attention\n",
    "class SelfAttentionMNISTModel(nn.Module):\n",
    "    def __init__(self, attention_type):\n",
    "        super().__init__()\n",
    "        # 输入层\n",
    "        self.input = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU())\n",
    "\n",
    "        # 注意力层\n",
    "        if attention_type == 'add':\n",
    "            self.attention = AdditiveAttention(query_size=128, key_size=128, num_hiddens=128, dropout=False)\n",
    "        elif attention_type == 'dot':\n",
    "            self.attention = DotProductAttention(query_size=128, key_size=128, value_size=128, num_hiddens=128, dropout=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid attention type: {attention_type}\")\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 28*28)\n",
    "        x = self.input(x)\n",
    "        ## x: (batch_size, 128) -> (batch_size, 1, 128)\n",
    "        x = x.unsqueeze(1)  # dim=1增加一个维度，即表示一个query维度是(1, 128)\n",
    "\n",
    "        # 自注意力即q、k、v相同\n",
    "        ## queries: (batch_size, 1, 128)\n",
    "        ## keys: (batch_size, 1, 128)\n",
    "        ## values: (batch_size, 1, 128)\n",
    "        ## 输出维度(batch_size, num_query, value_size) -> (batch_size, 1, 128) -> (batch_size, 128)\n",
    "        x = self.attention(queries=x, keys=x, values=x).squeeze(1) # dim=1的维度取消，只剩128\n",
    "\n",
    "        # 输出层\n",
    "        ## x: (batch_size, 128) -> (batch_size, 10)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "    print('='*100, '\\n', f\"Runing on {device}\", '\\n','='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "    net = nn.DataParallel(module=net).to(device)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上      \n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)# 计算loss\n",
    "            loss.backward()         # 计算梯度\n",
    "            opt.step()              # 更新网络参数\n",
    "        net.eval() \n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=1.6284608840942383, train_acc=84.38166809082031, test_acc=84.72999572753906\n",
      "epoch 2/10: train_loss=1.5520281791687012, train_acc=92.09333801269531, test_acc=92.5999984741211\n",
      "epoch 3/10: train_loss=1.532891869544983, train_acc=93.72167205810547, test_acc=93.73999786376953\n",
      "epoch 4/10: train_loss=1.5224930047988892, train_acc=94.61666870117188, test_acc=94.38999938964844\n",
      "epoch 5/10: train_loss=1.5171533823013306, train_acc=95.10333251953125, test_acc=94.72000122070312\n",
      "epoch 6/10: train_loss=1.5102174282073975, train_acc=95.7249984741211, test_acc=95.1199951171875\n",
      "epoch 7/10: train_loss=1.5049973726272583, train_acc=96.2316665649414, test_acc=95.65999603271484\n",
      "epoch 8/10: train_loss=1.5015963315963745, train_acc=96.56500244140625, test_acc=96.04000091552734\n",
      "epoch 9/10: train_loss=1.4988125562667847, train_acc=96.78500366210938, test_acc=96.23999786376953\n",
      "epoch 10/10: train_loss=1.4956179857254028, train_acc=97.11166381835938, test_acc=96.43000030517578\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 3.93339204788208 s\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "net = MLPMNISTModel()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)  \n",
    "\n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10: train_loss=0.15667781233787537, train_acc=95.52166748046875, test_acc=95.30999755859375\n",
      "epoch 2/10: train_loss=0.10943424701690674, train_acc=96.82833862304688, test_acc=96.33000183105469\n",
      "epoch 3/10: train_loss=0.08049079775810242, train_acc=97.67333221435547, test_acc=97.1500015258789\n",
      "epoch 4/10: train_loss=0.06135227158665657, train_acc=98.26499938964844, test_acc=97.38999938964844\n",
      "epoch 5/10: train_loss=0.04637853428721428, train_acc=98.6866683959961, test_acc=97.50999450683594\n",
      "epoch 6/10: train_loss=0.03937253728508949, train_acc=98.89167022705078, test_acc=97.62999725341797\n",
      "epoch 7/10: train_loss=0.03307609632611275, train_acc=99.14167022705078, test_acc=97.72999572753906\n",
      "epoch 8/10: train_loss=0.0306411050260067, train_acc=99.13333892822266, test_acc=97.75\n",
      "epoch 9/10: train_loss=0.02732059732079506, train_acc=99.26499938964844, test_acc=97.72999572753906\n",
      "epoch 10/10: train_loss=0.02051992528140545, train_acc=99.53666687011719, test_acc=97.9699935913086\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 8.129523754119873 s\n"
     ]
    }
   ],
   "source": [
    "# AdditiveAttention\n",
    "net = SelfAttentionMNISTModel(attention_type='add')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)  \n",
    "\n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=0.1637280136346817, train_acc=94.80833435058594, test_acc=94.56999969482422\n",
      "epoch 2/10: train_loss=0.08692066371440887, train_acc=97.27999877929688, test_acc=96.77000427246094\n",
      "epoch 3/10: train_loss=0.06666596978902817, train_acc=97.86500549316406, test_acc=96.94999694824219\n",
      "epoch 4/10: train_loss=0.05956593155860901, train_acc=98.12000274658203, test_acc=96.87999725341797\n",
      "epoch 5/10: train_loss=0.03826556354761124, train_acc=98.80500030517578, test_acc=97.7699966430664\n",
      "epoch 6/10: train_loss=0.0370258092880249, train_acc=98.836669921875, test_acc=97.47999572753906\n",
      "epoch 7/10: train_loss=0.037295542657375336, train_acc=98.7066650390625, test_acc=97.29000091552734\n",
      "epoch 8/10: train_loss=0.027073809877038002, train_acc=99.1116714477539, test_acc=97.58999633789062\n",
      "epoch 9/10: train_loss=0.016789773479104042, train_acc=99.47167205810547, test_acc=97.77999877929688\n",
      "epoch 10/10: train_loss=0.029497196897864342, train_acc=99.05166625976562, test_acc=97.29000091552734\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 12.49781608581543 s\n"
     ]
    }
   ],
   "source": [
    "# DotProductAttention\n",
    "net = SelfAttentionMNISTModel(attention_type='dot')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)  \n",
    "\n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.7. <a id='toc11_5_7_'></a>[多头注意力机制-h个q、k和v对](#toc0_)\n",
    "上述只求一次注意力的过程可以叫做单头注意力。多头注意力就是对同样的Q, K, V求多次注意力，并行计算h个得到h个不同的attention，再把这些不同的h个attention连接起来得到最终的attentions，每一个attention都是一个head（头），总共有h个head（头）。  \n",
    "\n",
    "<img src=\"./Pytorch_Pictures/Attention/multi_head.jpg\" width = \"300\" height = \"350\" alt=\"多头注意力机制\" align=center />\n",
    "\n",
    "在实现过程中通常选择`缩放点积注意力`作为每一个注意力头，除以根号d可以使计算数值减小。\n",
    "\n",
    "```python\n",
    "# summary \n",
    "    # Input:\n",
    "            # queries:                  (batch_size, num_query, query_size)\n",
    "            # keys:                     (batch_size, k_v_pair_num,  key_size)\n",
    "            # values:                   (batch_size, k_v_pair_num, value_size)\n",
    "    # Output:                           (batch_size, num_query, value_size)\n",
    "\n",
    "# 先 transpose_input()\n",
    "# 后 transpose_output()\n",
    "# 终 self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=exitBias)   # 最后concat所有head的结果 (其实就是投影)\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 简洁实现  \n",
    "query_size = key_size = value_size = d  \n",
    "nn.MultiheadAttention(embed_dim=value_size, num_heads=num_heads, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 1, 4])\n",
      "注意力权重形状: torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 定义参数\n",
    "batch_size = 2\n",
    "query_num, query_size = 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4\n",
    "\n",
    "num_heads = 2\n",
    "# 创建多头注意力模块\n",
    "multihead_attention = nn.MultiheadAttention(embed_dim=value_size, num_heads=num_heads, batch_first=True)\n",
    "# 使用缩放点积注意力机制，所以 query_size = key_size = value_size = d\n",
    "\n",
    "# 创建输入张量\n",
    "# 输入形状为 (批次大小, 序列长度, 嵌入维度)\n",
    "query = torch.ones(batch_size, query_num, query_size)\n",
    "key = torch.ones(batch_size, k_v_pair_num, key_size)\n",
    "value = torch.rand(batch_size, k_v_pair_num, value_size)\n",
    "\n",
    "# 可选的注意力掩码\n",
    "# mask = torch.zeros(batch_size, seq_length, seq_length).type(torch.bool)\n",
    "\n",
    "# 计算多头注意力\n",
    "# 如果需要掩码，可以传入 mask 参数\n",
    "output, attention_weights = multihead_attention(query, key, value)\n",
    "\n",
    "print(\"输出形状:\", output.shape)  # 输出形状: (batch_size, query_num, value_size)\n",
    "print(\"注意力权重形状:\", attention_weights.shape)  # 注意力权重形状: (batch_size, num_query, k_v_pair_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T15:01:54.504646</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 -0 \n",
       "L 0 -0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pce2502b9eb)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"image8b9140ecac\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m48ddec5d8d\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m48ddec5d8d\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m48ddec5d8d\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m48ddec5d8d\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m48ddec5d8d\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m48ddec5d8d\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m2c98e12bea\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c98e12bea\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c98e12bea\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2c98e12bea\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 0 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"imagec29139692a\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"mcb4f18ab42\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcb4f18ab42\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mcb4f18ab42\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pce2502b9eb\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T15:01:54.636712</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p2542ad3aa2)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"imageb331e5856b\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m94db934255\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m94db934255\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m94db934255\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m94db934255\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m94db934255\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m94db934255\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m1cd02930cf\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1cd02930cf\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1cd02930cf\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1cd02930cf\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 1 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"image3f1ba1134f\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m521f789f77\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m521f789f77\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m521f789f77\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2542ad3aa2\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 案例-李沐 (修改)\n",
    "  - 去除掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (W_q): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (W_k): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (W_v): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (attention): AdditiveAttentionForMultiHeadAttention(\n",
      "    (dropout): Dropout(p=False, inplace=False)\n",
      "    (w_v): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "raw queries size:  torch.Size([2, 1, 4])\n",
      "raw keys size:  torch.Size([2, 10, 4])\n",
      "raw values size:  torch.Size([2, 10, 4])\n",
      "attention_values size:  torch.Size([2, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn  \n",
    "import math \n",
    "\n",
    "\n",
    "# 加性注意力\n",
    "class AdditiveAttentionForMultiHeadAttention(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # queries:              (batch_size, num_query, num_hiddens)\n",
    "        # keys:                 (batch_size, k_v_pair_num, num_hiddens)\n",
    "        # 在维度扩展后，\n",
    "        # queries的形状：       (batch_size，num_query，        1，        num_hiddens)\n",
    "        # key的形状：           (batch_size，    1，    k_v_pair_num，  num_hiddens)\n",
    "        # 使用广播方式进行求和  (batch_size, num_query, 1, num_hiddens) + (batch_size, 1, k_v_pair_num, num_hiddens) = (batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # features的形状：(batch_size, num_query, k_v_pair_num, num_hiddens)\n",
    "        \n",
    "        # self.w_v: (num_hiddens, 1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num, 1)\n",
    "        # 移除最后一个维度squeeze(-1)\n",
    "        # scores的形状：(batch_size，num_query，k_v_pair_num)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "\n",
    "        # 注意力权重\n",
    "        # 使用masked_softmax计算注意力权重, 有效长度为valid_lens\n",
    "        # attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\n",
    "        # values的形状：(batch_size，k_v_pair_num，value_size)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "# 点积注意力\n",
    "class DotProductAttentionForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dropout=False):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries, keys, values = self.W_q(queries), self.W_k(keys), self.w_v(values)\n",
    "        # queries: (batch_size, num_query, num_hiddens)\n",
    "        # keys: (batch_size, k_v_pair_num, num_hiddens)\n",
    "        # values: (batch_size, k_v_pair_num, value_size)\n",
    "        d = queries.shape[-1]\n",
    "\n",
    "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
    "        # (batch_size, num_query, num_hiddens) @ (batch_size, num_hiddens, k_v_pair_num) = (batch_size, num_query, k_v_pair_num)\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "\n",
    "        # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n",
    "        # 使用masked_softmax计算注意力权重\n",
    "        # attention_weights的形状：(batch_size, num_query, k_v_pair_num)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\n",
    "        # (batch_size, num_query, k_v_pair_num) @ (batch_size, k_v_pair_num, value_size) = (batch_size, num_query, value_size)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "# 多头注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, query_size, key_size, num_hiddens, value_size, attention_type, dropout=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens)\n",
    "        if num_hiddens % num_heads != 0:\n",
    "            # raise ValueError(f'num_hiddens must be divisible by num_heads, but got num_hiddens={num_hiddens} and num_heads={num_heads}')\n",
    "            raise ValueError(f'num_hiddens必须能整除num_heads，但是接受的num_hiddens={num_hiddens}，num_heads={num_heads}')\n",
    "        \n",
    "        # 选择注意力机制\n",
    "        if attention_type == 'add':\n",
    "            # self.attention = AdditiveAttention(query_size=query_size, key_size=key_size, num_hiddens=num_hiddens, dropout=dropout)\n",
    "            self.attention = AdditiveAttentionForMultiHeadAttention(num_hiddens=int(num_hiddens/num_heads), dropout=dropout)\n",
    "        elif attention_type == 'dot':\n",
    "            # self.attention = DotProductAttention(query_size=query_size, key_size=key_size, value_size=value_size, num_hiddens=num_hiddens, dropout=dropout)\n",
    "            self.attention = DotProductAttentionForMultiHeadAttention(dropout=dropout)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid attention type: {attention_type}')        \n",
    "\n",
    "        # 最后concat所有head的结果 (其实就是投影)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens)   \n",
    "\n",
    "    def transpose_input(self, X, num_heads):\n",
    "        \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "        # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        # 输出X的形状:(batch_size，查询或者“键－值”对的个数，`num_heads`，num_hiddens/num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "        # 输出X的形状:(batch_size，`num_heads`，查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)                                                                   # 调整顺序以便做广播 (向量化并行计算multi heads)\n",
    "\n",
    "        # 最终输出的形状:(batch_size*`num_heads`,查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def transpose_output(self, X, num_heads):\n",
    "        \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "        # 输入X的形状:(batch_size*`num_heads`,查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        # 输出X的形状:(batch_size,`num_heads``,查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "\n",
    "        # 输出X的形状:(batch_size,查询或者“键－值”对的个数，`num_heads`,num_hiddens/num_heads)                                        # 不改变顺序\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "        # 最终输出X的形状:(batch_size,查询或者“键－值”对的个数，num_hiddens)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        # queries: (batch_size, num_query, query_size)\n",
    "        queries = self.W_q(queries)\n",
    "        # queries: (batch_size, num_query, num_hiddens)\n",
    "        queries = self.transpose_input(queries, self.num_heads)\n",
    "        # queries: (batch_size*num_heads, num_query, num_hiddens/num_heads)\n",
    "\n",
    "        # keys: (batch_size, k_v_pair_num, key_size)    \n",
    "        keys = self.W_k(keys)\n",
    "        # keys: (batch_size, k_v_pair_num, num_hiddens)\n",
    "        keys = self.transpose_input(keys, self.num_heads)\n",
    "        # keys: (batch_size*num_heads, k_v_pair_num, num_hiddens/num_heads)\n",
    "\n",
    "        # values: (batch_size, k_v_pair_num, value_size)\n",
    "        values = self.W_v(values)    \n",
    "        # values: (batch_size, k_v_pair_num, num_hiddens)     \n",
    "        values = self.transpose_input(values, self.num_heads)\n",
    "        # values: (batch_size*num_heads, k_v_pair_num, num_hiddens/num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在0轴，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，num_query，num_hiddens/num_heads)\n",
    "        output = self.attention(queries=queries, keys=keys, values=values, valid_lens=valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，num_query，num_hiddens) \n",
    "        output_concat = self.transpose_output(output, self.num_heads)\n",
    "\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "\n",
    "# Test\n",
    "batch_size = 2\n",
    "num_query, query_size = 1, 4\n",
    "k_v_pair_num, key_size = 10, 4\n",
    "value_size = 4 \n",
    "num_hiddens = 128\n",
    "num_heads = 2\n",
    "\n",
    "# 实例化\n",
    "multiHeadAttention = MultiHeadAttention(\n",
    "    num_heads = num_heads, \n",
    "    query_size = query_size, \n",
    "    key_size = key_size, \n",
    "    num_hiddens = num_hiddens, \n",
    "    value_size = value_size,\n",
    "    attention_type='add'\n",
    ").eval()\n",
    "print(multiHeadAttention)\n",
    "\n",
    "# 传参\n",
    "query = torch.ones((batch_size, num_query, query_size)); print('raw queries size: ', query.shape)\n",
    "key = torch.ones((batch_size, k_v_pair_num, key_size)); print('raw keys size: ', key.shape)\n",
    "value = torch.randn((batch_size, k_v_pair_num, value_size)); print('raw values size: ', value.shape)\n",
    "\n",
    "attention_values = multiHeadAttention(queries=query, keys=key, values=value); print('attention_values size: ', attention_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiHeadAttention.attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T15:05:29.031020</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 -0 \n",
       "L 0 -0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p065313ce8b)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"image35ba780d6f\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"ma21bb102bc\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma21bb102bc\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma21bb102bc\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma21bb102bc\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma21bb102bc\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma21bb102bc\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m6d984b8c55\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d984b8c55\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d984b8c55\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d984b8c55\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 0 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-30\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"imagea26d957891\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"md35d1719f0\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md35d1719f0\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md35d1719f0\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p065313ce8b\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"396.091974pt\" height=\"101.97438pt\" viewBox=\"0 0 396.091974 101.97438\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T15:05:29.163930</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 101.97438 \n",
       "L 396.091974 101.97438 \n",
       "L 396.091974 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "L 52.160938 35.84853 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pb0391e461a)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAY0AAAAoCAYAAAACAaY4AAAAr0lEQVR4nO3VsQ3AIADAMOCSHszAx+0L2VAl+4Jsmc/Z7wCAYN0OAOA/TAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyEwDgMw0AMhMA4DMNADITAOAzDQAyD6n/wKLVEc9aQAAAABJRU5ErkJggg==\" id=\"image08aa237174\" transform=\"scale(1 -1) translate(0 -28.8)\" x=\"52.160938\" y=\"-35.61813\" width=\"285.84\" height=\"28.8\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m5c463eafe6\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5c463eafe6\" x=\"66.445738\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.264488 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5c463eafe6\" x=\"123.584938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(120.403688 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5c463eafe6\" x=\"180.724138\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(177.542888 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5c463eafe6\" x=\"237.863338\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(234.682088 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5c463eafe6\" x=\"295.002538\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(291.821288 79.016567) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- k-v pair -->\n",
       "     <g transform=\"translate(176.209719 92.694692) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n",
       "L 1997 2009 \n",
       "L 1997 1497 \n",
       "L 313 1497 \n",
       "L 313 2009 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-2d\" x=\"57.910156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"91.369141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"150.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"182.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"245.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"307.091797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"334.875\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m717faa6ff6\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m717faa6ff6\" x=\"52.160938\" y=\"35.84853\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 39.647749) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m717faa6ff6\" x=\"52.160938\" y=\"50.13333\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 53.932549) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m717faa6ff6\" x=\"52.160938\" y=\"64.41813\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 68.217349) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- query -->\n",
       "     <g transform=\"translate(14.798438 64.567705) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"188.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"229.492188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 52.160938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 337.856938 64.41813 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160938 64.41813 \n",
       "L 337.856938 64.41813 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160938 35.84853 \n",
       "L 337.856938 35.84853 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_11\">\n",
       "    <!-- batch 1 -->\n",
       "    <g transform=\"translate(172.344875 29.84853) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"314.111328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABsCAYAAACilHh3AAAA10lEQVR4nM2Vyw3DIBBE92vTWGpw/204icndPKRFvsTH0bydAQTWVxxdbl+o+12TEBI1ourEmVENEjPCMZ2chHdO1zJOYtkpBkGd0hdEmnnhihgftAmOTkzv4xFNe6KzLC70RPxZ+kOR0uUPy/vwWqz1JHw8N4lu5ORdAifiWi6vdpFIMwk3r+KmIDrhTpWMZqZ9CacgmjnBwRkkboSvOElUwHf7FPFUEBFHJ27Ibu8qTmLTKt44CMpvHHRWewqmUxA5k86okZh03xNegWgKP+iEKxf0NPwAlAtZrKzl2bAAAAAASUVORK5CYII=\" id=\"imagee221d80994\" transform=\"scale(1 -1) translate(0 -77.76)\" x=\"355.68\" y=\"-10.8\" width=\"3.6\" height=\"77.76\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m23a541d724\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m23a541d724\" x=\"359.626349\" y=\"50.133336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(366.626349 53.932555) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m23a541d724\" x=\"359.626349\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.11 -->\n",
       "      <g transform=\"translate(366.626349 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 355.712938 89.267448 \n",
       "L 357.669643 89.267448 \n",
       "L 359.626349 89.267448 \n",
       "L 359.626349 10.999212 \n",
       "L 357.669643 10.999212 \n",
       "L 355.712938 10.999212 \n",
       "L 355.712938 89.267448 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pb0391e461a\">\n",
       "   <rect x=\"52.160938\" y=\"35.84853\" width=\"285.696\" height=\"28.5696\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for batch in range(attention_weights.shape[0]):\n",
    "    plt.subplot(3, 1, batch + 1)\n",
    "    plt.imshow(attention_weights[batch, :].detach().numpy())\n",
    "    plt.title(f'batch {batch}')\n",
    "    plt.xlabel('k-v pair')\n",
    "    plt.ylabel('query')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从头手写\n",
    "  - 无掩码和Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10: train_loss=0.1438291221857071, train_acc=95.59166717529297, test_acc=95.43000030517578\n",
      "epoch 2/10: train_loss=0.11071320623159409, train_acc=96.6016616821289, test_acc=95.88999938964844\n",
      "epoch 3/10: train_loss=0.06381400674581528, train_acc=98.07500457763672, test_acc=97.23999786376953\n",
      "epoch 4/10: train_loss=0.06747393310070038, train_acc=97.86500549316406, test_acc=96.91000366210938\n",
      "epoch 5/10: train_loss=0.03673125430941582, train_acc=98.8116683959961, test_acc=97.57999420166016\n",
      "epoch 6/10: train_loss=0.037261512130498886, train_acc=98.79166412353516, test_acc=97.43000030517578\n",
      "epoch 7/10: train_loss=0.03205549716949463, train_acc=98.95166778564453, test_acc=97.27999877929688\n",
      "epoch 8/10: train_loss=0.03736620396375656, train_acc=98.77666473388672, test_acc=97.19000244140625\n",
      "epoch 9/10: train_loss=0.02184496819972992, train_acc=99.31666564941406, test_acc=97.6199951171875\n",
      "epoch 10/10: train_loss=0.033308517187833786, train_acc=98.88500213623047, test_acc=97.25\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 16.65029287338257 s\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn  \n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "dbs = './Pytorch_datasets/'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=dbs, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(), \n",
    "            #  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 迭代型数据方式\n",
    "train_iter = data.DataLoader(dataset=train_dataset, batch_size=128,  shuffle=True)\n",
    "# test_iter = data.DataLoader(dataset=test_dataset) # test不需要batch训练\n",
    "\n",
    "  \n",
    "# 4\n",
    "class MultiHeadAttentionMNISTModel(nn.Module):\n",
    "    def __init__(self, num_heads, attention_type):\n",
    "        super().__init__()\n",
    "        self.input = nn.Sequential(\n",
    "                nn.Flatten(), \n",
    "                nn.Linear(28 * 28, 128),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        num_hiddens = 128\n",
    "        # 多头注意力机制 from PyTorch\n",
    "        # self.attention = nn.MultiheadAttention(embed_dim=num_hiddens, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # 选择注意力机制 from 自己\n",
    "        if attention_type == 'add': \n",
    "            self.attention = MultiHeadAttention(\n",
    "                num_heads = num_heads, \n",
    "                query_size = num_hiddens, \n",
    "                key_size = num_hiddens, \n",
    "                num_hiddens = num_hiddens, \n",
    "                value_size = num_hiddens,\n",
    "                attention_type='add'\n",
    "            )\n",
    "        elif attention_type == 'dot':\n",
    "            self.attention = MultiHeadAttention(\n",
    "                num_heads = num_heads, \n",
    "                query_size = num_hiddens, \n",
    "                key_size = num_hiddens, \n",
    "                num_hiddens = num_hiddens, \n",
    "                value_size = num_hiddens,\n",
    "                attention_type='dot'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f'Invalid attention type: {attention_type}')\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.input(X)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.attention(queries=x, keys=x, values=x)\n",
    "        # x, weights = self.attention(query=x, key=x, value=x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 训练过程封装\n",
    "def train_steps(epochs, train_dataset, train_iter, test_dataset, net, loss_fn, opt, device):\n",
    "    '''\n",
    "    参数记录\n",
    "    epochs = epochs                         # epoch\n",
    "    train_dataset = train_dataset           # 全部train数据集\n",
    "    train_iter = train_iter                 # batch之后的train数据集\n",
    "    test_dataset = test_dataset             # 全部test数据集\n",
    "    net = net                               # 网络模型\n",
    "    loss_fn = loss_fn                       # 损失函数\n",
    "    opt = opt                               # 优化器\n",
    "    device = device                         # device GPU/CPU\n",
    "    '''\n",
    "    print('='*100, '\\n', f\"Runing on {device}\", '\\n','='*100)\n",
    "    train_all_data_gpu = train_dataset.data.to(device)\n",
    "    train_all_targets_gpu = train_dataset.targets.to(device)\n",
    "    test_all_data_gpu = test_dataset.data.to(device)\n",
    "    test_all_targets_gpu = test_dataset.targets.to(device)\n",
    "    net = nn.DataParallel(module=net).to(device)\n",
    "\n",
    "    # 开始迭代\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_record in train_iter:\n",
    "            X, y = batch_record                 # 分配X, y\n",
    "            X, y = X.to(device), y.to(device)   # 复制到device（GPU/CPU）上\n",
    "            # print(X[0])\n",
    "            # print(X[0].dtype)\n",
    "            # break\n",
    "            opt.zero_grad()                     # 默认是累加，此处从新求导\n",
    "            y_hat = net(X)          # 计算y_hat\n",
    "            loss = loss_fn(y_hat, y)# 计算loss\n",
    "            loss.backward()         # 计算梯度\n",
    "            opt.step()              # 更新网络参数\n",
    "\n",
    "        net.eval()  # 切换至评估模式\n",
    "                    # 模型默认是net.train()\n",
    "                    # 但是net中含有BN、Dropout等，在test时必须固定train时学好的参数，不能被test又改变了\n",
    "                    # 但net中没有BN、Dropout等时，加不加net.eval()都无所谓\n",
    "\n",
    "        with torch.no_grad(): # with下内容不进行grad计算，可以节省运算和内存\n",
    "            train_loss = loss_fn(net(train_all_data_gpu/256), train_all_targets_gpu)\n",
    "            # print(train_loss)\n",
    "            train_acc_cmp = net(train_all_data_gpu/256).argmax(axis=1) == train_all_targets_gpu\n",
    "            train_acc = (train_acc_cmp.sum() / len(train_acc_cmp)) * 100\n",
    "            # print(train_acc)\n",
    "            test_acc_cmp = net(test_all_data_gpu/256).argmax(axis=1) == test_all_targets_gpu\n",
    "            test_acc = (test_acc_cmp.sum() / len(test_acc_cmp)) * 100\n",
    "            # print(test_acc)\n",
    "            print(f\"epoch {epoch+1}/{epochs}: train_loss={train_loss}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "\n",
    "    stop = time.time()\n",
    "    seconds = stop - start\n",
    "    def convert_seconds(seconds):\n",
    "        days = seconds // (24 * 3600)\n",
    "        hours = (seconds % (24 * 3600)) // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        remaining_seconds = seconds % 60\n",
    "        return days, hours, minutes, remaining_seconds\n",
    "    days, hours, minutes, remaining_seconds = convert_seconds(seconds)\n",
    "    print('='*100, '\\n', f\"Total：{days} d/ {hours} h/ {minutes} m/ {remaining_seconds} s\")\n",
    "    # return (train_loss, train_acc, test_acc)\n",
    "    return None\n",
    "\n",
    "# lr 0.01 -> 0.5\n",
    "# 结果表明还是会快一点收敛\n",
    "net = MultiHeadAttentionMNISTModel(num_heads=2, attention_type='add')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.5)   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      " Runing on cuda:0 \n",
      " ====================================================================================================\n",
      "epoch 1/10: train_loss=1.7392207384109497, train_acc=54.72500228881836, test_acc=54.97999954223633\n",
      "epoch 2/10: train_loss=0.6780784726142883, train_acc=81.1933364868164, test_acc=81.5199966430664\n",
      "epoch 3/10: train_loss=0.48317480087280273, train_acc=86.11333465576172, test_acc=86.43999481201172\n",
      "epoch 4/10: train_loss=0.41290122270584106, train_acc=88.25333404541016, test_acc=88.36000061035156\n",
      "epoch 5/10: train_loss=0.3759009540081024, train_acc=89.16666412353516, test_acc=89.41999816894531\n",
      "epoch 6/10: train_loss=0.3502875566482544, train_acc=89.97333526611328, test_acc=90.25\n",
      "epoch 7/10: train_loss=0.3304363787174225, train_acc=90.50333404541016, test_acc=90.76000213623047\n",
      "epoch 8/10: train_loss=0.31311121582984924, train_acc=91.0250015258789, test_acc=91.22999572753906\n",
      "epoch 9/10: train_loss=0.29658740758895874, train_acc=91.57666778564453, test_acc=91.80999755859375\n",
      "epoch 10/10: train_loss=0.28062403202056885, train_acc=91.98833465576172, test_acc=92.0999984741211\n",
      "==================================================================================================== \n",
      " Total：0.0 d/ 0.0 h/ 1.0 m/ 13.107177972793579 s\n"
     ]
    }
   ],
   "source": [
    "net = MultiHeadAttentionMNISTModel(num_heads=2, attention_type='dot')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=net.parameters(), lr=0.01)   \n",
    "train_steps(\n",
    "    epochs=10, \n",
    "    train_dataset=train_dataset, \n",
    "    train_iter=train_iter, \n",
    "    test_dataset=test_dataset, \n",
    "    net=net,                        \n",
    "    loss_fn=loss_fn, \n",
    "    opt=opt, \n",
    "    device=device \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AdditiveAttentionForMultiHeadAttention' object has no attribute 'attention_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[360], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_weights\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AdditiveAttentionForMultiHeadAttention' object has no attribute 'attention_weights'"
     ]
    }
   ],
   "source": [
    "net.attention.attention.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.0.0 (20240803.0821)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"431pt\" height=\"1318pt\"\n",
       " viewBox=\"0.00 0.00 430.75 1318.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.768513 0.768513) rotate(0) translate(4 1711)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1711 556.5,-1711 556.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"343.38,-1707 208.62,-1707 208.62,-1671.5 343.38,-1671.5 343.38,-1707\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"208.62,-1671.5 208.62,-1707 277.88,-1707 277.88,-1671.5 208.62,-1671.5\"/>\n",
       "<text text-anchor=\"start\" x=\"213.62\" y=\"-1692.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"224.88\" y=\"-1679.75\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"277.88,-1671.5 277.88,-1707 343.38,-1707 343.38,-1671.5 277.88,-1671.5\"/>\n",
       "<text text-anchor=\"start\" x=\"282.88\" y=\"-1686.12\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 28, 28)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"356.5,-1635.5 195.5,-1635.5 195.5,-1591.5 356.5,-1591.5 356.5,-1635.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"195.5,-1591.5 195.5,-1635.5 241.5,-1635.5 241.5,-1591.5 195.5,-1591.5\"/>\n",
       "<text text-anchor=\"start\" x=\"201.25\" y=\"-1616.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Flatten</text>\n",
       "<text text-anchor=\"start\" x=\"200.12\" y=\"-1604\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"241.5,-1613.5 241.5,-1635.5 288.5,-1635.5 288.5,-1613.5 241.5,-1613.5\"/>\n",
       "<text text-anchor=\"start\" x=\"251.12\" y=\"-1621\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"288.5,-1613.5 288.5,-1635.5 356.5,-1635.5 356.5,-1613.5 288.5,-1613.5\"/>\n",
       "<text text-anchor=\"start\" x=\"293.25\" y=\"-1621\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"241.5,-1591.5 241.5,-1613.5 288.5,-1613.5 288.5,-1591.5 241.5,-1591.5\"/>\n",
       "<text text-anchor=\"start\" x=\"246.25\" y=\"-1599\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"288.5,-1591.5 288.5,-1613.5 356.5,-1613.5 356.5,-1591.5 288.5,-1591.5\"/>\n",
       "<text text-anchor=\"start\" x=\"299.25\" y=\"-1599\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 784) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1671.51C276,-1664.15 276,-1655.24 276,-1646.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1646.9 276,-1636.9 272.5,-1646.9 279.5,-1646.9\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"350.5,-1555.5 201.5,-1555.5 201.5,-1511.5 350.5,-1511.5 350.5,-1555.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"201.5,-1511.5 201.5,-1555.5 247.5,-1555.5 247.5,-1511.5 201.5,-1511.5\"/>\n",
       "<text text-anchor=\"start\" x=\"209.5\" y=\"-1536.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"206.12\" y=\"-1524\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"247.5,-1533.5 247.5,-1555.5 294.5,-1555.5 294.5,-1533.5 247.5,-1533.5\"/>\n",
       "<text text-anchor=\"start\" x=\"257.12\" y=\"-1541\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"294.5,-1533.5 294.5,-1555.5 350.5,-1555.5 350.5,-1533.5 294.5,-1533.5\"/>\n",
       "<text text-anchor=\"start\" x=\"299.25\" y=\"-1541\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 784) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"247.5,-1511.5 247.5,-1533.5 294.5,-1533.5 294.5,-1511.5 247.5,-1511.5\"/>\n",
       "<text text-anchor=\"start\" x=\"252.25\" y=\"-1519\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"294.5,-1511.5 294.5,-1533.5 350.5,-1533.5 350.5,-1511.5 294.5,-1511.5\"/>\n",
       "<text text-anchor=\"start\" x=\"299.25\" y=\"-1519\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 128) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1591.6C276,-1583.99 276,-1575.2 276,-1566.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1566.97 276,-1556.97 272.5,-1566.97 279.5,-1566.97\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"350.5,-1475.5 201.5,-1475.5 201.5,-1431.5 350.5,-1431.5 350.5,-1475.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"201.5,-1431.5 201.5,-1475.5 247.5,-1475.5 247.5,-1431.5 201.5,-1431.5\"/>\n",
       "<text text-anchor=\"start\" x=\"211.75\" y=\"-1456.75\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"206.12\" y=\"-1444\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"247.5,-1453.5 247.5,-1475.5 294.5,-1475.5 294.5,-1453.5 247.5,-1453.5\"/>\n",
       "<text text-anchor=\"start\" x=\"257.12\" y=\"-1461\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"294.5,-1453.5 294.5,-1475.5 350.5,-1475.5 350.5,-1453.5 294.5,-1453.5\"/>\n",
       "<text text-anchor=\"start\" x=\"299.25\" y=\"-1461\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"247.5,-1431.5 247.5,-1453.5 294.5,-1453.5 294.5,-1431.5 247.5,-1431.5\"/>\n",
       "<text text-anchor=\"start\" x=\"252.25\" y=\"-1439\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"294.5,-1431.5 294.5,-1453.5 350.5,-1453.5 350.5,-1431.5 294.5,-1431.5\"/>\n",
       "<text text-anchor=\"start\" x=\"299.25\" y=\"-1439\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 128) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1511.6C276,-1503.99 276,-1495.2 276,-1486.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1486.97 276,-1476.97 272.5,-1486.97 279.5,-1486.97\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"364.5,-1395.5 187.5,-1395.5 187.5,-1351.5 364.5,-1351.5 364.5,-1395.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"187.5,-1351.5 187.5,-1395.5 249.5,-1395.5 249.5,-1351.5 187.5,-1351.5\"/>\n",
       "<text text-anchor=\"start\" x=\"192.25\" y=\"-1376.75\" font-family=\"Linux libertine\" font-size=\"10.00\">unsqueeze</text>\n",
       "<text text-anchor=\"start\" x=\"200.12\" y=\"-1364\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"249.5,-1373.5 249.5,-1395.5 296.5,-1395.5 296.5,-1373.5 249.5,-1373.5\"/>\n",
       "<text text-anchor=\"start\" x=\"259.12\" y=\"-1381\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296.5,-1373.5 296.5,-1395.5 364.5,-1395.5 364.5,-1373.5 296.5,-1373.5\"/>\n",
       "<text text-anchor=\"start\" x=\"307.25\" y=\"-1381\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"249.5,-1351.5 249.5,-1373.5 296.5,-1373.5 296.5,-1351.5 249.5,-1351.5\"/>\n",
       "<text text-anchor=\"start\" x=\"254.25\" y=\"-1359\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"296.5,-1351.5 296.5,-1373.5 364.5,-1373.5 364.5,-1351.5 296.5,-1351.5\"/>\n",
       "<text text-anchor=\"start\" x=\"301.25\" y=\"-1359\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1431.6C276,-1423.99 276,-1415.2 276,-1406.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1406.97 276,-1396.97 272.5,-1406.97 279.5,-1406.97\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"356.5,-1315.5 195.5,-1315.5 195.5,-1271.5 356.5,-1271.5 356.5,-1315.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"195.5,-1271.5 195.5,-1315.5 241.5,-1315.5 241.5,-1271.5 195.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"203.5\" y=\"-1296.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"200.12\" y=\"-1284\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"241.5,-1293.5 241.5,-1315.5 288.5,-1315.5 288.5,-1293.5 241.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"251.12\" y=\"-1301\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"288.5,-1293.5 288.5,-1315.5 356.5,-1315.5 356.5,-1293.5 288.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"293.25\" y=\"-1301\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"241.5,-1271.5 241.5,-1293.5 288.5,-1293.5 288.5,-1271.5 241.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"246.25\" y=\"-1279\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"288.5,-1271.5 288.5,-1293.5 356.5,-1293.5 356.5,-1271.5 288.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"293.25\" y=\"-1279\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1351.6C276,-1343.99 276,-1335.2 276,-1326.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1326.97 276,-1316.97 272.5,-1326.97 279.5,-1326.97\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"539.5,-1315.5 378.5,-1315.5 378.5,-1271.5 539.5,-1271.5 539.5,-1315.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"378.5,-1271.5 378.5,-1315.5 424.5,-1315.5 424.5,-1271.5 378.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"386.5\" y=\"-1296.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"383.12\" y=\"-1284\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"424.5,-1293.5 424.5,-1315.5 471.5,-1315.5 471.5,-1293.5 424.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"434.12\" y=\"-1301\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"471.5,-1293.5 471.5,-1315.5 539.5,-1315.5 539.5,-1293.5 471.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"476.25\" y=\"-1301\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"424.5,-1271.5 424.5,-1293.5 471.5,-1293.5 471.5,-1271.5 424.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"429.25\" y=\"-1279\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"471.5,-1271.5 471.5,-1293.5 539.5,-1293.5 539.5,-1271.5 471.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"476.25\" y=\"-1279\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M325.04,-1351.6C347.91,-1341.85 375.33,-1330.16 399.39,-1319.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"400.6,-1323.19 408.43,-1316.05 397.86,-1316.76 400.6,-1323.19\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"173.5,-1315.5 12.5,-1315.5 12.5,-1271.5 173.5,-1271.5 173.5,-1315.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"12.5,-1271.5 12.5,-1315.5 58.5,-1315.5 58.5,-1271.5 12.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"20.5\" y=\"-1296.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"17.12\" y=\"-1284\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-1293.5 58.5,-1315.5 105.5,-1315.5 105.5,-1293.5 58.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"68.12\" y=\"-1301\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"105.5,-1293.5 105.5,-1315.5 173.5,-1315.5 173.5,-1293.5 105.5,-1293.5\"/>\n",
       "<text text-anchor=\"start\" x=\"110.25\" y=\"-1301\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-1271.5 58.5,-1293.5 105.5,-1293.5 105.5,-1271.5 58.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"63.25\" y=\"-1279\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"105.5,-1271.5 105.5,-1293.5 173.5,-1293.5 173.5,-1271.5 105.5,-1271.5\"/>\n",
       "<text text-anchor=\"start\" x=\"110.25\" y=\"-1279\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;13 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M226.96,-1351.6C204.09,-1341.85 176.67,-1330.16 152.61,-1319.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"154.14,-1316.76 143.57,-1316.05 151.4,-1323.19 154.14,-1316.76\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"361,-1235.5 191,-1235.5 191,-1191.5 361,-1191.5 361,-1235.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"191,-1191.5 191,-1235.5 240,-1235.5 240,-1191.5 191,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"196\" y=\"-1216.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"197.12\" y=\"-1204\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"240,-1213.5 240,-1235.5 287,-1235.5 287,-1213.5 240,-1213.5\"/>\n",
       "<text text-anchor=\"start\" x=\"249.62\" y=\"-1221\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"287,-1213.5 287,-1235.5 361,-1235.5 361,-1213.5 287,-1213.5\"/>\n",
       "<text text-anchor=\"start\" x=\"294.75\" y=\"-1221\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"240,-1191.5 240,-1213.5 287,-1213.5 287,-1191.5 240,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"244.75\" y=\"-1199\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"287,-1191.5 287,-1213.5 361,-1213.5 361,-1191.5 287,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"291.75\" y=\"-1199\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1271.6C276,-1263.99 276,-1255.2 276,-1246.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1246.97 276,-1236.97 272.5,-1246.97 279.5,-1246.97\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"362,-1155.5 190,-1155.5 190,-1111.5 362,-1111.5 362,-1155.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"190,-1111.5 190,-1155.5 241,-1155.5 241,-1111.5 190,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"194.88\" y=\"-1136.75\" font-family=\"Linux libertine\" font-size=\"10.00\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"197.12\" y=\"-1124\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"241,-1133.5 241,-1155.5 288,-1155.5 288,-1133.5 241,-1133.5\"/>\n",
       "<text text-anchor=\"start\" x=\"250.62\" y=\"-1141\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"288,-1133.5 288,-1155.5 362,-1155.5 362,-1133.5 288,-1133.5\"/>\n",
       "<text text-anchor=\"start\" x=\"292.75\" y=\"-1141\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"241,-1111.5 241,-1133.5 288,-1133.5 288,-1111.5 241,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"245.75\" y=\"-1119\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"288,-1111.5 288,-1133.5 362,-1133.5 362,-1111.5 288,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"292.75\" y=\"-1119\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1191.6C276,-1183.99 276,-1175.2 276,-1166.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1166.97 276,-1156.97 272.5,-1166.97 279.5,-1166.97\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"361,-995.5 191,-995.5 191,-951.5 361,-951.5 361,-995.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"191,-951.5 191,-995.5 240,-995.5 240,-951.5 191,-951.5\"/>\n",
       "<text text-anchor=\"start\" x=\"196\" y=\"-976.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"197.12\" y=\"-964\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"240,-973.5 240,-995.5 287,-995.5 287,-973.5 240,-973.5\"/>\n",
       "<text text-anchor=\"start\" x=\"249.62\" y=\"-981\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"287,-973.5 287,-995.5 361,-995.5 361,-973.5 287,-973.5\"/>\n",
       "<text text-anchor=\"start\" x=\"291.75\" y=\"-981\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"240,-951.5 240,-973.5 287,-973.5 287,-951.5 240,-951.5\"/>\n",
       "<text text-anchor=\"start\" x=\"244.75\" y=\"-959\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"287,-951.5 287,-973.5 361,-973.5 361,-951.5 287,-951.5\"/>\n",
       "<text text-anchor=\"start\" x=\"294.75\" y=\"-959\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276,-1111.52C276,-1084.57 276,-1037.56 276,-1006.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.5,-1006.58 276,-996.58 272.5,-1006.58 279.5,-1006.58\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"412.5,-915.5 189.5,-915.5 189.5,-871.5 412.5,-871.5 412.5,-915.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"189.5,-871.5 189.5,-915.5 235.5,-915.5 235.5,-871.5 189.5,-871.5\"/>\n",
       "<text text-anchor=\"start\" x=\"199.75\" y=\"-896.75\" font-family=\"Linux libertine\" font-size=\"10.00\">bmm</text>\n",
       "<text text-anchor=\"start\" x=\"194.12\" y=\"-884\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"235.5,-893.5 235.5,-915.5 282.5,-915.5 282.5,-893.5 235.5,-893.5\"/>\n",
       "<text text-anchor=\"start\" x=\"245.12\" y=\"-901\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"282.5,-893.5 282.5,-915.5 412.5,-915.5 412.5,-893.5 282.5,-893.5\"/>\n",
       "<text text-anchor=\"start\" x=\"287.5\" y=\"-901\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 64), (128, 64, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"235.5,-871.5 235.5,-893.5 282.5,-893.5 282.5,-871.5 235.5,-871.5\"/>\n",
       "<text text-anchor=\"start\" x=\"240.25\" y=\"-879\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"282.5,-871.5 282.5,-893.5 412.5,-893.5 412.5,-871.5 282.5,-871.5\"/>\n",
       "<text text-anchor=\"start\" x=\"321.25\" y=\"-879\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;18 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>8&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.7,-951.6C285.19,-943.82 288.08,-934.8 290.82,-926.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.1,-927.49 293.82,-916.9 287.43,-925.36 294.1,-927.49\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"550,-1235.5 380,-1235.5 380,-1191.5 550,-1191.5 550,-1235.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"380,-1191.5 380,-1235.5 429,-1235.5 429,-1191.5 380,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"385\" y=\"-1216.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"386.12\" y=\"-1204\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"429,-1213.5 429,-1235.5 476,-1235.5 476,-1213.5 429,-1213.5\"/>\n",
       "<text text-anchor=\"start\" x=\"438.62\" y=\"-1221\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"476,-1213.5 476,-1235.5 550,-1235.5 550,-1213.5 476,-1213.5\"/>\n",
       "<text text-anchor=\"start\" x=\"483.75\" y=\"-1221\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"429,-1191.5 429,-1213.5 476,-1213.5 476,-1191.5 429,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"433.75\" y=\"-1199\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"476,-1191.5 476,-1213.5 550,-1213.5 550,-1191.5 476,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"480.75\" y=\"-1199\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M460.61,-1271.6C461.19,-1263.99 461.87,-1255.2 462.51,-1246.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"465.99,-1247.21 463.27,-1236.97 459.01,-1246.67 465.99,-1247.21\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"552,-1155.5 380,-1155.5 380,-1111.5 552,-1111.5 552,-1155.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"380,-1111.5 380,-1155.5 431,-1155.5 431,-1111.5 380,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"384.88\" y=\"-1136.75\" font-family=\"Linux libertine\" font-size=\"10.00\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"387.12\" y=\"-1124\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"431,-1133.5 431,-1155.5 478,-1155.5 478,-1133.5 431,-1133.5\"/>\n",
       "<text text-anchor=\"start\" x=\"440.62\" y=\"-1141\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"478,-1133.5 478,-1155.5 552,-1155.5 552,-1133.5 478,-1133.5\"/>\n",
       "<text text-anchor=\"start\" x=\"482.75\" y=\"-1141\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"431,-1111.5 431,-1133.5 478,-1133.5 478,-1111.5 431,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"435.75\" y=\"-1119\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"478,-1111.5 478,-1133.5 552,-1133.5 552,-1111.5 478,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"482.75\" y=\"-1119\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M465.27,-1191.6C465.37,-1183.99 465.48,-1175.2 465.59,-1166.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"469.08,-1167.02 465.71,-1156.97 462.08,-1166.93 469.08,-1167.02\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"551,-1075.5 381,-1075.5 381,-1031.5 551,-1031.5 551,-1075.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"381,-1031.5 381,-1075.5 430,-1075.5 430,-1031.5 381,-1031.5\"/>\n",
       "<text text-anchor=\"start\" x=\"386\" y=\"-1056.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"387.12\" y=\"-1044\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"430,-1053.5 430,-1075.5 477,-1075.5 477,-1053.5 430,-1053.5\"/>\n",
       "<text text-anchor=\"start\" x=\"439.62\" y=\"-1061\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"477,-1053.5 477,-1075.5 551,-1075.5 551,-1053.5 477,-1053.5\"/>\n",
       "<text text-anchor=\"start\" x=\"481.75\" y=\"-1061\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"430,-1031.5 430,-1053.5 477,-1053.5 477,-1031.5 430,-1031.5\"/>\n",
       "<text text-anchor=\"start\" x=\"434.75\" y=\"-1039\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"477,-1031.5 477,-1053.5 551,-1053.5 551,-1031.5 477,-1031.5\"/>\n",
       "<text text-anchor=\"start\" x=\"484.75\" y=\"-1039\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M466,-1111.6C466,-1103.99 466,-1095.2 466,-1086.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"469.5,-1086.97 466,-1076.97 462.5,-1086.97 469.5,-1086.97\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"552.5,-995.5 379.5,-995.5 379.5,-951.5 552.5,-951.5 552.5,-995.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"379.5,-951.5 379.5,-995.5 437.5,-995.5 437.5,-951.5 379.5,-951.5\"/>\n",
       "<text text-anchor=\"start\" x=\"384.5\" y=\"-976.75\" font-family=\"Linux libertine\" font-size=\"10.00\">transpose</text>\n",
       "<text text-anchor=\"start\" x=\"390.12\" y=\"-964\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"437.5,-973.5 437.5,-995.5 484.5,-995.5 484.5,-973.5 437.5,-973.5\"/>\n",
       "<text text-anchor=\"start\" x=\"447.12\" y=\"-981\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"484.5,-973.5 484.5,-995.5 552.5,-995.5 552.5,-973.5 484.5,-973.5\"/>\n",
       "<text text-anchor=\"start\" x=\"489.25\" y=\"-981\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"437.5,-951.5 437.5,-973.5 484.5,-973.5 484.5,-951.5 437.5,-951.5\"/>\n",
       "<text text-anchor=\"start\" x=\"442.25\" y=\"-959\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"484.5,-951.5 484.5,-973.5 552.5,-973.5 552.5,-951.5 484.5,-951.5\"/>\n",
       "<text text-anchor=\"start\" x=\"489.25\" y=\"-959\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 64, 1) </text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;17 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>12&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M466,-1031.6C466,-1023.99 466,-1015.2 466,-1006.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"469.5,-1006.97 466,-996.97 462.5,-1006.97 469.5,-1006.97\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"172,-1235.5 2,-1235.5 2,-1191.5 172,-1191.5 172,-1235.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2,-1191.5 2,-1235.5 51,-1235.5 51,-1191.5 2,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"7\" y=\"-1216.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"8.12\" y=\"-1204\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"51,-1213.5 51,-1235.5 98,-1235.5 98,-1213.5 51,-1213.5\"/>\n",
       "<text text-anchor=\"start\" x=\"60.62\" y=\"-1221\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"98,-1213.5 98,-1235.5 172,-1235.5 172,-1213.5 98,-1213.5\"/>\n",
       "<text text-anchor=\"start\" x=\"105.75\" y=\"-1221\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"51,-1191.5 51,-1213.5 98,-1213.5 98,-1191.5 51,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"55.75\" y=\"-1199\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"98,-1191.5 98,-1213.5 172,-1213.5 172,-1191.5 98,-1191.5\"/>\n",
       "<text text-anchor=\"start\" x=\"102.75\" y=\"-1199\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.39,-1271.6C90.81,-1263.99 90.13,-1255.2 89.49,-1246.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.99,-1246.67 88.73,-1236.97 86.01,-1247.21 92.99,-1246.67\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"172,-1155.5 0,-1155.5 0,-1111.5 172,-1111.5 172,-1155.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-1111.5 0,-1155.5 51,-1155.5 51,-1111.5 0,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"4.88\" y=\"-1136.75\" font-family=\"Linux libertine\" font-size=\"10.00\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"7.12\" y=\"-1124\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"51,-1133.5 51,-1155.5 98,-1155.5 98,-1133.5 51,-1133.5\"/>\n",
       "<text text-anchor=\"start\" x=\"60.62\" y=\"-1141\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"98,-1133.5 98,-1155.5 172,-1155.5 172,-1133.5 98,-1133.5\"/>\n",
       "<text text-anchor=\"start\" x=\"102.75\" y=\"-1141\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"51,-1111.5 51,-1133.5 98,-1133.5 98,-1111.5 51,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"55.75\" y=\"-1119\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"98,-1111.5 98,-1133.5 172,-1133.5 172,-1111.5 98,-1111.5\"/>\n",
       "<text text-anchor=\"start\" x=\"102.75\" y=\"-1119\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.73,-1191.6C86.63,-1183.99 86.52,-1175.2 86.41,-1166.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.92,-1166.93 86.29,-1156.97 82.92,-1167.02 89.92,-1166.93\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"171,-915.5 1,-915.5 1,-871.5 171,-871.5 171,-915.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1,-871.5 1,-915.5 50,-915.5 50,-871.5 1,-871.5\"/>\n",
       "<text text-anchor=\"start\" x=\"6\" y=\"-896.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"7.12\" y=\"-884\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"50,-893.5 50,-915.5 97,-915.5 97,-893.5 50,-893.5\"/>\n",
       "<text text-anchor=\"start\" x=\"59.62\" y=\"-901\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"97,-893.5 97,-915.5 171,-915.5 171,-893.5 97,-893.5\"/>\n",
       "<text text-anchor=\"start\" x=\"101.75\" y=\"-901\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"50,-871.5 50,-893.5 97,-893.5 97,-871.5 50,-871.5\"/>\n",
       "<text text-anchor=\"start\" x=\"54.75\" y=\"-879\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"97,-871.5 97,-893.5 171,-893.5 171,-871.5 97,-871.5\"/>\n",
       "<text text-anchor=\"start\" x=\"104.75\" y=\"-879\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86,-1111.8C86,-1070.21 86,-976.29 86,-926.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.5,-926.84 86,-916.84 82.5,-926.84 89.5,-926.84\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"301.5,-595.5 84.5,-595.5 84.5,-551.5 301.5,-551.5 301.5,-595.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"84.5,-551.5 84.5,-595.5 130.5,-595.5 130.5,-551.5 84.5,-551.5\"/>\n",
       "<text text-anchor=\"start\" x=\"94.75\" y=\"-576.75\" font-family=\"Linux libertine\" font-size=\"10.00\">bmm</text>\n",
       "<text text-anchor=\"start\" x=\"89.12\" y=\"-564\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"130.5,-573.5 130.5,-595.5 177.5,-595.5 177.5,-573.5 130.5,-573.5\"/>\n",
       "<text text-anchor=\"start\" x=\"140.12\" y=\"-581\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"177.5,-573.5 177.5,-595.5 301.5,-595.5 301.5,-573.5 177.5,-573.5\"/>\n",
       "<text text-anchor=\"start\" x=\"182.5\" y=\"-581\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1), (128, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"130.5,-551.5 130.5,-573.5 177.5,-573.5 177.5,-551.5 130.5,-551.5\"/>\n",
       "<text text-anchor=\"start\" x=\"135.25\" y=\"-559\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"177.5,-551.5 177.5,-573.5 301.5,-573.5 301.5,-551.5 177.5,-551.5\"/>\n",
       "<text text-anchor=\"start\" x=\"210.25\" y=\"-559\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>16&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M93.02,-871.64C111.31,-817.28 160.51,-671.04 182.33,-606.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.6,-607.47 185.47,-596.88 178.96,-605.24 185.6,-607.47\"/>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.78,-951.6C401.44,-941.98 377.1,-930.48 355.62,-920.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"357.24,-917.22 346.71,-916.11 354.25,-923.55 357.24,-917.22\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"371.5,-835.5 216.5,-835.5 216.5,-791.5 371.5,-791.5 371.5,-835.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"216.5,-791.5 216.5,-835.5 262.5,-835.5 262.5,-791.5 216.5,-791.5\"/>\n",
       "<text text-anchor=\"start\" x=\"232\" y=\"-816.75\" font-family=\"Linux libertine\" font-size=\"10.00\">div</text>\n",
       "<text text-anchor=\"start\" x=\"221.12\" y=\"-804\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"262.5,-813.5 262.5,-835.5 309.5,-835.5 309.5,-813.5 262.5,-813.5\"/>\n",
       "<text text-anchor=\"start\" x=\"272.12\" y=\"-821\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309.5,-813.5 309.5,-835.5 371.5,-835.5 371.5,-813.5 309.5,-813.5\"/>\n",
       "<text text-anchor=\"start\" x=\"314.25\" y=\"-821\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"262.5,-791.5 262.5,-813.5 309.5,-813.5 309.5,-791.5 262.5,-791.5\"/>\n",
       "<text text-anchor=\"start\" x=\"267.25\" y=\"-799\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"309.5,-791.5 309.5,-813.5 371.5,-813.5 371.5,-791.5 309.5,-791.5\"/>\n",
       "<text text-anchor=\"start\" x=\"314.25\" y=\"-799\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M299.12,-871.6C298.44,-863.99 297.65,-855.2 296.9,-846.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"300.4,-846.61 296.02,-836.97 293.42,-847.24 300.4,-846.61\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"366,-755.5 208,-755.5 208,-711.5 366,-711.5 366,-755.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"208,-711.5 208,-755.5 257,-755.5 257,-711.5 208,-711.5\"/>\n",
       "<text text-anchor=\"start\" x=\"212.62\" y=\"-736.75\" font-family=\"Linux libertine\" font-size=\"10.00\">softmax</text>\n",
       "<text text-anchor=\"start\" x=\"214.12\" y=\"-724\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"257,-733.5 257,-755.5 304,-755.5 304,-733.5 257,-733.5\"/>\n",
       "<text text-anchor=\"start\" x=\"266.62\" y=\"-741\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"304,-733.5 304,-755.5 366,-755.5 366,-733.5 304,-733.5\"/>\n",
       "<text text-anchor=\"start\" x=\"308.75\" y=\"-741\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"257,-711.5 257,-733.5 304,-733.5 304,-711.5 257,-711.5\"/>\n",
       "<text text-anchor=\"start\" x=\"261.75\" y=\"-719\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"304,-711.5 304,-733.5 366,-733.5 366,-711.5 304,-711.5\"/>\n",
       "<text text-anchor=\"start\" x=\"308.75\" y=\"-719\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.12,-791.6C291.44,-783.99 290.65,-775.2 289.9,-766.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"293.4,-766.61 289.02,-756.97 286.42,-767.24 293.4,-766.61\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"353,-675.5 195,-675.5 195,-631.5 353,-631.5 353,-675.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"195,-631.5 195,-675.5 244,-675.5 244,-631.5 195,-631.5\"/>\n",
       "<text text-anchor=\"start\" x=\"200\" y=\"-656.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"201.12\" y=\"-644\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:3</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"244,-653.5 244,-675.5 291,-675.5 291,-653.5 244,-653.5\"/>\n",
       "<text text-anchor=\"start\" x=\"253.62\" y=\"-661\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"291,-653.5 291,-675.5 353,-675.5 353,-653.5 291,-653.5\"/>\n",
       "<text text-anchor=\"start\" x=\"295.75\" y=\"-661\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"244,-631.5 244,-653.5 291,-653.5 291,-631.5 244,-631.5\"/>\n",
       "<text text-anchor=\"start\" x=\"248.75\" y=\"-639\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"291,-631.5 291,-653.5 353,-653.5 353,-631.5 291,-631.5\"/>\n",
       "<text text-anchor=\"start\" x=\"295.75\" y=\"-639\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 1) </text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;21 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>20&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M283.52,-711.6C282.23,-703.9 280.75,-695 279.34,-686.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"282.84,-686.24 277.74,-676.95 275.93,-687.39 282.84,-686.24\"/>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;22 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>21&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M252.29,-631.6C243.3,-622.94 232.73,-612.76 223.02,-603.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"225.49,-600.93 215.86,-596.51 220.63,-605.97 225.49,-600.93\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"278,-515.5 108,-515.5 108,-471.5 278,-471.5 278,-515.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108,-471.5 108,-515.5 157,-515.5 157,-471.5 108,-471.5\"/>\n",
       "<text text-anchor=\"start\" x=\"113\" y=\"-496.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"114.12\" y=\"-484\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"157,-493.5 157,-515.5 204,-515.5 204,-493.5 157,-493.5\"/>\n",
       "<text text-anchor=\"start\" x=\"166.62\" y=\"-501\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"204,-493.5 204,-515.5 278,-515.5 278,-493.5 204,-493.5\"/>\n",
       "<text text-anchor=\"start\" x=\"211.75\" y=\"-501\" font-family=\"Linux libertine\" font-size=\"10.00\">(128, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"157,-471.5 157,-493.5 204,-493.5 204,-471.5 157,-471.5\"/>\n",
       "<text text-anchor=\"start\" x=\"161.75\" y=\"-479\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"204,-471.5 204,-493.5 278,-493.5 278,-471.5 204,-471.5\"/>\n",
       "<text text-anchor=\"start\" x=\"208.75\" y=\"-479\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;23 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>22&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193,-551.6C193,-543.99 193,-535.2 193,-526.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.5,-526.97 193,-516.97 189.5,-526.97 196.5,-526.97\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"279,-435.5 107,-435.5 107,-391.5 279,-391.5 279,-435.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"107,-391.5 107,-435.5 158,-435.5 158,-391.5 107,-391.5\"/>\n",
       "<text text-anchor=\"start\" x=\"111.88\" y=\"-416.75\" font-family=\"Linux libertine\" font-size=\"10.00\">permute</text>\n",
       "<text text-anchor=\"start\" x=\"114.12\" y=\"-404\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"158,-413.5 158,-435.5 205,-435.5 205,-413.5 158,-413.5\"/>\n",
       "<text text-anchor=\"start\" x=\"167.62\" y=\"-421\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205,-413.5 205,-435.5 279,-435.5 279,-413.5 205,-413.5\"/>\n",
       "<text text-anchor=\"start\" x=\"209.75\" y=\"-421\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2, 1, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"158,-391.5 158,-413.5 205,-413.5 205,-391.5 158,-391.5\"/>\n",
       "<text text-anchor=\"start\" x=\"162.75\" y=\"-399\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205,-391.5 205,-413.5 279,-413.5 279,-391.5 205,-391.5\"/>\n",
       "<text text-anchor=\"start\" x=\"209.75\" y=\"-399\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;24 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>23&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193,-471.6C193,-463.99 193,-455.2 193,-446.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.5,-446.97 193,-436.97 189.5,-446.97 196.5,-446.97\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"278,-355.5 108,-355.5 108,-311.5 278,-311.5 278,-355.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108,-311.5 108,-355.5 157,-355.5 157,-311.5 108,-311.5\"/>\n",
       "<text text-anchor=\"start\" x=\"113\" y=\"-336.75\" font-family=\"Linux libertine\" font-size=\"10.00\">reshape</text>\n",
       "<text text-anchor=\"start\" x=\"114.12\" y=\"-324\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"157,-333.5 157,-355.5 204,-355.5 204,-333.5 157,-333.5\"/>\n",
       "<text text-anchor=\"start\" x=\"166.62\" y=\"-341\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"204,-333.5 204,-355.5 278,-355.5 278,-333.5 204,-333.5\"/>\n",
       "<text text-anchor=\"start\" x=\"208.75\" y=\"-341\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 2, 64) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"157,-311.5 157,-333.5 204,-333.5 204,-311.5 157,-311.5\"/>\n",
       "<text text-anchor=\"start\" x=\"161.75\" y=\"-319\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"204,-311.5 204,-333.5 278,-333.5 278,-311.5 204,-311.5\"/>\n",
       "<text text-anchor=\"start\" x=\"211.75\" y=\"-319\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193,-391.6C193,-383.99 193,-375.2 193,-366.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.5,-366.97 193,-356.97 189.5,-366.97 196.5,-366.97\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"273.5,-275.5 112.5,-275.5 112.5,-231.5 273.5,-231.5 273.5,-275.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"112.5,-231.5 112.5,-275.5 158.5,-275.5 158.5,-231.5 112.5,-231.5\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-256.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"117.12\" y=\"-244\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"158.5,-253.5 158.5,-275.5 205.5,-275.5 205.5,-253.5 158.5,-253.5\"/>\n",
       "<text text-anchor=\"start\" x=\"168.12\" y=\"-261\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205.5,-253.5 205.5,-275.5 273.5,-275.5 273.5,-253.5 205.5,-253.5\"/>\n",
       "<text text-anchor=\"start\" x=\"210.25\" y=\"-261\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"158.5,-231.5 158.5,-253.5 205.5,-253.5 205.5,-231.5 158.5,-231.5\"/>\n",
       "<text text-anchor=\"start\" x=\"163.25\" y=\"-239\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205.5,-231.5 205.5,-253.5 273.5,-253.5 273.5,-231.5 205.5,-231.5\"/>\n",
       "<text text-anchor=\"start\" x=\"210.25\" y=\"-239\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;26 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>25&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193,-311.6C193,-303.99 193,-295.2 193,-286.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.5,-286.97 193,-276.97 189.5,-286.97 196.5,-286.97\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"275.5,-195.5 110.5,-195.5 110.5,-151.5 275.5,-151.5 275.5,-195.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"110.5,-151.5 110.5,-195.5 160.5,-195.5 160.5,-151.5 110.5,-151.5\"/>\n",
       "<text text-anchor=\"start\" x=\"115.25\" y=\"-176.75\" font-family=\"Linux libertine\" font-size=\"10.00\">squeeze</text>\n",
       "<text text-anchor=\"start\" x=\"117.12\" y=\"-164\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"160.5,-173.5 160.5,-195.5 207.5,-195.5 207.5,-173.5 160.5,-173.5\"/>\n",
       "<text text-anchor=\"start\" x=\"170.12\" y=\"-181\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"207.5,-173.5 207.5,-195.5 275.5,-195.5 275.5,-173.5 207.5,-173.5\"/>\n",
       "<text text-anchor=\"start\" x=\"212.25\" y=\"-181\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"160.5,-151.5 160.5,-173.5 207.5,-173.5 207.5,-151.5 160.5,-151.5\"/>\n",
       "<text text-anchor=\"start\" x=\"165.25\" y=\"-159\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"207.5,-151.5 207.5,-173.5 275.5,-173.5 275.5,-151.5 207.5,-151.5\"/>\n",
       "<text text-anchor=\"start\" x=\"218.25\" y=\"-159\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 128) </text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;27 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>26&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193,-231.6C193,-223.99 193,-215.2 193,-206.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.5,-206.97 193,-196.97 189.5,-206.97 196.5,-206.97\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"267.5,-115.5 118.5,-115.5 118.5,-71.5 267.5,-71.5 267.5,-115.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"118.5,-71.5 118.5,-115.5 164.5,-115.5 164.5,-71.5 118.5,-71.5\"/>\n",
       "<text text-anchor=\"start\" x=\"126.5\" y=\"-96.75\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"123.12\" y=\"-84\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"164.5,-93.5 164.5,-115.5 211.5,-115.5 211.5,-93.5 164.5,-93.5\"/>\n",
       "<text text-anchor=\"start\" x=\"174.12\" y=\"-101\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"211.5,-93.5 211.5,-115.5 267.5,-115.5 267.5,-93.5 211.5,-93.5\"/>\n",
       "<text text-anchor=\"start\" x=\"216.25\" y=\"-101\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"164.5,-71.5 164.5,-93.5 211.5,-93.5 211.5,-71.5 164.5,-71.5\"/>\n",
       "<text text-anchor=\"start\" x=\"169.25\" y=\"-79\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"211.5,-71.5 211.5,-93.5 267.5,-93.5 267.5,-71.5 211.5,-71.5\"/>\n",
       "<text text-anchor=\"start\" x=\"219.25\" y=\"-79\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 10) </text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;28 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>27&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193,-151.6C193,-143.99 193,-135.2 193,-126.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.5,-126.97 193,-116.97 189.5,-126.97 196.5,-126.97\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"254.75,-35.5 131.25,-35.5 131.25,0 254.75,0 254.75,-35.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"131.25,0 131.25,-35.5 207.25,-35.5 207.25,0 131.25,0\"/>\n",
       "<text text-anchor=\"start\" x=\"136.25\" y=\"-21\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"150.88\" y=\"-8.25\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"207.25,0 207.25,-35.5 254.75,-35.5 254.75,0 207.25,0\"/>\n",
       "<text text-anchor=\"start\" x=\"212.25\" y=\"-14.62\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 10)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;29 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>28&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193,-71.56C193,-63.78 193,-54.83 193,-46.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.5,-46.56 193,-36.56 189.5,-46.56 196.5,-46.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f99dadbcbf0>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "\n",
    "model_graph = draw_graph(\n",
    "    model=MultiHeadAttentionMNISTModel(num_heads=2, attention_type='dot'), \n",
    "    input_size=(64, 28, 28),\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.8. <a id='toc11_5_8_'></a>[attention-seq2seq](#toc0_)\n",
    "\n",
    "- 加入attention机制的Seq2Seq；\n",
    "\n",
    "- 基于Attention的Seq2Seq。\n",
    "\n",
    "![attention_seq2seq](./Pytorch_Pictures/Attention/seq2seq_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "class AttentionDecoder(d2l.Decoder):\n",
    "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16]))"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = d2l.AdditiveAttention(num_hiddens, num_hiddens, num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        # Shape of `outputs`: (`num_steps`, `batch_size`, `num_hiddens`).\n",
    "        # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n",
    "        # `num_hiddens`)\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Shape of `enc_outputs`: (`batch_size`, `num_steps`, `num_hiddens`).\n",
    "        # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n",
    "        # `num_hiddens`)\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        # Shape of the output `X`: (`num_steps`, `batch_size`, `embed_size`)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            # 解码器输入，query是编码器输出最后时刻、最后层的隐状态，key和value是编码器输出所有时刻、所有层的隐状态\n",
    "            # hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "            # hidden_state[-1]: (batch_size, num_hiddens)\n",
    "            # hidden_state[-1].unsqueeze(1): (batch_size, 1, num_hiddens) \n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            # Shape of `context`: (`batch_size`, 1, `num_hiddens`)\n",
    "            context = self.attention(queries=query, keys=enc_outputs, values=enc_outputs, valid_lens=enc_valid_lens)\n",
    "\n",
    "            # Concatenate on the feature dimension\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
    "            # Reshape `x` as (1, `batch_size`, `embed_size` + `num_hiddens`)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        # After fully-connected layer transformation, shape of `outputs`:\n",
    "        # (`num_steps`, `batch_size`, `vocab_size`)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens]\n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n",
    "    \n",
    "\n",
    "# test\n",
    "#@tab pytorch\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval()\n",
    "\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "decoder.eval()\n",
    "\n",
    "X = d2l.zeros((4, 7), dtype=torch.long)  # (`batch_size`, `num_steps`)\n",
    "\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "\n",
    "output, state = decoder(X, state)\n",
    "\n",
    "output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.021, 7160.9 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T15:37:44.065724</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 262.1875 183.35625 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 82.69375 145.8 \n",
       "L 82.69375 7.2 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"md8492971bd\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md8492971bd\" x=\"82.69375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(76.33125 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 123.38125 145.8 \n",
       "L 123.38125 7.2 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md8492971bd\" x=\"123.38125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(113.8375 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 164.06875 145.8 \n",
       "L 164.06875 7.2 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md8492971bd\" x=\"164.06875\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(154.525 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 204.75625 145.8 \n",
       "L 204.75625 7.2 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md8492971bd\" x=\"204.75625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(195.2125 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md8492971bd\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(235.9 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 50.14375 118.916269 \n",
       "L 245.44375 118.916269 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m5542e6aea2\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5542e6aea2\" x=\"50.14375\" y=\"118.916269\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 122.715487) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 83.591518 \n",
       "L 245.44375 83.591518 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5542e6aea2\" x=\"50.14375\" y=\"83.591518\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 87.390737) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 50.14375 48.266767 \n",
       "L 245.44375 48.266767 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5542e6aea2\" x=\"50.14375\" y=\"48.266767\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 52.065986) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 50.14375 12.942017 \n",
       "L 245.44375 12.942017 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5542e6aea2\" x=\"50.14375\" y=\"12.942017\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 16.741236) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_19\">\n",
       "    <path d=\"M 50.14375 13.5 \n",
       "L 58.28125 56.319599 \n",
       "L 66.41875 78.830246 \n",
       "L 74.55625 94.058939 \n",
       "L 82.69375 105.535274 \n",
       "L 90.83125 113.796988 \n",
       "L 98.96875 119.992546 \n",
       "L 107.10625 123.844395 \n",
       "L 115.24375 128.112068 \n",
       "L 123.38125 130.26147 \n",
       "L 131.51875 132.372023 \n",
       "L 139.65625 134.260435 \n",
       "L 147.79375 135.056427 \n",
       "L 155.93125 135.765734 \n",
       "L 164.06875 136.818003 \n",
       "L 172.20625 136.778261 \n",
       "L 180.34375 137.997905 \n",
       "L 188.48125 138.093578 \n",
       "L 196.61875 137.964007 \n",
       "L 204.75625 138.993802 \n",
       "L 212.89375 139.139283 \n",
       "L 221.03125 139.244454 \n",
       "L 229.16875 139.5 \n",
       "L 237.30625 139.298933 \n",
       "L 245.44375 139.424578 \n",
       "\" clip-path=\"url(#p4907535705)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p4907535705\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab all\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 250, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !,  bleu 1.000\n",
      "i lost . => j'ai perdu .,  bleu 1.000\n",
      "he's calm . => il est paresseux .,  bleu 0.658\n",
      "i'm home . => je suis chez moi .,  bleu 1.000\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = d2l.predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}, ', f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "attention_weights = d2l.reshape(d2l.concat([step[0][0][0] for step in dec_attention_weight_seq], 0), (1, 1, -1, num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"167.87675pt\" height=\"183.35625pt\" viewBox=\"0 0 167.87675 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T15:38:04.217695</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 167.87675 183.35625 \n",
       "L 167.87675 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 34.240625 145.8 \n",
       "L 126.640625 145.8 \n",
       "L 126.640625 7.2 \n",
       "L 34.240625 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pe1f8629825)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAIEAAADBCAYAAADo+NM8AAACI0lEQVR4nO3cPUpeURhG0W28ISBKyE9jZ21tn0k4AieVaaTNKAJpUgVCBEFUUDSIfsn9MozXYq0JPG+xOeXZWS9/bRv0/eTT5HxfLu9G96uuN/9G91+NrvMiiAARIAISAYmAREAiIBGQCEgEJAISAYmAREAiIBGQCEgEJAISAYmAREAiIBGQCEgEJAISAYmAREC19Pw4esDn85vR/dOPB6P7VU/r6BcRXgJEQCIgEZAISAQkAhIBiYBEQCIgEZAISAQkAhIBiYBEQCIgEZAISAQkAhIBiYBEQCIgEZAISAQkAqpl5/3h6AEHu7MdXm3+ju6X/wl4AUSACBABiYBEQCIgEZAISAQkAhIBiYBEQCIgEZAISAQkAhIBiYBEQCIgEZAISAQkAhIBiYBEQCIgEVAt26vfowecHX0Y3f96cTe6X3X+NPtRhpcAESACEgGJgERAIiARkAhIBCQCEgGJgERAIiARkAhIBCQCEgGJgERAIiARkAhIBCQCEgGJgERAIqBa1oufowfc3m9G94/33ozuV/1Z19F9LwEiQAQkAhIBiYBEQCIgEZAISAQkAhIBiYBEQCIgEZAISAQkAhIBiYBEQCIgEZAISAQkAhIBiYBEQLX049voAW/3X4/ubx+2o/tV75bd0X0vASJABCQCEgGJgERAIiARkAhIBCQCEgGJgERAIiARkAhIBCQCEgGJgERAIiARkAhIBCQCEgGJgERA9R+LRDBCU0FHKwAAAABJRU5ErkJggg==\" id=\"imagec832c0a7cf\" transform=\"scale(1 -1) translate(0 -138.96)\" x=\"34.240625\" y=\"-6.84\" width=\"92.88\" height=\"138.96\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mba890f2254\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mba890f2254\" x=\"45.790625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(42.609375 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mba890f2254\" x=\"91.990625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(88.809375 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- Key posistions -->\n",
       "     <g transform=\"translate(44.772656 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 2694 \n",
       "L 3353 4666 \n",
       "L 4166 4666 \n",
       "L 1850 2491 \n",
       "L 4331 0 \n",
       "L 3500 0 \n",
       "L 1259 2247 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-4b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"60.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"122.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"181.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"213.066406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"276.542969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"337.724609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"389.824219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"417.607422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"469.707031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"508.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"536.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"597.880859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"661.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m427fa75bd0\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m427fa75bd0\" x=\"34.240625\" y=\"18.75\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(20.878125 22.549219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m427fa75bd0\" x=\"34.240625\" y=\"41.85\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(20.878125 45.649219) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m427fa75bd0\" x=\"34.240625\" y=\"64.95\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(20.878125 68.749219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m427fa75bd0\" x=\"34.240625\" y=\"88.05\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(20.878125 91.849219) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m427fa75bd0\" x=\"34.240625\" y=\"111.15\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(20.878125 114.949219) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m427fa75bd0\" x=\"34.240625\" y=\"134.25\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(20.878125 138.049219) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- Query posistions -->\n",
       "     <g transform=\"translate(14.798438 118.299219) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-51\" d=\"M 2522 4238 \n",
       "Q 1834 4238 1429 3725 \n",
       "Q 1025 3213 1025 2328 \n",
       "Q 1025 1447 1429 934 \n",
       "Q 1834 422 2522 422 \n",
       "Q 3209 422 3611 934 \n",
       "Q 4013 1447 4013 2328 \n",
       "Q 4013 3213 3611 3725 \n",
       "Q 3209 4238 2522 4238 \n",
       "z\n",
       "M 3406 84 \n",
       "L 4238 -825 \n",
       "L 3475 -825 \n",
       "L 2784 -78 \n",
       "Q 2681 -84 2626 -87 \n",
       "Q 2572 -91 2522 -91 \n",
       "Q 1538 -91 948 567 \n",
       "Q 359 1225 359 2328 \n",
       "Q 359 3434 948 4092 \n",
       "Q 1538 4750 2522 4750 \n",
       "Q 3503 4750 4090 4092 \n",
       "Q 4678 3434 4678 2328 \n",
       "Q 4678 1516 4351 937 \n",
       "Q 4025 359 3406 84 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"78.710938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"142.089844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"203.613281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"244.726562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"303.90625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"335.693359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"399.169922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"460.351562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"512.451172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"540.234375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"592.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"631.542969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"659.326172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"720.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"783.886719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 34.240625 145.8 \n",
       "L 34.240625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 126.640625 145.8 \n",
       "L 126.640625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 34.240625 145.8 \n",
       "L 126.640625 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 34.240625 7.2 \n",
       "L 126.640625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 133.615625 118.08 \n",
       "L 137.773625 118.08 \n",
       "L 137.773625 34.92 \n",
       "L 133.615625 34.92 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAUAAABzCAYAAABQFMg5AAAAxklEQVR4nLWVOw7DMAxDWcD3v2qXTok+3eNngIHhjARF0pLsfPr3bT2+oconpqEuF6xJcskEo2am747h/Ui2UdyuOx4zXaaSjPxyBCl8++W+e8SWJoN2Q9LeECz3R/ximn6XKDwb4X6Spl2+2E/baDM8dwnvEYZHTQCLNJHpG3XCa9MnjOyGFEUqeqgRDCy3NdM2yjigSTkRLPpNIDj3XRqJzBnTKKERaRJz0wiZqDlv0ooJc9MIYt575ci8DmjiMS+8Ha7RHygaEE7sVIc4AAAAAElFTkSuQmCC\" id=\"image6833f06dc6\" transform=\"scale(1 -1) translate(0 -82.8)\" x=\"133.92\" y=\"-34.56\" width=\"3.6\" height=\"82.8\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"md848321b88\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md848321b88\" x=\"137.773625\" y=\"111.108273\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.1 -->\n",
       "      <g transform=\"translate(144.773625 114.907491) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md848321b88\" x=\"137.773625\" y=\"77.970959\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(144.773625 81.770178) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md848321b88\" x=\"137.773625\" y=\"44.833645\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.3 -->\n",
       "      <g transform=\"translate(144.773625 48.632864) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 133.615625 118.08 \n",
       "L 135.694625 118.08 \n",
       "L 137.773625 118.08 \n",
       "L 137.773625 34.92 \n",
       "L 135.694625 34.92 \n",
       "L 133.615625 34.92 \n",
       "L 133.615625 118.08 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pe1f8629825\">\n",
       "   <rect x=\"34.240625\" y=\"7.2\" width=\"92.4\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "# Plus one to include the end-of-sequence token\n",
    "d2l.show_heatmaps(attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(), xlabel='Key posistions', ylabel='Query posistions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6. <a id='toc11_6_'></a>[Transformer](#toc0_)\n",
    "Transformer是一种神经网络架构，最初由Vaswani等人在2017年的论文《Attention is All You Need》中提出。它主要用于处理序列到序列的任务，如机器翻译。Transformer架构的核心是自注意力机制（Self-Attention），它能够捕捉序列中不同位置之间的依赖关系。\n",
    "```python\n",
    "完全基于注意力机制的Encoder-Decoder架构。\n",
    "1.多头自注意力机制；\n",
    "2.掩码；\n",
    "3.Encoder-Decoder框架。\n",
    "```\n",
    "\n",
    "![Transformer](./Pytorch_Pictures/Transformer/Transformer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.1. <a id='toc11_6_1_'></a>[简洁实现](#toc0_)\n",
    "PyTorch中提供了nn.Transformer类，可以方便地实现Transformer模型。同时，nn.TransformerEncoder和nn.TransformerDecoder类分别用于实现编码器和解码器。以及nn.TransformerEncoderLayer和nn.TransformerDecoderLayer类分别用于实现编码器和解码器的每一层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "trans = nn.Transformer(\n",
    "    d_model = 512, \n",
    "    nhead = 8, \n",
    "    num_encoder_layers = 6, \n",
    "    num_decoder_layers = 6, \n",
    "    dim_feedforward = 2048, \n",
    "    dropout = 0.1, \n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "\n",
    "# 创建输入张量\n",
    "src = torch.rand(2, 32, 512)  # (batch_size, seq_len, embed_size)\n",
    "tgt = torch.rand(2, 32, 512)  # (batch_size, seq_len, embed_size)\n",
    "\n",
    "# 前向传播\n",
    "output = trans(src, tgt)\n",
    "\n",
    "print(\"输出形状:\", output.shape)  # 输出形状: (batch_size, seq_len, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "# ----------------\n",
    "# parameters\n",
    "# ----------------\n",
    "d_model = 512 \n",
    "nhead = 8\n",
    "dim_feedforward = 2048 \n",
    "dropout = 0.1\n",
    "batch_first = True \n",
    "num_layers = 6\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# model\n",
    "# ----------------\n",
    "encoder = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(\n",
    "        d_model = d_model, \n",
    "        nhead = nhead, \n",
    "        dim_feedforward = dim_feedforward, \n",
    "        dropout = dropout, \n",
    "        batch_first = batch_first\n",
    "    ), \n",
    "    num_layers = num_layers\n",
    ")\n",
    "\n",
    "decoder = nn.TransformerDecoder(\n",
    "    nn.TransformerDecoderLayer(\n",
    "        d_model = d_model, \n",
    "        nhead = nhead, \n",
    "        dim_feedforward = dim_feedforward, \n",
    "        dropout = dropout, \n",
    "        batch_first = batch_first\n",
    "    ), \n",
    "    num_layers = num_layers\n",
    ")\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_outputs = self.encoder(src)\n",
    "        dec_state = self.init_state(enc_outputs)\n",
    "        dec_outputs, dec_state = self.decoder(tgt, dec_state)\n",
    "        return dec_outputs, dec_state \n",
    "    \n",
    "    def init_state(self, enc_outputs):\n",
    "        '''直接在Seq2SeqModel中实现对Decoder的初始化'''\n",
    "        return enc_outputs, [[None] * self.decoder.num_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.2. <a id='toc11_6_2_'></a>[位置编码](#toc0_)\n",
    "为什么需要位置编码？  \n",
    "  - 在Transformer中，所有输入数据是并行处理的，因此模型并不直接知道每个单词或元素在序列中的顺序。例如，输入序列 [A, B, C] 和 [C, B, A] 对于Transformer的自注意力机制来说，如果没有位置编码，将会完全一样。 \n",
    "  \n",
    "位置编码的原理：\n",
    "  - 位置编码将位置信息通过某种方式编码为向量，添加到每个输入元素的表示中，使得每个元素不仅包含其自身的特征，还包括它在序列中的位置。 \n",
    "\n",
    "\n",
    "常见的两种位置编码方式\n",
    "  - 绝对位置编码（Absolute Positional Encoding）：每个位置都有唯一的编码，通常采用正弦和余弦函数生成。Google一帮人发明了利用sin和cos函数编码位置信息并添加到输入X中。\n",
    "  - 相对位置编码（Relative Positional Encoding）：编码的是元素间的相对位置关系，而非具体的绝对位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.6.2.1. <a id='toc11_6_2_1_'></a>[绝对位置编码](#toc0_)\n",
    "\n",
    "$\\begin{aligned}&\\text{假设输入序列的长度为 }L,\\text{ 每个输入的词问量维度为 }d,\\text{ 那么对于位置 }pos\\text{ 和维度 }i\\text{ 的位置编码}\\\\&PE(pos,i)\\text{,定义如下:}\\\\&PE(pos,2i)=\\sin\\left(\\frac{pos}{10000\\frac{2i}{d}}\\right)\\\\&PE(pos,2i+1)=\\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\\\\&\\bullet\\quad pos:\\text{ 表示序列中每个元素的位置。}\\\\&\\bullet\\quad i:\\text{ 向量的维度索引。}\\\\&\\bullet\\quad d:\\quad\\text{嵌入向量的维度，偶数维度使用正弦，奇数维度使用余弦。}\\end{aligned}$\n",
    "\n",
    "假设我们有一个序列长度 𝐿=4，embedding维度𝑑=4。用简单的正弦和余弦函数计算得出位置编码的矩阵：\n",
    "\n",
    "|Position|\tPE(pos, 0) (sin)|\tPE(pos, 1) (cos)|\tPE(pos, 2) (sin)|\tPE(pos, 3) (cos)|\n",
    "|-|-|-|-|-|\n",
    "|0\t|0\t|1\t|0\t|1|\n",
    "|1\t|0.8415\t|0.5403\t|0.00999\t|0.99995|\n",
    "|2\t|0.9093\t|-0.4161\t|0.01998\t|0.9998|\n",
    "|3\t|0.1411\t|-0.98999\t|0.02997\t|0.99955|\n",
    "\n",
    "最终，Transformer会将这些位置编码与输入embedding相加，得到位置敏感的嵌入表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3]]),\n",
       " tensor([  1., 100.]),\n",
       " tensor([[0.0000, 0.0000],\n",
       "         [1.0000, 0.0100],\n",
       "         [2.0000, 0.0200],\n",
       "         [3.0000, 0.0300]]),\n",
       " tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "         [ 0.1411, -0.9900,  0.0300,  0.9996]]))"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_len = 4\n",
    "num_hiddens = 4\n",
    "\n",
    "# 创建一个位置编码矩阵框架\n",
    "# shape: (pos_len, num_hiddens)\n",
    "pos_matrix = torch.zeros(size=(pos_len, num_hiddens))\n",
    "\n",
    "# 创建位置索引\n",
    "pos = torch.arange(pos_len).reshape(-1, 1) \n",
    "# 创建位置编码的除数\n",
    "div_term = torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "# 计算位置除以除数，自动横向做了广播\n",
    "pos_div_term = pos / div_term\n",
    "\n",
    "# 计算位置编码\n",
    "## 偶数列使用正弦函数\n",
    "pos_matrix[:, 0::2] = torch.sin(pos_div_term)\n",
    "## 奇数列使用余弦函数\n",
    "pos_matrix[:, 1::2] = torch.cos(pos_div_term)\n",
    "\n",
    "pos, div_term, pos_div_term, pos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99dceeaae0>"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"380.482812pt\" height=\"183.35625pt\" viewBox=\"0 0 380.482812 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T18:41:18.001846</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 183.35625 \n",
       "L 380.482812 183.35625 \n",
       "L 380.482812 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 373.282813 145.8 \n",
       "L 373.282813 7.2 \n",
       "L 38.482813 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"ma03b8e7408\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma03b8e7408\" x=\"53.700994\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(50.519744 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma03b8e7408\" x=\"105.288051\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(98.925551 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma03b8e7408\" x=\"156.875108\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(150.512608 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma03b8e7408\" x=\"208.462165\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(202.099665 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma03b8e7408\" x=\"260.049222\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(253.686722 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma03b8e7408\" x=\"311.636279\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(305.273779 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma03b8e7408\" x=\"363.223336\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(356.860836 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- Row (position) -->\n",
       "     <g transform=\"translate(170.189844 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-52\" d=\"M 2841 2188 \n",
       "Q 3044 2119 3236 1894 \n",
       "Q 3428 1669 3622 1275 \n",
       "L 4263 0 \n",
       "L 3584 0 \n",
       "L 2988 1197 \n",
       "Q 2756 1666 2539 1819 \n",
       "Q 2322 1972 1947 1972 \n",
       "L 1259 1972 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2853 4666 3247 4331 \n",
       "Q 3641 3997 3641 3322 \n",
       "Q 3641 2881 3436 2590 \n",
       "Q 3231 2300 2841 2188 \n",
       "z\n",
       "M 1259 4147 \n",
       "L 1259 2491 \n",
       "L 2053 2491 \n",
       "Q 2509 2491 2742 2702 \n",
       "Q 2975 2913 2975 3322 \n",
       "Q 2975 3731 2742 3939 \n",
       "Q 2509 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n",
       "L 844 3500 \n",
       "L 1563 769 \n",
       "L 2278 3500 \n",
       "L 2956 3500 \n",
       "L 3675 769 \n",
       "L 4391 3500 \n",
       "L 4966 3500 \n",
       "L 4050 0 \n",
       "L 3372 0 \n",
       "L 2619 2869 \n",
       "L 1863 0 \n",
       "L 1184 0 \n",
       "L 269 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"64.982422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-77\" x=\"126.164062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"207.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"239.738281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"278.751953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"342.228516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"403.410156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"455.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"483.292969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"522.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"550.285156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"611.466797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"674.845703\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path id=\"m41a480bd1f\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m41a480bd1f\" x=\"38.482813\" y=\"139.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(7.2 143.299219) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m41a480bd1f\" x=\"38.482813\" y=\"108\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(7.2 111.799219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m41a480bd1f\" x=\"38.482813\" y=\"76.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(15.579688 80.299219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m41a480bd1f\" x=\"38.482813\" y=\"45\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(15.579688 48.799219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m41a480bd1f\" x=\"38.482813\" y=\"13.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(15.579688 17.299219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path d=\"M 53.700994 76.5 \n",
       "L 58.8597 65.355792 \n",
       "L 64.018406 54.563068 \n",
       "L 69.177111 44.462222 \n",
       "L 74.335817 35.371837 \n",
       "L 79.494523 27.578612 \n",
       "L 84.653229 21.328343 \n",
       "L 89.811934 16.818165 \n",
       "L 94.97064 14.190325 \n",
       "L 100.129346 13.527701 \n",
       "L 105.288051 14.851191 \n",
       "L 110.446757 18.119061 \n",
       "L 115.605463 23.22824 \n",
       "L 120.764168 30.017578 \n",
       "L 125.922874 38.272962 \n",
       "L 131.08158 47.733995 \n",
       "L 136.240286 58.102294 \n",
       "L 141.398991 69.050862 \n",
       "L 146.557697 80.234358 \n",
       "L 151.716403 91.300088 \n",
       "L 156.875108 101.899017 \n",
       "L 162.033814 111.696885 \n",
       "L 167.19252 120.384648 \n",
       "L 172.351225 127.688311 \n",
       "L 177.509931 133.377536 \n",
       "L 182.668637 137.272851 \n",
       "L 187.827343 139.251424 \n",
       "L 192.986048 139.250846 \n",
       "L 198.144754 137.271135 \n",
       "L 203.30346 133.37473 \n",
       "L 208.462165 127.684526 \n",
       "L 213.620871 120.379992 \n",
       "L 218.779577 111.691504 \n",
       "L 223.938282 101.893063 \n",
       "L 229.096988 91.293765 \n",
       "L 234.255694 80.227879 \n",
       "L 239.4144 69.044417 \n",
       "L 244.573105 58.096071 \n",
       "L 249.731811 47.728208 \n",
       "L 254.890517 38.267791 \n",
       "L 260.049222 30.0132 \n",
       "L 265.207928 23.224759 \n",
       "L 270.366634 18.116616 \n",
       "L 275.525339 14.849854 \n",
       "L 280.684045 13.52751 \n",
       "L 285.842751 14.191283 \n",
       "L 291.001457 16.820238 \n",
       "L 296.160162 21.331467 \n",
       "L 301.318868 27.582724 \n",
       "L 306.477574 35.376772 \n",
       "L 311.636279 44.467828 \n",
       "L 316.794985 54.569164 \n",
       "L 321.953691 65.362187 \n",
       "L 327.112396 76.50649 \n",
       "L 332.271102 87.650589 \n",
       "L 337.429808 98.443004 \n",
       "L 342.588514 108.543399 \n",
       "L 347.747219 117.633104 \n",
       "L 352.905925 125.425496 \n",
       "L 358.064631 131.6748 \n",
       "\" clip-path=\"url(#p5c3294a624)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 53.700994 13.5 \n",
       "L 58.8597 14.493496 \n",
       "L 64.018406 17.442644 \n",
       "L 69.177111 22.254439 \n",
       "L 74.335817 28.77711 \n",
       "L 79.494523 36.804942 \n",
       "L 84.653229 46.084737 \n",
       "L 89.811934 56.323816 \n",
       "L 94.97064 67.199234 \n",
       "L 100.129346 78.368 \n",
       "L 105.288051 89.477851 \n",
       "L 110.446757 100.178386 \n",
       "L 115.605463 110.132123 \n",
       "L 120.764168 119.025106 \n",
       "L 125.922874 126.576875 \n",
       "L 131.08158 132.549237 \n",
       "L 136.240286 136.753833 \n",
       "L 141.398991 139.058056 \n",
       "L 146.557697 139.389225 \n",
       "L 151.716403 137.736898 \n",
       "L 156.875108 134.153187 \n",
       "L 162.033814 128.751119 \n",
       "L 167.19252 121.701079 \n",
       "L 172.351225 113.225425 \n",
       "L 177.509931 103.591439 \n",
       "L 182.668637 93.103029 \n",
       "L 187.827343 82.090967 \n",
       "L 192.986048 70.902569 \n",
       "L 198.144754 59.890681 \n",
       "L 203.30346 49.402673 \n",
       "L 208.462165 39.769302 \n",
       "L 213.620871 31.2944 \n",
       "L 218.779577 24.245258 \n",
       "L 223.938282 18.844191 \n",
       "L 229.096988 15.261578 \n",
       "L 234.255694 13.610392 \n",
       "L 239.4144 13.94271 \n",
       "L 244.573105 16.248067 \n",
       "L 249.731811 20.453733 \n",
       "L 254.890517 26.427072 \n",
       "L 260.049222 33.979686 \n",
       "L 265.207928 42.87339 \n",
       "L 270.366634 52.827643 \n",
       "L 275.525339 63.528508 \n",
       "L 280.684045 74.638487 \n",
       "L 285.842751 85.807178 \n",
       "L 291.001457 96.682325 \n",
       "L 296.160162 106.920933 \n",
       "L 301.318868 116.200123 \n",
       "L 306.477574 124.227145 \n",
       "L 311.636279 130.748869 \n",
       "L 316.794985 135.55962 \n",
       "L 321.953691 138.507653 \n",
       "L 327.112396 139.5 \n",
       "L 332.271102 138.505359 \n",
       "L 337.429808 135.555099 \n",
       "L 342.588514 130.742238 \n",
       "L 347.747219 124.218628 \n",
       "L 352.905925 116.19 \n",
       "L 358.064631 106.909564 \n",
       "\" clip-path=\"url(#p5c3294a624)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 53.700994 76.5 \n",
       "L 58.8597 70.210494 \n",
       "L 64.018406 63.983832 \n",
       "L 69.177111 57.882226 \n",
       "L 74.335817 51.966643 \n",
       "L 79.494523 46.29619 \n",
       "L 84.653229 40.927523 \n",
       "L 89.811934 35.914287 \n",
       "L 94.97064 31.306567 \n",
       "L 100.129346 27.150408 \n",
       "L 105.288051 23.48733 \n",
       "L 110.446757 20.353934 \n",
       "L 115.605463 17.781538 \n",
       "L 120.764168 15.795834 \n",
       "L 125.922874 14.416667 \n",
       "L 131.08158 13.657815 \n",
       "L 136.240286 13.526864 \n",
       "L 141.398991 14.025116 \n",
       "L 146.557697 15.147599 \n",
       "L 151.716403 16.883094 \n",
       "L 156.875108 19.214263 \n",
       "L 162.033814 22.117806 \n",
       "L 167.19252 25.56473 \n",
       "L 172.351225 29.520569 \n",
       "L 177.509931 33.945826 \n",
       "L 182.668637 38.796257 \n",
       "L 187.827343 44.023409 \n",
       "L 192.986048 49.57507 \n",
       "L 198.144754 55.395743 \n",
       "L 203.30346 61.427298 \n",
       "L 208.462165 67.60944 \n",
       "L 213.620871 73.880412 \n",
       "L 218.779577 80.177574 \n",
       "L 223.938282 86.437976 \n",
       "L 229.096988 92.599096 \n",
       "L 234.255694 98.599343 \n",
       "L 239.4144 104.378782 \n",
       "L 244.573105 109.879679 \n",
       "L 249.731811 115.047043 \n",
       "L 254.890517 119.829274 \n",
       "L 260.049222 124.178557 \n",
       "L 265.207928 128.051455 \n",
       "L 270.366634 131.409266 \n",
       "L 275.525339 134.218458 \n",
       "L 280.684045 136.450932 \n",
       "L 285.842751 138.084398 \n",
       "L 291.001457 139.102531 \n",
       "L 296.160162 139.495163 \n",
       "L 301.318868 139.258367 \n",
       "L 306.477574 138.394512 \n",
       "L 311.636279 136.91223 \n",
       "L 316.794985 134.826329 \n",
       "L 321.953691 132.157649 \n",
       "L 327.112396 128.932843 \n",
       "L 332.271102 125.18416 \n",
       "L 337.429808 120.949039 \n",
       "L 342.588514 116.269803 \n",
       "L 347.747219 111.193199 \n",
       "L 352.905925 105.769927 \n",
       "L 358.064631 100.054224 \n",
       "\" clip-path=\"url(#p5c3294a624)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 53.700994 13.5 \n",
       "L 58.8597 13.814737 \n",
       "L 64.018406 14.755804 \n",
       "L 69.177111 16.3138 \n",
       "L 74.335817 18.473158 \n",
       "L 79.494523 21.212299 \n",
       "L 84.653229 24.503856 \n",
       "L 89.811934 28.314941 \n",
       "L 94.97064 32.607477 \n",
       "L 100.129346 37.338571 \n",
       "L 105.288051 42.460953 \n",
       "L 110.446757 47.923445 \n",
       "L 115.605463 53.671465 \n",
       "L 120.764168 59.647571 \n",
       "L 125.922874 65.792069 \n",
       "L 131.08158 72.043557 \n",
       "L 136.240286 78.339571 \n",
       "L 141.398991 84.617206 \n",
       "L 146.557697 90.813729 \n",
       "L 151.716403 96.867241 \n",
       "L 156.875108 102.717251 \n",
       "L 162.033814 108.3053 \n",
       "L 167.19252 113.575573 \n",
       "L 172.351225 118.475387 \n",
       "L 177.509931 122.955809 \n",
       "L 182.668637 126.972046 \n",
       "L 187.827343 130.483989 \n",
       "L 192.986048 133.456546 \n",
       "L 198.144754 135.860008 \n",
       "L 203.30346 137.670365 \n",
       "L 208.462165 138.869527 \n",
       "L 213.620871 139.445514 \n",
       "L 218.779577 139.392571 \n",
       "L 223.938282 138.711228 \n",
       "L 229.096988 137.408286 \n",
       "L 234.255694 135.496771 \n",
       "L 239.4144 132.995782 \n",
       "L 244.573105 129.9303 \n",
       "L 249.731811 126.330969 \n",
       "L 254.890517 122.233731 \n",
       "L 260.049222 117.679547 \n",
       "L 265.207928 112.713914 \n",
       "L 270.366634 107.386442 \n",
       "L 275.525339 101.750337 \n",
       "L 280.684045 95.861965 \n",
       "L 285.842751 89.780136 \n",
       "L 291.001457 83.565615 \n",
       "L 296.160162 77.280498 \n",
       "L 301.318868 70.987552 \n",
       "L 306.477574 64.749714 \n",
       "L 311.636279 58.629281 \n",
       "L 316.794985 52.687407 \n",
       "L 321.953691 46.983461 \n",
       "L 327.112396 41.574405 \n",
       "L 332.271102 36.514343 \n",
       "L 337.429808 31.853804 \n",
       "L 342.588514 27.639354 \n",
       "L 347.747219 23.913101 \n",
       "L 352.905925 20.712264 \n",
       "L 358.064631 18.068855 \n",
       "\" clip-path=\"url(#p5c3294a624)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 38.482813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 373.282813 145.8 \n",
       "L 373.282813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 373.282812 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 38.482813 7.2 \n",
       "L 373.282812 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 45.482813 140.8 \n",
       "L 101.41875 140.8 \n",
       "Q 103.41875 140.8 103.41875 138.8 \n",
       "L 103.41875 81.0875 \n",
       "Q 103.41875 79.0875 101.41875 79.0875 \n",
       "L 45.482813 79.0875 \n",
       "Q 43.482813 79.0875 43.482813 81.0875 \n",
       "L 43.482813 138.8 \n",
       "Q 43.482813 140.8 45.482813 140.8 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 47.482813 87.185938 \n",
       "L 57.482813 87.185938 \n",
       "L 67.482812 87.185938 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- col 6 -->\n",
       "     <g transform=\"translate(75.482812 90.685938) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"143.945312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-36\" x=\"175.732422\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 47.482813 101.864063 \n",
       "L 57.482813 101.864063 \n",
       "L 67.482812 101.864063 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- col 7 -->\n",
       "     <g transform=\"translate(75.482812 105.364063) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"143.945312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-37\" x=\"175.732422\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 47.482813 116.542188 \n",
       "L 57.482813 116.542188 \n",
       "L 67.482812 116.542188 \n",
       "\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- col 8 -->\n",
       "     <g transform=\"translate(75.482812 120.042188) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"143.945312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-38\" x=\"175.732422\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\">\n",
       "     <path d=\"M 47.482813 131.220313 \n",
       "L 57.482813 131.220313 \n",
       "L 67.482812 131.220313 \n",
       "\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- col 9 -->\n",
       "     <g transform=\"translate(75.482812 134.720313) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"143.945312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-39\" x=\"175.732422\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5c3294a624\">\n",
       "   <rect x=\"38.482813\" y=\"7.2\" width=\"334.8\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 600x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "#@save\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 创建一个足够长的P\n",
    "        # shape: (1, max_len, num_hiddens)\n",
    "        self.P = torch.zeros(size=(1, max_len, num_hiddens))\n",
    "\n",
    "        # pos / 10000^(2i/d)\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, (torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens))\n",
    "        \n",
    "        # 计算位置编码\n",
    "        ## 偶数列使用正弦函数\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        ## 奇数列使用余弦函数\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, seq_len, embedding_dim) = (batch_size, pos_len, num_hiddens)\n",
    "        # P[:, :X.shape[1], :] : (1, pos_len, num_hiddens)\n",
    "        # 自动广播了batch维度：(batch_size, pos_len, num_hiddens) + (1, pos_len, num_hiddens) = (batch_size, pos_len, num_hiddens)\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)  # On the same device\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, num_steps, encoding_dim = 32, 60, 32\n",
    "pos_encoding = PositionalEncoding(num_hiddens=encoding_dim, max_len=10000, dropout=0.1)\n",
    "pos_encoding.eval()\n",
    "\n",
    "# 创建一个随机输入张量\n",
    "X = torch.rand(size=(batch_size, num_steps, encoding_dim))\n",
    "\n",
    "# 对输入张量进行位置编码\n",
    "X = pos_encoding(X=X)\n",
    "\n",
    "# 获取位置编码矩阵\n",
    "batch_size, num_steps, encoding_dim = X.shape\n",
    "P = pos_encoding.P[:, :num_steps, :encoding_dim]\n",
    "\n",
    "# Draw a plot picture\n",
    "plt.figure(figsize=(6, 2.5))\n",
    "# num_steps: torch.arange(num_steps)\n",
    "# P[0, :, 6:10].T[0]: 取P的第0个batch，num_hiddens的第6到10列，并转置\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[0], label='col 6')\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[1], label='col 7')\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[2], label='col 8')\n",
    "plt.plot(torch.arange(num_steps), P[0, :, 6:10].T[3], label='col 9')\n",
    "plt.xlabel('Row (position)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.6.2.2. <a id='toc11_6_2_2_'></a>[相对位置编码](#toc0_)\n",
    "相对位置编码（Relative Positional Encoding）可以通过学习矩阵或嵌入层来表示位置差值。相对位置编码不直接关心位置 𝑝𝑜𝑠 的绝对位置，而是关心两个位置之间的距离 𝑝𝑜𝑠𝑖−𝑝𝑜𝑠𝑗。这在长序列中非常有用，因为它不局限于特定位置，而是考虑相对距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class RelativePositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 创建一个足够长的相对位置编码矩阵\n",
    "        self.relative_positions = torch.zeros(size=(max_len, max_len, num_hiddens))\n",
    "        for i in range(max_len):\n",
    "            for j in range(max_len):\n",
    "                relative_position = i - j\n",
    "                self.relative_positions[i, j, :] = self._get_relative_position_encoding(relative_position)\n",
    "\n",
    "    def _get_relative_position_encoding(self, relative_position):\n",
    "        # 使用正弦和余弦函数计算相对位置编码\n",
    "        encoding = torch.zeros(self.num_hiddens)\n",
    "        for i in range(0, self.num_hiddens, 2):\n",
    "            encoding[i] = torch.sin(torch.tensor(relative_position / (10000 ** (i / self.num_hiddens))))\n",
    "            if i + 1 < self.num_hiddens:\n",
    "                encoding[i + 1] = torch.cos(torch.tensor(relative_position / (10000 ** ((i + 1) / self.num_hiddens))))\n",
    "        return encoding\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "        relative_positions = self.relative_positions[:seq_len, :seq_len, :].to(X.device)\n",
    "        return X + relative_positions\n",
    "\n",
    "\n",
    "# 测试相对位置编码\n",
    "batch_size, encoding_dim, num_steps = 32, 60, 32\n",
    "relative_pos_encoding = RelativePositionalEncoding(num_hiddens=encoding_dim, max_len=num_steps)\n",
    "relative_pos_encoding.eval()\n",
    "\n",
    "# 创建一个随机输入张量\n",
    "X = torch.zeros(size=(batch_size, num_steps, encoding_dim))\n",
    "X_encoded = relative_pos_encoding(X=X)\n",
    "\n",
    "print(X_encoded.shape)  # 应输出: torch.Size([32, 60, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.6.2.3. <a id='toc11_6_2_3_'></a>[可学习的位置编码](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        # 可学习的参数\n",
    "        self.position_encoding = nn.Parameter(torch.zeros(size=(max_len, num_hiddens)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, seq_len, embedding_dim)\n",
    "        seq_len = X.size(1)\n",
    "        X = X + self.position_encoding[:seq_len, :].unsqueeze(0)\n",
    "        return self.dropout(X)\n",
    "    \n",
    "\n",
    "# Test\n",
    "batch_size, num_steps, encoding_dim = 32, 60, 32\n",
    "\n",
    "learned_pos_encoding = LearnedPositionalEncoding(num_hiddens=encoding_dim, dropout=0, max_len=num_steps)\n",
    "learned_pos_encoding.eval()\n",
    "\n",
    "X = torch.zeros(size=(batch_size, num_steps, encoding_dim))\n",
    "X = learned_pos_encoding(X=X)\n",
    "\n",
    "print(X.shape)  # 应输出: torch.Size([32, 60, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.3. <a id='toc11_6_3_'></a>[基于位置的前馈网络](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "#@save\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, seq_len, embed_size = 2, 3, 4\n",
    "\n",
    "# 实例化对象\n",
    "ffn = PositionWiseFFN(ffn_num_input=embed_size, ffn_num_hiddens=4, ffn_num_outputs=embed_size)\n",
    "ffn.eval()\n",
    "\n",
    "# (batch_size, seq_len, embed_size)\n",
    "x = torch.ones(size=(batch_size, seq_len, embed_size))\n",
    "\n",
    "# (batch_size, seq_len, ffn_num_outputs)\n",
    "y = ffn(x)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.4. <a id='toc11_6_4_'></a>[残差连接和层规范化](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tensor([[-1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>) \n",
      "batch norm: tensor([[-1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)\n",
    "\n",
    "\n",
    "# 测试\n",
    "X = torch.tensor([[1, 2], \n",
    "                  [2, 3]], dtype=torch.float32)\n",
    "\n",
    "# 在训练模式下计算X的均值和方差\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, seq_len, embed_size = 2, 3, 4\n",
    "\n",
    "# 实例化对象\n",
    "add_norm = AddNorm(normalized_shape=embed_size, dropout=0.5)\n",
    "add_norm.eval()\n",
    "\n",
    "# 测试\n",
    "X = torch.ones(size=(batch_size, seq_len, embed_size))\n",
    "\n",
    "add_norm(X=X, Y=X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.5. <a id='toc11_6_5_'></a>[编码器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        # From d2l\n",
    "        # self.attention = d2l.MultiHeadAttention(\n",
    "        #     key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n",
    "        #     use_bias)\n",
    "\n",
    "        # From 自己\n",
    "        self.attention = MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            query_size = query_size, \n",
    "            key_size = key_size, \n",
    "            num_hiddens = num_hiddens, \n",
    "            value_size = value_size,\n",
    "            attention_type='dot',\n",
    "            dropout = dropout\n",
    "        )\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        Y = self.addnorm1(X=X, Y=self.attention(queries=X, keys=X, values=X, valid_lens=valid_lens))\n",
    "        return self.addnorm2(X=Y, Y=self.ffn(Y))\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size, seq_len, embed_size = 2, 100, 24\n",
    "key_size, query_size, value_size, num_heads, num_hiddens, dropout = 24, 24, 24, 8, 24, 0.5 \n",
    "norm_shape = [100, 24]\n",
    "ffn_num_input, ffn_num_hiddens = 24, 48\n",
    " \n",
    "encoder_blk = EncoderBlock(key_size=key_size, query_size=query_size, value_size=value_size, num_hiddens=num_hiddens, norm_shape=norm_shape, ffn_num_input=ffn_num_input, ffn_num_hiddens=ffn_num_hiddens, num_heads=num_heads, dropout=dropout)\n",
    "encoder_blk.eval()\n",
    "\n",
    "X = torch.ones(size=(batch_size, seq_len, embed_size))\n",
    "\n",
    "encoder_blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer编码器\"\"\"\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 因为位置编码值在-1和1之间，\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放，\n",
    "        # 然后再与位置编码相加。\n",
    "        # X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        X = self.pos_encoding(self.embedding(X) * torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X)\n",
    "            # self.attention_weights[i] = blk.attention.attention.attention_weights_numpy\n",
    "        return X\n",
    "\n",
    "\n",
    "# 测试\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=200, \n",
    "    key_size=24, \n",
    "    query_size=24, \n",
    "    value_size=24, \n",
    "    num_hiddens=24, \n",
    "    norm_shape=[100, 24], \n",
    "    ffn_num_input=24, \n",
    "    ffn_num_hiddens=48, \n",
    "    num_heads=8, \n",
    "    num_layers=2, \n",
    "    dropout=0.5\n",
    ")\n",
    "encoder.eval()\n",
    "\n",
    "encoder(torch.ones((2, 100), dtype=torch.long)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.6. <a id='toc11_6_6_'></a>[解码器](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "class DecoderBlock(nn.Module):\n",
    "    # The `i`-th block in the decoder\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        # self.attention1 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.attention1 = MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            query_size = query_size, \n",
    "            key_size = key_size, \n",
    "            num_hiddens = num_hiddens, \n",
    "            value_size = value_size,\n",
    "            attention_type='dot',\n",
    "            dropout = dropout\n",
    "        )   \n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        # self.attention2 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.attention2 = MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            query_size = query_size, \n",
    "            key_size = key_size, \n",
    "            num_hiddens = num_hiddens, \n",
    "            value_size = value_size,\n",
    "            attention_type='dot'\n",
    "        )\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # During training, all the tokens of any output sequence are processed\n",
    "        # at the same time, so `state[2][self.i]` is `None` as initialized.\n",
    "        # When decoding any output sequence token by token during prediction,\n",
    "        # `state[2][self.i]` contains representations of the decoded output at\n",
    "        # the `i`-th block up to the current time step\n",
    "        # X: (batch_size, seq_len, embed_size)\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        \n",
    "        if self.training:  \n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            # Shape of `dec_valid_lens`: (`batch_size`, `num_steps`), where\n",
    "            # every row is [1, 2, ..., `num_steps`]\n",
    "            dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "\n",
    "        # Self-attention\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        # Encoder-decoder attention. Shape of `enc_outputs`:\n",
    "        # (`batch_size`, `num_steps`, `num_hiddens`)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state\n",
    "    \n",
    "\n",
    "# 测试\n",
    "decoder_blk = DecoderBlock(\n",
    "    key_size=key_size, \n",
    "    query_size=query_size, \n",
    "    value_size=value_size, \n",
    "    num_hiddens=num_hiddens, \n",
    "    norm_shape=norm_shape, \n",
    "    ffn_num_input=ffn_num_input, \n",
    "    ffn_num_hiddens=ffn_num_hiddens, \n",
    "    num_heads=num_heads, \n",
    "    dropout=dropout, \n",
    "    i=0\n",
    ")\n",
    "decoder_blk.eval()\n",
    "\n",
    "X = d2l.ones((2, 100, 24))\n",
    "\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): Embedding(200, 24)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (blks): Sequential(\n",
       "    (block0): DecoderBlock(\n",
       "      (attention1): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attention2): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=False, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionWiseFFN(\n",
       "        (dense1): Linear(in_features=24, out_features=48, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=48, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm3): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block1): DecoderBlock(\n",
       "      (attention1): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attention2): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_k): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (W_v): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (attention): DotProductAttentionForMultiHeadAttention(\n",
       "          (dropout): Dropout(p=False, inplace=False)\n",
       "        )\n",
       "        (W_o): Linear(in_features=24, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionWiseFFN(\n",
       "        (dense1): Linear(in_features=24, out_features=48, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=48, out_features=24, bias=True)\n",
       "      )\n",
       "      (addnorm3): AddNorm(\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=24, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            # Decoder self-attention weights\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # Encoder-decoder attention weights\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n",
    "    \n",
    "\n",
    "# 测试\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size=200, \n",
    "    key_size=24, \n",
    "    query_size=24, \n",
    "    value_size=24, \n",
    "    num_hiddens=24, \n",
    "    norm_shape=[100, 24], \n",
    "    ffn_num_input=24, \n",
    "    ffn_num_hiddens=48, \n",
    "    num_heads=8, \n",
    "    num_layers=2, \n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.7. <a id='toc11_6_7_'></a>[基于Transformer的Seq2Seq网络](#toc0_)\n",
    "```shell\n",
    "基于Transformer的Seq2Seq神经网络框架。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.029, 7064.6 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-07T15:48:10.944309</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 262.1875 183.35625 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 91.259539 145.8 \n",
       "L 91.259539 7.2 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m0e1645aec4\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0e1645aec4\" x=\"91.259539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(84.897039 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 142.654276 145.8 \n",
       "L 142.654276 7.2 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0e1645aec4\" x=\"142.654276\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(133.110526 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 194.049013 145.8 \n",
       "L 194.049013 7.2 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0e1645aec4\" x=\"194.049013\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(184.505263 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0e1645aec4\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(235.9 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 50.14375 122.104882 \n",
       "L 245.44375 122.104882 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"m6921abb2a9\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6921abb2a9\" x=\"50.14375\" y=\"122.104882\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 125.904101) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 50.14375 80.909231 \n",
       "L 245.44375 80.909231 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6921abb2a9\" x=\"50.14375\" y=\"80.909231\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 84.70845) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 39.71358 \n",
       "L 245.44375 39.71358 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6921abb2a9\" x=\"50.14375\" y=\"39.71358\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 43.512798) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 50.14375 13.5 \n",
       "L 60.422697 54.549547 \n",
       "L 70.701645 80.003832 \n",
       "L 80.980592 95.787202 \n",
       "L 91.259539 109.788249 \n",
       "L 101.538487 117.858018 \n",
       "L 111.817434 124.364808 \n",
       "L 122.096382 125.898477 \n",
       "L 132.375329 128.600444 \n",
       "L 142.654276 131.591171 \n",
       "L 152.933224 133.629076 \n",
       "L 163.212171 134.936159 \n",
       "L 173.491118 136.543166 \n",
       "L 183.770066 135.950349 \n",
       "L 194.049013 138.630156 \n",
       "L 204.327961 138.403006 \n",
       "L 214.606908 138.247982 \n",
       "L 224.885855 139.5 \n",
       "L 235.164803 139.05767 \n",
       "L 245.44375 139.025031 \n",
       "\" clip-path=\"url(#pb5e4914ba3)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pb5e4914ba3\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10\n",
    "lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=len(src_vocab), \n",
    "    key_size=key_size, \n",
    "    query_size=query_size, \n",
    "    value_size=value_size, \n",
    "    num_hiddens=num_hiddens, \n",
    "    norm_shape=norm_shape, \n",
    "    ffn_num_input=ffn_num_input, \n",
    "    ffn_num_hiddens=ffn_num_hiddens, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout\n",
    ")\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size=len(tgt_vocab), \n",
    "    key_size=key_size, \n",
    "    query_size=query_size, \n",
    "    value_size=value_size, \n",
    "    num_hiddens=num_hiddens, \n",
    "    norm_shape=norm_shape, \n",
    "    ffn_num_input=ffn_num_input, \n",
    "    ffn_num_hiddens=ffn_num_hiddens, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout\n",
    ")\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !,  bleu 1.000\n",
      "i lost . => j'ai perdu .,  bleu 1.000\n",
      "he's calm . => il est calme .,  bleu 1.000\n",
      "i'm home . => je suis chez moi .,  bleu 1.000\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = d2l.predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}, ',\n",
    "          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7. <a id='toc11_7_'></a>[BERT](#toc0_)\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是基于Transformer架构的一种预训练语言模型，由Google在2018年提出。BERT使用了Transformer的编码器部分，并通过双向训练来捕捉上下文信息。BERT的主要创新在于它的预训练任务（如Masked Language Model和Next Sentence Prediction），使其在各种自然语言处理任务中表现出色。\n",
    "\n",
    "- 就是Encoder部分\n",
    "\n",
    "- Base：12层，768维，12个注意力头，110M参数\n",
    "\n",
    "- Large：24层，1024维，16个注意力头，340M参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.1. <a id='toc11_7_1_'></a>[BERT encode block](#toc0_)\n",
    "\n",
    "\n",
    "![BERT](./Pytorch_Pictures/BERT/BERT.jpg)\n",
    "\n",
    "1. 词元化：将原始文本转化为词元列表（tokenization）。\n",
    "2. 添加特殊标记：在词元列表前后添加 '<cls>' 和 '<sep>'。\n",
    "3. 生成段标识：为每个词元分配相应的段标识。\n",
    "4. 转换为ID：使用词汇表（vocab）将词元转换为对应的ID。\n",
    "5. 位置编码：为每个词元添加位置编码。\n",
    "6. 输入模型：将处理后的序列输入到 BERT 模型中进行训练或推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['<cls>', 'You', 'are', 'the', 'best', '<sep>', 'You', 'are', 'the', 'worst', '<sep>']\n",
      "segments: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"\n",
    "    Get tokens of the BERT input sequence and their segment IDs.\n",
    "    Args:\n",
    "        tokens_a: List[str] 第一段文本的词元列表（即第一句话的分词结果）。\n",
    "        tokens_b: List[str] （可选）第二段文本的词元列表（即第二句话的分词结果）。在单句任务中，此参数可以省略。\n",
    "    Returns:\n",
    "        tokens: List[str] 词元列表，其中第一个词元是'<cls>'，表示序列的开始，最后一个词元是'<sep>'，表示序列的结束。\n",
    "        segments: List[int] 段标识列表，其中0表示第一段，1表示第二段。\n",
    "    \"\"\"\n",
    "    # classification (cls) and separator (sep) tokens are added\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments\n",
    "\n",
    "\n",
    "# 测试\n",
    "tokens_a = ['You', 'are', 'the', 'best']\n",
    "tokens_b = ['You', 'are', 'the', 'worst']\n",
    "\n",
    "tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "\n",
    "print(f'tokens: {tokens}')\n",
    "print(f'segments: {segments}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.shape: torch.Size([2, 8])\n",
      "segments.shape: torch.Size([2, 8])\n",
      "encoded_X.shape: torch.Size([2, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            # 此处调用TransformerEncoder中的EncoderBlock\n",
    "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        # 此处用nn.Parameter来创建一个可学习的参数，用于存储位置编码, 形状为(1, max_len, num_hiddens)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        # 词元嵌入和段嵌入相加\n",
    "        # (batch_size, seq_len, num_hiddens) + (batch_size, seq_len, num_hiddens) = (batch_size, seq_len, num_hiddens)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        # 位置编码\n",
    "        # (batch_size, seq_len, num_hiddens) + (1, seq_len, num_hiddens) = (batch_size, seq_len, num_hiddens)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        # X: (batch_size, seq_len, num_hiddens)\n",
    "        return X\n",
    "    \n",
    "\n",
    "# 测试\n",
    "#@tab pytorch\n",
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "\n",
    "#@tab pytorch\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "tokens = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "\n",
    "encoded_X = encoder(tokens, segments, None) \n",
    "# encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "\n",
    "# tokens.shape, segments.shape, encoded_X.shape\n",
    "print(f'tokens.shape: {tokens.shape}')\n",
    "print(f'segments.shape: {segments.shape}')\n",
    "print(f'encoded_X.shape: {encoded_X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.2. <a id='toc11_7_2_'></a>[Masked Language Modeling](#toc0_)\n",
    "MaskLM 类通过多层感知机（MLP）对被遮蔽的位置进行预测，输出每个被遮蔽位置的词汇表概率分布。这是 BERT 模型在预训练阶段的核心任务之一，旨在让模型学习上下文关系和词汇之间的语义联系。通过这样的设计，模型能够在处理自然语言理解任务时表现出色，因为它已经通过大量的无监督数据学习到了丰富的语言表示。\n",
    "\n",
    "- 选择一些位置进行预测，这些位置被称为被遮蔽的位置。\n",
    "- 被遮蔽的位置上的词元被替换为特殊的“<mask>”词元。\n",
    "- 模型需要预测这些被遮蔽位置上的原始词元。\n",
    "- `从经过BERTEncoder编码后的序列中，提取出被遮蔽位置（<mask>, mlm_positions）上的子序列，然后通过多层感知机（MLP）进行预测。`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_X.shape: torch.Size([2, 8, 768])\n",
      "mlm_positions.shape: torch.Size([2, 3])\n",
      "mlm_Y_hat.shape: torch.Size([2, 3, 10000])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "#@save\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size)) # 输出： (batch_size, num_pred_positions, vocab_size)\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        # pred_positions: (batch_size, num_pred_positions)\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        # 将预测位置展平成一维，用于后续的索引操作, 形状为 (batch_size * num_pred_positions), e.g., torch.tensor([1, 5, 2, 6, 1, 5])\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "\n",
    "        # 生成批次索引\n",
    "        ## X: (batch_size, seq_len, num_hiddens)\n",
    "        batch_size = X.shape[0]\n",
    "        ## 创建一个包含batch_size个元素的索引            \n",
    "        batch_idx = torch.arange(0, batch_size) \n",
    "        ## Suppose that `batch_size` = 2, `num_pred_positions` = 3, then `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "\n",
    "        # 根据batch_idx和pred_positions从X中提取出对应的子序列\n",
    "        ## masked_X: (batch_size * num_pred_positions, num_hiddens), 索引：encoded_X[[0, 0, 0, 1, 1, 1], [1, 5, 2, 6, 1, 5]]\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        ## masked_X: (batch_size, num_pred_positions, num_hiddens)\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        ## (batch_size, num_pred_positions, vocab_size)\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat\n",
    "    \n",
    "\n",
    "# 测试\n",
    "#@tab pytorch\n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])    # (batch_size, num_pred_positions)\n",
    "\n",
    "# encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "# mlm_positions: (batch_size, num_pred_positions)\n",
    "# mlm_Y_hat: (batch_size, num_pred_positions, vocab_size)\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)               \n",
    "\n",
    "print(f'encoded_X.shape: {encoded_X.shape}')\n",
    "print(f'mlm_positions.shape: {mlm_positions.shape}')\n",
    "print(f'mlm_Y_hat.shape: {mlm_Y_hat.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.3. <a id='toc11_7_3_'></a>[Next Sentence Prediction](#toc0_)\n",
    "NextSentencePred 类通常与 BERT 模型的编码器部分结合使用。在预训练 BERT 模型时，除了进行 Masked Language Modeling（MLM）任务，还会同时进行 NSP 任务。通过 NSP 任务，模型能够学习句子之间的关系，这对于诸如问答系统、自然语言推理等下游任务具有重要意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_X.shape: torch.Size([2, 6144])\n",
      "nsp_Y_hat.shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)\n",
    "    \n",
    "\n",
    "# 测试\n",
    "# PyTorch by default won't flatten the tensor as seen in mxnet where, if flatten=True, all but the first axis of input data are collapsed together\n",
    "## encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "## flattened_encoded_X: (batch_size, seq_len * num_hiddens)\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "\n",
    "# input_shape for NSP: (batch size, `num_hiddens`)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "\n",
    "# encoded_X: (batch_size, seq_len * num_hiddens)\n",
    "# nsp_Y_hat: (batch_size, 2)\n",
    "nsp_Y_hat = nsp(encoded_X) \n",
    "\n",
    "print(f'encoded_X.shape: {encoded_X.shape}')\n",
    "print(f'nsp_Y_hat.shape: {nsp_Y_hat.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.4. <a id='toc11_7_4_'></a>[BERT模型](#toc0_)\n",
    "\n",
    "![BERT模型](./Pytorch_Pictures/BERT/BERT_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768, mlm_in_features=768, nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        # tokens: (batch_size, seq_len)\n",
    "        # segments: (batch_size, seq_len)\n",
    "        # valid_lens: (batch_size,)\n",
    "        # pred_positions: (batch_size, num_pred_positions)\n",
    "        # encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            # mlm_Y_hat: (batch_size, num_pred_positions, vocab_size)\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # The hidden layer of the MLP classifier for next sentence prediction. 0 is the index of the '<cls>' token\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        # encoded_X: (batch_size, seq_len, num_hiddens)\n",
    "        # mlm_Y_hat: (batch_size, num_pred_positions, vocab_size)\n",
    "        # nsp_Y_hat: (batch_size, 2)\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n",
    "    \n",
    "\n",
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.5. <a id='toc11_7_5_'></a>[Datasets for Pre-training](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import random\n",
    "\n",
    "\n",
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    # file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    # with open(file_name, 'r') as f:\n",
    "    #     lines = f.readlines()\n",
    "    file_name = os.path.join(data_dir, 'train-00000-of-00001.parquet')\n",
    "    df = pd.read_parquet(data_dir)\n",
    "    lines = df['text'].tolist()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.1. <a id='toc11_7_5_1_'></a>[生成下一句预测任务的数据](#toc0_)\n",
    "下一句预测任务 是BERT预训练的两个主要任务之一（另一个是遮蔽语言模型）。NSP任务的目的是让模型理解句子之间的关系，判断一个句子是否是另一个句子的真实下一句。这对于下游任务如问答系统、自然语言推理等具有重要意义。  \n",
    "\n",
    "函数 _get_next_sentence 的具体作用：\n",
    "  - 正样本生成：以50%的概率，函数返回的 next_sentence 是 sentence 的真实下一句，对应标签 is_next=True。\n",
    "  - 负样本生成：以另外50%的概率，函数返回的 next_sentence 是随机选择的其他段落中的句子，对应标签 is_next=False。\n",
    "通过这种方式，模型在训练过程中能够接触到正负两类样本，从而学习句子之间的逻辑关系和上下文关联。\n",
    "\n",
    "函数 _get_nsp_data_from_paragraph 的主要作用是从给定的段落中生成用于下一句预测任务的训练数据。具体步骤包括遍历段落中的句子对、生成句子对及其标签、过滤超长的句子对、格式化词元和段落标记，并将符合条件的句子对数据收集起来。通过这种方式，模型在训练过程中能够学习到句子之间的逻辑关系和上下文关联，从而提高其在诸如问答系统、自然语言推理等下游任务中的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['句子1A'], ['句子1B'], True)\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    '''\n",
    "    生成下一句预测任务的数据\n",
    "    Args:\n",
    "        sentence: 当前句子\n",
    "        next_sentence: 下一句句子\n",
    "        paragraphs: 段落列表\n",
    "    Returns:\n",
    "        sentence: 当前句子\n",
    "        next_sentence: 下一句句子\n",
    "        is_next: 是否是下一句\n",
    "    '''\n",
    "    # 1. 随机决定是否使用真实的下一句：使用 random.random() 生成一个 [0,1) 之间的随机数，如果随机数小于 0.5，则 is_next 设置为 True，表示使用真实的下一句\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 生成负样本（不真实的下一句）：从 paragraphs 中随机选择一个段落，再从中随机选择一句作为 next_sentence。\n",
    "        ## 设置 is_next 为 False，表示 next_sentence 不是 sentence 的真实下一句。\n",
    "        # paragraphs是三重列表的嵌套\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next\n",
    "\n",
    "\n",
    "# 测试\n",
    "paragraphs = [\n",
    "    [[\"句子1A\"], [\"句子1B\"], [\"句子1C\"]],\n",
    "    [[\"句子2A\"], [\"句子2B\"], [\"句子2C\"]],\n",
    "    [[\"句子3A\"], [\"句子3B\"], [\"句子3C\"]]\n",
    "]\n",
    "sentence = [\"句子1A\"]\n",
    "next_sentence = [\"句子1B\"]\n",
    "result = _get_next_sentence(sentence, next_sentence, paragraphs)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsp_data_from_paragraph: [(['<cls>', '句子1A', '<sep>', '句子2C', '<sep>'], [0, 0, 0, 1, 1], False), (['<cls>', '句子1B', '<sep>', '句子2C', '<sep>'], [0, 0, 0, 1, 1], False)]\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    ''' \n",
    "    生成下一句预测任务的数据\n",
    "    Args:\n",
    "        paragraph: 段落, 当前处理的段落，通常是由多个句子组成的列表。\n",
    "        paragraphs: 段落列表, 所有段落的集合，用于在生成负样本时随机选择其他句子。\n",
    "        vocab: 词汇表, 用于将词元转换为对应的索引或其他形式。\n",
    "        max_len: 最大长度, 模型接受的最大句子对长度，包含了特殊词元 <cls> 和 <sep>。\n",
    "    Returns:\n",
    "        nsp_data_from_paragraph: 下一句预测任务的数据\n",
    "    '''\n",
    "    nsp_data_from_paragraph = []\n",
    "    # 遍历当前段落中的每一个句子，除了最后一个句子，因为需要成对处理句子与下一句的关系。\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(sentence=paragraph[i], next_sentence=paragraph[i + 1], paragraphs=paragraphs)\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph\n",
    "\n",
    "\n",
    "# 测试\n",
    "paragraphs = [\n",
    "    [[\"句子1A\"], [\"句子1B\"], [\"句子1C\"]],\n",
    "    [[\"句子2A\"], [\"句子2B\"], [\"句子2C\"]],\n",
    "    [[\"句子3A\"], [\"句子3B\"], [\"句子3C\"]]\n",
    "]\n",
    "result = _get_nsp_data_from_paragraph(paragraph=paragraphs[0], paragraphs=paragraphs, vocab=None, max_len=100)\n",
    "\n",
    "print(f'nsp_data_from_paragraph: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.2. <a id='toc11_7_5_2_'></a>[生成遮蔽语言模型任务的数据](#toc0_)\n",
    "函数 _replace_mlm_tokens 的主要功能是为遮蔽语言模型（Masked Language Model, MLM）任务生成新的输入词元序列，其中部分词元被替换为特殊的 <mask> 词元或随机词元。这是BERT等预训练模型在进行自监督学习时常用的策略。  \n",
    "函数 _replace_mlm_tokens 实现了MLM任务中词元的随机替换，按照BERT的策略进行：\n",
    "- 80% 的词元被替换为 <mask>。\n",
    "- 10% 的词元保持不变。\n",
    "- 10% 的词元被替换为随机词元。  \n",
    "\n",
    "通过这种方式，模型在训练过程中能够学习到预测被遮蔽词元的能力，从而理解上下文关系和词汇之间的语义联系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: \t ['<cls>', '我', '爱', '中', '国', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
      "mlm_input_tokens: \t ['<cls>', '<mask>', '<mask>', '中', '<mask>', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
      "pred_positions_and_labels: \t [(2, '爱'), (1, '我'), (4, '国')]\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    ''' \n",
    "    为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    Args:\n",
    "        tokens :list: 输入词元序列列表，通常是一个句子的词元化结果。\n",
    "        candidate_pred_positions :list: 候选预测位置列表，表示哪些词元有可能被遮蔽和预测。\n",
    "        num_mlm_preds :int: 需要遮蔽和预测的词元数量。\n",
    "        vocab :Vocab: 词汇表对象，包含词元到索引的映射（idx_to_token）。\n",
    "    Returns:\n",
    "        mlm_input_tokens: 返回修改后的词元序列，包含被替换的词元。\n",
    "        pred_positions_and_labels: 返回被替换词元的位置及其对应的原始词元，用于模型训练时的预测目标。\n",
    "    '''\n",
    "    # 创建一个输入词元的副本，准备在其中进行替换操作。\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    # 初始化一个空列表，用于存储被遮蔽词元的位置及其原始标签。\n",
    "    pred_positions_and_labels = []\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        # 如果已经替换了所需数量的词元（num_mlm_preds），则退出循环。\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        # 替换操作: 根据上述概率策略，对指定位置的词元进行替换。\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 将预测位置 及其 原始标签添 加到列表中，用于后续的损失计算和模型训练。\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "\n",
    "# 测试\n",
    "tokens = ['<cls>', '我', '爱', '中', '国', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
    "# 候选预测位置，需要遮蔽和预测的词元位置，1，2，3，4表示列表索引位置\n",
    "candidate_pred_positions = [1, 2, 3, 4]\n",
    "# 需要遮蔽和预测的词元数量\n",
    "num_mlm_preds = 3\n",
    "vocab = d2l.Vocab()\n",
    "result = _replace_mlm_tokens(tokens=tokens, candidate_pred_positions=candidate_pred_positions, num_mlm_preds=num_mlm_preds, vocab=vocab)\n",
    "\n",
    "print(f'tokens: \\t {tokens}')\n",
    "print(f'mlm_input_tokens: \\t {result[0]}')\n",
    "print(f'pred_positions_and_labels: \\t {result[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 只是替换15%的词元：\n",
    "  - 在这被替换的15%词元中，有80%被替换为`<mask>`，\n",
    "  - 有10%被替换为`其他词元`，\n",
    "  - 有10%`保持不变`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_input_tokens: \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "pred_positions: \t [1, 5]\n",
      "mlm_pred_labels: \t [0, 0]\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    ''' \n",
    "    生成遮蔽语言模型任务的数据\n",
    "    Args:\n",
    "        tokens :list: 输入词元序列列表，通常是一个句子的词元化结果。\n",
    "        vocab :Vocab: 词汇表对象，包含词元到索引的映射（idx_to_token）。\n",
    "    Returns:\n",
    "        vocab[mlm_input_tokens]：经过遮蔽处理后的词元序列，通常会被转换为词汇表中的索引。\n",
    "        pred_positions：被遮蔽词元的位置索引列表。\n",
    "        vocab[mlm_pred_labels]：被遮蔽词元的原始标签，通常也是词汇表中的索引。\n",
    "    '''\n",
    "    # 初始化候选预测位置列表\n",
    "    candidate_pred_positions = []\n",
    "    # 过滤特殊词元\n",
    "    ## tokens是一个字符串列表\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    # 生成遮蔽后的词元和标签\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    # 对被遮蔽的位置和标签进行排序,按照位置索引进行排序，确保顺序的一致性。这对于后续处理和训练时的批量操作非常重要。\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    # 分离位置索引和标签\n",
    "    ## pred_positions：仅包含被遮蔽词元的位置索引。\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    ## mlm_pred_labels：包含这些位置上被遮蔽词元的原始标签。\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
    "\n",
    "\n",
    "# 测试\n",
    "tokens = ['<cls>', '我', '爱', '中', '国', '的', '美', '食', '<sep>', '我', '爱', '中', '国', '<sep>']\n",
    "result = _get_mlm_data_from_tokens(tokens=tokens, vocab=vocab)\n",
    "\n",
    "print(f'mlm_input_tokens: \\t {result[0]}')\n",
    "print(f'pred_positions: \\t {result[1]}')\n",
    "print(f'mlm_pred_labels: \\t {result[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.3. <a id='toc11_7_5_3_'></a>[将文本转换为预训练数据集](#toc0_)\n",
    "函数 _pad_bert_inputs 的主要作用是为BERT模型的`预训练任务`（包括遮蔽语言模型任务 MLM 和下一句预测任务 NSP）准备和`填充`输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_token_ids: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "all_segments: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "valid_lens: [tensor(10.)]\n",
      "all_pred_positions: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "all_mlm_weights: [tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
      "all_mlm_labels: [tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "nsp_labels: [tensor(1)]\n"
     ]
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    ''' \n",
    "    为BERT模型的预训练任务（包括遮蔽语言模型任务 MLM 和下一句预测任务 NSP）准备和填充输入数据\n",
    "    Args:\n",
    "        examples :list:  包含多个样本的数据集。每个样本是一个元组，通常包含以下内容：\n",
    "                        - token_ids：词元ID列表，表示一个句子的词元序列。\n",
    "                        - pred_positions：被遮蔽词元的位置索引列表。\n",
    "                        - mlm_pred_label_ids：被遮蔽词元的原始标签列表。\n",
    "                        - segments：段落ID列表，表示句子在段落中的位置。\n",
    "                        - is_next：布尔值，表示是否为下一句预测任务的标签。\n",
    "        max_len :int: 模型接受的最大句子对长度，包含了特殊词元 <cls> 和 <sep>,所有样本将被填充或截断到这个长度。\n",
    "        vocab :Vocab: 词汇表对象，包含词元到索引的映射（idx_to_token）。\n",
    "    Returns:\n",
    "        all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels\n",
    "    '''\n",
    "    # 根据最大长度 max_len 计算出最多可以进行遮蔽预测的词元数量，通常占总长度的15%。\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    # 初始化列表，用于存储填充后的数据\n",
    "    ## all_token_ids: 存储填充后的词元索引。\n",
    "    ## all_segments: 存储填充后的段落ID。\n",
    "    ## valid_lens: 存储每个样本的有效长度，不包括'<pad>'的计数。    \n",
    "    all_token_ids, all_segments, valid_lens = [], [], []\n",
    "    ## all_pred_positions: 存储填充后的被遮蔽词元的位置索引。\n",
    "    ## all_mlm_weights: 存储填充后的被遮蔽词元的权重。\n",
    "    ## all_mlm_labels: 存储填充后的被遮蔽词元的原始标签。\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    ## nsp_labels: 存储填充后的下一句预测任务的标签。\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # 词元索引填充: 若 token_ids 的长度小于 max_len，则使用 <pad> 词元的索引进行填充，确保每个序列长度一致。\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))\n",
    "        # 段落ID填充: 若 segments 的长度小于 max_len，则使用0进行填充，确保每个序列长度一致。\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens: 记录每个样本中实际有效的词元数量，即不包括填充的 <pad> 词元的数量。\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        # 被遮蔽词元的位置索引填充: 若 pred_positions 的长度小于 max_num_mlm_preds，则使用0进行填充，确保每个序列长度一致。\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # MLM 权重填充: 对于实际的遮蔽词元位置，赋予权重 1.0；对于填充的位置，赋予权重 0.0。这样在计算损失时，填充部分不会影响结果。\n",
    "        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.float32))\n",
    "        # MLM 标签填充: 对于实际的遮蔽词元，使用其真实的词元索引作为标签；对于填充的位置，使用 0 作为占位符。\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        # NSP 标签: 记录每个样本的下一句预测标签，通常为 0 或 1，表示是否为真实的下一句。\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)\n",
    "\n",
    "\n",
    "# 测试\n",
    "examples = [\n",
    "    ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], True)\n",
    "]\n",
    "result = _pad_bert_inputs(examples=examples, max_len=10, vocab=vocab)\n",
    "\n",
    "print(f'all_token_ids: {result[0]}')\n",
    "print(f'all_segments: {result[1]}')\n",
    "print(f'valid_lens: {result[2]}')\n",
    "print(f'all_pred_positions: {result[3]}')\n",
    "print(f'all_mlm_weights: {result[4]}')\n",
    "print(f'all_mlm_labels: {result[5]}')\n",
    "print(f'nsp_labels: {result[6]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.7.5.4. <a id='toc11_7_5_4_'></a>[创建数据集](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
    "        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n",
    "        paragraphs = [d2l.tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # 获取下一句子预测任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(paragraph, paragraphs, self.vocab, max_len))\n",
    "        # 获取遮蔽语言模型任务的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        # 填充输入\n",
    "        (self.all_token_ids, \n",
    "         self.all_segments, \n",
    "         self.valid_lens,\n",
    "         self.all_pred_positions, \n",
    "         self.all_mlm_weights,\n",
    "         self.all_mlm_labels, \n",
    "         self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], \n",
    "                self.all_segments[idx],\n",
    "                self.valid_lens[idx], \n",
    "                self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], \n",
    "                self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    # data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    data_dir = './data/wikipedia_text'\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, \n",
    "          segments_X.shape, \n",
    "          valid_lens_x.shape,\n",
    "          pred_positions_X.shape, \n",
    "          mlm_weights_X.shape, \n",
    "          mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.6. <a id='toc11_7_6_'></a>[预训练BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y):\n",
    "    ''' \n",
    "    计算BERT模型的遮蔽语言模型损失和下一句预测任务损失。\n",
    "    Args:\n",
    "        net :BERTModel: BERT模型实例。\n",
    "        loss :nn.CrossEntropyLoss: 损失函数实例。\n",
    "        vocab_size :int: 词汇表大小。\n",
    "        tokens_X :torch.Tensor: 输入词元索引。\n",
    "        segments_X :torch.Tensor: 输入段落ID。\n",
    "        valid_lens_x :torch.Tensor: 有效长度。\n",
    "    Returns:\n",
    "        mlm_l :torch.Tensor: 遮蔽语言模型损失。\n",
    "        nsp_l :torch.Tensor: 下一句预测任务损失。\n",
    "        l :torch.Tensor: 总损失。\n",
    "    '''\n",
    "    # 前向传播\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X, valid_lens_x.reshape(-1), pred_positions_X)\n",
    "    # 计算遮蔽语言模型损失\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) * mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # 计算下一句子预测任务的损失\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM loss 5.616, NSP loss 0.743\n",
      "7495.3 sentence pairs/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"252.646875pt\" height=\"183.35625pt\" viewBox=\"0 0 252.646875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-11T18:27:11.282085</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 252.646875 183.35625 \n",
       "L 252.646875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "L 40.603125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 78.084943 145.8 \n",
       "L 78.084943 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m7bc6efeab6\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"78.084943\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(71.722443 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 117.539489 145.8 \n",
       "L 117.539489 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"117.539489\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(111.176989 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 156.994034 145.8 \n",
       "L 156.994034 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"156.994034\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(150.631534 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 196.44858 145.8 \n",
       "L 196.44858 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"196.44858\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(190.08608 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7bc6efeab6\" x=\"235.903125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(226.359375 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(127.4375 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 40.603125 123.024186 \n",
       "L 235.903125 123.024186 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"mb078a3a8e8\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"123.024186\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(27.240625 126.823405) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 40.603125 96.910243 \n",
       "L 235.903125 96.910243 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"96.910243\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(27.240625 100.709462) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 40.603125 70.7963 \n",
       "L 235.903125 70.7963 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"70.7963\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(27.240625 74.595519) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 40.603125 44.682358 \n",
       "L 235.903125 44.682358 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"44.682358\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(27.240625 48.481576) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 40.603125 18.568415 \n",
       "L 235.903125 18.568415 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb078a3a8e8\" x=\"40.603125\" y=\"18.568415\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 22.367634) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 40.603125 13.5 \n",
       "L 42.575852 25.661385 \n",
       "L 44.54858 30.721015 \n",
       "L 46.521307 38.846162 \n",
       "L 48.494034 45.134569 \n",
       "L 50.466761 49.840032 \n",
       "L 52.439489 53.348194 \n",
       "L 54.412216 55.998856 \n",
       "L 56.384943 58.039534 \n",
       "L 58.35767 59.523746 \n",
       "L 60.330398 60.651841 \n",
       "L 62.303125 61.620033 \n",
       "L 64.275852 62.590861 \n",
       "L 66.24858 63.471262 \n",
       "L 68.221307 64.142753 \n",
       "L 70.194034 64.739469 \n",
       "L 72.166761 65.387778 \n",
       "L 74.139489 65.88051 \n",
       "L 76.112216 66.317057 \n",
       "L 78.084943 66.786108 \n",
       "L 80.05767 67.188831 \n",
       "L 82.030398 67.576942 \n",
       "L 84.003125 67.972549 \n",
       "L 85.975852 68.30938 \n",
       "L 87.94858 68.629127 \n",
       "L 89.921307 68.862662 \n",
       "L 91.894034 69.207132 \n",
       "L 93.866761 69.55001 \n",
       "L 95.839489 69.848155 \n",
       "L 97.812216 70.058182 \n",
       "L 99.784943 70.303884 \n",
       "L 101.75767 70.507851 \n",
       "L 103.730398 70.657058 \n",
       "L 105.703125 70.841539 \n",
       "L 107.675852 71.027889 \n",
       "L 109.64858 71.233382 \n",
       "L 111.621307 71.42783 \n",
       "L 113.594034 71.613844 \n",
       "L 115.566761 71.758865 \n",
       "L 117.539489 71.90429 \n",
       "L 119.512216 72.06756 \n",
       "L 121.484943 72.210509 \n",
       "L 123.45767 72.349104 \n",
       "L 125.430398 72.467599 \n",
       "L 127.403125 72.587853 \n",
       "L 129.375852 72.709511 \n",
       "L 131.34858 72.817607 \n",
       "L 133.321307 72.933981 \n",
       "L 135.294034 72.996189 \n",
       "L 137.266761 73.102321 \n",
       "L 139.239489 73.212049 \n",
       "L 141.212216 73.30338 \n",
       "L 143.184943 73.373167 \n",
       "L 145.15767 73.443248 \n",
       "L 147.130398 73.530904 \n",
       "L 149.103125 73.605939 \n",
       "L 151.075852 73.684886 \n",
       "L 153.04858 73.776422 \n",
       "L 155.021307 73.849052 \n",
       "L 156.994034 73.927017 \n",
       "L 158.966761 73.980969 \n",
       "L 160.939489 74.070449 \n",
       "L 162.912216 74.128658 \n",
       "L 164.884943 74.19497 \n",
       "L 166.85767 74.260885 \n",
       "L 168.830398 74.310094 \n",
       "L 170.803125 74.368606 \n",
       "L 172.775852 74.442467 \n",
       "L 174.74858 74.492515 \n",
       "L 176.721307 74.548386 \n",
       "L 178.694034 74.596607 \n",
       "L 180.666761 74.642948 \n",
       "L 182.639489 74.700606 \n",
       "L 184.612216 74.772941 \n",
       "L 186.584943 74.807425 \n",
       "L 188.55767 74.86277 \n",
       "L 190.530398 74.916105 \n",
       "L 192.503125 74.951362 \n",
       "L 194.475852 75.000177 \n",
       "L 196.44858 75.035135 \n",
       "L 198.421307 75.075498 \n",
       "L 200.394034 75.120261 \n",
       "L 202.366761 75.174091 \n",
       "L 204.339489 75.214076 \n",
       "L 206.312216 75.228521 \n",
       "L 208.284943 75.256377 \n",
       "L 210.25767 75.302434 \n",
       "L 212.230398 75.350611 \n",
       "L 214.203125 75.402255 \n",
       "L 216.175852 75.445684 \n",
       "L 218.14858 75.475246 \n",
       "L 220.121307 75.513269 \n",
       "L 222.094034 75.556111 \n",
       "L 224.066761 75.599085 \n",
       "L 226.039489 75.642944 \n",
       "L 228.012216 75.678068 \n",
       "L 229.984943 75.710396 \n",
       "L 231.95767 75.74417 \n",
       "L 233.930398 75.774387 \n",
       "L 235.903125 75.811024 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_22\">\n",
       "    <path d=\"M 40.603125 139.5 \n",
       "L 42.575852 137.108483 \n",
       "L 44.54858 129.767098 \n",
       "L 46.521307 128.981284 \n",
       "L 48.494034 130.797116 \n",
       "L 50.466761 132.180952 \n",
       "L 52.439489 133.087804 \n",
       "L 54.412216 133.914726 \n",
       "L 56.384943 134.383507 \n",
       "L 58.35767 134.705106 \n",
       "L 60.330398 135.151464 \n",
       "L 62.303125 135.557287 \n",
       "L 64.275852 135.782361 \n",
       "L 66.24858 136.003891 \n",
       "L 68.221307 136.270284 \n",
       "L 70.194034 136.500998 \n",
       "L 72.166761 136.667052 \n",
       "L 74.139489 136.816249 \n",
       "L 76.112216 136.970833 \n",
       "L 78.084943 137.126161 \n",
       "L 80.05767 137.251917 \n",
       "L 82.030398 137.334579 \n",
       "L 84.003125 137.435436 \n",
       "L 85.975852 137.543338 \n",
       "L 87.94858 137.636781 \n",
       "L 89.921307 137.711951 \n",
       "L 91.894034 137.782432 \n",
       "L 93.866761 137.864181 \n",
       "L 95.839489 137.94024 \n",
       "L 97.812216 138.009755 \n",
       "L 99.784943 138.06656 \n",
       "L 101.75767 138.124856 \n",
       "L 103.730398 138.183478 \n",
       "L 105.703125 138.238981 \n",
       "L 107.675852 138.292134 \n",
       "L 109.64858 138.338831 \n",
       "L 111.621307 138.381482 \n",
       "L 113.594034 138.425971 \n",
       "L 115.566761 138.468694 \n",
       "L 117.539489 138.505792 \n",
       "L 119.512216 138.543333 \n",
       "L 121.484943 138.5782 \n",
       "L 123.45767 138.6123 \n",
       "L 125.430398 138.643668 \n",
       "L 127.403125 138.674632 \n",
       "L 129.375852 138.701133 \n",
       "L 131.34858 138.73022 \n",
       "L 133.321307 138.758786 \n",
       "L 135.294034 138.786784 \n",
       "L 137.266761 138.808848 \n",
       "L 139.239489 138.83537 \n",
       "L 141.212216 138.8598 \n",
       "L 143.184943 138.883176 \n",
       "L 145.15767 138.905363 \n",
       "L 147.130398 138.926825 \n",
       "L 149.103125 138.947044 \n",
       "L 151.075852 138.967389 \n",
       "L 153.04858 138.986126 \n",
       "L 155.021307 139.004576 \n",
       "L 156.994034 139.022287 \n",
       "L 158.966761 139.0399 \n",
       "L 160.939489 139.058224 \n",
       "L 162.912216 139.073238 \n",
       "L 164.884943 139.088934 \n",
       "L 166.85767 139.103486 \n",
       "L 168.830398 139.118382 \n",
       "L 170.803125 139.132383 \n",
       "L 172.775852 139.143206 \n",
       "L 174.74858 139.156424 \n",
       "L 176.721307 139.167468 \n",
       "L 178.694034 139.18164 \n",
       "L 180.666761 139.19177 \n",
       "L 182.639489 139.203878 \n",
       "L 184.612216 139.215785 \n",
       "L 186.584943 139.226793 \n",
       "L 188.55767 139.23733 \n",
       "L 190.530398 139.248572 \n",
       "L 192.503125 139.259306 \n",
       "L 194.475852 139.269773 \n",
       "L 196.44858 139.280112 \n",
       "L 198.421307 139.290201 \n",
       "L 200.394034 139.298813 \n",
       "L 202.366761 139.308033 \n",
       "L 204.339489 139.317115 \n",
       "L 206.312216 139.32591 \n",
       "L 208.284943 139.334348 \n",
       "L 210.25767 139.343088 \n",
       "L 212.230398 139.351701 \n",
       "L 214.203125 139.359098 \n",
       "L 216.175852 139.366249 \n",
       "L 218.14858 139.374265 \n",
       "L 220.121307 139.381281 \n",
       "L 222.094034 139.388853 \n",
       "L 224.066761 139.396269 \n",
       "L 226.039489 139.403586 \n",
       "L 228.012216 139.410363 \n",
       "L 229.984943 139.417148 \n",
       "L 231.95767 139.424252 \n",
       "L 233.930398 139.431052 \n",
       "L 235.903125 139.437225 \n",
       "\" clip-path=\"url(#p927c2fb8b1)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 40.603125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 7.2 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 174.64375 44.55625 \n",
       "L 228.903125 44.55625 \n",
       "Q 230.903125 44.55625 230.903125 42.55625 \n",
       "L 230.903125 14.2 \n",
       "Q 230.903125 12.2 228.903125 12.2 \n",
       "L 174.64375 12.2 \n",
       "Q 172.64375 12.2 172.64375 14.2 \n",
       "L 172.64375 42.55625 \n",
       "Q 172.64375 44.55625 174.64375 44.55625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_23\">\n",
       "     <path d=\"M 176.64375 20.298438 \n",
       "L 186.64375 20.298438 \n",
       "L 196.64375 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- mlm -->\n",
       "     <g transform=\"translate(204.64375 23.798438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"97.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"125.195312\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 176.64375 34.976562 \n",
       "L 186.64375 34.976562 \n",
       "L 196.64375 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- nsp -->\n",
       "     <g transform=\"translate(204.64375 38.476562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"63.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"115.478516\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p927c2fb8b1\">\n",
       "   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss', xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(net, loss, vocab_size, tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1, (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')\n",
    "    \n",
    "\n",
    "#@tab mxnet, pytorch\n",
    "# train_bert(train_iter, net, loss, len(vocab), devices, 100000)\n",
    "train_bert(train_iter, net, loss, len(vocab), devices, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), 'Pytorch_params/BERT/bert_5000.pt')\n",
    "# torch.save(net.state_dict(), 'Pytorch_params/BERT/bert_50000.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.7. <a id='toc11_7_7_'></a>[用BERT表示文本](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    ''' \n",
    "    获取BERT模型的文本表示。\n",
    "    Args:\n",
    "        net :BERTModel: BERT模型实例。\n",
    "        tokens_a :list: 第一个文本的词元列表。\n",
    "        tokens_b :list: 第二个文本的词元列表（可选）。\n",
    "    Returns:\n",
    "        encoded_X :torch.Tensor: 文本的BERT表示。\n",
    "    '''\n",
    "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_text.shape: torch.Size([1, 6, 128])\n",
      "encoded_text_cls.shape: torch.Size([1, 128])\n",
      "encoded_text_crane.shape: torch.Size([1, 128])\n",
      "encoded_text_crane[0][:3]: tensor([-0.6918, -0.0320,  0.0174], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# 词元：'<cls>','a','crane','is','flying','<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "\n",
    "# encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]\n",
    "print(f'encoded_text.shape: {encoded_text.shape}')\n",
    "print(f'encoded_text_cls.shape: {encoded_text_cls.shape}')\n",
    "print(f'encoded_text_crane.shape: {encoded_text_crane.shape}')\n",
    "print(f'encoded_text_crane[0][:3]: {encoded_text_crane[0][:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_pair.shape: torch.Size([1, 10, 128])\n",
      "encoded_pair_cls.shape: torch.Size([1, 128])\n",
      "encoded_pair_crane.shape: torch.Size([1, 128])\n",
      "encoded_pair_crane[0][:3]: tensor([-0.7072,  0.0097,  1.1122], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\n",
    "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
    "# 词元：'<cls>','a','crane','driver','came','<sep>','he','just','left','<sep>'\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "\n",
    "# encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]\n",
    "print(f'encoded_pair.shape: {encoded_pair.shape}')\n",
    "print(f'encoded_pair_cls.shape: {encoded_pair_cls.shape}')\n",
    "print(f'encoded_pair_crane.shape: {encoded_pair_crane.shape}')\n",
    "print(f'encoded_pair_crane[0][:3]: {encoded_pair_crane[0][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.8. <a id='toc11_8_'></a>[用BERT做微调](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.8.1. <a id='toc11_8_1_'></a>[情感分析](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['aclImdb'] = (\n",
    "    'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "    '01ada507287d82875905620988597833ad4e0903')\n",
    "\n",
    "data_dir = d2l.download_extract('aclImdb', 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainings: 25000\n",
      "label: 1 review: Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postw\n",
      "label: 1 review: Zentropa is the most original movie I've seen in years. If you like unique thrillers that are influe\n",
      "label: 1 review: Lars Von Trier is never backward in trying out new techniques. Some of them are very original while \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "#@tab all\n",
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label in ('pos', 'neg'):\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test', label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data = read_imdb(data_dir, is_train=True)\n",
    "print('# trainings:', len(train_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('label:', y, 'review:', x[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"255.828125pt\" height=\"183.35625pt\" viewBox=\"0 0 255.828125 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-12T17:52:24.823105</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 255.828125 183.35625 \n",
       "L 255.828125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 53.328125 145.8 \n",
       "L 248.628125 145.8 \n",
       "L 248.628125 7.2 \n",
       "L 53.328125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 62.205398 145.8 \n",
       "L 71.549895 145.8 \n",
       "L 71.549895 135.096774 \n",
       "L 62.205398 135.096774 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 71.549895 145.8 \n",
       "L 80.894393 145.8 \n",
       "L 80.894393 99.870968 \n",
       "L 71.549895 99.870968 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 80.894393 145.8 \n",
       "L 90.238891 145.8 \n",
       "L 90.238891 13.8 \n",
       "L 80.894393 13.8 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 90.238891 145.8 \n",
       "L 99.583388 145.8 \n",
       "L 99.583388 52.23871 \n",
       "L 90.238891 52.23871 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 99.583388 145.8 \n",
       "L 108.927886 145.8 \n",
       "L 108.927886 91.277419 \n",
       "L 99.583388 91.277419 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 108.927886 145.8 \n",
       "L 118.272383 145.8 \n",
       "L 118.272383 110.032258 \n",
       "L 108.927886 110.032258 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 118.272383 145.8 \n",
       "L 127.616881 145.8 \n",
       "L 127.616881 119.090323 \n",
       "L 118.272383 119.090323 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 127.616881 145.8 \n",
       "L 136.961379 145.8 \n",
       "L 136.961379 126.348387 \n",
       "L 127.616881 126.348387 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 136.961379 145.8 \n",
       "L 146.305876 145.8 \n",
       "L 146.305876 131.109677 \n",
       "L 136.961379 131.109677 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 146.305876 145.8 \n",
       "L 155.650374 145.8 \n",
       "L 155.650374 134.554839 \n",
       "L 146.305876 134.554839 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 155.650374 145.8 \n",
       "L 164.994871 145.8 \n",
       "L 164.994871 137.341935 \n",
       "L 155.650374 137.341935 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 164.994871 145.8 \n",
       "L 174.339369 145.8 \n",
       "L 174.339369 139.045161 \n",
       "L 164.994871 139.045161 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 174.339369 145.8 \n",
       "L 183.683867 145.8 \n",
       "L 183.683867 140.825806 \n",
       "L 174.339369 140.825806 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 183.683867 145.8 \n",
       "L 193.028364 145.8 \n",
       "L 193.028364 141.793548 \n",
       "L 183.683867 141.793548 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 193.028364 145.8 \n",
       "L 202.372862 145.8 \n",
       "L 202.372862 142.432258 \n",
       "L 193.028364 142.432258 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 202.372862 145.8 \n",
       "L 211.717359 145.8 \n",
       "L 211.717359 143.225806 \n",
       "L 202.372862 143.225806 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 211.717359 145.8 \n",
       "L 221.061857 145.8 \n",
       "L 221.061857 143.554839 \n",
       "L 211.717359 143.554839 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 221.061857 145.8 \n",
       "L 230.406355 145.8 \n",
       "L 230.406355 144.154839 \n",
       "L 221.061857 144.154839 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 230.406355 145.8 \n",
       "L 239.750852 145.8 \n",
       "L 239.750852 144.348387 \n",
       "L 230.406355 144.348387 \n",
       "z\n",
       "\" clip-path=\"url(#p33ba718331)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m14c49d08f1\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"62.205398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(59.024148 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"99.583388\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(90.039638 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"136.961379\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(127.417629 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"174.339369\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(164.795619 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14c49d08f1\" x=\"211.717359\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(202.173609 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- # tokens per review -->\n",
       "     <g transform=\"translate(100.597656 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-23\" d=\"M 3272 2816 \n",
       "L 2363 2816 \n",
       "L 2100 1772 \n",
       "L 3016 1772 \n",
       "L 3272 2816 \n",
       "z\n",
       "M 2803 4594 \n",
       "L 2478 3297 \n",
       "L 3391 3297 \n",
       "L 3719 4594 \n",
       "L 4219 4594 \n",
       "L 3897 3297 \n",
       "L 4872 3297 \n",
       "L 4872 2816 \n",
       "L 3775 2816 \n",
       "L 3519 1772 \n",
       "L 4513 1772 \n",
       "L 4513 1294 \n",
       "L 3397 1294 \n",
       "L 3072 0 \n",
       "L 2572 0 \n",
       "L 2894 1294 \n",
       "L 1978 1294 \n",
       "L 1656 0 \n",
       "L 1153 0 \n",
       "L 1478 1294 \n",
       "L 494 1294 \n",
       "L 494 1772 \n",
       "L 1594 1772 \n",
       "L 1856 2816 \n",
       "L 850 2816 \n",
       "L 850 3297 \n",
       "L 1978 3297 \n",
       "L 2297 4594 \n",
       "L 2803 4594 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n",
       "L 844 3500 \n",
       "L 1563 769 \n",
       "L 2278 3500 \n",
       "L 2956 3500 \n",
       "L 3675 769 \n",
       "L 4391 3500 \n",
       "L 4966 3500 \n",
       "L 4050 0 \n",
       "L 3372 0 \n",
       "L 2619 2869 \n",
       "L 1863 0 \n",
       "L 1184 0 \n",
       "L 269 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-23\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"83.789062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"115.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"154.785156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\" x=\"215.966797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"270.251953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"331.775391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"395.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"447.253906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"479.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"542.517578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"604.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"645.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"676.941406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"715.804688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" x=\"777.328125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"836.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"864.291016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-77\" x=\"925.814453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m62e23531bd\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(39.965625 149.599219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"107.090323\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(20.878125 110.889541) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"68.380645\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 4000 -->\n",
       "      <g transform=\"translate(20.878125 72.179864) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62e23531bd\" x=\"53.328125\" y=\"29.670968\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 6000 -->\n",
       "      <g transform=\"translate(20.878125 33.470186) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- count -->\n",
       "     <g transform=\"translate(14.798437 90.60625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"179.541016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"242.919922\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 53.328125 145.8 \n",
       "L 53.328125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 248.628125 145.8 \n",
       "L 248.628125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 53.328125 145.8 \n",
       "L 248.628125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 53.328125 7.2 \n",
       "L 248.628125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p33ba718331\">\n",
       "   <rect x=\"53.328125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab all\n",
    "d2l.set_figsize()\n",
    "d2l.plt.xlabel('# tokens per review')\n",
    "d2l.plt.ylabel('count')\n",
    "d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 500])\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "num_steps = 500  # sequence length\n",
    "train_features = d2l.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([64, 500]) , y: torch.Size([64])\n",
      "# batches: 391\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), 64)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X:', X.shape, ', y:', y.shape)\n",
    "    break\n",
    "print('# batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\n",
    "    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = torch.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])), batch_size, is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.1.1. <a id='toc11_8_1_1_'></a>[使用RNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.1.2. <a id='toc11_8_1_2_'></a>[使用CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.8.2. <a id='toc11_8_2_'></a>[自然语言推断](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.2.1. <a id='toc11_8_2_1_'></a>[使用Attention](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.8.2.2. <a id='toc11_8_2_2_'></a>[微调BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import json\n",
    "import multiprocessing\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices, bert_type='small'):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # Define an empty vocabulary to load the predefined vocabulary\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n",
    "    if bert_type == 'small':    \n",
    "        # parameters of BERT-small\n",
    "        bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=num_heads, num_layers=num_layers, dropout=dropout,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    else:\n",
    "        # parameters of BERT-base\n",
    "        bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[768],\n",
    "                         ffn_num_input=768, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=num_heads, num_layers=num_layers, dropout=dropout,\n",
    "                         max_len=max_len, key_size=768, query_size=768,\n",
    "                         value_size=768, hid_in_features=768,\n",
    "                         mlm_in_features=768, nsp_in_features=768)\n",
    "    # Load pretrained BERT parameters\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.100967 M parameters\n",
      "122.46 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509516/614722111.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "# BERT-small\n",
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    pretrained_model='bert.small', \n",
    "    num_hiddens=256, \n",
    "    ffn_num_hiddens=512, \n",
    "    num_heads=4,\n",
    "    num_layers=2, \n",
    "    dropout=0.1, \n",
    "    max_len=512, \n",
    "    devices=devices,\n",
    "    bert_type='small')\n",
    "parameter_size(bert, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509516/614722111.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.861159 M parameters\n",
      "682.30 MB\n"
     ]
    }
   ],
   "source": [
    "# BERT-base\n",
    "bert, vocab = load_pretrained_model(\n",
    "    pretrained_model='bert.base',\n",
    "    num_hiddens=768, \n",
    "    ffn_num_hiddens=3072, \n",
    "    num_heads=12,\n",
    "    num_layers=12, \n",
    "    dropout=0.1, \n",
    "    max_len=512, \n",
    "    devices=devices,\n",
    "    bert_type='base')\n",
    "parameter_size(bert, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[p_tokens, h_tokens] for p_tokens, h_tokens in zip(*[d2l.tokenize([s.lower() for s in sentences]) for sentences in dataset[:2]])]\n",
    "        \n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long), \n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n",
    "        # input\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "# Reduce `batch_size` if there is an out of memory error. In the original BERT model, `max_len` = 512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, bert_type='small'):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        if bert_type == 'small':\n",
    "            self.output = nn.Linear(256, 3)\n",
    "        else:\n",
    "            self.output = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))\n",
    "\n",
    "\n",
    "#@tab pytorch\n",
    "net = BERTClassifier(bert, bert_type='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.022, train acc 0.993, test acc 0.860\n",
      "386.6 examples/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"187.155469pt\" viewBox=\"0 0 238.965625 187.155469\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-13T14:00:07.476575</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 187.155469 \n",
       "L 238.965625 187.155469 \n",
       "L 238.965625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "L 30.103125 10.999219 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 65.974554 149.599219 \n",
       "L 65.974554 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m79ce69da6b\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"65.974554\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(59.612054 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 105.831696 149.599219 \n",
       "L 105.831696 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"105.831696\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(99.469196 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 145.688839 149.599219 \n",
       "L 145.688839 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"145.688839\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(139.326339 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 185.545982 149.599219 \n",
       "L 185.545982 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"185.545982\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(179.183482 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m79ce69da6b\" x=\"225.403125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(219.040625 164.197656) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(112.525 177.875781) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m190ef7832f\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"149.599219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(7.2 153.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 30.103125 121.879219 \n",
       "L 225.403125 121.879219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"121.879219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(7.2 125.678438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 30.103125 94.159219 \n",
       "L 225.403125 94.159219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"94.159219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 97.958438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 30.103125 66.439219 \n",
       "L 225.403125 66.439219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"66.439219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 70.238437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 30.103125 38.719219 \n",
       "L 225.403125 38.719219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"38.719219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 42.518438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 30.103125 10.999219 \n",
       "L 225.403125 10.999219 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m190ef7832f\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 26.912325 58.125151 \n",
       "L 27.707239 68.381335 \n",
       "L 28.502153 73.805893 \n",
       "L 29.297067 77.41677 \n",
       "L 30.103125 80.104161 \n",
       "L 30.898039 99.361541 \n",
       "L 31.692953 99.61591 \n",
       "L 32.487867 99.661185 \n",
       "L 33.282782 99.915327 \n",
       "L 34.088839 99.959084 \n",
       "L 34.883753 110.985948 \n",
       "L 35.678668 110.398307 \n",
       "L 36.473582 109.963923 \n",
       "L 38.074554 109.41106 \n",
       "L 38.869468 118.502536 \n",
       "L 39.664382 117.660466 \n",
       "L 42.060268 116.425186 \n",
       "L 42.855182 124.378462 \n",
       "L 43.650096 123.510346 \n",
       "L 45.239924 122.485848 \n",
       "L 46.045982 122.10765 \n",
       "L 46.840896 127.792179 \n",
       "L 48.430725 126.802854 \n",
       "L 50.031696 126.099038 \n",
       "L 50.826611 131.176623 \n",
       "L 52.416439 130.082259 \n",
       "L 54.017411 129.444282 \n",
       "L 54.812325 134.113213 \n",
       "L 55.607239 133.083603 \n",
       "L 56.402153 132.648314 \n",
       "L 58.003125 131.95916 \n",
       "L 58.798039 135.786756 \n",
       "L 61.988839 134.212472 \n",
       "L 62.783753 137.429235 \n",
       "L 63.578668 136.88134 \n",
       "L 65.168496 136.252734 \n",
       "L 65.974554 135.914784 \n",
       "L 66.769468 138.768584 \n",
       "L 69.960268 137.345953 \n",
       "L 70.755182 139.860581 \n",
       "L 71.550096 139.245357 \n",
       "L 73.945982 138.503516 \n",
       "L 74.740896 140.95419 \n",
       "L 77.931696 139.642648 \n",
       "L 78.726611 141.702016 \n",
       "L 79.521525 141.253081 \n",
       "L 81.111353 140.654439 \n",
       "L 81.917411 140.356366 \n",
       "L 82.712325 142.410966 \n",
       "L 84.302153 141.790849 \n",
       "L 85.903125 141.250588 \n",
       "L 86.698039 142.941165 \n",
       "L 87.492953 142.658437 \n",
       "L 88.287867 142.254109 \n",
       "L 89.888839 141.756079 \n",
       "L 90.683753 143.435878 \n",
       "L 93.874554 142.424393 \n",
       "L 94.669468 143.780791 \n",
       "L 97.860268 142.65379 \n",
       "L 98.655182 143.992376 \n",
       "L 101.039924 143.245149 \n",
       "L 101.845982 143.098493 \n",
       "L 102.640896 144.276378 \n",
       "L 104.230725 143.696571 \n",
       "L 105.831696 143.352003 \n",
       "L 106.626611 144.720697 \n",
       "L 107.421525 144.32933 \n",
       "L 109.011353 143.910828 \n",
       "L 109.817411 143.705235 \n",
       "L 110.612325 144.554295 \n",
       "L 112.202153 144.362637 \n",
       "L 113.803125 144.016578 \n",
       "L 114.598039 144.90786 \n",
       "L 117.788839 144.269661 \n",
       "L 118.583753 145.088036 \n",
       "L 120.173582 144.771726 \n",
       "L 121.774554 144.415468 \n",
       "L 122.569468 145.32131 \n",
       "L 124.159296 144.749907 \n",
       "L 125.760268 144.467221 \n",
       "L 126.555182 145.457272 \n",
       "L 129.745982 144.794953 \n",
       "L 130.540896 145.552129 \n",
       "L 131.33581 145.484043 \n",
       "L 132.130725 145.16472 \n",
       "L 133.731696 144.932733 \n",
       "L 134.526611 145.839737 \n",
       "L 137.717411 145.096789 \n",
       "L 138.512325 145.591672 \n",
       "L 140.102153 145.353143 \n",
       "L 141.703125 145.050634 \n",
       "L 142.498039 145.970592 \n",
       "L 145.688839 145.338215 \n",
       "L 146.483753 145.885664 \n",
       "L 147.278668 145.826291 \n",
       "L 148.868496 145.39645 \n",
       "L 149.674554 145.300129 \n",
       "L 150.469468 146.248235 \n",
       "L 152.85421 145.699968 \n",
       "L 153.660268 145.57984 \n",
       "L 154.455182 146.254604 \n",
       "L 156.04501 145.83425 \n",
       "L 157.645982 145.56669 \n",
       "L 158.440896 146.292321 \n",
       "L 160.030725 145.972181 \n",
       "L 161.631696 145.718664 \n",
       "L 162.426611 146.220652 \n",
       "L 165.617411 145.712153 \n",
       "L 166.412325 146.2914 \n",
       "L 168.797067 145.914004 \n",
       "L 169.603125 145.778291 \n",
       "L 170.398039 146.446668 \n",
       "L 171.192953 146.173309 \n",
       "L 173.588839 145.85444 \n",
       "L 174.383753 146.475535 \n",
       "L 175.178668 146.391609 \n",
       "L 176.768496 146.042559 \n",
       "L 177.574554 145.927707 \n",
       "L 178.369468 146.550791 \n",
       "L 181.560268 145.995037 \n",
       "L 182.355182 146.450987 \n",
       "L 185.545982 145.968765 \n",
       "L 186.340896 146.17449 \n",
       "L 187.930725 146.240451 \n",
       "L 189.531696 145.98479 \n",
       "L 190.326611 146.571855 \n",
       "L 193.517411 146.245112 \n",
       "L 194.312325 146.73362 \n",
       "L 195.902153 146.439496 \n",
       "L 197.503125 146.198527 \n",
       "L 198.298039 146.933209 \n",
       "L 199.092953 146.592397 \n",
       "L 201.488839 146.261457 \n",
       "L 202.283753 146.945829 \n",
       "L 203.078668 146.692927 \n",
       "L 205.474554 146.366696 \n",
       "L 206.269468 146.795164 \n",
       "L 208.65421 146.366878 \n",
       "L 209.460268 146.304738 \n",
       "L 210.255182 147.098318 \n",
       "L 211.050096 146.809251 \n",
       "L 213.445982 146.474802 \n",
       "L 214.240896 147.064407 \n",
       "L 217.431696 146.44592 \n",
       "L 218.226611 146.964882 \n",
       "L 221.417411 146.464633 \n",
       "L 222.212325 146.94643 \n",
       "L 225.403125 146.595698 \n",
       "L 225.403125 146.595698 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_24\">\n",
       "    <path d=\"M 26.912325 50.191718 \n",
       "L 27.707239 44.60562 \n",
       "L 28.502153 41.852629 \n",
       "L 29.297067 40.136173 \n",
       "L 30.103125 38.814231 \n",
       "L 30.898039 29.787787 \n",
       "L 34.088839 29.639692 \n",
       "L 34.883753 25.003678 \n",
       "L 36.473582 25.462861 \n",
       "L 38.074554 25.696646 \n",
       "L 38.869468 22.017089 \n",
       "L 40.459296 22.560182 \n",
       "L 42.060268 22.867967 \n",
       "L 42.855182 19.853994 \n",
       "L 43.650096 20.21957 \n",
       "L 45.239924 20.573444 \n",
       "L 46.045982 20.715678 \n",
       "L 46.840896 18.520718 \n",
       "L 48.430725 18.933941 \n",
       "L 50.031696 19.206477 \n",
       "L 50.826611 17.273459 \n",
       "L 53.211353 17.837635 \n",
       "L 54.017411 17.946538 \n",
       "L 54.812325 16.279195 \n",
       "L 55.607239 16.646668 \n",
       "L 58.003125 17.072857 \n",
       "L 58.798039 15.66442 \n",
       "L 61.988839 16.307156 \n",
       "L 62.783753 15.138194 \n",
       "L 65.96341 15.693515 \n",
       "L 65.974554 15.694847 \n",
       "L 66.769468 14.663831 \n",
       "L 69.960268 15.210449 \n",
       "L 70.755182 14.31217 \n",
       "L 72.34501 14.625038 \n",
       "L 73.945982 14.824193 \n",
       "L 74.740896 13.960509 \n",
       "L 77.931696 14.436675 \n",
       "L 78.726611 13.684746 \n",
       "L 81.111353 14.052535 \n",
       "L 81.917411 14.172779 \n",
       "L 82.712325 13.451991 \n",
       "L 85.097067 13.79543 \n",
       "L 85.903125 13.908379 \n",
       "L 86.698039 13.263511 \n",
       "L 89.888839 13.704024 \n",
       "L 90.683753 13.154724 \n",
       "L 93.874554 13.497903 \n",
       "L 94.669468 13.047202 \n",
       "L 97.860268 13.415656 \n",
       "L 98.655182 12.977629 \n",
       "L 101.845982 13.284465 \n",
       "L 102.640896 12.814448 \n",
       "L 105.025639 13.131006 \n",
       "L 105.831696 13.195911 \n",
       "L 106.626611 12.698071 \n",
       "L 109.011353 12.967825 \n",
       "L 109.817411 13.042266 \n",
       "L 110.612325 12.761319 \n",
       "L 112.202153 12.827941 \n",
       "L 113.803125 12.941602 \n",
       "L 114.598039 12.656327 \n",
       "L 116.982782 12.81002 \n",
       "L 117.788839 12.8697 \n",
       "L 118.583753 12.531095 \n",
       "L 121.774554 12.793256 \n",
       "L 122.569468 12.498206 \n",
       "L 124.95421 12.728746 \n",
       "L 125.760268 12.802086 \n",
       "L 126.555182 12.426103 \n",
       "L 129.745982 12.658532 \n",
       "L 130.540896 12.357794 \n",
       "L 133.731696 12.59874 \n",
       "L 134.526611 12.304666 \n",
       "L 137.717411 12.548786 \n",
       "L 138.512325 12.381829 \n",
       "L 140.897067 12.515283 \n",
       "L 141.703125 12.569726 \n",
       "L 142.498039 12.232562 \n",
       "L 145.688839 12.469567 \n",
       "L 146.483753 12.264187 \n",
       "L 148.868496 12.429897 \n",
       "L 149.674554 12.459475 \n",
       "L 150.469468 12.173109 \n",
       "L 153.660268 12.395394 \n",
       "L 154.455182 12.161724 \n",
       "L 157.645982 12.393123 \n",
       "L 158.440896 12.104801 \n",
       "L 161.631696 12.318193 \n",
       "L 162.426611 12.156664 \n",
       "L 165.617411 12.343674 \n",
       "L 166.412325 12.108596 \n",
       "L 169.603125 12.290693 \n",
       "L 170.398039 12.084561 \n",
       "L 173.588839 12.274294 \n",
       "L 174.383753 12.040287 \n",
       "L 177.574554 12.249065 \n",
       "L 178.369468 12.032698 \n",
       "L 181.560268 12.212988 \n",
       "L 182.355182 12.095946 \n",
       "L 186.340896 12.14275 \n",
       "L 189.531696 12.224088 \n",
       "L 190.326611 11.997278 \n",
       "L 193.517411 12.156475 \n",
       "L 194.312325 11.945415 \n",
       "L 195.902153 12.065165 \n",
       "L 197.503125 12.13831 \n",
       "L 198.298039 11.902406 \n",
       "L 200.682782 12.097211 \n",
       "L 201.488839 12.126957 \n",
       "L 202.283753 11.907466 \n",
       "L 203.873582 12.024686 \n",
       "L 205.474554 12.103998 \n",
       "L 206.269468 11.94668 \n",
       "L 209.460268 12.114594 \n",
       "L 210.255182 11.848012 \n",
       "L 212.639924 12.016253 \n",
       "L 213.445982 12.050008 \n",
       "L 214.240896 11.870782 \n",
       "L 217.431696 12.066155 \n",
       "L 218.226611 11.856867 \n",
       "L 221.417411 12.045719 \n",
       "L 222.212325 11.908731 \n",
       "L 225.403125 12.021752 \n",
       "L 225.403125 12.021752 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_25\">\n",
       "    <path d=\"M 30.103125 30.05954 \n",
       "L 34.088839 28.832118 \n",
       "L 38.074554 28.930876 \n",
       "L 42.060268 28.761576 \n",
       "L 46.045982 28.719251 \n",
       "L 50.031696 29.410558 \n",
       "L 54.017411 29.847916 \n",
       "L 58.003125 29.593966 \n",
       "L 61.988839 29.438775 \n",
       "L 65.974554 29.340017 \n",
       "L 69.960268 29.495208 \n",
       "L 73.945982 29.593966 \n",
       "L 77.931696 29.678616 \n",
       "L 81.917411 29.819699 \n",
       "L 85.903125 29.706833 \n",
       "L 89.888839 29.664508 \n",
       "L 93.874554 29.706833 \n",
       "L 97.860268 29.6504 \n",
       "L 101.845982 29.904349 \n",
       "L 105.831696 29.763266 \n",
       "L 109.817411 29.340017 \n",
       "L 113.803125 29.56575 \n",
       "L 117.788839 29.777374 \n",
       "L 121.774554 29.622183 \n",
       "L 125.760268 29.622183 \n",
       "L 129.745982 30.242948 \n",
       "L 133.731696 29.862024 \n",
       "L 137.717411 29.946674 \n",
       "L 141.703125 30.327598 \n",
       "L 145.688839 30.257057 \n",
       "L 149.674554 29.6504 \n",
       "L 153.660268 29.763266 \n",
       "L 157.645982 30.172407 \n",
       "L 161.631696 29.509316 \n",
       "L 165.617411 29.340017 \n",
       "L 169.603125 29.932566 \n",
       "L 173.588839 29.523425 \n",
       "L 177.574554 30.003107 \n",
       "L 181.560268 29.636291 \n",
       "L 185.545982 29.678616 \n",
       "L 189.531696 29.636291 \n",
       "L 193.517411 29.833808 \n",
       "L 197.503125 29.678616 \n",
       "L 201.488839 29.932566 \n",
       "L 205.474554 30.299382 \n",
       "L 209.460268 30.454573 \n",
       "L 213.445982 30.567439 \n",
       "L 217.431696 30.200623 \n",
       "L 221.417411 30.31349 \n",
       "L 225.403125 30.355815 \n",
       "\" clip-path=\"url(#p54fb815d60)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 30.103125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 225.403125 149.599219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 149.599219 \n",
       "L 225.403125 149.599219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 10.999219 \n",
       "L 225.403125 10.999219 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 140.634375 144.599219 \n",
       "L 218.403125 144.599219 \n",
       "Q 220.403125 144.599219 220.403125 142.599219 \n",
       "L 220.403125 99.564844 \n",
       "Q 220.403125 97.564844 218.403125 97.564844 \n",
       "L 140.634375 97.564844 \n",
       "Q 138.634375 97.564844 138.634375 99.564844 \n",
       "L 138.634375 142.599219 \n",
       "Q 138.634375 144.599219 140.634375 144.599219 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_26\">\n",
       "     <path d=\"M 142.634375 105.663281 \n",
       "L 152.634375 105.663281 \n",
       "L 162.634375 105.663281 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- train loss -->\n",
       "     <g transform=\"translate(170.634375 109.163281) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_27\">\n",
       "     <path d=\"M 142.634375 120.341406 \n",
       "L 152.634375 120.341406 \n",
       "L 162.634375 120.341406 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- train acc -->\n",
       "     <g transform=\"translate(170.634375 123.841406) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_28\">\n",
       "     <path d=\"M 142.634375 135.019531 \n",
       "L 152.634375 135.019531 \n",
       "L 162.634375 135.019531 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- test acc -->\n",
       "     <g transform=\"translate(170.634375 138.519531) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"100.732422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"152.832031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"192.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"223.828125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"285.107422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"340.087891\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p54fb815d60\">\n",
       "   <rect x=\"30.103125\" y=\"10.999219\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 1e-4, 50\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.125955 M parameters\n",
      "504.02 MB\n"
     ]
    }
   ],
   "source": [
    "parameter_size(net, torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9. <a id='toc11_9_'></a>[GPT](#toc0_)\n",
    "GPT（Generative Pre-trained Transformer）是OpenAI提出的基于Transformer架构的生成模型。GPT仅使用Transformer的解码器部分，通过单向训练（Unidirectional）进行预训练。GPT擅长文本生成任务，能够生成连贯且有意义的文本。GPT-1于2018年发布，随后是GPT-2（2019年）和GPT-3（2020年），每一代模型的生成能力都得到了显著提升。\n",
    "```python\n",
    "就是Transformer的Decoder部分。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoE\n",
    "专家混合模型（Mixture of Experts, MoE）是一种用于处理大规模数据和模型的深度学习架构。MoE模型由多个专家网络和一个门控网络组成，专家网络负责处理不同的输入数据子集，门控网络负责动态地选择合适的专家网络。MoE模型能够有效地处理大规模数据和模型，提高模型的泛化能力和性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[-0.0175,  0.4796, -0.3629,  0.3384, -1.2579, -0.1807, -0.7914, -0.1752,\n",
      "         -3.3145, -2.1341],\n",
      "        [ 0.6781,  1.5992,  0.0348,  0.2335,  0.0074,  0.0514,  0.8723,  0.3556,\n",
      "         -0.5605,  2.6458],\n",
      "        [ 1.7146, -0.7278, -2.3194,  0.2181, -0.2695,  0.9692, -0.0338,  1.4140,\n",
      "          0.2758, -0.4186],\n",
      "        [ 1.0461, -1.0277,  0.4058,  0.2847, -0.4015, -0.0565, -1.6594, -0.6853,\n",
      "         -0.1258, -0.1962]])\n",
      "Output: tensor([[[-1.5784,  2.4017,  1.1923, -0.4639,  0.3437],\n",
      "         [-1.2390,  1.8958,  0.9395, -0.3653,  0.2784]],\n",
      "\n",
      "        [[ 0.6973, -0.4446, -0.9515,  1.1556,  0.0600],\n",
      "         [ 0.5507, -0.3590, -0.7446,  0.9127,  0.0317]],\n",
      "\n",
      "        [[ 0.2116,  0.2233, -0.5717,  0.1656, -0.5791],\n",
      "         [ 0.1630,  0.1762, -0.4485,  0.1383, -0.4475]],\n",
      "\n",
      "        [[-0.0688,  0.4920,  0.2282,  0.6065,  0.0124],\n",
      "         [-0.0523,  0.3917,  0.1894,  0.4814,  0.0165]]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "This module implements a Mixture of Experts (MoE) model using PyTorch.\n",
    "\n",
    "Classes:\n",
    "    MoE: A PyTorch module implementing the Mixture of Experts model.\n",
    "\n",
    "MoE class:\n",
    "    __init__(self, input_dim, output_dim, num_experts, k=1):\n",
    "        Initializes the MoE model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): The dimension of the input features.\n",
    "            output_dim (int): The dimension of the output features.\n",
    "            num_experts (int): The number of expert networks.\n",
    "            k (int): The number of experts to use for each input. Default is 1.\n",
    "\n",
    "    forward(self, x):\n",
    "        Forward pass of the MoE model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, output_dim).\n",
    "\n",
    "Example usage:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts, k=1):\n",
    "        super(MoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k  # Number of experts to use\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the gating values\n",
    "        gate_values = self.gate(x)\n",
    "        gate_values = F.softmax(gate_values, dim=1)\n",
    "\n",
    "        # Select top-k experts\n",
    "        topk_gate_values, topk_indices = torch.topk(gate_values, self.k, dim=1)\n",
    "\n",
    "        # Compute the output of the selected experts\n",
    "        expert_outputs = torch.stack([self.experts[i](x) for i in range(self.num_experts)], dim=1)\n",
    "        topk_expert_outputs = torch.stack([expert_outputs[:, i, :] for i in topk_indices], dim=1)\n",
    "\n",
    "        # Compute the final output\n",
    "        output = torch.sum(topk_gate_values.unsqueeze(2) * topk_expert_outputs, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "output_dim = 5\n",
    "num_experts = 3\n",
    "k = 2\n",
    "batch_size = 4\n",
    "\n",
    "moe = MoE(input_dim, output_dim, num_experts, k)\n",
    "x = torch.randn(batch_size, input_dim)\n",
    "output = moe(x)\n",
    "\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.10. <a id='toc11_10_'></a>[Mamba](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义状态空间模型（SSM）模块\n",
    "class SSM(nn.Module):\n",
    "    def __init__(self, input_size, state_size):\n",
    "        super(SSM, self).__init__()\n",
    "        # 状态空间模型的参数矩阵 A, B, C, D\n",
    "        self.A = nn.Parameter(torch.randn(state_size, state_size))\n",
    "        self.B = nn.Parameter(torch.randn(state_size, input_size))\n",
    "        self.C = nn.Parameter(torch.randn(input_size, state_size))\n",
    "        self.D = nn.Parameter(torch.randn(input_size, input_size))\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        # 状态更新: h_t = A * h_{t-1} + B * x_t\n",
    "        h_next = torch.tanh(self.A @ h_prev + self.B @ x)\n",
    "        # 输出: y_t = C * h_t + D * x_t\n",
    "        y = self.C @ h_next + self.D @ x\n",
    "        return y, h_next\n",
    "\n",
    "# 定义自注意力机制模块\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x 的形状: [seq_len, batch_size, embed_size]\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "# 定义 Mamba 模型\n",
    "class MambaModel(nn.Module):\n",
    "    def __init__(self, input_size, state_size, embed_size, num_heads, num_layers, output_size, select_threshold=0.5):\n",
    "        super(MambaModel, self).__init__()\n",
    "        self.ssm = SSM(input_size, state_size)\n",
    "        self.attention_module = AttentionModule(embed_size, num_heads)\n",
    "        self.num_layers = num_layers\n",
    "        self.fc = nn.Linear(embed_size, output_size)\n",
    "        self.select_threshold = select_threshold\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状: [seq_len, batch_size, input_size]\n",
    "        seq_len, batch_size, input_size = x.size()\n",
    "        h = torch.zeros(batch_size, input_size).to(x.device)  # 初始化状态\n",
    "        \n",
    "        # 用于选择是否应用状态空间模型的掩码\n",
    "        select_mask = (torch.rand(seq_len) > self.select_threshold).to(x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            if select_mask[t]:\n",
    "                # 使用状态空间模型\n",
    "                y, h = self.ssm(x[t], h)\n",
    "            else:\n",
    "                # 使用自注意力机制\n",
    "                y = self.attention_module(x[t].unsqueeze(0)).squeeze(0)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        \n",
    "        # 将输出送入全连接层\n",
    "        outputs = self.fc(outputs.mean(dim=0))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# 模型参数配置\n",
    "input_size = 512     # 输入特征维度\n",
    "state_size = 256     # 状态空间模型的状态维度\n",
    "embed_size = 512     # 自注意力机制的嵌入维度\n",
    "num_heads = 8        # 注意力头的数量\n",
    "num_layers = 6       # 模型的层数\n",
    "output_size = 10     # 输出类别的数量\n",
    "select_threshold = 0.5  # 选择机制的阈值\n",
    "\n",
    "# 创建模型实例\n",
    "model = MambaModel(input_size, state_size, embed_size, num_heads, num_layers, output_size, select_threshold)\n",
    "\n",
    "# 输入示例 (假设序列长度为 30，batch size 为 16)\n",
    "input_data = torch.rand(30, 16, input_size)\n",
    "output = model(input_data)\n",
    "\n",
    "print(output.shape)  # 输出形状: [batch_size, output_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_path = 'kuleshov-group/PlantCaduceus_l24'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path, trust_remote_code=True, device_map=device)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "sequence = \"ATGCGTACGATCGTAG\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(input_ids=input_ids, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. <a id='toc12_'></a>[==============](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. <a id='toc13_'></a>[炼丹心得](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1. <a id='toc13_1_'></a>[关于调参](#toc0_)\n",
    "1. Pytorch没有变量、常量之分，不需要定义说明什么是变量，全部都是张量；\n",
    "\n",
    "2. 因为变量定义后需要初始化，就相当于常量；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5604])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    '''带参数的，自定义层'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(2, requires_grad=True))  # 变量，立即初始化，相当于常量\n",
    "        self.bias = nn.Parameter(torch.zeros(1, requires_grad=True))    # 同上\n",
    "    \n",
    "    def forward(self, X):\n",
    "        y_hat = self.weight.data@X + self.bias.data\n",
    "        # y_hat = torch.matmul(self.weight.data, X) + self.bias.data    # 同上\n",
    "        return F.relu(y_hat)\n",
    "\n",
    "\n",
    "# Test\n",
    "myLayer = MyLayer()\n",
    "\n",
    "X = torch.ones(2)\n",
    "\n",
    "myLayer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([ 2.0576, -0.4972])), ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLayer.state_dict() # 访问神经网络参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2. <a id='toc13_2_'></a>[模型选择](#toc0_)\n",
    "模型的复杂度应该合适，不能太大，也不能太小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3. <a id='toc13_3_'></a>[离散数据](#toc0_)\n",
    "- 离散数据：数据是离散的，不能连续的，比如性别，颜色，类别等。\n",
    "- 连续数据：数据是连续的，可以连续的，比如身高，体重，温度等。\n",
    "\n",
    "\n",
    "在 PyTorch 中，one-hot 编码和embedding（嵌入）是两种常见的用于处理离散数据的方法，尤其是在自然语言处理（NLP）任务中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.1. <a id='toc13_3_1_'></a>[one-hot](#toc0_)\n",
    "One-hot 编码将离散的分类变量（如单词、字符等）转换为高维稀疏向量，其中一个位置为 1，其余位置为 0。  \n",
    "有向量无偏差表示；  \n",
    "One-hot 编码生成的张量是稀疏的，随着类别数量增加，存储效率较低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3496130/1598271952.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(col_raw == raw) # 只是bool\n",
      "/tmp/ipykernel_3496130/1598271952.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  one_hot = torch.tensor(col_raw == raw, dtype=torch.float32) # bool -> torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4]]),\n",
       " tensor([[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# 先做广播，后比较即可\n",
    "raw = [0, 1, 2, 3, 4]\n",
    "raw = torch.tensor(raw)\n",
    "col_raw = raw.reshape(5, 1)\n",
    "col_raw == raw # （5， 1） 和 （1， 5）先广播后比较\n",
    "torch.tensor(col_raw == raw) # 只是bool\n",
    "one_hot = torch.tensor(col_raw == raw, dtype=torch.float32) # bool -> torch.float32\n",
    "\n",
    "col_raw, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# 先做广播，后比较即可\n",
    "raw = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long)\n",
    "\n",
    "# help(F.one_hot)\n",
    "F.one_hot(raw, num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.2. <a id='toc13_3_2_'></a>[embedding](#toc0_)\n",
    "Embedding 是一种将离散的索引映射为稠密向量的方式，用于学习类别之间的语义关系。  \n",
    "相比 One-Hot 编码，Embedding 提供了一个低维的稠密向量表示（比如词向量）。  \n",
    "\n",
    "Embedding 的特点：\n",
    "- 输入是类别索引（通常是整数）。\n",
    "- 输出是一个稠密向量，其维度由用户定义。\n",
    "- 通常作为模型的一部分，通过反向传播自动更新。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.3.2.1. <a id='toc13_3_2_1_'></a>[使用 torch.nn.Embedding](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3292, -0.7713,  0.6218],\n",
      "        [-2.0660, -1.0436,  0.2645],\n",
      "        [-0.3133,  0.4514, -0.9506]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "# 定义类别数量和嵌入维度\n",
    "num_classes = 5 \n",
    "embedding_dim = 3 \n",
    "\n",
    "# 创建嵌入层\n",
    "embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "# 输入类别索引\n",
    "indices = torch.tensor([0, 2, 4])\n",
    "\n",
    "# 获取嵌入向量\n",
    "embedded = embedding(indices)\n",
    "\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.3.2.2. <a id='toc13_3_2_2_'></a>[初始化 Embedding 层](#toc0_)\n",
    "嵌入层可以通过预训练向量（如 Word2Vec、GloVe）初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Tensor:\n",
      "tensor([[0.1000, 0.2000, 0.3000],\n",
      "        [0.7000, 0.8000, 0.9000],\n",
      "        [1.3000, 1.4000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn  \n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "# 假设预训练向量\n",
    "pretrained_weights = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],  # 类别 0\n",
    "    [0.4, 0.5, 0.6],  # 类别 1\n",
    "    [0.7, 0.8, 0.9],  # 类别 2\n",
    "    [1.0, 1.1, 1.2],  # 类别 3\n",
    "    [1.3, 1.4, 1.5]   # 类别 4\n",
    "])\n",
    "\n",
    "# 创建嵌入层并加载权重\n",
    "embedding = nn.Embedding.from_pretrained(pretrained_weights)\n",
    "\n",
    "# 输入类别索引\n",
    "indices = torch.tensor([0, 2, 4])\n",
    "embedded = embedding(indices)\n",
    "print(\"Embedded Tensor:\")\n",
    "print(embedded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4. <a id='toc13_4_'></a>[BN和LN](#toc0_)\n",
    "Batch norm和Layer norm之间的区别  \n",
    "\n",
    "* BatchNorm：在同一特征（同一列），不同样品之间（不同行）之间做的normalization？ standerlization？\n",
    "\n",
    "* LayerNorm：在同一样品（同一行），不同特征（不同列）之间做的normalization？ standerlization？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "torch.nn.BatchNorm1d()\n",
    "torch.nn.LayerNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2., 3., 4.],\n",
       "         [5., 6., 7., 8., 9.]]),\n",
       " tensor([[-1.5667, -1.2185, -0.8704, -0.5222, -0.1741],\n",
       "         [ 0.1741,  0.5222,  0.8704,  1.2185,  1.5667]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "x = torch.arange(10, dtype=torch.float32).reshape(2, 5)\n",
    "\n",
    "ln = nn.LayerNorm(normalized_shape=x.shape)\n",
    "\n",
    "\n",
    "x, ln(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5. <a id='toc13_5_'></a>[掩码 (mask)](#toc0_)\n",
    "在深度学习中，掩码（mask） 是一种用于选择性地处理或忽略数据元素的机制。掩码的常见形式是一个`布尔张量`或`数值张量`，与`目标张量的形状相匹配`，用来标记哪些元素应被关注或忽略。\n",
    "掩码机制广泛应用于自然语言处理（NLP）、计算机视觉（CV）和其他深度学习任务中。  \n",
    "```python \n",
    "scores: (3 x 5)\n",
    "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
    "        [ 5.,  6.,  7.,  8.,  9.],\n",
    "        [10., 11., 12., 13., 14.]])\n",
    "        \n",
    "mask: (3 x 5)\n",
    "tensor([[ True,  True,  True, False, False],\n",
    "        [ True,  True, False, False, False],\n",
    "        [ True,  True,  True,  True,  True]])\n",
    "```\n",
    "\n",
    "掩码的作用和常见场景\n",
    "1. 忽略无效数据  \n",
    "在 NLP 中，序列可能具有不同的长度，为了对齐这些序列，通常会填充（padding）较短的序列。\n",
    "掩码可以帮助模型忽略填充的部分，使得它们不会影响计算。\n",
    "2. 选择性操作  \n",
    "在计算损失或应用注意力机制时，掩码可以用来只关注有效的部分。\n",
    "3. 实现自定义操作  \n",
    "掩码还可以用于筛选数据、执行条件更新等操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.1. <a id='toc13_5_1_'></a>[简单演示](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.1.1. <a id='toc13_5_1_1_'></a>[忽略填充](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 序列张量，填充部分为 0\n",
    "sequences = torch.tensor([\n",
    "    [1, 2, 3, 0, 0],\n",
    "    [4, 5, 0, 0, 0],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "\n",
    "# 创建掩码，标记非填充部分\n",
    "# True 表示非填充部分，False 表示填充部分\n",
    "mask = sequences != 0\n",
    "print(\"Mask:\", mask, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于布尔掩码索引数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用掩码过滤张量，此处过滤掉0元素\n",
    "sequences[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6,  0,  0],\n",
       "        [ 8, 10,  0,  0,  0],\n",
       "        [12, 14, 16, 18, 20]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用掩码来选择指定张量，并进行替换操作\n",
    "sequences[mask] = sequences[mask] * 2 \n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于掩码SoftMax计算\n",
    "\n",
    "![基于掩码SoftMax计算](./Pytorch_Pictures/Mask/Masked_SoftMax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.]])\n",
      "valid_length:\n",
      "tensor([[1., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "scores:\n",
      "tensor([[ 0.,  1.,  2., -inf, -inf],\n",
      "        [ 5.,  6., -inf, -inf, -inf],\n",
      "        [10., 11., 12., 13., 14.]])\n",
      "probabilities:\n",
      "tensor([[0.0900, 0.2447, 0.6652, 0.0000, 0.0000],\n",
      "        [0.2689, 0.7311, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]])\n"
     ]
    }
   ],
   "source": [
    "# 基于掩码SoftMax计算\n",
    "# 掩码通常与 PyTorch 的模块结合使用，例如注意力机制\n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "# 假设注意力分数\n",
    "scores = torch.arange(15, dtype=torch.float32).reshape(3, 5)\n",
    "print('scores:', scores, sep='\\n')\n",
    "\n",
    "# 创建掩码\n",
    "## 数值掩码：序列张量，填充部分为 0\n",
    "valid_length = torch.tensor([[1, 1, 1, 0, 0], \n",
    "                             [1, 1, 0, 0, 0], \n",
    "                             [1, 1, 1, 1, 1]], dtype=torch.float32)\n",
    "print('valid_length:', valid_length, sep='\\n')\n",
    "\n",
    "## bool掩码：序列张量，填充部分为 False\n",
    "mask = (valid_length != 0)\n",
    "print('mask:', mask, sep='\\n')\n",
    "\n",
    "# 使用掩码填充分数\n",
    "scores = scores.masked_fill(~mask, float('-inf')) # 填充部分设置为 -inf \n",
    "print('scores:', scores, sep='\\n')\n",
    "\n",
    "# 计算概率，-inf 会变成 0\n",
    "probabilities = F.softmax(scores, dim=-1) # 计算概率\n",
    "print('probabilities:', probabilities, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.1.2. <a id='toc13_5_1_2_'></a>[加权忽略](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "loss:\n",
      "tensor([[ 0.8100,  3.2400,  7.2900,  0.1600,  0.2500],\n",
      "        [12.2500, 19.3600,  0.4900,  0.6400,  0.8100],\n",
      "        [26.0100, 38.4400, 53.2900, 70.5600, 90.2500]])\n",
      "masked_loss:\n",
      "tensor([[ 0.8100,  3.2400,  7.2900,  0.0000,  0.0000],\n",
      "        [12.2500, 19.3600,  0.0000,  0.0000,  0.0000],\n",
      "        [26.0100, 38.4400, 53.2900, 70.5600, 90.2500]])\n",
      "Final loss:\n",
      "tensor(32.1500)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# 假设模型输出和目标值\n",
    "output = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "])\n",
    "\n",
    "target = torch.tensor([\n",
    "    [1, 2, 3, 0, 0],\n",
    "    [4, 5, 0, 0, 0],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "\n",
    "mask = (target !=0)\n",
    "print('mask:', mask, sep='\\n')\n",
    "\n",
    "# 损失函数\n",
    "loss = (output - target.float())**2\n",
    "print('loss:', loss, sep='\\n')\n",
    "\n",
    "# 使用掩码忽略填充部分\n",
    "masked_loss = loss * mask\n",
    "print('masked_loss:', masked_loss, sep='\\n')\n",
    "\n",
    "final_loss = masked_loss.sum() / mask.sum()  # 仅对非填充部分求平均\n",
    "print(\"Final loss:\", final_loss, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4],\n",
       "         [5, 6, 7, 8, 9]]),\n",
       " tensor(45))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10).reshape(2, 5)\n",
    "\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.2. <a id='toc13_5_2_'></a>[注意力机制中的掩码](#toc0_)\n",
    "在 Transformer 中，掩码常用于：\n",
    "  1. Padding Mask：防止模型关注填充的部分。\n",
    "  2. Causal Mask（未来掩码）：防止模型在自回归任务中关注未来的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.2.1. <a id='toc13_5_2_1_'></a>[Padding Mask](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:\n",
      "tensor([[ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "~mask:\n",
      "tensor([[False, False, False,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False]])\n",
      "Masked Attention:\n",
      "tensor([[0.1000, 0.2000, 0.3000,   -inf,   -inf],\n",
      "        [0.5000, 0.6000,   -inf,   -inf,   -inf],\n",
      "        [0.9000, 0.8000, 0.7000, 0.6000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# 0表示填充，1表示有效\n",
    "valid_length = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "mask = (valid_length !=0)\n",
    "print('mask:', mask, sep='\\n')\n",
    "# 取反操作\n",
    "print('~mask:', ~mask, sep='\\n')\n",
    "\n",
    "\n",
    "# 假设一批序列的注意力权重\n",
    "attention_weights = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "])\n",
    "\n",
    "# 利用掩码设置填充部分的权重为 -inf\n",
    "masked_attention = attention_weights.masked_fill(~mask, float('-inf'))\n",
    "print(\"Masked Attention:\", masked_attention, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5.2.2. <a id='toc13_5_2_2_'></a>[Causal Mask](#toc0_)\n",
    "用于防止模型在解码时看到未来的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask:\n",
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "\n",
    "# True 表示需要屏蔽的部分\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "print(\"Causal Mask:\", causal_mask, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.3. <a id='toc13_5_3_'></a>[掩码注意力计算](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3, 0, 0],\n",
       "         [4, 5, 0, 0, 0],\n",
       "         [6, 7, 8, 9, 0]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F  \n",
    "\n",
    "\n",
    "# 假设有一个批次的序列，长度为5\n",
    "sequences = torch.tensor([[1, 2, 3, 0, 0],  # 0 表示填充\n",
    "                          [4, 5, 0, 0, 0],\n",
    "                          [6, 7, 8, 9, 0]])\n",
    "sequences, sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ True,  True,  True, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True, False]]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个mask，1表示有效部分，0表示填充部分\n",
    "mask = (sequences != 0)\n",
    "mask, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1088, 0.1637, 0.7025, 0.6790, 0.9155],\n",
       "          [0.2418, 0.1591, 0.7653, 0.2979, 0.8035],\n",
       "          [0.3813, 0.7860, 0.1115, 0.2477, 0.6524],\n",
       "          [0.6057, 0.3725, 0.7980, 0.8399, 0.1374],\n",
       "          [0.2331, 0.9578, 0.3313, 0.3227, 0.0162]],\n",
       " \n",
       "         [[0.2137, 0.6249, 0.4340, 0.1371, 0.5117],\n",
       "          [0.1585, 0.0758, 0.2247, 0.0624, 0.1816],\n",
       "          [0.9998, 0.5944, 0.6541, 0.0337, 0.1716],\n",
       "          [0.3336, 0.5782, 0.0600, 0.2846, 0.2007],\n",
       "          [0.5014, 0.3139, 0.4654, 0.1612, 0.1568]],\n",
       " \n",
       "         [[0.2083, 0.3289, 0.1054, 0.9192, 0.4008],\n",
       "          [0.9302, 0.6558, 0.0766, 0.8460, 0.3624],\n",
       "          [0.3083, 0.0850, 0.0029, 0.6431, 0.3908],\n",
       "          [0.6947, 0.0897, 0.8712, 0.1330, 0.4137],\n",
       "          [0.6044, 0.7581, 0.9037, 0.9555, 0.1035]]]),\n",
       " torch.Size([3, 5, 5]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设有一个注意力得分矩阵\n",
    "attention_scores = torch.rand(3, 5, 5)  # (batch_size, seq_length, seq_length)\n",
    "attention_scores, attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True, False, False]],\n",
       " \n",
       "         [[ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True, False, False, False]],\n",
       " \n",
       "         [[ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True, False]]]),\n",
       " torch.Size([3, 5, 5]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用masked_fill来忽略填充部分\n",
    "# mask需要扩展到与attention_scores相同的形状\n",
    "mask = mask.unsqueeze(1).expand(-1, sequences.size(1), -1)\n",
    "mask, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True]],\n",
       "\n",
       "        [[False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7284, 0.8325, 0.7335,   -inf,   -inf],\n",
       "         [0.4551, 0.3431, 0.4233,   -inf,   -inf],\n",
       "         [0.6648, 0.7469, 0.0081,   -inf,   -inf],\n",
       "         [0.1827, 0.9752, 0.4536,   -inf,   -inf],\n",
       "         [0.2960, 0.3019, 0.9114,   -inf,   -inf]],\n",
       "\n",
       "        [[0.7884, 0.0216,   -inf,   -inf,   -inf],\n",
       "         [0.7880, 0.1560,   -inf,   -inf,   -inf],\n",
       "         [0.8611, 0.5480,   -inf,   -inf,   -inf],\n",
       "         [0.0806, 0.9418,   -inf,   -inf,   -inf],\n",
       "         [0.2446, 0.0949,   -inf,   -inf,   -inf]],\n",
       "\n",
       "        [[0.0106, 0.8559, 0.8870, 0.3322,   -inf],\n",
       "         [0.2074, 0.1152, 0.6055, 0.7251,   -inf],\n",
       "         [0.3443, 0.9339, 0.3960, 0.9770,   -inf],\n",
       "         [0.1821, 0.4533, 0.3604, 0.4188,   -inf],\n",
       "         [0.3007, 0.6403, 0.9883, 0.0820,   -inf]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将填充部分的注意力得分设置为一个非常小的值\n",
    "attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重:\n",
      " tensor([[[0.3210, 0.3563, 0.3227, 0.0000, 0.0000],\n",
      "         [0.3493, 0.3123, 0.3384, 0.0000, 0.0000],\n",
      "         [0.3840, 0.4169, 0.1991, 0.0000, 0.0000],\n",
      "         [0.2212, 0.4887, 0.2901, 0.0000, 0.0000],\n",
      "         [0.2593, 0.2608, 0.4798, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6828, 0.3172, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6529, 0.3471, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5776, 0.4224, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2971, 0.7029, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5373, 0.4627, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1406, 0.3275, 0.3378, 0.1940, 0.0000],\n",
      "         [0.1969, 0.1795, 0.2932, 0.3304, 0.0000],\n",
      "         [0.1743, 0.3142, 0.1835, 0.3281, 0.0000],\n",
      "         [0.2095, 0.2747, 0.2504, 0.2654, 0.0000],\n",
      "         [0.1924, 0.2702, 0.3827, 0.1546, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# 计算注意力权重\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "print(\"注意力权重:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.6. <a id='toc13_6_'></a>[MLP、FC、FNN、CNN、RNN](#toc0_)\n",
    "Linear()：线性网络，即没有非线性激活函数  \n",
    "MLP()：多层感知机，有非线性激活函数  \n",
    "FNN()：前馈神经网络，同MLP（）  \n",
    "CNN()：卷积神经网络    \n",
    "RNN()：循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.7. <a id='toc13_7_'></a>[优化显存使用](#toc0_)\n",
    "PyTorch 在进行深度学习训练的时候，有 4 大部分的显存开销，分别是`模型参数(parameters)`，`模型参数的梯度(gradients)`，`优化器状态(optimizer states)` 以及 `中间激活值(intermediate activations) 或者叫中间结果(intermediate results)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.1. <a id='toc13_7_1_'></a>[删除中间暂时不用的变量](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.randn(1000, 1000)\n",
    "\n",
    "# 删除变量\n",
    "del x \n",
    "\n",
    "# 释放显存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.2. <a id='toc13_7_2_'></a>[混合精度训练(Mixed Precision Training)](#toc0_)\n",
    "使用半精度（FP16）或混合精度（AMP）可以减少显存占用，提高训练速度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1140599250793457\n",
      "Epoch 1, Loss: 1.1137373447418213\n",
      "Epoch 2, Loss: 1.1134288311004639\n",
      "Epoch 3, Loss: 1.1131079196929932\n",
      "Epoch 4, Loss: 1.1127952337265015\n",
      "Epoch 5, Loss: 1.1124804019927979\n",
      "Epoch 6, Loss: 1.112164855003357\n",
      "Epoch 7, Loss: 1.11184823513031\n",
      "Epoch 8, Loss: 1.111536979675293\n",
      "Epoch 9, Loss: 1.1112210750579834\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# 定义一个简单的模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(1000, 1000)\n",
    "        self.layer2 = nn.Linear(1000, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 创建模型、损失函数和优化器\n",
    "model = SimpleModel().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# 创建输入数据和目标\n",
    "input_data = torch.randn(10, 1000).cuda()\n",
    "target = torch.randn(10, 1000).cuda()\n",
    "\n",
    "# 创建GradScaler用于缩放梯度\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 混合精度训练循环\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 使用autocast进行前向传播\n",
    "    with autocast(device_type='cuda'):\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output, target)\n",
    "    \n",
    "    # 使用GradScaler进行反向传播和优化\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.3. <a id='toc13_7_3_'></a>[梯度检查点（Gradient Checkpointing）](#toc0_)\n",
    "- 时间换空间：通过在前向传播过程中保存较少的中间激活值，AlphaFold2可以在反向传播时重新计算这些值，从而减少显存占用。  \n",
    "\n",
    "- 具体地来说，在前向传递中，传入的function将以torch.no_grad的方式运行，即不保存中间激活值。取而代之的是，前向传递保存了输入元组以及function参数。在反向传递中，保存下来的输入元组与function参数将会被重新取回，并且前向传递将会在function上重新计算，此时会追踪中间激活值，然后梯度将会根据这些中间激活值计算得到。\n",
    "\n",
    "\n",
    "```python\n",
    "torch.utils.checkpoint.checkpoint(function, *args, use_reentrant: Optional[bool] = None)\n",
    "# function：在前向传播时调用的函数（通常是模型的某一部分）。\n",
    "# *args：传递给 function 的输入参数。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1000]), torch.Size([10, 1000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils import checkpoint\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(1000, 1000)\n",
    "        self.layer2 = nn.Linear(1000, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 使用梯度检查点来减少内存使用\n",
    "        x = checkpoint.checkpoint(self.layer1, x)\n",
    "        x = checkpoint.checkpoint(self.layer2, x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 创建模型和输入数据\n",
    "model = SimpleModel().cuda()\n",
    "input_data = torch.randn(10, 1000).cuda()\n",
    "\n",
    "# 前向传播\n",
    "output = model(input_data)\n",
    "\n",
    "input_data.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7.4. <a id='toc13_7_4_'></a>[分块计算 (Chunking)](#toc0_)\n",
    "AlphaFold2将计算过程分成多个较小的块来处理。这种方法可以减少一次性需要加载到显存中的数据量，从而降低显存的使用。  \n",
    "在 PyTorch 中，chunk 是一种用于将张量沿指定维度分割为多个小张量的操作。其主要功能是将一个大的张量分成多个`小块（chunk）`，以便于并行处理或其他需要分割数据的场景。\n",
    "\n",
    "```python\n",
    "torch.chunk(input, chunks, dim=0)\n",
    "# input: 要分割的张量\n",
    "# chunks: 分割的块数\n",
    "# dim: 沿哪个维度进行分割，默认为0\n",
    "# 返回：一个包含分割后的小张量的元组\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.7.4.1. <a id='toc13_7_4_1_'></a>[简单演示](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "\n",
      "Chunks:\n",
      "Chunk 0:\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "Chunk 1:\n",
      "tensor([[ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "# 示例 1：按行分割张量\n",
    "import torch\n",
    "\n",
    "\n",
    "# 创建一个 4x4 的张量\n",
    "x = torch.arange(16).view(4, 4)\n",
    "print(\"Original Tensor:\")\n",
    "print(x)\n",
    "\n",
    "# 按行分割为 2 块\n",
    "chunks = torch.chunk(x, chunks=2, dim=0)\n",
    "print(\"\\nChunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks by Columns:\n",
      "Chunk 0:\n",
      "tensor([[ 0],\n",
      "        [ 4],\n",
      "        [ 8],\n",
      "        [12]])\n",
      "Chunk 1:\n",
      "tensor([[ 1],\n",
      "        [ 5],\n",
      "        [ 9],\n",
      "        [13]])\n",
      "Chunk 2:\n",
      "tensor([[ 2],\n",
      "        [ 6],\n",
      "        [10],\n",
      "        [14]])\n",
      "Chunk 3:\n",
      "tensor([[ 3],\n",
      "        [ 7],\n",
      "        [11],\n",
      "        [15]])\n"
     ]
    }
   ],
   "source": [
    "# 示例 2：按列分割张量\n",
    "# 按列分割为 4 块\n",
    "chunks = torch.chunk(x, chunks=4, dim=1)\n",
    "print(\"\\nChunks by Columns:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.7.4.2. <a id='toc13_7_4_2_'></a>[重要演示](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def chunked_attention(query, key, value, chunk_size):\n",
    "    \"\"\"\n",
    "    计算分块的自注意力。\n",
    "    \"\"\"\n",
    "    num_chunks = query.size(0) // chunk_size\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # 获取当前块的query, key, value\n",
    "        q_chunk = query[i * chunk_size:(i + 1) * chunk_size]\n",
    "        k_chunk = key[i * chunk_size:(i + 1) * chunk_size]\n",
    "        v_chunk = value[i * chunk_size:(i + 1) * chunk_size]\n",
    "\n",
    "        # 计算注意力得分\n",
    "        scores = torch.matmul(q_chunk, k_chunk.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        output = torch.matmul(attn_weights, v_chunk)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # 将所有块的输出拼接在一起\n",
    "    return torch.cat(outputs, dim=0)\n",
    "\n",
    "# 示例输入\n",
    "seq_length = 1024\n",
    "d_model = 64\n",
    "chunk_size = 256\n",
    "\n",
    "query = torch.randn(seq_length, d_model)\n",
    "key = torch.randn(seq_length, d_model)\n",
    "value = torch.randn(seq_length, d_model)\n",
    "\n",
    "# 使用分块注意力计算\n",
    "output = chunked_attention(query, key, value, chunk_size)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.8. <a id='toc13_8_'></a>[模型参数量](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数总数: 0.20 M\n",
      "参数占用内存: 0.78 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def parameter_size(model, dtype=torch.float32):\n",
    "    bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "    total_params = count_parameters(model)\n",
    "    total_size = total_params * bytes_per_param\n",
    "    return total_params, total_size\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = SampleModel()\n",
    "    params, size = parameter_size(model, dtype=torch.float32)\n",
    "    print(f\"参数总数: {params / 1000000:.2f} M\")\n",
    "    print(f\"参数占用内存: {size / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.9. <a id='toc13_9_'></a>[大模型微调](#toc0_)\n",
    "\n",
    "|特性|全模型微调（SFT）|参数高效微调（PEFT）|指令微调（Instruction Tuning）|强化学习微调（RLHF）|\n",
    "|---|---|---|---|---|\n",
    "|训练目标|优化单一任务|优化少量参数，提高训练效率|提高多任务能力，适应自然语言指令|生成符合人类反馈的内容|\n",
    "|计算开销|高|低（训练少量参数）|中等，取决于任务复杂度|高，训练奖励模型和强化学习|\n",
    "|数据需求|大量带标注的训练数据|训练数据较少，但任务数据需要多样|需要多样化的指令数据|需要人类反馈数据|\n",
    "|适用场景|单一任务的优化|资源有限的场景，快速微调|多任务学习，灵活的指令处理|开放式任务生成，基于人类偏好的优化|\n",
    "|优点|可以大幅提升单一任务性能|节省资源，减少训练成本|提升多任务能力和灵活性|增强生成质量，符合人类期望|\n",
    "|缺点|计算资源消耗大，容易过拟合|微调效果可能不如全模型微调|数据准备复杂，训练时间长|实现复杂，资源需求高|\n",
    "\n",
    "\n",
    "不需要更新参数：\n",
    " - 提示词微调（Promot tuning）\n",
    " \n",
    " 更新参数：\n",
    " - 全量微调（Full Fine-Tuning）\n",
    " - 高效参数微调（Efficient Fine-Tuning）\n",
    " - 强化学习微调（RLHF）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.10. <a id='toc13_10_'></a>[加速器](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据并行、模型并行、混合并行与工具包的关系比较\n",
    "\n",
    "\n",
    "|特性|数据并行|模型并行|混合并行|\n",
    "|---|---|---|---|\n",
    "|定义|将训练数据划分到多个设备（GPU/节点）上，并进行并行计算，最后合并梯度。|将模型划分为多个部分，并分配到多个设备（GPU/节点）上进行并行计算。|结合数据并行和模型并行，既拆分数据也拆分模型以提高训练效率。|\n",
    "|适用场景|数据量大，但模型相对较小。适用于大规模数据训练，适合处理常见的深度学习任务。|模型大，单个设备无法存下整个模型，适用于非常大的模型（如Transformer、BERT类模型）。|适用于需要同时处理超大模型和大规模数据的训练任务，如训练GPT类模型等。|\n",
    "|工具包支持|DeepSpeed（通过DistributedDataParallel和ZeRO），Horovod（基于Ring-AllReduce），TensorFlow（tf.distribute.Strategy）|Megatron-LM（张量并行），DeepSpeed（ZeRO阶段3），FairScale（FSDP）|DeepSpeed（结合数据并行和ZeRO），Megatron-LM（结合数据并行和模型并行）|\n",
    "|性能优化|高效的梯度同步和数据拆分，适用于多GPU/多节点训练。|模型拆分，适合超大规模模型，减少单个设备内存压力。|综合了数据并行和模型并行的优势，适用于超大规模数据和模型训练。|\n",
    "|内存优化|通过数据拆分来减少每个设备的内存需求。|通过模型拆分来减轻设备的内存压力。|结合了数据和模型并行的内存优化，特别适用于超大模型和数据的训练。|\n",
    "|分布式训练支持|DeepSpeed（支持NCCL后端），Horovod（Ring-AllReduce），TensorFlow（MirroredStrategy）|Megatron-LM（张量并行），FairScale（FSDP），DeepSpeed（ZeRO阶段3）|DeepSpeed（支持两者的混合模式），Megatron-LM（数据和模型并行结合）|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|特性/工具|Hugging Face Trainer|DeepSpeed|Horovod|Megatron-LM|FairScale|\n",
    "|---|---|---|---|---|---|\n",
    "|功能|高层封装的训练API，简化训练过程（适用于NLP任务）|低层次优化，支持大规模模型训练和数据并行，提供多种内存优化方案|分布式训练框架，适用于大规模数据并行训练，基于Horovod接口|专注于超大规模模型（如GPT-3），通过张量并行进行大规模训练|专注于大模型训练的内存优化，支持模型并行和数据并行|\n",
    "|适用场景|主要用于NLP任务，适用于Transformer类模型训练，方便与 Hugging Face 数据集和模型库集成|适用于大规模模型和数据的分布式训练，尤其适合超大模型（如GPT、BERT）|适用于大规模分布式训练，尤其是多节点环境中的数据并行|适用于需要极大计算和内存资源的超大规模模型训练（如GPT-3）|适用于内存受限的情况下进行大规模分布式训练（如BERT、GPT模型）|\n",
    "|易用性|非常易用，高层API，少量代码即可完成训练、微调、评估等任务|需要较多配置，适合需要高度定制化训练的高级用户|需要较高的分布式训练经验，配置较为复杂|需要深入了解模型并行、数据并行的概念，配置复杂|相对复杂，需要开发者了解内存优化和并行训练技术|\n",
    "|分布式训练支持|支持数据并行，集成了 Accelerate 库，支持多GPU训练|支持数据并行、模型并行和混合并行，尤其在大规模模型训练中表现优异|支持数据并行，分布式梯度同步（基于 Ring-AllReduce 或 NCCL）|支持模型并行和数据并行的结合，专门针对超大规模模型（如GPT-3）|支持数据并行和模型并行，内存优化，适合大规模训练\n",
    "|模型并行支持|不支持复杂的模型并行，主要聚焦于数据并行和微调|支持模型并行，尤其是在 ZeRO 和混合并行模式下支持大规模模型训练|不直接支持模型并行，专注于数据并行|通过张量并行（Tensor Parallelism）支持模型并行|支持模型并行，尤其是通过 FSDP（Fully Sharded Data Parallel）模式优化内存\n",
    "|内存优化|提供微调、自动混合精度（AMP）等基本优化|ZeRO（Zero Redundancy Optimizer）优化，支持多种内存优化技术|主要通过数据并行和全局梯度同步优化内存|张量并行和模型分片，通过分布式内存管理优化超大规模模型训练|通过 FSDP 和混合并行优化内存，减少训练时内存占用\n",
    "|性能|性能主要依赖于配置，适用于中小规模模型训练和微调|在大规模模型和数据训练中提供显著性能提升，特别是在分布式环境下|在多节点环境下性能较强，尤其是在数据并行模式下|适合极大规模的模型训练，提供高效的张量并行支持|在模型并行和内存优化方面提供较好性能，适合内存受限的场景\n",
    "|集成度|与 Hugging Face 模型库和数据集无缝集成，极大简化了训练过程|可以与 Hugging Face 集成，但需要更多配置和自定义|需要与 PyTorch 集成，配置较为复杂|可以与 Hugging Face 集成，但适用于大规模训练和模型开发者|适用于与 PyTorch 结合，专注于内存优化和并行训练\n",
    "|自动化功能|自动保存、评估、调优、日志记录、早期停止等功能|提供 ZeRO、FP16、混合精度等自动优化功能，但配置较为复杂|通过 Horovod 提供分布式训练的自动化控制|支持大规模模型的自动优化，尤其是通过模型并行与数据并行的结合|提供分布式训练的内存优化和自动化控制，尤其是 FSDP 优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.10.1. <a id='toc13_10_1_'></a>[deepspeed](#toc0_)\n",
    "DeepSpeed 是一个由 Microsoft 提供的深度学习优化库，旨在提高深度学习模型训练的效率，特别是对于超大规模模型的训练。它提供了多种性能优化技术，包括内存优化、分布式训练、混合精度训练和模型并行等。DeepSpeed 的目标是让研究人员和开发者能够训练更大规模的模型，同时保持高效的内存利用和计算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.10.1.1. <a id='toc13_10_1_1_'></a>[数据并行](#toc0_)\n",
    "在 DeepSpeed 中，数据并行使用 DeepSpeed 和 torch.nn.DataParallel 的结合来加速训练，特别是在多GPU环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-16 15:01:45,498] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=056f307, git-branch=HEAD\n",
      "[2025-01-16 15:01:45,499] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1\n",
      "[2025-01-16 15:01:45,501] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-01-16 15:01:45,502] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-01-16 15:01:45,502] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-01-16 15:01:45,503] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = SGD\n",
      "[2025-01-16 15:01:45,503] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=SGD type=<class 'torch.optim.sgd.SGD'>\n",
      "[2025-01-16 15:01:45,504] [WARNING] [engine.py:1244:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-01-16 15:01:45,504] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2025-01-16 15:01:45,505] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-01-16 15:01:45,505] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-01-16 15:01:45,505] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-01-16 15:01:45,506] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-01-16 15:01:45,649] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-01-16 15:01:45,650] [INFO] [utils.py:782:see_memory_usage] MA 0.36 GB         Max_MA 0.36 GB         CA 0.36 GB         Max_CA 0 GB \n",
      "[2025-01-16 15:01:45,651] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 148.31 GB, percent = 14.7%\n",
      "[2025-01-16 15:01:45,786] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-01-16 15:01:45,787] [INFO] [utils.py:782:see_memory_usage] MA 0.36 GB         Max_MA 0.36 GB         CA 0.36 GB         Max_CA 0 GB \n",
      "[2025-01-16 15:01:45,787] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 148.32 GB, percent = 14.7%\n",
      "[2025-01-16 15:01:45,788] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized\n",
      "[2025-01-16 15:01:45,918] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-01-16 15:01:45,919] [INFO] [utils.py:782:see_memory_usage] MA 0.36 GB         Max_MA 0.36 GB         CA 0.36 GB         Max_CA 0 GB \n",
      "[2025-01-16 15:01:45,920] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 148.28 GB, percent = 14.7%\n",
      "[2025-01-16 15:01:45,921] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-01-16 15:01:45,921] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-01-16 15:01:45,922] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-01-16 15:01:45,922] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.01], mom=[0]\n",
      "[2025-01-16 15:01:45,923] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-01-16 15:01:45,923] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-01-16 15:01:45,924] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-01-16 15:01:45,924] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-01-16 15:01:45,924] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-01-16 15:01:45,925] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-01-16 15:01:45,925] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-01-16 15:01:45,926] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-01-16 15:01:45,926] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-01-16 15:01:45,927] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-01-16 15:01:45,927] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-01-16 15:01:45,927] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdf1987b190>\n",
      "[2025-01-16 15:01:45,928] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-01-16 15:01:45,928] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-01-16 15:01:45,928] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-01-16 15:01:45,929] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-01-16 15:01:45,929] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-01-16 15:01:45,929] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-01-16 15:01:45,931] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-01-16 15:01:45,931] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-01-16 15:01:45,932] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-01-16 15:01:45,932] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-01-16 15:01:45,932] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-01-16 15:01:45,933] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-01-16 15:01:45,933] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-01-16 15:01:45,933] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-01-16 15:01:45,934] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-01-16 15:01:45,934] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-01-16 15:01:45,935] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-01-16 15:01:45,935] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-01-16 15:01:45,935] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-01-16 15:01:45,936] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-01-16 15:01:45,936] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2025-01-16 15:01:45,936] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2025-01-16 15:01:45,937] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-01-16 15:01:45,937] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-01-16 15:01:45,937] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-01-16 15:01:45,938] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-01-16 15:01:45,938] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-01-16 15:01:45,938] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-01-16 15:01:45,939] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-01-16 15:01:45,939] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-01-16 15:01:45,939] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-01-16 15:01:45,940] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-01-16 15:01:45,940] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-01-16 15:01:45,940] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-01-16 15:01:45,941] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-01-16 15:01:45,941] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-01-16 15:01:45,941] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-01-16 15:01:45,942] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-01-16 15:01:45,942] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-01-16 15:01:45,945] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2025-01-16 15:01:45,945] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2025-01-16 15:01:45,946] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-01-16 15:01:45,946] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-01-16 15:01:45,946] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-01-16 15:01:45,947] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-01-16 15:01:45,947] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-01-16 15:01:45,947] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-01-16 15:01:45,948] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-01-16 15:01:45,948] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-01-16 15:01:45,948] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-01-16 15:01:45,949] [INFO] [config.py:1003:print]   steps_per_print .............. 200\n",
      "[2025-01-16 15:01:45,949] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-01-16 15:01:45,949] [INFO] [config.py:1003:print]   train_batch_size ............. 32\n",
      "[2025-01-16 15:01:45,950] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  32\n",
      "[2025-01-16 15:01:45,950] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-01-16 15:01:45,950] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-01-16 15:01:45,951] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-01-16 15:01:45,951] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-01-16 15:01:45,951] [INFO] [config.py:1003:print]   world_size ................... 1\n",
      "[2025-01-16 15:01:45,952] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True\n",
      "[2025-01-16 15:01:45,952] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-01-16 15:01:45,952] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-01-16 15:01:45,953] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-01-16 15:01:45,953] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2\n",
      "[2025-01-16 15:01:45,953] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 32, \n",
      "    \"steps_per_print\": 200, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 启动DeepSpeed训练\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 62\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# 创建数据集\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data_size):\n",
    "        self.data = torch.randn(data_size, 10)\n",
    "        self.labels = torch.randint(0, 2, (data_size,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# 分布式训练配置\n",
    "def train():\n",
    "    model = SimpleModel()\n",
    "\n",
    "    # DeepSpeed配置\n",
    "    config = {\n",
    "        \"train_batch_size\": 32,\n",
    "        \"steps_per_print\": 200,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2\n",
    "        },\n",
    "        # \"fp16\": {\n",
    "        #     \"enabled\": True\n",
    "        # },\n",
    "        # \"cpu_offload\": False\n",
    "        \"zero_allow_untested_optimizer\": True\n",
    "    }\n",
    "\n",
    "    # 创建数据加载器\n",
    "    dataset = SimpleDataset(1000)\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    # 初始化DeepSpeed\n",
    "    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optim.SGD(model.parameters(), lr=0.01), config_params=config)\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            model.backward(loss)\n",
    "            model.step()\n",
    "\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# 启动DeepSpeed训练\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.10.1.2. <a id='toc13_10_1_2_'></a>[模型并行](#toc0_)\n",
    "在 DeepSpeed 中，模型并行允许将模型划分为多个部分并分配到不同的设备。通过这种方式，我们能够训练超大模型，超出单个GPU内存限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-16 14:34:38,491] [WARNING] [real_accelerator.py:181:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-16 14:34:38,493] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3340546/4292986897.py\", line 1, in <module>\n",
      "    import deepspeed\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/__init__.py\", line 26, in <module>\n",
      "    from . import module_inject\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/module_inject/__init__.py\", line 6, in <module>\n",
      "    from .replace_module import replace_transformer_layer, revert_transformer_layer, ReplaceWithTensorSlicing, GroupQuantizer, generic_injection\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/module_inject/replace_module.py\", line 652, in <module>\n",
      "    from ..pipe import PipelineModule\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/pipe/__init__.py\", line 6, in <module>\n",
      "    from ..runtime.pipe import PipelineModule, LayerSpec, TiedLayerSpec\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/pipe/__init__.py\", line 6, in <module>\n",
      "    from .module import PipelineModule, LayerSpec, TiedLayerSpec\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/pipe/module.py\", line 19, in <module>\n",
      "    from ..activation_checkpointing import checkpointing\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py\", line 26, in <module>\n",
      "    from deepspeed.runtime.config import DeepSpeedConfig\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/config.py\", line 29, in <module>\n",
      "    from .zero.config import get_zero_config, ZeroStageEnum\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/zero/__init__.py\", line 15, in <module>\n",
      "    from .mics import MiCS_Init\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/zero/mics.py\", line 19, in <module>\n",
      "    from deepspeed.runtime.zero.stage3 import DeepSpeedZeroOptimizer_Stage3\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py\", line 33, in <module>\n",
      "    from deepspeed.checkpoint.constants import OPTIMIZER_STATE_DICT, FP32_FLAT_GROUPS, PARTITION_COUNT, ZERO_STAGE, LOSS_SCALER\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/checkpoint/__init__.py\", line 10, in <module>\n",
      "    from .utils import (get_layer_ckpt_name_for_rank, get_model_ckpt_name_for_rank, get_zero_ckpt_name_for_rank)\n",
      "  File \"/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/checkpoint/utils.py\", line 41, in <module>\n",
      "    def clone_tensors_for_torch_save(item, device=torch.device('cpu')):\n",
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch1/lib/python3.11/site-packages/deepspeed/checkpoint/utils.py:41: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1682343904035/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  def clone_tensors_for_torch_save(item, device=torch.device('cpu')):\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 启动DeepSpeed训练\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m():\n\u001b[0;32m---> 40\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# DeepSpeed配置\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_print\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         }\n\u001b[1;32m     52\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart1 \u001b[38;5;241m=\u001b[39m \u001b[43mModelPart1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart2 \u001b[38;5;241m=\u001b[39m ModelPart2()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch1/lib/python3.11/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 定义模型的多个部分\n",
    "class ModelPart1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart1, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.fc1(x))\n",
    "\n",
    "\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# 模型组合\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.part1 = ModelPart1().to('cuda:0')\n",
    "        self.part2 = ModelPart2().to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.part1(x)\n",
    "        return self.part2(x)\n",
    "\n",
    "\n",
    "# 分布式训练配置\n",
    "def train():\n",
    "    model = Model()\n",
    "\n",
    "    # DeepSpeed配置\n",
    "    config = {\n",
    "        \"train_batch_size\": 32,\n",
    "        \"steps_per_print\": 200,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 初始化DeepSpeed\n",
    "    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optim.SGD(model.parameters(), lr=0.01), config_params=config)\n",
    "\n",
    "    # 开始训练\n",
    "    data = torch.randn(32, 10).to('cuda:0')  # 模拟输入数据\n",
    "    labels = torch.randint(0, 2, (32,)).to('cuda:1')  # 标签\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.backward(loss)\n",
    "        model.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 启动DeepSpeed训练\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.10.1.3. <a id='toc13_10_1_3_'></a>[混合并行](#toc0_)\n",
    "混合并行结合了数据并行和模型并行的优势。数据被拆分到多个设备上，同时每个设备上存储模型的不同部分。DeepSpeed 提供了一个简单的API来实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 定义模型的多个部分\n",
    "class ModelPart1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart1, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.fc1(x))\n",
    "\n",
    "\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelPart2, self).__init__()\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# 模型组合\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.part1 = ModelPart1().to('cuda:0')\n",
    "        self.part2 = ModelPart2().to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.part1(x)\n",
    "        return self.part2(x)\n",
    "\n",
    "\n",
    "# 分布式训练配置\n",
    "def train():\n",
    "    model = Model()\n",
    "\n",
    "    # DeepSpeed配置\n",
    "    config = {\n",
    "        \"train_batch_size\": 32,\n",
    "        \"steps_per_print\": 200,\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 初始化DeepSpeed\n",
    "    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optim.SGD(model.parameters(), lr=0.01), config_params=config)\n",
    "\n",
    "    # 创建数据\n",
    "    data = torch.randn(32, 10).to('cuda:0')\n",
    "    labels = torch.randint(0, 2, (32,)).to('cuda:1')\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.backward(loss)\n",
    "        model.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 启动DeepSpeed训练\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.10.2. <a id='toc13_10_2_'></a>[huggingface trainer and accelerate](#toc0_)\n",
    "#### 13.10.2.1. <a id='toc13_10_2_1_'></a>[数据并行](#toc0_)\n",
    "#### 13.10.2.2. <a id='toc13_10_2_2_'></a>[模型并行](#toc0_)\n",
    "#### 13.10.2.3. <a id='toc13_10_2_3_'></a>[混合并行](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. <a id='toc14_'></a>[PyTorch做迁移学习](#toc0_)\n",
    "- 在今后的很长时间，深度学习的模型创新上会有很大的难度，基于已有的模型的微调（Fine-tuning）应用于新的可解决的问题是趋势。\n",
    "\n",
    "- Fine-tuning in CV：\n",
    "\n",
    "    - 1.用Pre-trained的参数初始化特征提取器如Encoder的参数，而不是随机初始化；\n",
    "\n",
    "    - 2.用小的lerning-rate和小的epochs；\n",
    "\n",
    "    - 3.固定模型层的（其实就是learning-rate为0）。\n",
    "\n",
    "- 如何找到Pre-trained model？\n",
    "\n",
    "    - TIMM（pytorch）-一个叫Ross的小哥自己维护的；\n",
    "\n",
    "    - HugginFace - 一个早期只是东抄抄西抄抄的公司，逐渐发展为比较好的社区公司。\n",
    "\n",
    "- Fine-tuning in NLP：\n",
    "\n",
    "    - 1.Self-supervised pre-training;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1. <a id='toc14_1_'></a>[Fine-tuning](#toc0_)\n",
    "- 目前已知两种方式进行Fine-tuning:\n",
    "    - 设置非常小的lr\n",
    "    - param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.1. <a id='toc14_1_1_'></a>[小的lr](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 1)\n",
    "        self.hidden = nn.Linear(1, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.fc = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.input(X)\n",
    "        y = self.hidden(y)\n",
    "        y = self.output(y)\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "param_1x = [param for name, param in net.named_parameters() if name not in ['fc.weight', 'fc.bias']]    # 提取出fc以外的所有参数\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "opt = optim.SGD(\n",
    "    params=[\n",
    "        {'params': param_1x},                                           # lr不变\n",
    "        {'params': net.fc.parameters(), 'lr': learning_rate * 0.001}    # lr缩小\n",
    "    ], \n",
    "    lr=learning_rate, \n",
    "    weight_decay=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.2. <a id='toc14_1_2_'></a>[停止计算梯度](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认参数信息：\n",
      "input.weight >>> True\n",
      "input.bias >>> True\n",
      "hidden.weight >>> True\n",
      "hidden.bias >>> True\n",
      "output.weight >>> True\n",
      "output.bias >>> True\n",
      "fc.weight >>> True\n",
      "fc.bias >>> True\n",
      "========== \n",
      " 修改后参数信息：\n",
      "input.weight >>> False\n",
      "input.bias >>> False\n",
      "hidden.weight >>> False\n",
      "hidden.bias >>> False\n",
      "output.weight >>> False\n",
      "output.bias >>> False\n",
      "fc.weight >>> True\n",
      "fc.bias >>> True\n"
     ]
    }
   ],
   "source": [
    "from torch import optim \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 1)\n",
    "        self.hidden = nn.Linear(1, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.fc = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.input(X)\n",
    "        y = self.hidden(y)\n",
    "        y = self.output(y)\n",
    "        y = self.fc(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print('默认参数信息：')\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, '>>>', param.requires_grad)\n",
    "\n",
    "print('='*10, '\\n', '修改后参数信息：')\n",
    "for name, param in net.named_parameters():\n",
    "    if name not in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = False\n",
    "    print(name, '>>>', param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2. <a id='toc14_2_'></a>[torchvision的应用案例](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3. <a id='toc14_3_'></a>[迁移学习案例](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    '''重载torch.utils.data.Dataset类'''\n",
    "    def __init__(self, dirname, transform=None):\n",
    "        super(MyDataset, self).__init__() # 要不要都行\n",
    "        self.classes = os.listdir(dirname)\n",
    "        self.images = []\n",
    "        self.transform = transform\n",
    "        for i, classes in enumerate(self.classes):\n",
    "            classes_path = os.path.join(dirname, classes)\n",
    "            for image_name in os.listdir(classes_path):\n",
    "                self.images.append((os.path.join(classes_path, image_name), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        '''改写__len__()方法'''\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''改写__getitem__()方法'''\n",
    "        image_name, classes = self.images[idx]\n",
    "        image = Image.open(image_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, classes\n",
    "    \n",
    "    def get_claesses(self):\n",
    "        return self.classes\n",
    "    \n",
    "# 分布实现训练和预测的transform\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.RandomResizedCrop(224), #随机裁剪一个area然后再resize\n",
    "        transforms.RandomHorizontalFlip(), #随机水平翻转\n",
    "        transforms.Resize(size=(256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.Resize(size=(256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 分别实现loader\n",
    "# ws = 'Pytorch_datasets/hymenoptera_data/'\n",
    "train_dataset = MyDataset('Pytorch_datasets/hymenoptera_data/train/', train_transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "\n",
    "val_dataset = MyDataset('Pytorch_datasets/hymenoptera_data/val/', val_transform)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择预训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练的模型\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0350, -0.0355,  ...,  0.0068,  0.0349,  0.0407],\n",
      "        [-0.0257,  0.0340, -0.0237,  ..., -0.0052, -0.0351,  0.0249]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0364,  0.0310], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 停止权重更新，并将model最后一层替换掉\n",
    "only_train_fc = True\n",
    "if only_train_fc:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "fc_in_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(fc_in_features, 2, bias=True)\n",
    "\n",
    "# 查看\n",
    "for i in model.parameters():\n",
    "    if i.requires_grad:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练主体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.31711972 0.88244045\n",
      "1 0.30389076 0.85200006\n",
      "0 0.33484977 0.858817\n",
      "1 0.4615616 0.80550003\n",
      "0.85200006\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "device = 'cuda:0'\n",
    "\n",
    "epochs = 2\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(lr=0.01, params=model.parameters())\n",
    "opt_step = torch.optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.1)\n",
    "max_acc = 0\n",
    "epoch_acc = []\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for type_id, loader in enumerate([train_loader, val_loader]):\n",
    "        # print('type_id:',type_id)\n",
    "        mean_loss = []\n",
    "        mean_acc = []\n",
    "        for images, labels in loader:\n",
    "            if type_id == 0:\n",
    "                # opt_step.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            opt.zero_grad()\n",
    "            with torch.set_grad_enabled(type_id==0):\n",
    "                outputs = model(images)\n",
    "                _, pre_labels = torch.max(outputs, 1)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            if type_id == 0:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            acc = torch.sum(pre_labels==labels) / torch.tensor(labels.shape[0], dtype=torch.float32)        \n",
    "            mean_loss.append(loss.detach().cpu().numpy())\n",
    "            mean_acc.append(acc.detach().cpu().numpy())\n",
    "        if type_id == 1:\n",
    "            epoch_acc.append(np.mean(mean_acc))\n",
    "            epoch_loss.append(np.mean(mean_loss))\n",
    "            if max_acc < np.mean(mean_acc):\n",
    "                max_acc = np.mean(mean_acc)\n",
    "        print(type_id, np.mean(mean_loss),np.mean(mean_acc))\n",
    "print(max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. <a id='toc15_'></a>[Metrics](#toc0_)\n",
    "\n",
    "- 图像分类任务中，需要计算各种评估指标，如准确率、精确率、召回率等。\n",
    "\n",
    "- 文本分类任务中，需要计算评估指标，如 F1 分数。\n",
    "\n",
    "- 生成对抗网络（GAN）的训练中，需要计算生成图片的质量指标，如 Frechet Inception Distance（FID）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1. <a id='toc15_1_'></a>[TorchMetrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.1. <a id='toc15_1_1_'></a>[准确率、精确率、召回率和F1分数](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: tensor(0.5000)\n",
      "prec: tensor(0.5000)\n",
      "rec: tensor(0.5000)\n",
      "F1 score: tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchmetrics \n",
    "\n",
    "\n",
    "# 模拟预测和真实标签\n",
    "preds = torch.tensor([0, 2, 1, 3])\n",
    "target = torch.tensor([0, 1, 2, 3])\n",
    "\n",
    "# 准确率： Accuracy\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
    "acc = accuracy(preds, target)\n",
    "print('acc:', acc)\n",
    "\n",
    "# 精确率： precision\n",
    "precision = torchmetrics.Precision(task='multiclass', num_classes=4)\n",
    "prec = precision(preds, target)\n",
    "print('prec:', prec)\n",
    "\n",
    "# 召回率： recall\n",
    "recall = torchmetrics.Recall(task='multiclass', num_classes=4)\n",
    "rec = recall(preds, target)\n",
    "print('rec:', rec)\n",
    "\n",
    "# F1分数\n",
    "f1_score = torchmetrics.F1Score(task='multiclass', num_classes=4)\n",
    "f1 = f1_score(preds, target)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.2. <a id='toc15_1_2_'></a>[自定义计算指标](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchmetrics \n",
    "\n",
    "\n",
    "class CustomMetrics(torchmetrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def compute(self):\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.3. <a id='toc15_1_3_'></a>[于PyTorch Lightning联合使用](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pytorch_lightning as L \n",
    "import torchmetrics  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2903, 0.3722, 0.0199, 0.3067, 0.0109],\n",
       "         [0.4634, 0.0822, 0.1153, 0.1082, 0.2309],\n",
       "         [0.2371, 0.4400, 0.0101, 0.0359, 0.2770],\n",
       "         [0.1717, 0.0242, 0.0344, 0.6931, 0.0766],\n",
       "         [0.1555, 0.2979, 0.2756, 0.0383, 0.2328],\n",
       "         [0.2484, 0.1408, 0.1883, 0.0630, 0.3596],\n",
       "         [0.3829, 0.0814, 0.1469, 0.3578, 0.0310],\n",
       "         [0.0218, 0.2064, 0.3804, 0.2019, 0.1896],\n",
       "         [0.1798, 0.3152, 0.1579, 0.1464, 0.2007],\n",
       "         [0.0955, 0.0309, 0.1288, 0.2167, 0.5280]]),\n",
       " tensor([0, 2, 1, 1, 1, 3, 1, 0, 2, 2]),\n",
       " tensor(0.2000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# import our library\n",
    "import torchmetrics\n",
    "\n",
    "# simulate a classification problem\n",
    "preds = torch.randn(10, 5).softmax(dim=-1)\n",
    "target = torch.randint(5, (10,))\n",
    "# target = torch.randn(5, (10,))\n",
    "\n",
    "acc = torchmetrics.functional.accuracy(preds, target, task=\"multiclass\", num_classes=5)\n",
    "preds, target, acc\n",
    "# preds.dtype, target.dtype, acc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2. <a id='toc15_2_'></a>[分类问题的评估指标](#toc0_)\n",
    "分类问题的目标是将输入数据分配到预定义的类别中。评估指标主要基于模型预测的类别与实际类别的匹配程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.1. <a id='toc15_2_1_'></a>[混淆矩阵](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.1. <a id='toc15_2_1_1_'></a>[二分类混淆矩阵](#toc0_)\n",
    "1. 对于二分类问题，混淆矩阵可以表示为：\n",
    "\n",
    "    |实际正例 (Positive)|实际负例 (Negative)|\n",
    "    |:---:|:---:|\n",
    "    |预测正例|真正例 (TP)|假正例 (FP)|\n",
    "    |预测负例|假负例 (FN)|真负例 (TN)|\n",
    "\n",
    "    - TP（True Positive）: 实际是正例，且预测为正例。\n",
    "\n",
    "    - FP（False Positive）: 实际是负例，但预测为正例（误报）。\n",
    "\n",
    "    - FN（False Negative）: 实际是正例，但预测为负例（漏报）。\n",
    "\n",
    "    - TN（True Negative）: 实际是负例，且预测为负例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.2. <a id='toc15_2_1_2_'></a>[多分类混淆矩阵](#toc0_)\n",
    "2. 对于多分类任务，混淆矩阵会扩展为𝐶×𝐶的结构，其中𝐶是类别数。\n",
    "\n",
    "    |预测\\实际|类别 1|类别 2|类别 3|...|类别 𝐶|\n",
    "    |:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "    |类别 1|TP|FP|FP|...|FP|\n",
    "    |类别 2|FN|TP|FP|...|FP|\n",
    "    |类别 3|FN|FN|TP|...|FP|\n",
    "    |...|...|...|...|...|...|\n",
    "    |类别 𝐶|FN|FN|FN|...|TP|\n",
    "\n",
    "    - 对角线上的值为正确分类的数量。\n",
    "    - 非对角线上的值为分类错误的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.3. <a id='toc15_2_1_3_'></a>[可视化混淆矩阵](#toc0_)\n",
    "通过热力图或颜色编码的矩阵图可以直观展示分类器的性能。Python 中可以使用 sklearn.metrics 模块生成混淆矩阵，并用 seaborn 可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAG2CAYAAACNs6TQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuhklEQVR4nO3dfVzV9f3/8edB5cIEDBOEiYpWiJpX4JKWprlwuPnT29xmt/qqldaXWZGR2dCVVityOSMrQUvFLvzONtJ0mdNtola6QrEr0XWBQgahtSQxQfDz+8M4t46gnuM5h9M578fd2+d223mfz8VLZ754vd7vz+djsyzLEgAACAhBvg4AAAB4DokdAIAAQmIHACCAkNgBAAggJHYAAAIIiR0AgABCYgcAIICQ2AEACCAkdgAAAgiJHQCAAEJiBwDAC/Ly8tS/f39FREQoIiJCqampev311895zNatW5WcnKzQ0FD17NlT+fn5Ll+XxA4AgBd07dpVjz32mIqLi1VcXKxrr71W48aN04cfftji/mVlZRozZoyGDRumkpISzZ49W5mZmSosLHTpujZeAgMAQOuIiorS448/rqlTpzb77r777tO6detUWlpqH8vIyNC7776rHTt2OH2Nth6J1EdOnTqlzz//XOHh4bLZbL4OBwDgIsuy9M033yguLk5BQd5rIp84cUL19fVun8eyrGb5JiQkRCEhIec8rrGxUX/5y19UW1ur1NTUFvfZsWOH0tLSHMZGjx6tZcuW6eTJk2rXrp1TMfp1Yv/8888VHx/v6zAAAG6qqKhQ165dvXLuEydOKCy8k9Rw3O1zdejQQceOHXMYmzt3rubNm9fi/u+//75SU1N14sQJdejQQWvWrFGfPn1a3LeqqkoxMTEOYzExMWpoaNCRI0cUGxvrVIx+ndjDw8MlScF9psjWJtjH0QDeUV60wNchAF7zTU2NLk2It/977g319fVSw3GF9JkiuZMrGut1bO9KVVRUKCIiwj58rmo9MTFRe/bs0ddff63CwkJNmTJFW7duPWtyP7Mb0DRb7kpX2q8Te9Nv1NYmmMSOgPX9f0CAQNUq06ltQ93KFZbt9FRB0yp3ZwQHB+vSSy+VJKWkpOidd97Rk08+qSVLljTbt0uXLqqqqnIYq66uVtu2bdWpUyen4/TrxA4AgNNsktz5AcIDP3tYlqW6uroWv0tNTdX69esdxjZt2qSUlBSn59clbncDAJjCFuT+5oLZs2dr+/btOnDggN5//33NmTNHRUVFuvHGGyVJ2dnZmjx5sn3/jIwMHTx4UFlZWSotLdXy5cu1bNkyzZw506XrUrEDAOAFX3zxhSZNmqTKykpFRkaqf//+2rhxo6677jpJUmVlpcrLy+37JyQkaMOGDbr77rv1zDPPKC4uTosWLdKECRNcui6JHQBgBpvNzVa8a8cuW7bsnN8XFBQ0G7vmmmu0e/dul65zJhI7AMAMF9BOb3a8H/CPKAEAgFOo2AEAZmjlVryvkNgBAIZwsxXvJ01u/4gSAAA4hYodAGAGWvEAAAQQVsUDAAB/Q8UOADADrXgAAAKIIa14EjsAwAyGVOz+8eMHAABwChU7AMAMtOIBAAggNpubiZ1WPAAAaGVU7AAAMwTZTm/uHO8HSOwAADMYMsfuH1ECAACnULEDAMxgyH3sJHYAgBloxQMAAH9DxQ4AMAOteAAAAoghrXgSOwDADIZU7P7x4wcAAHAKFTsAwAy04gEACCC04gEAgL+hYgcAGMLNVryf1MIkdgCAGWjFAwAAf0PFDgAwg83m5qp4/6jYSewAADMYcrubf0QJAACcQsUOADCDIYvnSOwAADMY0oonsQMAzGBIxe4fP34AAACnULEDAMxAKx4AgABCKx4AAPgbKnYAgBFsNptsBlTsJHYAgBFMSey04gEACCBU7AAAM9i+29w53g+Q2AEARqAVDwAA/A4VOwDACKZU7CR2AIARSOwAAAQQUxI7c+wAAAQQEjsAwAw2D2wuyMnJ0ZAhQxQeHq7o6GiNHz9e+/fvP+cxRUVF9s7C97d9+/Y5fV0SOwDACC0lTFc3V2zdulW33367du7cqc2bN6uhoUFpaWmqra0977H79+9XZWWlfbvsssucvi5z7AAAeMHGjRsdPq9YsULR0dHatWuXhg8ffs5jo6Oj1bFjxwu6LhU7AMAIp9/a6k7Ffvo8NTU1DltdXZ1T1z969KgkKSoq6rz7Dho0SLGxsRo1apS2bNni0u+TxA4AMIJNbrbiv5tkj4+PV2RkpH3Lyck577Uty1JWVpauvvpq9evX76z7xcbGaunSpSosLNQrr7yixMREjRo1Stu2bXP690krHgAAF1RUVCgiIsL+OSQk5LzH3HHHHXrvvff0xhtvnHO/xMREJSYm2j+npqaqoqJCCxYsOG/7vgkVOwDACJ5aPBcREeGwnS+x33nnnVq3bp22bNmirl27uhz30KFD9dFHHzm9PxU7AMAMrfx2N8uydOedd2rNmjUqKipSQkLCBV22pKREsbGxTu9PYgcAwAtuv/12rVq1Sq+++qrCw8NVVVUlSYqMjFRYWJgkKTs7W4cOHdLzzz8vScrNzVWPHj3Ut29f1dfX68UXX1RhYaEKCwudvi6JHQBgBjcfKWu5eGxeXp4kacSIEQ7jK1as0E033SRJqqysVHl5uf27+vp6zZw5U4cOHVJYWJj69u2r1157TWPGjHH6uiR2AIAR3H1WvKvHWpZ13n0KCgocPs+aNUuzZs1y6TpnIrEDAIzQ2ondV1gVDwBAAKFiBwCYoZVXxfsKiR0AYARa8QAAwO9QsQMAjGBKxU5iBwAYwZTETiseAIAAQsUOADCCKRU7iR0AYAZDbnejFQ8AQAChYgcAGIFWPAAAAYTEDgBAADElsTPHDgBAAKFiBwCYwZBV8SR2AIARaMUDAAC/Q8WO87plwtW6ZcIwxcdGSZL2fVqlx5e9rn+8tdfHkQGe9dxftumpF/+pL44cVe+esXo0a4KuGnSpr8OCh1Cxt5LFixcrISFBoaGhSk5O1vbt230dEs7wefXXevDpV3XtlMd17ZTHtb34P3ppwW3q3bOLr0MDPOaVTbs0e2Gh7rl5tLa++DulDuyl39y1WBVVX/k6NHiITTZ7cr+gzU8m2X2a2FevXq0ZM2Zozpw5Kikp0bBhw5Senq7y8nJfhoUzbNz+gTa/tVeflFfrk/Jq/SFvvWqP1ymlX4KvQwM8ZvGqf+l/xqVq8virlJjQRTn3/Eo/irlYy/9KsQH/4tPEvnDhQk2dOlXTpk1TUlKScnNzFR8fr7y8PF+GhXMICrLpl9clq31YsN55v8zX4QAeUX+yQXv2VejaK5McxkdemaS33+PveaBwq1p3s43fmnw2x15fX69du3bpd7/7ncN4Wlqa3nrrLR9FhbPp0ytOf19+j0KD26r22zpNuvdZ7S+r8nVYgEd8+fUxNTaeUueocIfxzp3CVf1ljY+igsdxu5t3HTlyRI2NjYqJiXEYj4mJUVVVywmjrq5OdXV19s81NfwH11o+OviFht+Yo8jw9vp/1w7U4nmT9Iv/fZLkjoByZkFmWZbfVGlAE58vnjvzP5pz/YeUk5OjyMhI+xYfH98aIULSyYZGlX12RHtKy/XQM+v0wUeHlHH9CF+HBXhEp44d1KZNkKq//MZh/MhXx5pV8fBfprTifZbYL7nkErVp06ZZdV5dXd2sim+SnZ2to0eP2reKiorWCBUtsNlsCg7mbkkEhuB2bTWwd7y2/Hufw3jR2/v04/4sEg0UJHYvCw4OVnJysjZv3uwwvnnzZl111VUtHhMSEqKIiAiHDd53//SxSh3YS/GxUerTK06//+1YXT34Mv3l9WJfhwZ4zPQbrtULr76lF9ft0P6yKs1eWKjPqr7SzROG+To0eIjN5v7mD3xacmVlZWnSpElKSUlRamqqli5dqvLycmVkZPgyLJyhc1S48h+crJhLIlRz7IQ+/PiQfpW5WEVv7zv/wYCf+GVasr46Wqs/Pve6vjhSo6ResVqdO13dvnswE+AvfJrYJ06cqC+//FIPPfSQKisr1a9fP23YsEHdu3f3ZVg4Q+YfVvk6BKBVTPv1cE379XBfhwEvOV11u/PkOQ8G40U+nySdPn26pk+f7uswAACBzt12up8kdp+vigcAAJ7j84odAIDWYMpLYEjsAAAjuLuy3U/yOq14AAACCRU7AMAIQUE2BQVdeNltuXFsayKxAwCMQCseAAD4HSp2AIARWBUPAEAAMaUVT2IHABjBlIqdOXYAAAIIFTsAwAimVOwkdgCAEUyZY6cVDwBAAKFiBwAYwSY3W/F+8t5WEjsAwAi04gEAgN+hYgcAGIFV8QAABBBa8QAAwO+Q2AEARmhqxbuzuSInJ0dDhgxReHi4oqOjNX78eO3fv/+8x23dulXJyckKDQ1Vz549lZ+f79J1SewAACM0teLd2VyxdetW3X777dq5c6c2b96shoYGpaWlqba29qzHlJWVacyYMRo2bJhKSko0e/ZsZWZmqrCw0OnrMscOADBCay+e27hxo8PnFStWKDo6Wrt27dLw4cNbPCY/P1/dunVTbm6uJCkpKUnFxcVasGCBJkyY4NR1qdgBAHBBTU2Nw1ZXV+fUcUePHpUkRUVFnXWfHTt2KC0tzWFs9OjRKi4u1smTJ526DokdAGAGd9vw3xXs8fHxioyMtG85OTnnvbRlWcrKytLVV1+tfv36nXW/qqoqxcTEOIzFxMSooaFBR44cceq3SSseAGAET7XiKyoqFBERYR8PCQk577F33HGH3nvvPb3xxhtOX6eJZVktjp8NiR0AABdEREQ4JPbzufPOO7Vu3Tpt27ZNXbt2Pee+Xbp0UVVVlcNYdXW12rZtq06dOjl1PRI7AMAIrf2AGsuydOedd2rNmjUqKipSQkLCeY9JTU3V+vXrHcY2bdqklJQUtWvXzqnrMscOADBCa9/Hfvvtt+vFF1/UqlWrFB4erqqqKlVVVenbb7+175Odna3JkyfbP2dkZOjgwYPKyspSaWmpli9frmXLlmnmzJlOX5fEDgCAF+Tl5eno0aMaMWKEYmNj7dvq1avt+1RWVqq8vNz+OSEhQRs2bFBRUZEGDhyohx9+WIsWLXL6VjeJVjwAwBC+aMWfT0FBQbOxa665Rrt373btYt9DYgcAGMGUt7vRigcAIIBQsQMAjGBKxU5iBwAYwZT3sZPYAQBGMKViZ44dAIAAQsUOADACrXgAAAIIrXgAAOB3qNgBAEawyc1WvMci8S4SOwDACEE2m4LcyOzuHNuaaMUDABBAqNgBAEZgVTwAAAHElFXxJHYAgBGCbKc3d473B8yxAwAQQKjYAQBmsLnZTveTip3EDgAwgimL52jFAwAQQKjYAQBGsH33y53j/QGJHQBgBFbFAwAAv0PFDgAwAg+o+Z5FixY5fcLMzMwLDgYAAG8xZVW8U4n9iSeecOpkNpuNxA4AgA85ldjLysq8HQcAAF7Fa1vPo76+Xvv371dDQ4Mn4wEAwCuaWvHubP7A5cR+/PhxTZ06Ve3bt1ffvn1VXl4u6fTc+mOPPebxAAEA8ISmxXPubP7A5cSenZ2td999V0VFRQoNDbWP//SnP9Xq1as9GhwAAHCNy7e7rV27VqtXr9bQoUMdfnrp06ePPvnkE48GBwCAp7Aq/iwOHz6s6OjoZuO1tbV+06YAAJiHxXNnMWTIEL322mv2z03J/Nlnn1VqaqrnIgMAAC5zuWLPycnRz372M+3du1cNDQ168skn9eGHH2rHjh3aunWrN2IEAMBtNrn3SnX/qNcvoGK/6qqr9Oabb+r48ePq1auXNm3apJiYGO3YsUPJycneiBEAALeZsir+gp4Vf8UVV2jlypWejgUAALjpghJ7Y2Oj1qxZo9LSUtlsNiUlJWncuHFq25Z3ygAAfphMeW2ry5n4gw8+0Lhx41RVVaXExERJ0n/+8x917txZ69at0xVXXOHxIAEAcJcpb3dzeY592rRp6tu3rz777DPt3r1bu3fvVkVFhfr376/bbrvNGzECAAAnuVyxv/vuuyouLtbFF19sH7v44ov1yCOPaMiQIR4NDgAAT/KTotstLlfsiYmJ+uKLL5qNV1dX69JLL/VIUAAAeBqr4r+npqbG/r8fffRRZWZmat68eRo6dKgkaefOnXrooYc0f/5870QJAICbWDz3PR07dnT4ScWyLP3mN7+xj1mWJUkaO3asGhsbvRAmAABwhlOJfcuWLd6OAwAArzJlVbxTif2aa67xdhwAAHiVKY+UveAnyhw/flzl5eWqr693GO/fv7/bQQEAgAtzQa9tvfnmm/X666+3+D1z7ACAHyJe23oWM2bM0H//+1/t3LlTYWFh2rhxo1auXKnLLrtM69at80aMAAC4zWZzf/MHLlfs//rXv/Tqq69qyJAhCgoKUvfu3XXdddcpIiJCOTk5+vnPf+6NOAEAgBNcrthra2sVHR0tSYqKitLhw4clnX7j2+7duz0bHQAAHmLKA2ou6Mlz+/fvlyQNHDhQS5Ys0aFDh5Sfn6/Y2FiPBwgAgCfQij+LGTNmqLKyUpI0d+5cjR49Wi+99JKCg4NVUFDg6fgAAIALXK7Yb7zxRt10002SpEGDBunAgQN65513VFFRoYkTJ3o6PgAAPKJpVbw7myu2bdumsWPHKi4uTjabTWvXrj3n/kVFRS22//ft2+fSdS/4PvYm7du31+DBg909DQAAXuVuO93VY2trazVgwADdfPPNmjBhgtPH7d+/XxEREfbPnTt3dum6TiX2rKwsp0+4cOFClwIAAKA1tPYjZdPT05Wenu7ydaKjo9WxY0eXj2viVGIvKSlx6mT+smIQAIAL9f03nkpSSEiIQkJCPHb+QYMG6cSJE+rTp49+//vfa+TIkS4dHxAvgVm6+B617xDu6zAAr7h4yB2+DgHwGqux/vw7eUiQLmBh2RnHS1J8fLzD+Ny5czVv3jw3znxabGysli5dquTkZNXV1emFF17QqFGjVFRUpOHDhzt9Hrfn2AEA8AeeasVXVFQ4zIF7qlpPTExUYmKi/XNqaqoqKiq0YMEClxK7Oz+8AABgnIiICIfNk234Mw0dOlQfffSRS8dQsQMAjGCzSUGtuCreE0pKSlx++BuJHQBghCA3E7urxx47dkwff/yx/XNZWZn27NmjqKgodevWTdnZ2Tp06JCef/55SVJubq569Oihvn37qr6+Xi+++KIKCwtVWFjo0nVJ7AAAeEFxcbHDivamW8enTJmigoICVVZWqry83P59fX29Zs6cqUOHDiksLEx9+/bVa6+9pjFjxrh03QtK7C+88ILy8/NVVlamHTt2qHv37srNzVVCQoLGjRt3IacEAMCrWvs+9hEjRsiyrLN+f+Zj2GfNmqVZs2ZdSGgOXF48l5eXp6ysLI0ZM0Zff/21GhsbJUkdO3ZUbm6u2wEBAOANTa14dzZ/4HJif+qpp/Tss89qzpw5atOmjX08JSVF77//vkeDAwAArnG5FV9WVqZBgwY1Gw8JCVFtba1HggIAwNNa+1nxvuJyxZ6QkKA9e/Y0G3/99dfVp08fT8QEAIDHtfbb3XzF5Yr93nvv1e23364TJ07Isiy9/fbb+r//+z/l5OToueee80aMAAC4zVOPlP2hczmx33zzzWpoaNCsWbN0/Phx3XDDDfrRj36kJ598Utdff703YgQAAE66oNvdbr31Vt166606cuSITp06pejoaE/HBQCAR5kyx+7WA2ouueQST8UBAIBXBcm9efIg+UdmdzmxJyQknPMm/U8//dStgAAAwIVzObHPmDHD4fPJkydVUlKijRs36t577/VUXAAAeBSt+LO46667Whx/5plnVFxc7HZAAAB4Q2u/BMZXPLZ6Pz093eU30AAAAM/y2Nvd/vrXvyoqKspTpwMAwKNOv4/dnZfAeDAYL3I5sQ8aNMhh8ZxlWaqqqtLhw4e1ePFijwYHAICnMMd+FuPHj3f4HBQUpM6dO2vEiBHq3bu3p+ICAAAXwKXE3tDQoB49emj06NHq0qWLt2ICAMDjWDzXgrZt2+q3v/2t6urqvBUPAABeYfPAL3/g8qr4K6+8UiUlJd6IBQAAr2mq2N3Z/IHLc+zTp0/XPffco88++0zJycm66KKLHL7v37+/x4IDAACucTqx33LLLcrNzdXEiRMlSZmZmfbvbDabLMuSzWZTY2Oj56MEAMBNpsyxO53YV65cqccee0xlZWXejAcAAK+w2WznfNeJM8f7A6cTu2VZkqTu3bt7LRgAAOAel+bY/eWnFQAAzkQrvgWXX375eZP7V1995VZAAAB4A0+ea8GDDz6oyMhIb8UCAADc5FJiv/766xUdHe2tWAAA8Jogm82tl8C4c2xrcjqxM78OAPBnpsyxO/3kuaZV8QAA4IfL6Yr91KlT3owDAADvcnPxnJ88Kt71R8oCAOCPgmRTkBvZ2Z1jWxOJHQBgBFNud3P57W4AAOCHi4odAGAEU1bFk9gBAEYw5T52WvEAAAQQKnYAgBFMWTxHYgcAGCFIbrbi/eR2N1rxAAAEECp2AIARaMUDABBAguRem9pfWtz+EicAAHACFTsAwAg2m82tV5D7y+vLSewAACPY5N4L2vwjrZPYAQCG4MlzAADA71CxAwCM4R81t3tI7AAAI5hyHzuteAAAAggVOwDACNzuBgBAAOHJcwAA4IJt27ZNY8eOVVxcnGw2m9auXXveY7Zu3ark5GSFhoaqZ8+eys/Pd/m6JHYAgBGaWvHubK6ora3VgAED9PTTTzu1f1lZmcaMGaNhw4appKREs2fPVmZmpgoLC126Lq14AIARWvvJc+np6UpPT3d6//z8fHXr1k25ubmSpKSkJBUXF2vBggWaMGGC0+ehYgcA4Adgx44dSktLcxgbPXq0iouLdfLkSafPQ8UOADCCp1bF19TUOIyHhIQoJCTErdgkqaqqSjExMQ5jMTExamho0JEjRxQbG+vUeajYAQBGCPLAJknx8fGKjIy0bzk5OR6L8cwfPCzLanH8XKjYAQBG8FTFXlFRoYiICPu4J6p1SerSpYuqqqocxqqrq9W2bVt16tTJ6fOQ2AEAcEFERIRDYveU1NRUrV+/3mFs06ZNSklJUbt27Zw+D614AIARbB7YXHHs2DHt2bNHe/bskXT6drY9e/aovLxckpSdna3Jkyfb98/IyNDBgweVlZWl0tJSLV++XMuWLdPMmTNdui4VOwDACK39Epji4mKNHDnS/jkrK0uSNGXKFBUUFKiystKe5CUpISFBGzZs0N13361nnnlGcXFxWrRokUu3ukkkdgAAvGLEiBH2xW8tKSgoaDZ2zTXXaPfu3W5dl8QOADBCkGwKcuMRNe4c25pI7AAAI/A+dgAA4Heo2AEARrB998ud4/0BiR0AYARa8QAAwO9QsQMAjGBzc1U8rXgAAH5ATGnFk9gBAEYwJbEzxw4AQAChYgcAGIHb3QAACCBBttObO8f7A1rxAAAEECp2AIARaMUDABBAWBUPAAD8DhU7AMAINrnXTveTgp3EDgAwA6viAQCA36Fih1P27S/Xa6/v1IGDVfr662O6684JShmc6OuwAI+4ZcLVumXCMMXHRkmS9n1apceXva5/vLXXx5HBk0xZFe/Tin3btm0aO3as4uLiZLPZtHbtWl+Gg3OoqzupbvHRmnxjmq9DATzu8+qv9eDTr+raKY/r2imPa3vxf/TSgtvUu2cXX4cGD2paFe/O5g98WrHX1tZqwIABuvnmmzVhwgRfhoLzGNC/lwb07+XrMACv2Lj9A4fPf8hbr1smXK2Ufgna92mVj6KCp9nk3gI4P8nrvk3s6enpSk9P92UIAOAgKMim8aMGq31YsN55v8zX4QAu86s59rq6OtXV1dk/19TU+DAaAIGkT684/X35PQoNbqvab+s06d5ntb+Maj2QBMmmIDf66UF+UrP71ar4nJwcRUZG2rf4+HhfhwQgQHx08AsNvzFH193yJy0vfEOL501SYgJz7IHE5oHNH/hVYs/OztbRo0ftW0VFha9DAhAgTjY0quyzI9pTWq6HnlmnDz46pIzrR/g6LMBlftWKDwkJUUhIiK/DAGAAm82m4GC/+icS52PI6jn+1sIpJ07U64vq/9o/Hz58VAfLv9BFF4Xqkk6RPowMcN/908fqH2/t1Wdf/Ffh7UP1y7RkXT34Mv0qc7GvQ4MHmXIfu08T+7Fjx/Txxx/bP5eVlWnPnj2KiopSt27dfBgZzlR2oFKPzn/J/nnVn/8hSbr6J1fof6eN9VVYgEd0jgpX/oOTFXNJhGqOndCHHx/SrzIXq+jtfb4ODXCZTxN7cXGxRo4caf+clZUlSZoyZYoKCgp8FBVaktS7u15YMdvXYQBekfmHVb4OAa3B3YfM+EfB7tvEPmLECFmW5csQAACGMGSK3b9WxQMAgHNj8RwAwAyGlOwkdgCAEVgVDwBAAHH3DW3+8nY35tgBAAggVOwAACMYMsVOYgcAGMKQzE4rHgCAAELFDgAwAqviAQAIIKyKBwAAfoeKHQBgBEPWzpHYAQCGMCSz04oHACCAULEDAIzAqngAAAKIKaviSewAACMYMsXOHDsAAIGEih0AYAZDSnYSOwDACKYsnqMVDwCAFy1evFgJCQkKDQ1VcnKytm/fftZ9i4qKZLPZmm379u1z+npU7AAAI/hiVfzq1as1Y8YMLV68WD/5yU+0ZMkSpaena+/everWrdtZj9u/f78iIiLsnzt37uz0NanYAQBGsHlgc9XChQs1depUTZs2TUlJScrNzVV8fLzy8vLOeVx0dLS6dOli39q0aeP0NUnsAAC4oKamxmGrq6trcb/6+nrt2rVLaWlpDuNpaWl66623znmNQYMGKTY2VqNGjdKWLVtcio/EDgAwg4dK9vj4eEVGRtq3nJycFi935MgRNTY2KiYmxmE8JiZGVVVVLR4TGxurpUuXqrCwUK+88ooSExM1atQobdu2zenfJnPsAAAjeGpVfEVFhcP8d0hIyLmPO2Ny3rKsZmNNEhMTlZiYaP+cmpqqiooKLViwQMOHD3cqTip2AABcEBER4bCdLbFfcsklatOmTbPqvLq6ulkVfy5Dhw7VRx995PT+JHYAgBGaVsW7s7kiODhYycnJ2rx5s8P45s2bddVVVzl9npKSEsXGxjq9P614AIARfPHguaysLE2aNEkpKSlKTU3V0qVLVV5eroyMDElSdna2Dh06pOeff16SlJubqx49eqhv376qr6/Xiy++qMLCQhUWFjp9TRI7AMAMPsjsEydO1JdffqmHHnpIlZWV6tevnzZs2KDu3btLkiorK1VeXm7fv76+XjNnztShQ4cUFhamvn376rXXXtOYMWOcD9OyLMv1UH8YampqFBkZqZXb96l9h3BfhwN4xaSbH/V1CIDXWI31qnv/WR09etRhQZonNeWKXR9VqkP4hV/j2Dc1Sr4s1quxegIVOwDACKY8K57EDgAwg5uPlPWTvM6qeAAAAgkVOwDACIa8jp3EDgAwhCGZnVY8AAABhIodAGAEVsUDABBALuSxsGce7w9oxQMAEECo2AEARjBk7RyJHQBgCEMyO4kdAGAEUxbPMccOAEAAoWIHABjBJjdXxXssEu8isQMAjGDIFDuteAAAAgkVOwDACKY8oIbEDgAwhBnNeFrxAAAEECp2AIARaMUDABBAzGjE04oHACCgULEDAIxAKx4AgABiyrPiSewAADMYMsnOHDsAAAGEih0AYARDCnYSOwDADKYsnqMVDwBAAKFiBwAYgVXxAAAEEkMm2WnFAwAQQKjYAQBGMKRgJ7EDAMzAqngAAOB3qNgBAIZwb1W8vzTjSewAACPQigcAAH6HxA4AQAChFQ8AMIIprXgSOwDACKY8UpZWPAAAAYSKHQBgBFrxAAAEEFMeKUsrHgCAAELFDgAwgyElO4kdAGAEVsUDAAC/Q8UOADACq+IBAAgghkyxk9gBAIYwJLMzxw4AgBctXrxYCQkJCg0NVXJysrZv337O/bdu3ark5GSFhoaqZ8+eys/Pd+l6JHYAgBFsHvjlqtWrV2vGjBmaM2eOSkpKNGzYMKWnp6u8vLzF/cvKyjRmzBgNGzZMJSUlmj17tjIzM1VYWOj0NUnsAAAjNC2ec2dz1cKFCzV16lRNmzZNSUlJys3NVXx8vPLy8lrcPz8/X926dVNubq6SkpI0bdo03XLLLVqwYIHT1/TrOXbLsiRJ39Ye83EkgPdYjfW+DgHwmqa/303/nntTTU2NR44/8zwhISEKCQlptn99fb127dql3/3udw7jaWlpeuutt1q8xo4dO5SWluYwNnr0aC1btkwnT55Uu3btzhunXyf2b775RpKU8bMUH0cCAHDHN998o8jISK+cOzg4WF26dNFlCfFun6tDhw6Kj3c8z9y5czVv3rxm+x45ckSNjY2KiYlxGI+JiVFVVVWL56+qqmpx/4aGBh05ckSxsbHnjdGvE3tcXJwqKioUHh4um7/cYOjnampqFB8fr4qKCkVERPg6HMCj+Pvd+izL0jfffKO4uDivXSM0NFRlZWWqr3e/+2VZVrN801K1/n1n7t/SOc63f0vjZ+PXiT0oKEhdu3b1dRhGioiI4B8+BCz+frcub1Xq3xcaGqrQ0FCvX+f7LrnkErVp06ZZdV5dXd2sKm/SpUuXFvdv27atOnXq5NR1WTwHAIAXBAcHKzk5WZs3b3YY37x5s6666qoWj0lNTW22/6ZNm5SSkuLU/LpEYgcAwGuysrL03HPPafny5SotLdXdd9+t8vJyZWRkSJKys7M1efJk+/4ZGRk6ePCgsrKyVFpaquXLl2vZsmWaOXOm09f061Y8Wl9ISIjmzp173jklwB/x9xueNnHiRH355Zd66KGHVFlZqX79+mnDhg3q3r27JKmystLhnvaEhARt2LBBd999t5555hnFxcVp0aJFmjBhgtPXtFmtcY8BAABoFbTiAQAIICR2AAACCIkdAIAAQmIHACCAkNjhNFdfPQj4i23btmns2LGKi4uTzWbT2rVrfR0ScMFI7HCKq68eBPxJbW2tBgwYoKefftrXoQBu43Y3OOXKK6/U4MGDHV41mJSUpPHjxysnJ8eHkQGeZbPZtGbNGo0fP97XoQAXhIod59X06sEzXyV4rlcPAgB8g8SO87qQVw8CAHyDxA6nufrqQQBA6yOx47wu5NWDAADfILHjvC7k1YMAAN/g7W5wSlZWliZNmqSUlBSlpqZq6dKlDq8eBPzZsWPH9PHHH9s/l5WVac+ePYqKilK3bt18GBngOm53g9MWL16sP/7xj/ZXDz7xxBMaPny4r8MC3FZUVKSRI0c2G58yZYoKCgpaPyDADSR2AAACCHPsAAAEEBI7AAABhMQOAEAAIbEDABBASOwAAAQQEjsAAAGExA4AQAAhsQNumjdvngYOHGj/fNNNN/nkXd4HDhyQzWbTnj17zrpPjx49lJub6/Q5CwoK1LFjR7djs9lsWrt2rdvnAXB+JHYEpJtuukk2m002m03t2rVTz549NXPmTNXW1nr92k8++aTTTytzJhkDgCt4VjwC1s9+9jOtWLFCJ0+e1Pbt2zVt2jTV1tYqLy+v2b4nT55Uu3btPHLdyMhIj5wHAC4EFTsCVkhIiLp06aL4+HjdcMMNuvHGG+3t4Kb2+fLly9WzZ0+FhITIsiwdPXpUt912m6KjoxUREaFrr71W7777rsN5H3vsMcXExCg8PFxTp07ViRMnHL4/sxV/6tQpzZ8/X5deeqlCQkLUrVs3PfLII5KkhIQESdKgQYNks9k0YsQI+3ErVqxQUlKSQkND1bt3by1evNjhOm+//bYGDRqk0NBQpaSkqKSkxOU/o4ULF+qKK67QRRddpPj4eE2fPl3Hjh1rtt/atWt1+eWXKzQ0VNddd50qKiocvl+/fr2Sk5MVGhqqnj176sEHH1RDQ4PL8QBwH4kdxggLC9PJkyftnz/++GO9/PLLKiwstLfCf/7zn6uqqkobNmzQrl27NHjwYI0aNUpfffWVJOnll1/W3Llz9cgjj6i4uFixsbHNEu6ZsrOzNX/+fN1///3au3evVq1aZX+P/dtvvy1J+sc//qHKykq98sorkqRnn31Wc+bM0SOPPKLS0lI9+uijuv/++7Vy5UpJUm1trX7xi18oMTFRu3bt0rx58zRz5kyX/0yCgoK0aNEiffDBB1q5cqX+9a9/adasWQ77HD9+XI888ohWrlypN998UzU1Nbr++uvt3//973/X//zP/ygzM1N79+7VkiVLVFBQYP/hBUArs4AANGXKFGvcuHH2z//+97+tTp06Wb/5zW8sy7KsuXPnWu3atbOqq6vt+/zzn/+0IiIirBMnTjicq1evXtaSJUssy7Ks1NRUKyMjw+H7K6+80howYECL166pqbFCQkKsZ599tsU4y8rKLElWSUmJw3h8fLy1atUqh7GHH37YSk1NtSzLspYsWWJFRUVZtbW19u/z8vJaPNf3de/e3XriiSfO+v3LL79sderUyf55xYoVliRr586d9rHS0lJLkvXvf//bsizLGjZsmPXoo486nOeFF16wYmNj7Z8lWWvWrDnrdQF4DnPsCFh/+9vf1KFDBzU0NOjkyZMaN26cnnrqKfv33bt3V+fOne2fd+3apWPHjqlTp04O5/n222/1ySefSJJKS0ubvYM+NTVVW7ZsaTGG0tJS1dXVadSoUU7HffjwYVVUVGjq1Km69dZb7eMNDQ32+fvS0lINGDBA7du3d4jDVVu2bNGjjz6qvXv3qqamRg0NDTpx4oRqa2t10UUXSZLatm2rlJQU+zG9e/dWx44dVVpaqh//+MfatWuX3nnnHYcKvbGxUSdOnNDx48cdYgTgfSR2BKyRI0cqLy9P7dq1U1xcXLPFcU2Jq8mpU6cUGxuroqKiZue60Fu+wsLCXD7m1KlTkk6346+88kqH79q0aSNJsjzwtuWDBw9qzJgxysjI0MMPP6yoqCi98cYbmjp1qsOUhXT6drUzNY2dOnVKDz74oH75y1822yc0NNTtOAG4hsSOgHXRRRfp0ksvdXr/wYMHq6qqSm3btlWPHj1a3CcpKUk7d+7U5MmT7WM7d+486zkvu+wyhYWF6Z///KemTZvW7Pvg4GBJpyvcJjExMfrRj36kTz/9VDfeeGOL5+3Tp49eeOEFffvtt/YfHs4VR0uKi4vV0NCgP/3pTwoKOr3c5uWXX262X0NDg4qLi/XjH/9YkrR//359/fXX6t27t6TTf2779+936c8agPeQ2IHv/PSnP1VqaqrGjx+v+fPnKzExUZ9//rk2bNig8ePHKyUlRXfddZemTJmilJQUXX311XrppZf04YcfqmfPni2eMzQ0VPfdd59mzZql4OBg/eQnP9Hhw4f14YcfaurUqYqOjlZYWJg2btyorl27KjQ0VJGRkZo3b54yMzMVERGh9PR01dXVqbi4WP/973+VlZWlG264QXPmzNHUqVP1+9//XgcOHNCCBQtc+v326tVLDQ0NeuqppzR27Fi9+eabys/Pb7Zfu3btdOedd2rRokVq166d7rjjDg0dOtSe6B944AH94he/UHx8vH79618rKChI7733nt5//3394Q9/cP3/CABuYVU88B2bzaYNGzZo+PDhuuWWW3T55Zfr+uuv14EDB+yr2CdOnKgHHnhA9913n5KTk3Xw4EH99re/Ped577//ft1zzz164IEHlJSUpIkTJ6q6ulrS6fnrRYsWacmSJYqLi9O4ceMkSdOmTdNzzz2ngoICXXHFFbrmmmtUUFBgvz2uQ4cOWr9+vfbu3atBgwZpzpw5mj9/vku/34EDB2rhwoWaP3+++vXrp5deekk5OTnN9mvfvr3uu+8+3XDDDUpNTVVYWJj+/Oc/278fPXq0/va3v2nz5s0aMmSIhg4dqoULF6p79+4uxQPAM2yWJybrAADADwIVOwAAAYTEDgBAACGxAwAQQEjsAAAEEBI7AAABhMQOAEAAIbEDABBASOwAAAQQEjsAAAGExA4AQAAhsQMAEEBI7AAABJD/D2wd0rep5596AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 示例: 真实标签和预测标签\n",
    "y_true = [0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(2, 2))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2.1.4. <a id='toc15_2_1_4_'></a>[混淆矩阵的优点与局限性](#toc0_)\n",
    "优点：\n",
    "- 提供了分类结果的详细信息（TP、FP、FN、TN），帮助分析分类器的性能。\n",
    "- 可用于计算多种指标（如 Precision、Recall、F1-Score 等）。\n",
    "- 易于扩展到多分类任务。\n",
    "\n",
    "局限性：\n",
    "- 难以直接提供一个全局的性能评分（需要其他指标辅助）。\n",
    "- 当类别过多或类别不平衡时，可能难以直观理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.2. <a id='toc15_2_2_'></a>[准确率 (Accuracy)](#toc0_)\n",
    "准确率是最基本的评估指标，表示模型预测正确的样本占总样本的比例。\n",
    "\n",
    "$\\text{公式: Accuracy}=\\frac{\\text{正确预测样本数}}{\\text{总样本数}}$\n",
    "\n",
    "适用场景: 数据类别分布均衡时效果较好。\n",
    "\n",
    "缺点: 对类别不平衡的数据不敏感（如正例远多于负例时）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.3. <a id='toc15_2_3_'></a>[精确率 (Precision)](#toc0_)\n",
    "\n",
    "$\\text{公式: Precision}=\\frac{\\text{真正例 }(\\mathrm{TP})}{\\text{真正例 }(\\mathrm{TP})+\\text{假正例 }(\\mathrm{FP})}$\n",
    "\n",
    "适用场景: 关注减少误报的场景（如垃圾邮件检测）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.4. <a id='toc15_2_4_'></a>[召回率 (Recall)](#toc0_)\n",
    "\n",
    "$\\text{公式: Recall}=\\frac{\\text{真正例 }(\\mathrm{TP})}{\\text{真正例 }(\\mathrm{TP})+\\text{假负例 }(\\mathrm{FN})}$\n",
    "\n",
    "适用场景: 关注尽可能找到所有正例的场景（如疾病筛查）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.5. <a id='toc15_2_5_'></a>[F1-Score](#toc0_)\n",
    "\n",
    "$\\text{公式: }F1=2\\times\\frac{\\mathrm{Precision}\\times\\mathrm{Recall}}{\\mathrm{Precision}+\\mathrm{Recall}}$\n",
    "\n",
    "适用场景: 在精确率和召回率之间需要平衡时。\n",
    "\n",
    "特点: F1-Score 是精确率和召回率的调和平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.6. <a id='toc15_2_6_'></a>[ROC 曲线和 AUC (Area Under Curve)](#toc0_)\n",
    "ROC 曲线: 通过绘制不同阈值下的 假正例率 (FPR) 和 真正例率 (TPR) 来评估分类器性能。\n",
    "\n",
    "AUC: 曲线下面积，表示模型区分正负例的能力。\n",
    "\n",
    "适用场景: 用于评估二分类模型，尤其是在类别分布不平衡的情况下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.7. <a id='toc15_2_7_'></a>[多分类问题指标](#toc0_)\n",
    "Top-k Accuracy: 预测中真实标签出现在模型输出概率前 k 个类别中。\n",
    "\n",
    "分类报告: 包含每个类别的 Precision、Recall 和 F1-Score，适用于多分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3. <a id='toc15_3_'></a>[回归问题的评估指标](#toc0_)\n",
    "回归问题的目标是预测连续数值。评估指标衡量预测值与真实值之间的偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.1. <a id='toc15_3_1_'></a>[平均绝对误差 (MAE)](#toc0_)\n",
    "公式: MAE = $\\frac{1}{N}\\sum_{i=1}^{N}|y_i-\\hat{y}_i|$\n",
    "\n",
    "特点: 衡量预测值与真实值之间的平均绝对差异。\n",
    "\n",
    "优点: 对异常值较为鲁棒。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.2. <a id='toc15_3_2_'></a>[均方误差 (MSE)](#toc0_)\n",
    "公式: MSE = $\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2$\n",
    "\n",
    "特点: 衡量预测值与真实值之间的平均平方差异。\n",
    "\n",
    "优点: 对异常值较为敏感。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.3. <a id='toc15_3_3_'></a>[均方根误差 (RMSE)](#toc0_)\n",
    "公式: RMSE = $\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}$\n",
    "\n",
    "特点: 是 MSE 的平方根，用于衡量预测值与真实值之间的平均误差。\n",
    "\n",
    "优点: 对异常值较为敏感。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.4. <a id='toc15_3_4_'></a>[R² (决定系数)](#toc0_)\n",
    "\n",
    "$\\text{公式: }R^2=1-\\frac{\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{N}(y_i-\\bar{y})^2}$\n",
    "\n",
    "特点: 衡量模型解释数据变异的能力。\n",
    "\n",
    "优点: 易于理解和解释。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. <a id='toc16_'></a>[Benchmark](#toc0_)\n",
    "Benchmark 是评估算法、模型、系统或硬件性能的关键步骤，通常用于比较不同方法的效率、准确性或资源消耗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1. <a id='toc16_1_'></a>[确定 Benchmark 目标](#toc0_)\n",
    "明确要评估的内容和目标。可能的目标包括：\n",
    "  - 模型性能：例如准确率、召回率、F1-score 等。\n",
    "  - 运行效率：如训练时间、推理时间、内存占用。\n",
    "  - 可扩展性：系统对大规模数据或复杂任务的适应性。\n",
    "  - 能耗：如 GPU 或 CPU 使用率。\n",
    "\n",
    "Benchmark 注意事项\n",
    "  - 公平性: 确保所有方法在相同条件下运行，消除外部干扰因素。\n",
    "  - 可靠性: 结果需稳定，避免因随机性导致的显著偏差。\n",
    "  - 多样性: 在不同数据集、不同任务上进行 Benchmark，提高结果的通用性。\n",
    "  - 资源消耗: 注意方法的时间和硬件需求，避免忽略效率问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2. <a id='toc16_2_'></a>[Benchmark模板](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking ModelA...\n",
      "Benchmarking ModelB...\n",
      "\n",
      "Benchmark Results:\n",
      "{'Model': 'ModelA', 'Train Time (s)': 0.17575478553771973, 'Inference Time (s)': 0.01085042953491211, 'Current Memory (MB)': 0.00067, 'Peak Memory (MB)': 0.042011, 'Accuracy': 0.218}\n",
      "{'Model': 'ModelB', 'Train Time (s)': 0.20154285430908203, 'Inference Time (s)': 0.012841463088989258, 'Current Memory (MB)': 0.00067, 'Peak Memory (MB)': 0.042011, 'Accuracy': 0.479}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 数据集定义\n",
    "# -----------------------------\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, input_dim=100, num_classes=10):\n",
    "        self.num_samples = num_samples\n",
    "        self.data = torch.randn(num_samples, input_dim)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 模型定义\n",
    "# -----------------------------\n",
    "class ModelA(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ModelA, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ModelB(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ModelB, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "# -----------------------------\n",
    "# 3. Benchmark 工具函数\n",
    "# -----------------------------\n",
    "def benchmark_training(model, dataloader, criterion, optimizer, device, num_epochs=5):\n",
    "    \"\"\" 测量模型训练时间 \"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return total_time\n",
    "\n",
    "\n",
    "def benchmark_inference(model, dataloader, device):\n",
    "    \"\"\" 测量模型推理时间 \"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "    total_time = time.time() - start_time\n",
    "    return total_time\n",
    "\n",
    "\n",
    "def measure_memory(model, dataloader, device):\n",
    "    \"\"\" 测量模型内存占用 \"\"\"\n",
    "    tracemalloc.start()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "            break  # 只测量一次前向传播\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    return current / 10**6, peak / 10**6  # 转换为 MB\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device):\n",
    "    \"\"\" 测量模型准确性 \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Benchmark 主流程\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    INPUT_DIM = 100\n",
    "    NUM_CLASSES = 10\n",
    "    NUM_SAMPLES = 1000\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 5\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 数据集和加载器\n",
    "    dataset = DummyDataset(num_samples=NUM_SAMPLES, input_dim=INPUT_DIM, num_classes=NUM_CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # 模型列表\n",
    "    models = {\n",
    "        \"ModelA\": ModelA(INPUT_DIM, NUM_CLASSES),\n",
    "        \"ModelB\": ModelB(INPUT_DIM, NUM_CLASSES)\n",
    "    }\n",
    "\n",
    "    # 损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Benchmark 结果\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Benchmarking {model_name}...\")\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        # 优化器\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        # 测试训练时间\n",
    "        train_time = benchmark_training(model, dataloader, criterion, optimizer, DEVICE, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "        # 测试推理时间\n",
    "        inference_time = benchmark_inference(model, dataloader, DEVICE)\n",
    "\n",
    "        # 测试内存占用\n",
    "        current_memory, peak_memory = measure_memory(model, dataloader, DEVICE)\n",
    "\n",
    "        # 测试准确性\n",
    "        accuracy = evaluate_accuracy(model, dataloader, DEVICE)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Train Time (s)\": train_time,\n",
    "            \"Inference Time (s)\": inference_time,\n",
    "            \"Current Memory (MB)\": current_memory,\n",
    "            \"Peak Memory (MB)\": peak_memory,\n",
    "            \"Accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "    # 打印 Benchmark 结果\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    for result in results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. <a id='toc17_'></a>[PyTorch lightning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.1. <a id='toc17_1_'></a>[训练逻辑](#toc0_)\n",
    "- PyTorch lightning官方给的`PyTorch lightning`教程：[https://lightning.ai/docs/pytorch/stable/expertise_levels.html](https://lightning.ai/docs/pytorch/stable/expertise_levels.html)\n",
    "\n",
    "- PyTorch lightning给的`PyTorch`教程：[https://lightning.ai/docs/pytorch/stable/tutorials.html](https://lightning.ai/docs/pytorch/stable/tutorials.html)\n",
    "\n",
    "- PuTorch lightning给的`PyTorch code to PyTorchLightning`：[https://lightning.ai/docs/pytorch/stable/starter/converting.html](https://lightning.ai/docs/pytorch/stable/starter/converting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Frame](./Pytorch_Pictures/PyTorch_lightning/Frame1.jpg)\n",
    "![Frame2](./Pytorch_Pictures/PyTorch_lightning/Frame2.jpg)\n",
    "![Frame3](./Pytorch_Pictures/PyTorch_lightning/Frame3.jpg)\n",
    "![Frame4](./Pytorch_Pictures/PyTorch_lightning/Frame4.jpg)\n",
    "![Frame5](./Pytorch_Pictures/PyTorch_lightning/Frame5.jpg)\n",
    "<!-- <img src=\"./Pytorch_Pictures/PyTorch_lightning/Frame1.jpg\" width = 600 height = 600 /> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch lightning version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "print(f'Pytorch lightning version: {L.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.2. <a id='toc17_2_'></a>[Data.py](#toc0_)\n",
    "数据部分应该单独处理，便于以后管理和维护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 2]),\n",
       " torch.Size([10000, 1]),\n",
       " tensor([ 0.1441, -0.0539]),\n",
       " tensor([3.7984]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 虚拟出一些数据\n",
    "# 仅供参考\n",
    "\n",
    "import torch \n",
    "\n",
    "def syn_datas(\n",
    "    w = torch.tensor([2.0, -3.0]),\n",
    "    b = torch.tensor([3.4]), \n",
    "    nums = 10000):\n",
    "    X = torch.normal(mean=0, std=0.1, size=(nums, w.shape[0]), dtype=torch.float32)\n",
    "    y = X @ w + b\n",
    "    y += torch.normal(mean=0, std=0.1, size=y.shape, dtype=torch.float)\n",
    "    return X, y \n",
    "\n",
    "# 预设参数，注意 形状/维度\n",
    "preset_weight = torch.tensor([2.0, -3.0], dtype=torch.float32).reshape(2, 1)\n",
    "preset_bias = torch.tensor([3.4], dtype=torch.float32)\n",
    "\n",
    "# 虚拟数据\n",
    "features, labels = syn_datas(w=preset_weight, b=preset_bias, nums=10000)\n",
    "\n",
    "# 初步查看\n",
    "features.shape, labels.shape, features[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data \n",
    "\n",
    "datasets = data.TensorDataset(features, labels)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=datasets, shuffle=True, batch_size=128, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.3. <a id='toc17_3_'></a>[Model.py](#toc0_)\n",
    "模型部分应该用纯PyTorch编写以便于理解和后续的维护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用纯PyTorch构建模型的网络结构\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AlphaFold2(nn.Module):\n",
    "    def __init__(self, in_features=2, out_features=1):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Sequential(nn.Linear(in_features, out_features))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.hidden(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.4. <a id='toc17_4_'></a>[ModelWrapper.py](#toc0_)\n",
    "利用PyTorch lightning训练框架进行训练只是方便调用，最后进行模型`魔改`后最好还是用纯PyTorch进行学习。  \n",
    "都是固定框架（API信息如下）\n",
    "- [L.LightningModule API](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#backward)\n",
    "- [Trainer API](https://lightning.ai/docs/pytorch/stable/common/trainer.html)\n",
    "    - Automatically enabling/disabling grads\n",
    "    - Running the training, validation and test dataloaders\n",
    "    - Calling the Callbacks at the appropriate times\n",
    "    - Putting batches and computations on the correct devices\n",
    "    \n",
    "![Trainer_API](./Pytorch_Pictures/PyTorch_lightning/Trainer_API.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# 用PyTorch lightning构建训练步骤\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class AlphaFold2Wrapper(L.LightningModule):\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        ## save hyperparameters\n",
    "        self.save_hyperparameters()                                 # 超参数保存\n",
    "        self.learning_rate = learning_rate                          # 超参数\n",
    "        ## model initiate from model constructed by pure PyTorch\n",
    "        self.demo_model = AlphaFold2(in_features=2, out_features=1)      ## 模型\n",
    "        ## loss_fn\n",
    "        self.loss_fn = torch.nn.MSELoss()                           ## 损失函数\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.demo_model(X)\n",
    "\n",
    "    def configure_optimizers(self):                                 ## 优化函数\n",
    "        opt = optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        return opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''训练步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)                 # 在进度条上显示出来\n",
    "\n",
    "        self.training_step_outputs.append(loss)                     # 保存结果，以备后续使用 (on_train_epoch_end(self))\n",
    "        return loss\n",
    "    \n",
    "    # def on_train_batch_start(self, batch, batch_idx):\n",
    "    #     '''\n",
    "    #     Called in the training loop before anything happens for that batch.\n",
    "    #     If you return -1 here, you will skip training for the rest of the current epoch.\n",
    "    #     '''\n",
    "    #     pass\n",
    "\n",
    "    # def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "    #     '''\n",
    "    #     Called in the training loop after the batch.\n",
    "    #     Parameters:\n",
    "    #             outputs (Union[Tensor, Mapping[str, Any], None]) – The outputs of training_step(x)\n",
    "    #             batch (Any) – The batched data as it is returned by the training DataLoader.\n",
    "    #             batch_idx (int) – the index of the batch\n",
    "    #     '''\n",
    "    #     pass\n",
    "\n",
    "    # def on_train_epoch_start(self):\n",
    "    #     '''Called in the training loop at the very beginning of the epoch.'''\n",
    "    #     pass\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Called in the training loop at the very end of the epoch.\n",
    "        To access all batch outputs at the end of the epoch, \n",
    "        you can cache step outputs as an attribute of the LightningModule and access them in this hook:\n",
    "        '''\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_outputs).mean() # 来自于training_step()计算结果\n",
    "        self.log(\"training_epoch_mean\", epoch_mean)\n",
    "        # free up the memory\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''验证步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X) \n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    # def on_validation_batch_start(self, batch, batch_idx, dataloader_idx=0):\n",
    "    #     '''Called in the validation loop before anything happens for that batch.'''\n",
    "    #     pass \n",
    "\n",
    "    # def on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "    #     '''Called in the validation loop after the batch.\n",
    "    #     Parameters:\n",
    "    #             outputs (Union[Tensor, Mapping[str, Any], None]) – The outputs of validation_step(x)\n",
    "    #             batch (Any) – The batched data as it is returned by the validation DataLoader.\n",
    "    #             batch_idx (int) – the index of the batch\n",
    "    #             dataloader_idx (int) – the index of the dataloader\n",
    "    #     '''\n",
    "    #     pass\n",
    "\n",
    "    # def on_validation_epoch_start(self):\n",
    "    #     '''Called in the validation loop at the very beginning of the epoch.'''\n",
    "    #     pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        '''Called in the validation loop at the very end of the epoch.'''\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''测试步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X) \n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(self, batch, batch_idx):\n",
    "        '''预测步骤'''\n",
    "        X, y = batch \n",
    "        y_hat = self.forward(X) \n",
    "        self.log('y_hat', y_hat)\n",
    "        return y_hat\n",
    "\n",
    "## 实例化一个对象\n",
    "alphafold2 = AlphaFold2Wrapper(learning_rate=0.01)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",              # cpu, gpu, tpu, auto\n",
    "    devices=1, \n",
    "    # strategy=\"ddp\",                 # ddp, ddp_spawn, ddp_notebook\n",
    "    # num_nodes=1,                    # Number of GPU nodes for distributed training.\n",
    "\n",
    "    # precision=\"32-true\",            # There are two different techniques to set the mixed precision. “True” precision and “Mixed” precision.\n",
    "\n",
    "    # callbacks = ,\n",
    "    \n",
    "    # min_epochs=1,\n",
    "    max_epochs=10, \n",
    "    # min_steps=None,                 # Force training for at least this number of global steps. Trainer will train model for at least min_steps or min_epochs (latest).\n",
    "    max_steps=-1,                   # Stop training after this number of global steps. Training will stop if max_steps or max_epochs have reached (earliest).\n",
    "    log_every_n_steps=1,           ## How often to add logging rows (does not write to disk)\n",
    "    check_val_every_n_epoch=1,      # default used by the Trainer\n",
    "\n",
    "    # default_root_dir=os.getcwd(),   # os.getcwd()\n",
    "    # enable_progress_bar=True,       # Whether to enable or disable the progress bar. Defaults to True.\n",
    "    # enable_model_summary=True,      # Whether to enable or disable the model summarization. Defaults to True.\n",
    "\n",
    "    profiler=None,                  # simple, advanced, None: To profile individual steps during training and assist in identifying bottlenecks.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.1. <a id='toc17_4_1_'></a>[Training and vlidation](#toc0_)\n",
    "Training的同时进行Validation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type       | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | demo_model | AlphaFold2 | 3      | train\n",
      "1 | loss_fn    | MSELoss    | 0      | train\n",
      "--------------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 79/79 [00:01<00:00, 75.30it/s, v_num=0, train_loss=0.217, val_loss=0.133] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 79/79 [00:01<00:00, 74.95it/s, v_num=0, train_loss=0.217, val_loss=0.133]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=alphafold2, \n",
    "    train_dataloaders=train_iter, \n",
    "    val_dataloaders=train_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.2. <a id='toc17_4_2_'></a>[Validation](#toc0_)\n",
    "只进行validation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 79/79 [00:00<00:00, 359.93it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            0.13269878923892975\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.13269878923892975}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model=alphafold2, dataloaders=train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.3. <a id='toc17_4_3_'></a>[Test](#toc0_)\n",
    "只进行test。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 79/79 [00:00<00:00, 404.56it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.13269877433776855\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.13269877433776855}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=alphafold2, dataloaders=train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4.4. <a id='toc17_4_4_'></a>[Prediction](#toc0_)\n",
    "\n",
    "进行预测。\n",
    "\n",
    "![Prediction summary](./Pytorch_Pictures/PyTorch_lightning/Frame4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.4.4.1. <a id='toc17_4_4_1_'></a>[PyTorch lightning自身Trainer直接predict](#toc0_)\n",
    "调用PyTorch lightning自身Trainer的predict，程序会自动使用：  \n",
    "- model.eval()\n",
    "- with torch.no_grad():\n",
    "- 或 torch.set_grad_enable(True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 10000/10000 [00:08<00:00, 1224.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([3.4124]),\n",
       " tensor([3.3868]),\n",
       " tensor([3.3909]),\n",
       " tensor([3.3817]),\n",
       " tensor([3.4208]),\n",
       " tensor([3.3964]),\n",
       " tensor([3.3973]),\n",
       " tensor([3.3862]),\n",
       " tensor([3.3966]),\n",
       " tensor([3.4032]),\n",
       " tensor([3.4066]),\n",
       " tensor([3.3905]),\n",
       " tensor([3.3932]),\n",
       " tensor([3.3774]),\n",
       " tensor([3.3949]),\n",
       " tensor([3.3956]),\n",
       " tensor([3.3845]),\n",
       " tensor([3.3931]),\n",
       " tensor([3.3935]),\n",
       " tensor([3.3898]),\n",
       " tensor([3.4090]),\n",
       " tensor([3.4059]),\n",
       " tensor([3.3987]),\n",
       " tensor([3.3881]),\n",
       " tensor([3.3913]),\n",
       " tensor([3.4166]),\n",
       " tensor([3.3755]),\n",
       " tensor([3.4028]),\n",
       " tensor([3.4021]),\n",
       " tensor([3.4224]),\n",
       " tensor([3.3956]),\n",
       " tensor([3.3914]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.3889]),\n",
       " tensor([3.3950]),\n",
       " tensor([3.3908]),\n",
       " tensor([3.4049]),\n",
       " tensor([3.4014]),\n",
       " tensor([3.3925]),\n",
       " tensor([3.3970]),\n",
       " tensor([3.4043]),\n",
       " tensor([3.4050]),\n",
       " tensor([3.4055]),\n",
       " tensor([3.4033]),\n",
       " tensor([3.3873]),\n",
       " tensor([3.3934]),\n",
       " tensor([3.4031]),\n",
       " tensor([3.4101]),\n",
       " tensor([3.3942]),\n",
       " tensor([3.3713]),\n",
       " tensor([3.3908]),\n",
       " tensor([3.3955]),\n",
       " tensor([3.3886]),\n",
       " tensor([3.4044]),\n",
       " tensor([3.3781]),\n",
       " tensor([3.4005]),\n",
       " tensor([3.4258]),\n",
       " tensor([3.3873]),\n",
       " tensor([3.3782]),\n",
       " tensor([3.3928]),\n",
       " tensor([3.4193]),\n",
       " tensor([3.3874]),\n",
       " tensor([3.3984]),\n",
       " tensor([3.3887]),\n",
       " tensor([3.4131]),\n",
       " tensor([3.4129]),\n",
       " tensor([3.4034]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.3874]),\n",
       " tensor([3.3870]),\n",
       " tensor([3.4152]),\n",
       " tensor([3.3893]),\n",
       " tensor([3.3938]),\n",
       " tensor([3.3955]),\n",
       " tensor([3.3921]),\n",
       " tensor([3.3895]),\n",
       " tensor([3.3981]),\n",
       " tensor([3.3871]),\n",
       " tensor([3.4101]),\n",
       " tensor([3.4051]),\n",
       " tensor([3.3864]),\n",
       " tensor([3.4033]),\n",
       " tensor([3.3987]),\n",
       " tensor([3.3935]),\n",
       " tensor([3.4010]),\n",
       " tensor([3.3961]),\n",
       " tensor([3.3918]),\n",
       " tensor([3.3886]),\n",
       " tensor([3.4129]),\n",
       " tensor([3.3801]),\n",
       " tensor([3.3956]),\n",
       " tensor([3.3981]),\n",
       " tensor([3.3972]),\n",
       " tensor([3.3699]),\n",
       " tensor([3.3770]),\n",
       " tensor([3.4040]),\n",
       " tensor([3.3986]),\n",
       " tensor([3.3855]),\n",
       " tensor([3.3998]),\n",
       " tensor([3.4040]),\n",
       " tensor([3.3861]),\n",
       " tensor([3.3966]),\n",
       " tensor([3.4106]),\n",
       " tensor([3.4038]),\n",
       " tensor([3.4075]),\n",
       " tensor([3.3972]),\n",
       " tensor([3.3843]),\n",
       " tensor([3.3849]),\n",
       " tensor([3.3885]),\n",
       " tensor([3.3999]),\n",
       " tensor([3.3865]),\n",
       " tensor([3.3973]),\n",
       " tensor([3.3804]),\n",
       " tensor([3.3916]),\n",
       " tensor([3.3607]),\n",
       " tensor([3.3936]),\n",
       " tensor([3.3948]),\n",
       " tensor([3.3984]),\n",
       " tensor([3.3910]),\n",
       " tensor([3.4078]),\n",
       " tensor([3.4075]),\n",
       " tensor([3.3888]),\n",
       " tensor([3.3929]),\n",
       " tensor([3.3968]),\n",
       " tensor([3.3875]),\n",
       " tensor([3.3859]),\n",
       " tensor([3.3864]),\n",
       " tensor([3.4090]),\n",
       " tensor([3.3977]),\n",
       " tensor([3.4027]),\n",
       " tensor([3.4120]),\n",
       " tensor([3.4087]),\n",
       " tensor([3.3966]),\n",
       " tensor([3.3961]),\n",
       " tensor([3.3787]),\n",
       " tensor([3.3978]),\n",
       " tensor([3.3952]),\n",
       " tensor([3.3897]),\n",
       " tensor([3.4006]),\n",
       " tensor([3.4003]),\n",
       " tensor([3.4146]),\n",
       " tensor([3.4132]),\n",
       " tensor([3.3967]),\n",
       " tensor([3.3859]),\n",
       " tensor([3.3974]),\n",
       " tensor([3.3895]),\n",
       " tensor([3.4091]),\n",
       " tensor([3.3953]),\n",
       " tensor([3.3847]),\n",
       " tensor([3.3809]),\n",
       " tensor([3.4056]),\n",
       " tensor([3.3944]),\n",
       " tensor([3.4027]),\n",
       " tensor([3.3968]),\n",
       " tensor([3.3890]),\n",
       " tensor([3.4088]),\n",
       " tensor([3.4081]),\n",
       " tensor([3.4046]),\n",
       " tensor([3.4032]),\n",
       " tensor([3.3874]),\n",
       " tensor([3.3803]),\n",
       " tensor([3.3998]),\n",
       " tensor([3.4051]),\n",
       " tensor([3.4086]),\n",
       " tensor([3.3976]),\n",
       " tensor([3.3843]),\n",
       " tensor([3.4084]),\n",
       " tensor([3.3904]),\n",
       " tensor([3.3955]),\n",
       " tensor([3.3869]),\n",
       " tensor([3.3834]),\n",
       " tensor([3.4043]),\n",
       " tensor([3.4136]),\n",
       " tensor([3.4164]),\n",
       " tensor([3.3850]),\n",
       " tensor([3.3986]),\n",
       " tensor([3.3877]),\n",
       " tensor([3.3948]),\n",
       " tensor([3.4094]),\n",
       " tensor([3.3838]),\n",
       " tensor([3.3951]),\n",
       " tensor([3.3938]),\n",
       " tensor([3.4015]),\n",
       " tensor([3.3974]),\n",
       " tensor([3.3974]),\n",
       " tensor([3.3943]),\n",
       " tensor([3.4021]),\n",
       " tensor([3.3885]),\n",
       " tensor([3.3933]),\n",
       " tensor([3.4221]),\n",
       " tensor([3.3944]),\n",
       " tensor([3.3702]),\n",
       " tensor([3.3849]),\n",
       " tensor([3.3721]),\n",
       " tensor([3.4130]),\n",
       " tensor([3.4109]),\n",
       " tensor([3.3931]),\n",
       " tensor([3.4006]),\n",
       " tensor([3.3931]),\n",
       " tensor([3.3830]),\n",
       " tensor([3.4076]),\n",
       " tensor([3.4150]),\n",
       " tensor([3.3928]),\n",
       " tensor([3.3854]),\n",
       " tensor([3.3955]),\n",
       " tensor([3.3874]),\n",
       " tensor([3.3804]),\n",
       " tensor([3.3996]),\n",
       " tensor([3.3997]),\n",
       " tensor([3.3945]),\n",
       " tensor([3.3966]),\n",
       " tensor([3.3844]),\n",
       " tensor([3.4151]),\n",
       " tensor([3.3878]),\n",
       " tensor([3.4081]),\n",
       " tensor([3.3861]),\n",
       " tensor([3.3918]),\n",
       " tensor([3.3909]),\n",
       " tensor([3.3790]),\n",
       " tensor([3.3976]),\n",
       " tensor([3.3993]),\n",
       " tensor([3.3754]),\n",
       " tensor([3.3959]),\n",
       " tensor([3.4031]),\n",
       " tensor([3.3949]),\n",
       " tensor([3.3995]),\n",
       " tensor([3.4028]),\n",
       " tensor([3.3978]),\n",
       " tensor([3.3845]),\n",
       " tensor([3.3905]),\n",
       " tensor([3.4012]),\n",
       " tensor([3.4248]),\n",
       " tensor([3.4041]),\n",
       " tensor([3.3908]),\n",
       " tensor([3.3819]),\n",
       " tensor([3.3824]),\n",
       " tensor([3.4074]),\n",
       " tensor([3.4040]),\n",
       " tensor([3.3797]),\n",
       " tensor([3.3876]),\n",
       " tensor([3.3864]),\n",
       " tensor([3.3888]),\n",
       " tensor([3.3748]),\n",
       " tensor([3.3762]),\n",
       " tensor([3.3839]),\n",
       " tensor([3.4036]),\n",
       " tensor([3.4012]),\n",
       " tensor([3.4112]),\n",
       " tensor([3.3962]),\n",
       " tensor([3.3846]),\n",
       " tensor([3.3814]),\n",
       " tensor([3.4060]),\n",
       " tensor([3.3960]),\n",
       " tensor([3.3785]),\n",
       " tensor([3.4101]),\n",
       " tensor([3.3812]),\n",
       " tensor([3.4179]),\n",
       " tensor([3.3958]),\n",
       " tensor([3.4003]),\n",
       " tensor([3.3811]),\n",
       " tensor([3.3881]),\n",
       " tensor([3.3697]),\n",
       " tensor([3.3896]),\n",
       " tensor([3.4057]),\n",
       " tensor([3.4168]),\n",
       " tensor([3.4068]),\n",
       " tensor([3.3944]),\n",
       " tensor([3.3939]),\n",
       " tensor([3.3898]),\n",
       " tensor([3.4055]),\n",
       " tensor([3.4003]),\n",
       " tensor([3.3825]),\n",
       " tensor([3.3823]),\n",
       " tensor([3.3900]),\n",
       " tensor([3.4036]),\n",
       " tensor([3.3910]),\n",
       " tensor([3.3669]),\n",
       " tensor([3.4105]),\n",
       " tensor([3.3908]),\n",
       " tensor([3.3930]),\n",
       " tensor([3.3747]),\n",
       " tensor([3.4005]),\n",
       " tensor([3.4129]),\n",
       " tensor([3.3842]),\n",
       " tensor([3.4011]),\n",
       " tensor([3.3984]),\n",
       " tensor([3.3772]),\n",
       " tensor([3.4038]),\n",
       " tensor([3.3999]),\n",
       " tensor([3.3987]),\n",
       " tensor([3.3911]),\n",
       " tensor([3.4013]),\n",
       " tensor([3.4021]),\n",
       " tensor([3.3991]),\n",
       " tensor([3.3840]),\n",
       " tensor([3.4171]),\n",
       " tensor([3.4068]),\n",
       " tensor([3.4061]),\n",
       " tensor([3.3943]),\n",
       " tensor([3.4080]),\n",
       " tensor([3.3757]),\n",
       " tensor([3.3950]),\n",
       " tensor([3.3848]),\n",
       " tensor([3.3827]),\n",
       " tensor([3.4070]),\n",
       " tensor([3.4035]),\n",
       " tensor([3.3793]),\n",
       " tensor([3.3813]),\n",
       " tensor([3.4006]),\n",
       " tensor([3.3937]),\n",
       " tensor([3.3855]),\n",
       " tensor([3.3849]),\n",
       " tensor([3.3879]),\n",
       " tensor([3.4032]),\n",
       " tensor([3.3842]),\n",
       " tensor([3.4141]),\n",
       " tensor([3.3924]),\n",
       " tensor([3.3830]),\n",
       " tensor([3.4130]),\n",
       " tensor([3.3903]),\n",
       " tensor([3.3871]),\n",
       " tensor([3.4162]),\n",
       " tensor([3.4080]),\n",
       " tensor([3.3905]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.3950]),\n",
       " tensor([3.3955]),\n",
       " tensor([3.4070]),\n",
       " tensor([3.4014]),\n",
       " tensor([3.3858]),\n",
       " tensor([3.3889]),\n",
       " tensor([3.3913]),\n",
       " tensor([3.4134]),\n",
       " tensor([3.3972]),\n",
       " tensor([3.4066]),\n",
       " tensor([3.4012]),\n",
       " tensor([3.3954]),\n",
       " tensor([3.3905]),\n",
       " tensor([3.3966]),\n",
       " tensor([3.3846]),\n",
       " tensor([3.4101]),\n",
       " tensor([3.4278]),\n",
       " tensor([3.4167]),\n",
       " tensor([3.3849]),\n",
       " tensor([3.4030]),\n",
       " tensor([3.3895]),\n",
       " tensor([3.3883]),\n",
       " tensor([3.3797]),\n",
       " tensor([3.3906]),\n",
       " tensor([3.3962]),\n",
       " tensor([3.3998]),\n",
       " tensor([3.3837]),\n",
       " tensor([3.3906]),\n",
       " tensor([3.3814]),\n",
       " tensor([3.3948]),\n",
       " tensor([3.4101]),\n",
       " tensor([3.3900]),\n",
       " tensor([3.3951]),\n",
       " tensor([3.3977]),\n",
       " tensor([3.3841]),\n",
       " tensor([3.4076]),\n",
       " tensor([3.4021]),\n",
       " tensor([3.3867]),\n",
       " tensor([3.3944]),\n",
       " tensor([3.3987]),\n",
       " tensor([3.3875]),\n",
       " tensor([3.3883]),\n",
       " tensor([3.3954]),\n",
       " tensor([3.4117]),\n",
       " tensor([3.3833]),\n",
       " tensor([3.4028]),\n",
       " tensor([3.3847]),\n",
       " tensor([3.3911]),\n",
       " tensor([3.4061]),\n",
       " tensor([3.3887]),\n",
       " tensor([3.4028]),\n",
       " tensor([3.3935]),\n",
       " tensor([3.4019]),\n",
       " tensor([3.3861]),\n",
       " tensor([3.3714]),\n",
       " tensor([3.3898]),\n",
       " tensor([3.4068]),\n",
       " tensor([3.3961]),\n",
       " tensor([3.4316]),\n",
       " tensor([3.3874]),\n",
       " tensor([3.3967]),\n",
       " tensor([3.4210]),\n",
       " tensor([3.4201]),\n",
       " tensor([3.4086]),\n",
       " tensor([3.3852]),\n",
       " tensor([3.3954]),\n",
       " tensor([3.4149]),\n",
       " tensor([3.4019]),\n",
       " tensor([3.4001]),\n",
       " tensor([3.3926]),\n",
       " tensor([3.4000]),\n",
       " tensor([3.4079]),\n",
       " tensor([3.3977]),\n",
       " tensor([3.3997]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.4179]),\n",
       " tensor([3.4061]),\n",
       " tensor([3.3923]),\n",
       " tensor([3.4027]),\n",
       " tensor([3.3974]),\n",
       " tensor([3.3904]),\n",
       " tensor([3.4022]),\n",
       " tensor([3.3988]),\n",
       " tensor([3.3825]),\n",
       " tensor([3.3840]),\n",
       " tensor([3.4073]),\n",
       " tensor([3.3974]),\n",
       " tensor([3.3830]),\n",
       " tensor([3.3974]),\n",
       " tensor([3.3870]),\n",
       " tensor([3.4090]),\n",
       " tensor([3.3719]),\n",
       " tensor([3.4009]),\n",
       " tensor([3.3729]),\n",
       " tensor([3.4007]),\n",
       " tensor([3.3830]),\n",
       " tensor([3.4080]),\n",
       " tensor([3.3862]),\n",
       " tensor([3.3794]),\n",
       " tensor([3.3971]),\n",
       " tensor([3.3994]),\n",
       " tensor([3.3896]),\n",
       " tensor([3.3780]),\n",
       " tensor([3.4054]),\n",
       " tensor([3.3893]),\n",
       " tensor([3.3879]),\n",
       " tensor([3.3838]),\n",
       " tensor([3.3832]),\n",
       " tensor([3.4018]),\n",
       " tensor([3.3888]),\n",
       " tensor([3.3988]),\n",
       " tensor([3.3934]),\n",
       " tensor([3.3899]),\n",
       " tensor([3.4056]),\n",
       " tensor([3.3880]),\n",
       " tensor([3.3884]),\n",
       " tensor([3.4088]),\n",
       " tensor([3.3989]),\n",
       " tensor([3.3885]),\n",
       " tensor([3.4058]),\n",
       " tensor([3.4244]),\n",
       " tensor([3.4051]),\n",
       " tensor([3.4101]),\n",
       " tensor([3.3924]),\n",
       " tensor([3.4058]),\n",
       " tensor([3.4032]),\n",
       " tensor([3.4103]),\n",
       " tensor([3.4106]),\n",
       " tensor([3.4228]),\n",
       " tensor([3.4213]),\n",
       " tensor([3.3915]),\n",
       " tensor([3.4078]),\n",
       " tensor([3.3992]),\n",
       " tensor([3.3945]),\n",
       " tensor([3.3877]),\n",
       " tensor([3.3812]),\n",
       " tensor([3.3730]),\n",
       " tensor([3.4031]),\n",
       " tensor([3.3698]),\n",
       " tensor([3.4008]),\n",
       " tensor([3.3902]),\n",
       " tensor([3.3899]),\n",
       " tensor([3.3998]),\n",
       " tensor([3.4109]),\n",
       " tensor([3.4035]),\n",
       " tensor([3.3786]),\n",
       " tensor([3.3959]),\n",
       " tensor([3.3848]),\n",
       " tensor([3.4000]),\n",
       " tensor([3.3748]),\n",
       " tensor([3.4036]),\n",
       " tensor([3.4022]),\n",
       " tensor([3.3908]),\n",
       " tensor([3.3926]),\n",
       " tensor([3.3972]),\n",
       " tensor([3.4044]),\n",
       " tensor([3.4077]),\n",
       " tensor([3.3990]),\n",
       " tensor([3.3943]),\n",
       " tensor([3.3992]),\n",
       " tensor([3.3849]),\n",
       " tensor([3.4102]),\n",
       " tensor([3.3814]),\n",
       " tensor([3.4102]),\n",
       " tensor([3.3866]),\n",
       " tensor([3.4003]),\n",
       " tensor([3.4128]),\n",
       " tensor([3.3874]),\n",
       " tensor([3.4031]),\n",
       " tensor([3.3985]),\n",
       " tensor([3.3962]),\n",
       " tensor([3.3875]),\n",
       " tensor([3.4017]),\n",
       " tensor([3.3998]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.3932]),\n",
       " tensor([3.3958]),\n",
       " tensor([3.3973]),\n",
       " tensor([3.3950]),\n",
       " tensor([3.3947]),\n",
       " tensor([3.3999]),\n",
       " tensor([3.4081]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.4067]),\n",
       " tensor([3.3806]),\n",
       " tensor([3.4036]),\n",
       " tensor([3.4196]),\n",
       " tensor([3.3900]),\n",
       " tensor([3.3872]),\n",
       " tensor([3.3964]),\n",
       " tensor([3.3935]),\n",
       " tensor([3.4003]),\n",
       " tensor([3.3981]),\n",
       " tensor([3.4003]),\n",
       " tensor([3.3864]),\n",
       " tensor([3.3797]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.4134]),\n",
       " tensor([3.4020]),\n",
       " tensor([3.3841]),\n",
       " tensor([3.3775]),\n",
       " tensor([3.3854]),\n",
       " tensor([3.3974]),\n",
       " tensor([3.3947]),\n",
       " tensor([3.4116]),\n",
       " tensor([3.4051]),\n",
       " tensor([3.3892]),\n",
       " tensor([3.4026]),\n",
       " tensor([3.3893]),\n",
       " tensor([3.4023]),\n",
       " tensor([3.3904]),\n",
       " tensor([3.3944]),\n",
       " tensor([3.4056]),\n",
       " tensor([3.4035]),\n",
       " tensor([3.3766]),\n",
       " tensor([3.3897]),\n",
       " tensor([3.3936]),\n",
       " tensor([3.4063]),\n",
       " tensor([3.3832]),\n",
       " tensor([3.4096]),\n",
       " tensor([3.3850]),\n",
       " tensor([3.4216]),\n",
       " tensor([3.4043]),\n",
       " tensor([3.3882]),\n",
       " tensor([3.3908]),\n",
       " tensor([3.3987]),\n",
       " tensor([3.3989]),\n",
       " tensor([3.4090]),\n",
       " tensor([3.3868]),\n",
       " tensor([3.3926]),\n",
       " tensor([3.3796]),\n",
       " tensor([3.3829]),\n",
       " tensor([3.4089]),\n",
       " tensor([3.3850]),\n",
       " tensor([3.3933]),\n",
       " tensor([3.3950]),\n",
       " tensor([3.3988]),\n",
       " tensor([3.4028]),\n",
       " tensor([3.3891]),\n",
       " tensor([3.4035]),\n",
       " tensor([3.3948]),\n",
       " tensor([3.3858]),\n",
       " tensor([3.3911]),\n",
       " tensor([3.3868]),\n",
       " tensor([3.3836]),\n",
       " tensor([3.3964]),\n",
       " tensor([3.4149]),\n",
       " tensor([3.3960]),\n",
       " tensor([3.3946]),\n",
       " tensor([3.4032]),\n",
       " tensor([3.3866]),\n",
       " tensor([3.4022]),\n",
       " tensor([3.3883]),\n",
       " tensor([3.3948]),\n",
       " tensor([3.3981]),\n",
       " tensor([3.4169]),\n",
       " tensor([3.3975]),\n",
       " tensor([3.4144]),\n",
       " tensor([3.3744]),\n",
       " tensor([3.4058]),\n",
       " tensor([3.3935]),\n",
       " tensor([3.3881]),\n",
       " tensor([3.3699]),\n",
       " tensor([3.4189]),\n",
       " tensor([3.3954]),\n",
       " tensor([3.3870]),\n",
       " tensor([3.3865]),\n",
       " tensor([3.3838]),\n",
       " tensor([3.3925]),\n",
       " tensor([3.3861]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.4019]),\n",
       " tensor([3.4072]),\n",
       " tensor([3.3948]),\n",
       " tensor([3.4002]),\n",
       " tensor([3.3793]),\n",
       " tensor([3.4097]),\n",
       " tensor([3.3896]),\n",
       " tensor([3.4005]),\n",
       " tensor([3.3942]),\n",
       " tensor([3.3988]),\n",
       " tensor([3.3954]),\n",
       " tensor([3.3904]),\n",
       " tensor([3.4143]),\n",
       " tensor([3.3803]),\n",
       " tensor([3.4204]),\n",
       " tensor([3.3789]),\n",
       " tensor([3.3836]),\n",
       " tensor([3.4065]),\n",
       " tensor([3.3721]),\n",
       " tensor([3.3925]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.4049]),\n",
       " tensor([3.3764]),\n",
       " tensor([3.3928]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.3951]),\n",
       " tensor([3.4004]),\n",
       " tensor([3.3832]),\n",
       " tensor([3.4129]),\n",
       " tensor([3.4088]),\n",
       " tensor([3.3887]),\n",
       " tensor([3.3962]),\n",
       " tensor([3.3855]),\n",
       " tensor([3.3939]),\n",
       " tensor([3.3819]),\n",
       " tensor([3.3905]),\n",
       " tensor([3.4249]),\n",
       " tensor([3.4143]),\n",
       " tensor([3.4135]),\n",
       " tensor([3.3960]),\n",
       " tensor([3.4036]),\n",
       " tensor([3.3716]),\n",
       " tensor([3.3956]),\n",
       " tensor([3.4005]),\n",
       " tensor([3.3983]),\n",
       " tensor([3.4125]),\n",
       " tensor([3.3915]),\n",
       " tensor([3.4220]),\n",
       " tensor([3.3756]),\n",
       " tensor([3.3922]),\n",
       " tensor([3.3857]),\n",
       " tensor([3.4030]),\n",
       " tensor([3.3887]),\n",
       " tensor([3.3770]),\n",
       " tensor([3.3992]),\n",
       " tensor([3.4071]),\n",
       " tensor([3.4107]),\n",
       " tensor([3.4151]),\n",
       " tensor([3.4113]),\n",
       " tensor([3.4054]),\n",
       " tensor([3.4038]),\n",
       " tensor([3.3863]),\n",
       " tensor([3.4015]),\n",
       " tensor([3.4036]),\n",
       " tensor([3.3857]),\n",
       " tensor([3.3983]),\n",
       " tensor([3.3962]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.3808]),\n",
       " tensor([3.4076]),\n",
       " tensor([3.4043]),\n",
       " tensor([3.3870]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.3880]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.3983]),\n",
       " tensor([3.3919]),\n",
       " tensor([3.4050]),\n",
       " tensor([3.3837]),\n",
       " tensor([3.4005]),\n",
       " tensor([3.3585]),\n",
       " tensor([3.4139]),\n",
       " tensor([3.3854]),\n",
       " tensor([3.4070]),\n",
       " tensor([3.3916]),\n",
       " tensor([3.3823]),\n",
       " tensor([3.3984]),\n",
       " tensor([3.4058]),\n",
       " tensor([3.3818]),\n",
       " tensor([3.4084]),\n",
       " tensor([3.3786]),\n",
       " tensor([3.3798]),\n",
       " tensor([3.3991]),\n",
       " tensor([3.3876]),\n",
       " tensor([3.3987]),\n",
       " tensor([3.4036]),\n",
       " tensor([3.4024]),\n",
       " tensor([3.3893]),\n",
       " tensor([3.3973]),\n",
       " tensor([3.3846]),\n",
       " tensor([3.4137]),\n",
       " tensor([3.3864]),\n",
       " tensor([3.3943]),\n",
       " tensor([3.3881]),\n",
       " tensor([3.3978]),\n",
       " tensor([3.4078]),\n",
       " tensor([3.3929]),\n",
       " tensor([3.3951]),\n",
       " tensor([3.3995]),\n",
       " tensor([3.4120]),\n",
       " tensor([3.4318]),\n",
       " tensor([3.3795]),\n",
       " tensor([3.3785]),\n",
       " tensor([3.3924]),\n",
       " tensor([3.3759]),\n",
       " tensor([3.3997]),\n",
       " tensor([3.3840]),\n",
       " tensor([3.4264]),\n",
       " tensor([3.4175]),\n",
       " tensor([3.3834]),\n",
       " tensor([3.3861]),\n",
       " tensor([3.3997]),\n",
       " tensor([3.4206]),\n",
       " tensor([3.3871]),\n",
       " tensor([3.3845]),\n",
       " tensor([3.4004]),\n",
       " tensor([3.3881]),\n",
       " tensor([3.3914]),\n",
       " tensor([3.4153]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.4035]),\n",
       " tensor([3.3940]),\n",
       " tensor([3.3738]),\n",
       " tensor([3.3919]),\n",
       " tensor([3.3889]),\n",
       " tensor([3.3880]),\n",
       " tensor([3.3940]),\n",
       " tensor([3.4042]),\n",
       " tensor([3.4017]),\n",
       " tensor([3.4033]),\n",
       " tensor([3.4213]),\n",
       " tensor([3.3893]),\n",
       " tensor([3.3788]),\n",
       " tensor([3.3851]),\n",
       " tensor([3.3969]),\n",
       " tensor([3.3791]),\n",
       " tensor([3.3855]),\n",
       " tensor([3.3906]),\n",
       " tensor([3.3865]),\n",
       " tensor([3.3909]),\n",
       " tensor([3.3916]),\n",
       " tensor([3.3898]),\n",
       " tensor([3.3884]),\n",
       " tensor([3.3852]),\n",
       " tensor([3.3982]),\n",
       " tensor([3.4141]),\n",
       " tensor([3.3893]),\n",
       " tensor([3.4094]),\n",
       " tensor([3.3684]),\n",
       " tensor([3.4163]),\n",
       " tensor([3.3763]),\n",
       " tensor([3.3887]),\n",
       " tensor([3.3932]),\n",
       " tensor([3.3873]),\n",
       " tensor([3.3944]),\n",
       " tensor([3.3999]),\n",
       " tensor([3.4012]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.4031]),\n",
       " tensor([3.3958]),\n",
       " tensor([3.3873]),\n",
       " tensor([3.3865]),\n",
       " tensor([3.4221]),\n",
       " tensor([3.4161]),\n",
       " tensor([3.3898]),\n",
       " tensor([3.3983]),\n",
       " tensor([3.3923]),\n",
       " tensor([3.4119]),\n",
       " tensor([3.3873]),\n",
       " tensor([3.3876]),\n",
       " tensor([3.3926]),\n",
       " tensor([3.3767]),\n",
       " tensor([3.4065]),\n",
       " tensor([3.3951]),\n",
       " tensor([3.4100]),\n",
       " tensor([3.3970]),\n",
       " tensor([3.3796]),\n",
       " tensor([3.4057]),\n",
       " tensor([3.3976]),\n",
       " tensor([3.3955]),\n",
       " tensor([3.3911]),\n",
       " tensor([3.3922]),\n",
       " tensor([3.3937]),\n",
       " tensor([3.3938]),\n",
       " tensor([3.3969]),\n",
       " tensor([3.3936]),\n",
       " tensor([3.3967]),\n",
       " tensor([3.3899]),\n",
       " tensor([3.3775]),\n",
       " tensor([3.4075]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.3970]),\n",
       " tensor([3.3836]),\n",
       " tensor([3.4081]),\n",
       " tensor([3.3845]),\n",
       " tensor([3.3839]),\n",
       " tensor([3.3944]),\n",
       " tensor([3.3899]),\n",
       " tensor([3.3913]),\n",
       " tensor([3.4027]),\n",
       " tensor([3.3920]),\n",
       " tensor([3.3838]),\n",
       " tensor([3.4055]),\n",
       " tensor([3.4126]),\n",
       " tensor([3.4025]),\n",
       " tensor([3.3967]),\n",
       " tensor([3.4007]),\n",
       " tensor([3.3952]),\n",
       " tensor([3.3953]),\n",
       " tensor([3.4139]),\n",
       " tensor([3.4124]),\n",
       " tensor([3.3936]),\n",
       " tensor([3.4005]),\n",
       " tensor([3.4197]),\n",
       " tensor([3.3932]),\n",
       " tensor([3.3995]),\n",
       " tensor([3.3759]),\n",
       " tensor([3.3923]),\n",
       " tensor([3.3989]),\n",
       " tensor([3.3915]),\n",
       " tensor([3.3882]),\n",
       " tensor([3.4209]),\n",
       " tensor([3.3956]),\n",
       " tensor([3.3979]),\n",
       " tensor([3.3857]),\n",
       " tensor([3.4188]),\n",
       " tensor([3.3918]),\n",
       " tensor([3.4094]),\n",
       " tensor([3.3897]),\n",
       " tensor([3.3953]),\n",
       " tensor([3.3892]),\n",
       " tensor([3.3970]),\n",
       " tensor([3.3870]),\n",
       " tensor([3.3946]),\n",
       " tensor([3.3978]),\n",
       " tensor([3.3755]),\n",
       " tensor([3.4047]),\n",
       " tensor([3.4014]),\n",
       " tensor([3.4015]),\n",
       " tensor([3.3726]),\n",
       " tensor([3.3975]),\n",
       " tensor([3.4013]),\n",
       " tensor([3.4140]),\n",
       " tensor([3.3905]),\n",
       " tensor([3.3780]),\n",
       " tensor([3.3963]),\n",
       " tensor([3.4032]),\n",
       " tensor([3.3950]),\n",
       " tensor([3.3960]),\n",
       " tensor([3.3978]),\n",
       " tensor([3.3896]),\n",
       " tensor([3.4156]),\n",
       " tensor([3.4089]),\n",
       " tensor([3.3999]),\n",
       " tensor([3.3753]),\n",
       " tensor([3.3867]),\n",
       " tensor([3.4000]),\n",
       " tensor([3.4068]),\n",
       " tensor([3.4117]),\n",
       " tensor([3.4168]),\n",
       " tensor([3.3935]),\n",
       " tensor([3.4048]),\n",
       " tensor([3.4027]),\n",
       " tensor([3.3758]),\n",
       " tensor([3.4010]),\n",
       " tensor([3.4005]),\n",
       " tensor([3.3967]),\n",
       " tensor([3.4026]),\n",
       " tensor([3.3929]),\n",
       " tensor([3.3893]),\n",
       " tensor([3.3895]),\n",
       " tensor([3.4025]),\n",
       " tensor([3.3774]),\n",
       " tensor([3.3910]),\n",
       " tensor([3.3889]),\n",
       " tensor([3.4063]),\n",
       " tensor([3.3899]),\n",
       " tensor([3.3971]),\n",
       " tensor([3.3923]),\n",
       " tensor([3.4122]),\n",
       " tensor([3.3859]),\n",
       " tensor([3.3962]),\n",
       " tensor([3.3989]),\n",
       " tensor([3.3804]),\n",
       " tensor([3.4029]),\n",
       " tensor([3.3983]),\n",
       " tensor([3.3745]),\n",
       " tensor([3.4155]),\n",
       " tensor([3.3889]),\n",
       " tensor([3.3940]),\n",
       " tensor([3.3936]),\n",
       " tensor([3.3784]),\n",
       " tensor([3.4200]),\n",
       " tensor([3.3796]),\n",
       " tensor([3.3854]),\n",
       " tensor([3.3860]),\n",
       " tensor([3.4193]),\n",
       " tensor([3.4039]),\n",
       " tensor([3.4040]),\n",
       " tensor([3.4007]),\n",
       " tensor([3.3829]),\n",
       " tensor([3.3982]),\n",
       " tensor([3.3808]),\n",
       " tensor([3.4081]),\n",
       " tensor([3.3952]),\n",
       " tensor([3.4156]),\n",
       " tensor([3.4112]),\n",
       " tensor([3.4128]),\n",
       " tensor([3.4121]),\n",
       " tensor([3.3957]),\n",
       " tensor([3.3943]),\n",
       " tensor([3.3799]),\n",
       " tensor([3.3915]),\n",
       " tensor([3.4176]),\n",
       " tensor([3.3785]),\n",
       " tensor([3.3834]),\n",
       " tensor([3.4043]),\n",
       " tensor([3.3938]),\n",
       " tensor([3.3826]),\n",
       " tensor([3.4024]),\n",
       " tensor([3.3841]),\n",
       " tensor([3.3795]),\n",
       " tensor([3.3810]),\n",
       " tensor([3.3850]),\n",
       " tensor([3.3951]),\n",
       " tensor([3.3959]),\n",
       " tensor([3.3843]),\n",
       " tensor([3.3688]),\n",
       " tensor([3.3970]),\n",
       " tensor([3.4021]),\n",
       " tensor([3.3768]),\n",
       " tensor([3.4121]),\n",
       " tensor([3.3758]),\n",
       " tensor([3.3795]),\n",
       " tensor([3.3867]),\n",
       " tensor([3.3911]),\n",
       " tensor([3.4070]),\n",
       " tensor([3.3904]),\n",
       " tensor([3.4122]),\n",
       " tensor([3.4100]),\n",
       " tensor([3.3890]),\n",
       " tensor([3.4108]),\n",
       " tensor([3.3873]),\n",
       " tensor([3.3842]),\n",
       " tensor([3.3999]),\n",
       " tensor([3.4127]),\n",
       " tensor([3.3960]),\n",
       " tensor([3.4027]),\n",
       " tensor([3.3887]),\n",
       " tensor([3.4074]),\n",
       " tensor([3.3842]),\n",
       " tensor([3.3875]),\n",
       " tensor([3.3933]),\n",
       " tensor([3.3945]),\n",
       " tensor([3.3959]),\n",
       " tensor([3.3963]),\n",
       " tensor([3.3896]),\n",
       " tensor([3.4010]),\n",
       " tensor([3.3924]),\n",
       " tensor([3.4082]),\n",
       " tensor([3.3833]),\n",
       " tensor([3.3936]),\n",
       " tensor([3.3899]),\n",
       " tensor([3.3990]),\n",
       " tensor([3.3942]),\n",
       " tensor([3.3891]),\n",
       " tensor([3.3880]),\n",
       " tensor([3.4155]),\n",
       " tensor([3.3919]),\n",
       " tensor([3.3955]),\n",
       " tensor([3.3861]),\n",
       " tensor([3.4238]),\n",
       " tensor([3.3786]),\n",
       " tensor([3.4097]),\n",
       " tensor([3.4214]),\n",
       " tensor([3.3743]),\n",
       " tensor([3.3897]),\n",
       " tensor([3.4080]),\n",
       " tensor([3.4082]),\n",
       " tensor([3.3964]),\n",
       " tensor([3.3845]),\n",
       " tensor([3.4070]),\n",
       " tensor([3.3896]),\n",
       " tensor([3.4056]),\n",
       " tensor([3.3756]),\n",
       " tensor([3.3853]),\n",
       " tensor([3.4129]),\n",
       " tensor([3.4044]),\n",
       " tensor([3.3836]),\n",
       " tensor([3.3902]),\n",
       " tensor([3.3959]),\n",
       " tensor([3.3827]),\n",
       " tensor([3.4051]),\n",
       " tensor([3.3900]),\n",
       " ...]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(model=alphafold2, dataloaders=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.4.4.2. <a id='toc17_4_4_2_'></a>[PyTorch lightning加载权重后预测](#toc0_)\n",
    "需要手动：\n",
    "- model.eval()\n",
    "- with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4124],\n",
       "        [3.3868],\n",
       "        [3.3909],\n",
       "        ...,\n",
       "        [3.4022],\n",
       "        [3.3898],\n",
       "        [3.3934]], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_alphafold2 = AlphaFold2Wrapper.load_from_checkpoint('./lightning_logs/version_0/checkpoints/epoch=9-step=790.ckpt')\n",
    "\n",
    "# 进行预测/推理\n",
    "pretrained_alphafold2.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = pretrained_alphafold2(features.to('cuda:0'))\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.4.4.3. <a id='toc17_4_4_3_'></a>[提取权重后加载至纯PyTorch模型](#toc0_)\n",
    "从checkpoint中`提取模型的权重参数`，`修改相关格式`后再加载到纯PyTorch的模型中，就是普通又熟悉的PyTorch的预测方式了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268120/3329860571.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 9,\n",
       " 'global_step': 790,\n",
       " 'pytorch-lightning_version': '2.4.0',\n",
       " 'state_dict': OrderedDict([('demo_model.hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('demo_model.hidden.0.bias',\n",
       "               tensor([3.3954], device='cuda:0'))]),\n",
       " 'loops': {'fit_loop': {'state_dict': {},\n",
       "   'epoch_loop.state_dict': {'_batches_that_stepped': 790},\n",
       "   'epoch_loop.batch_progress': {'total': {'ready': 790,\n",
       "     'completed': 790,\n",
       "     'started': 790,\n",
       "     'processed': 790},\n",
       "    'current': {'ready': 79, 'completed': 79, 'started': 79, 'processed': 79},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_loop.scheduler_progress': {'total': {'ready': 0, 'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.automatic_optimization.state_dict': {},\n",
       "   'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 790,\n",
       "       'completed': 790},\n",
       "      'current': {'ready': 79, 'completed': 79}},\n",
       "     'zero_grad': {'total': {'ready': 790, 'completed': 790, 'started': 790},\n",
       "      'current': {'ready': 79, 'completed': 79, 'started': 79}}}},\n",
       "   'epoch_loop.manual_optimization.state_dict': {},\n",
       "   'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0,\n",
       "     'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.val_loop.state_dict': {},\n",
       "   'epoch_loop.val_loop.batch_progress': {'total': {'ready': 79,\n",
       "     'completed': 79,\n",
       "     'started': 79,\n",
       "     'processed': 79},\n",
       "    'current': {'ready': 79, 'completed': 79, 'started': 79, 'processed': 79},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_progress': {'total': {'ready': 10,\n",
       "     'completed': 9,\n",
       "     'started': 10,\n",
       "     'processed': 10},\n",
       "    'current': {'ready': 10, 'completed': 9, 'started': 10, 'processed': 10}}},\n",
       "  'validate_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'test_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'predict_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}},\n",
       " 'callbacks': {\"ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': None,\n",
       "   'best_model_score': None,\n",
       "   'best_model_path': '/bmp/backup/zhaosy/ws/PyTorch_learning/lightning_logs/version_0/checkpoints/epoch=9-step=790.ckpt',\n",
       "   'current_score': None,\n",
       "   'dirpath': '/bmp/backup/zhaosy/ws/PyTorch_learning/lightning_logs/version_0/checkpoints',\n",
       "   'best_k_models': {},\n",
       "   'kth_best_model_path': '',\n",
       "   'kth_value': tensor(inf),\n",
       "   'last_model_path': ''}},\n",
       " 'optimizer_states': [{'state': {},\n",
       "   'param_groups': [{'lr': 0.01,\n",
       "     'momentum': 0,\n",
       "     'dampening': 0,\n",
       "     'weight_decay': 0,\n",
       "     'nesterov': False,\n",
       "     'maximize': False,\n",
       "     'foreach': None,\n",
       "     'differentiable': False,\n",
       "     'fused': None,\n",
       "     'params': [0, 1]}]}],\n",
       " 'lr_schedulers': [],\n",
       " 'hparams_name': 'kwargs',\n",
       " 'hyper_parameters': {'learning_rate': 0.01}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = './lightning_logs/version_0/checkpoints/epoch=9-step=790.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "checkpoint  # checkpoint的贮存格式，其中 'state_dict'就是模型权重信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AlphaFold2Wrapper(\n",
       "   (demo_model): AlphaFold2(\n",
       "     (hidden): Sequential(\n",
       "       (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (loss_fn): MSELoss()\n",
       " ),\n",
       " OrderedDict([('demo_model.hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('demo_model.hidden.0.bias',\n",
       "               tensor([3.3954], device='cuda:0'))]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphafold2, checkpoint['state_dict']    # with AlphaFold2Wrapper, 多了demo_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_model.hidden.0.weight\n",
      "demo_model.hidden.0.bias\n"
     ]
    }
   ],
   "source": [
    "for param in checkpoint['state_dict']:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `纯PyTorch的state_dict`如下，`如上的checkpoint`中的state_dict`不符合`相应格式，需要进行更改："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AlphaFold2(\n",
       "   (hidden): Sequential(\n",
       "     (0): Linear(in_features=2, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " OrderedDict([('hidden.0.weight', tensor([[-0.5207,  0.0861]])),\n",
       "              ('hidden.0.bias', tensor([0.0467]))]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphafold_with_pure_pytorch = AlphaFold2()\n",
    "alphafold_with_pure_pytorch, alphafold_with_pure_pytorch.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 更改操作如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = checkpoint['state_dict']\n",
    "\n",
    "# 把demo_model.删除即可\n",
    "for key in model_weights:\n",
    "    model_weights[key.replace(\"demo_model.\", \"\")] = model_weights.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('hidden.0.bias', tensor([3.3954], device='cuda:0'))]),\n",
       " OrderedDict([('hidden.0.weight',\n",
       "               tensor([[ 0.0967, -0.0567]], device='cuda:0')),\n",
       "              ('hidden.0.bias', tensor([3.3954], device='cuda:0'))]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['state_dict'], model_weights # 都更改了，什么鬼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4124],\n",
       "        [3.3868],\n",
       "        [3.3909],\n",
       "        ...,\n",
       "        [3.4022],\n",
       "        [3.3898],\n",
       "        [3.3934]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新实例化一个新的对象\n",
    "alphafold_with_pure_pytorch = AlphaFold2()\n",
    "\n",
    "# 加载修改后的权重\n",
    "alphafold_with_pure_pytorch.load_state_dict(model_weights)  # 加载修改后的model_weights\n",
    "\n",
    "# 进行预测/推理\n",
    "alphafold_with_pure_pytorch.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = alphafold_with_pure_pytorch(features)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. <a id='toc18_'></a>[Torchvision](#toc0_)\n",
    "Torchvision Docs: [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision version: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision \n",
    "\n",
    "\n",
    "print('torchvision version:', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1. <a id='toc18_1_'></a>[Models](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.1. <a id='toc18_1_1_'></a>[可用模型](#toc0_)\n",
    "可用`模型`见：[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'deeplabv3_mobilenet_v3_large',\n",
       " 'deeplabv3_resnet101',\n",
       " 'deeplabv3_resnet50',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'fasterrcnn_mobilenet_v3_large_320_fpn',\n",
       " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
       " 'fasterrcnn_resnet50_fpn',\n",
       " 'fasterrcnn_resnet50_fpn_v2',\n",
       " 'fcn_resnet101',\n",
       " 'fcn_resnet50',\n",
       " 'fcos_resnet50_fpn',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'keypointrcnn_resnet50_fpn',\n",
       " 'lraspp_mobilenet_v3_large',\n",
       " 'maskrcnn_resnet50_fpn',\n",
       " 'maskrcnn_resnet50_fpn_v2',\n",
       " 'maxvit_t',\n",
       " 'mc3_18',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mvit_v1_b',\n",
       " 'mvit_v2_s',\n",
       " 'quantized_googlenet',\n",
       " 'quantized_inception_v3',\n",
       " 'quantized_mobilenet_v2',\n",
       " 'quantized_mobilenet_v3_large',\n",
       " 'quantized_resnet18',\n",
       " 'quantized_resnet50',\n",
       " 'quantized_resnext101_32x8d',\n",
       " 'quantized_resnext101_64x4d',\n",
       " 'quantized_shufflenet_v2_x0_5',\n",
       " 'quantized_shufflenet_v2_x1_0',\n",
       " 'quantized_shufflenet_v2_x1_5',\n",
       " 'quantized_shufflenet_v2_x2_0',\n",
       " 'r2plus1d_18',\n",
       " 'r3d_18',\n",
       " 'raft_large',\n",
       " 'raft_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'retinanet_resnet50_fpn',\n",
       " 'retinanet_resnet50_fpn_v2',\n",
       " 's3d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'ssd300_vgg16',\n",
       " 'ssdlite320_mobilenet_v3_large',\n",
       " 'swin3d_b',\n",
       " 'swin3d_s',\n",
       " 'swin3d_t',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models \n",
    "\n",
    "\n",
    "models.list_models()    # List all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.2. <a id='toc18_1_2_'></a>[下载模型和权重](#toc0_)\n",
    "可用`权重`见：[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "alexnet = models.get_model(name='alexnet')\n",
    "\n",
    "# 1. Get weight\n",
    "weights = models.get_weight('AlexNet_Weights.IMAGENET1K_V1')\n",
    "# weights = models.get_weight('ResNet50_Weights.IMAGENET1K_V1')\n",
    "# weights = models.get_weight('ResNet50_Weights.IMAGENET1K_V2')\n",
    "\n",
    "# 2. (Recommendation) Get weight with model name\n",
    "weights = models.get_model_weights(name='alexnet')\n",
    "\n",
    "# Get the state_dict parameters from loaded weights wrapper\n",
    "state_dict = weights.IMAGENET1K_V1.get_state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.3. <a id='toc18_1_3_'></a>[模型加载权重](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "alexnet.load_state_dict(state_dict=state_dict)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1.4. <a id='toc18_1_4_'></a>[总结](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrapper to the following:\n",
    "def get_pretrained_model(model_name:str)-> torch.nn.Module:\n",
    "    '''Default to get: IMAGENET1K_V1'''\n",
    "    model = models.get_model(name=model_name)\n",
    "    weight_wrapper = models.get_model_weights(name=model_name)\n",
    "    state_dict = weight_wrapper.IMAGENET1K_V1.get_state_dict()\n",
    "    model.load_state_dict(state_dict=state_dict)\n",
    "    return model\n",
    "\n",
    "# pretrained_model = get_pretrained_model(model_name='resnet50')\n",
    "pretrained_model = get_pretrained_model(model_name='alexnet')\n",
    "pretrained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2. <a id='toc18_2_'></a>[Dataset](#toc0_)\n",
    "torchvision Docs: [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)  \n",
    "\n",
    "1. torchvision的datasets有很多，如：\n",
    "  \n",
    "    - Image classification\n",
    "        - FashionMNIST(root[, train, transform, ...])\n",
    "        - MNIST(root[, train, transform, ...])\n",
    "    - Image detection or segmentation\n",
    "        - CocoDetection(root, annFile[, transform, ...])\n",
    "    - Video classification\n",
    "        - HMDB51(root, annotation_path, frames_per_clip)\n",
    "    - Video prediction\n",
    "        - MovingMNIST(root[, split, split_ratio, ...])\n",
    "\n",
    "2. 另外，还可以自定义数据集，函数如下：\n",
    "\n",
    "    - Base classes for custom datasets\n",
    "        - `DatasetFolder`(root, loader[, extensions, ...])    # A generic data loader.\n",
    "        - `ImageFolder`(root, transform, ...)                 # A generic data loader where the images are arranged in this way by default: .\n",
    "        - `VisionDataset`([root, transforms, transform, ...]) # Base Class For making datasets which are compatible with torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.FashionMNIST,\n",
       " torchvision.datasets.mnist.FashionMNIST)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets \n",
    "from torchvision import transforms \n",
    "\n",
    "\n",
    "dbs = './Pytorch_datasets/'\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),                                  # PIL转换为tensor格式\n",
    "    transforms.Normalize(mean=(0.5,), std=(1.0,))           # 标准化\n",
    "])\n",
    "\n",
    "\n",
    "train_datasets = datasets.FashionMNIST(\n",
    "    root=dbs, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=trans\n",
    ")\n",
    "\n",
    "test_datasets = datasets.FashionMNIST(\n",
    "    root=dbs,  \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=trans\n",
    ")\n",
    "\n",
    "type(train_datasets), type(test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. <a id='toc19_'></a>[Hugging face](#toc0_)\n",
    "Hugging Face 是一个非常流行的自然语言处理（NLP）库，提供了大量预训练的模型和数据集，简化了 NLP 任务的实现。\n",
    "\n",
    "Hugging Face 的 PyTorch 库提供了许多预训练的模型，包括用于文本分类、情感分析、命名实体识别、文本生成等任务的模型。这些模型通常具有良好的性能，并且可以轻松地在 PyTorch 中使用。\n",
    "\n",
    "Hugging Face 还提供了许多数据集，包括用于文本分类、情感分析、命名实体识别、文本生成等任务的数据集。这些数据集通常具有良好的性能，并且可以轻松地在 PyTorch 中使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "res = classifier(\"Today is a nice day.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器\n",
    "model_name = \"bert-base-uncased\"  # 你也可以选择其他模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 输入文本示例\n",
    "text = \"I love Hugging Face!\"\n",
    "inputs = tokenizer(text=text, return_tensors=\"pt\")\n",
    "\n",
    "# 前向推理 (inference)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# 输出分类结果\n",
    "predicted_class = torch.argmax(logits, dim=1)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. <a id='toc20_'></a>[PyTorch hub](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# torch.hub.list()\n",
    "# torch.hub.load()\n",
    "# torch.hub.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. <a id='toc21_'></a>[监督学习 (Supervised learning)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import TensorDataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. <a id='toc22_'></a>[半监督学习 (Semi-supervised learning)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. <a id='toc23_'></a>[无监督学习 (Unsupervised learning)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. <a id='toc24_'></a>[深度强化学习 (DRL, Deep Reforcement Learning)](#toc0_)\n",
    "深度强化学习（Deep Reinforcement Learning, DRL） 是将`强化学习（Reinforcement Learning, RL）与深度学习（Deep Learning）`结合的一类方法，旨在通过智能体与环境的交互，学习如何采取最优行动策略，以最大化累积奖励。它在解决复杂、高维的决策问题方面表现出色，例如机器人控制、游戏 AI 和自动驾驶等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24.1. <a id='toc24_1_'></a>[强化学习基础概念](#toc0_)\n",
    "强化学习的核心思想是通过`试错（Trial and Error）`机制让智能体学习如何在环境中行动，逐步找到最优策略。它由以下几个主要元素构成：\n",
    "\n",
    "  - 环境（Environment）：智能体所处的外部系统，负责接收智能体的动作并提供相应的奖励和状态。\n",
    "  - 智能体（Agent）：在环境中行动的实体，目标是通过与环境交互获得奖励并学习如何行动。\n",
    "  - 状态（State, 𝑆）：环境在某一时刻的描述，智能体根据当前状态决定行动。\n",
    "  - 动作（Action, 𝐴）：智能体在当前状态下选择的行为，影响下一个状态。\n",
    "  - 奖励（Reward, 𝑅）：环境根据智能体的动作给予反馈，用来衡量智能体行为的好坏。\n",
    "  - 策略（Policy, 𝜋）：智能体根据当前状态选择动作的规则，可以是确定性策略或随机策略。\n",
    "  - 值函数（Value Function）：用于评估某个状态或动作的好坏，通常包括状态值函数 𝑉(𝑠) 和 动作值函数𝑄(𝑠,𝑎)。\n",
    "  \n",
    "强化学习的目标是找到一个最优策略 𝜋∗π∗ ，使得智能体在与环境交互时获得的累计奖励最大化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24.2. <a id='toc24_2_'></a>[深度强化学习的特点](#toc0_)\n",
    "深度强化学习通过使用`深度神经网络（DNN）`来近似复杂的值函数或策略，解决传统强化学习在高维状态空间中的表现不佳问题。结合深度学习的表征能力，深度强化学习可以处理图像、视频等高维感知输入，并在此基础上做出决策。\n",
    "\n",
    "主要特点：\n",
    "  - 处理高维输入：通过深度神经网络，DRL 可以处理像素级图像输入、复杂的传感器数据等高维特征，传统强化学习方法无法有效应对这些情况。\n",
    "  - 自动特征学习：无需手动设计特征，深度学习模型可以自动从输入数据中学习重要特征。\n",
    "  - 扩展性强：DRL 可以扩展到大规模的状态和动作空间，解决复杂的实际问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24.3. <a id='toc24_3_'></a>[马尔可夫](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24.4. <a id='toc24_4_'></a>[深度强化学习的主要方法](#toc0_)\n",
    "### 24.4.1. <a id='toc24_4_1_'></a>[深度 Q 网络（Deep Q-Network, DQN）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.2. <a id='toc24_4_2_'></a>[策略梯度方法（Policy Gradient Methods）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.3. <a id='toc24_4_3_'></a>[演员-评论家方法（Actor-Critic Methods）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.4. <a id='toc24_4_4_'></a>[深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG）](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24.5. <a id='toc24_5_'></a>[深度强化学习的应用](#toc0_)\n",
    "深度强化学习已经在多个领域取得了成功：\n",
    "\n",
    "  - 游戏 AI：最著名的例子是 AlphaGo，它通过强化学习击败了世界顶级围棋选手。DQN 和 A3C 也被广泛应用于 Atari 游戏、StarCraft 等复杂游戏环境中。\n",
    "  - 自动驾驶：通过 DRL，智能体可以在虚拟环境中学习如何驾驶，并在现实世界中进行应用。\n",
    "  - 机器人控制：DRL 可以帮助机器人学习复杂的控制任务，例如机械臂操作、无人机飞行等。\n",
    "  - 推荐系统：通过深度强化学习，推荐系统可以动态调整推荐策略，个性化地满足用户需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. <a id='toc25_'></a>[生成对抗网络 (GAN, Generative Adversarial Networks)](#toc0_)\n",
    "生成对抗网络（Generative Adversarial Networks） 是由 Ian Goodfellow 等人在 2014 年提出的一种深度学习生成模型。GAN 通过两个神经网络——生成器（Generator）和判别器（Discriminator）——相互对抗来生成与真实数据分布相似的合成数据。GAN 在生成图像、视频、文本和音频等领域具有广泛的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. <a id='toc26_'></a>[扩散模型 (DM, Diffusion Models)](#toc0_)\n",
    "扩散模型（Diffusion Models） 是近年来在生成模型领域非常流行的一类方法，尤其是在生成图像等数据方面取得了重要进展。它们已经成为了生成对抗网络（GAN）和变分自编码器（VAE）之外的一个有力替代方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. <a id='toc27_'></a>[图神经网络 (GNN, Graph Neural Networks)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch_geometric \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. <a id='toc28_'></a>[多模态 (ML, MultiModal Learning)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.1. <a id='toc28_1_'></a>[特征融合](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.1. <a id='toc28_1_1_'></a>[concatenate融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1880,  0.7078],\n",
       "        [ 0.1063, -0.2480],\n",
       "        [-0.8586, -1.1807],\n",
       "        [ 0.0430, -0.5358],\n",
       "        [-0.1850,  0.2209],\n",
       "        [-0.3914, -0.1025],\n",
       "        [-0.1250,  0.0920],\n",
       "        [-0.1198, -0.2499],\n",
       "        [ 0.3329, -0.4342],\n",
       "        [ 0.3291, -0.1004],\n",
       "        [ 0.1111, -0.3002],\n",
       "        [ 0.0725, -0.6321],\n",
       "        [ 0.1208, -0.0259],\n",
       "        [ 0.1791,  0.1676],\n",
       "        [-0.0622,  0.1658],\n",
       "        [-0.3677,  0.0363],\n",
       "        [ 0.1491,  0.3804],\n",
       "        [ 0.2784, -0.0216],\n",
       "        [ 0.4644,  0.2349],\n",
       "        [-0.2483, -0.2576]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class ConcatenationFusion(nn.Module):\n",
    "    def __init__(self, text_dim, hidden_dim, image_dim, num_classes): \n",
    "        super().__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, text, image):\n",
    "        text_embed = self.text_fc(text)\n",
    "        image_embed = self.image_fc(image)\n",
    "        embed = torch.cat((text_embed, image_embed), dim=0)  # 拼接融合\n",
    "        return self.classifier(embed)\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "text = torch.randn(batch_size, text_dim)\n",
    "image = torch.randn(batch_size, image_dim)\n",
    "\n",
    "model = ConcatenationFusion(text_dim, hidden_dim, image_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.2. <a id='toc28_1_2_'></a>[加权融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0170,  0.1555],\n",
       "        [ 0.0385, -0.1914],\n",
       "        [ 0.1036, -0.0659],\n",
       "        [-0.1466,  0.0531],\n",
       "        [-0.6206, -0.1544],\n",
       "        [-0.3399, -0.2346],\n",
       "        [-0.4070, -0.4799],\n",
       "        [-0.5645,  0.0331],\n",
       "        [-0.1366,  0.0349],\n",
       "        [-0.7428,  0.2844]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class WeightedFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(WeightedFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.text_weight = nn.Parameter(torch.randn(1))\n",
    "        self.image_weight = nn.Parameter(torch.randn(1))\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))\n",
    "        image_feat = torch.relu(self.image_fc(image))\n",
    "        combined = self.text_weight * text_feat + self.image_weight * image_feat  # 加权融合    \n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = WeightedFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.3. <a id='toc28_1_3_'></a>[元素级融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1509,  0.3871],\n",
       "        [-0.8885, -0.3169],\n",
       "        [-0.8079, -0.5716],\n",
       "        [-0.7261, -0.1003],\n",
       "        [-0.0701, -0.5493],\n",
       "        [-0.8524, -0.3561],\n",
       "        [-1.1294, -0.3606],\n",
       "        [-0.8689, -0.3579],\n",
       "        [-0.9081, -0.6309],\n",
       "        [-1.0464, -0.4657]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ElementWiseFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(ElementWiseFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))\n",
    "        image_feat = torch.relu(self.image_fc(image))\n",
    "        combined = text_feat + image_feat  # 元素级融合\n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = ElementWiseFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.4. <a id='toc28_1_4_'></a>[张量融合](#toc0_)\n",
    "通过构建高纬度张量来表示不同模态之间的交互关系，捕捉多阶的特征交互信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6352, -0.3399],\n",
       "        [-0.5262, -0.7934],\n",
       "        [-0.8739,  0.2307],\n",
       "        [ 0.1405,  0.0721],\n",
       "        [-0.4297, -0.4880],\n",
       "        [-1.2108, -0.8075],\n",
       "        [ 0.0606,  0.8979],\n",
       "        [ 0.2691,  0.4548],\n",
       "        [-0.1921, -0.7102],\n",
       "        [-0.3202,  0.6673]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "\n",
    "class TensorFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(TensorFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        # 双线性层进行张量融合\n",
    "        self.bilinear = nn.Bilinear(hidden_dim, hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))  # (batch, hidden_dim)\n",
    "        image_feat = torch.relu(self.image_fc(image))  # (batch, hidden_dim)\n",
    "        # 张量融合\n",
    "        fused_feat = self.bilinear(text_feat, image_feat)\n",
    "        output = self.classifier(fused_feat)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = TensorFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.5. <a id='toc28_1_5_'></a>[注意力机制融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2901, -0.0059],\n",
       "        [-0.0250,  0.1197],\n",
       "        [-0.2061, -0.1256],\n",
       "        [-0.3189,  0.2029],\n",
       "        [-0.0152,  0.1354],\n",
       "        [ 0.0839,  0.0148],\n",
       "        [-0.1621,  0.0316],\n",
       "        [ 0.0181,  0.2491],\n",
       "        [-0.0624,  0.0125],\n",
       "        [-0.2978, -0.0618]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models/attention_fusion.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AttentionFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim):\n",
    "        super(AttentionFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))  # (batch, hidden_dim)\n",
    "        image_feat = torch.relu(self.image_fc(image))  # (batch, hidden_dim)\n",
    "        # 拼接特征用于计算注意力\n",
    "        combined = torch.cat((text_feat, image_feat), dim=1)\n",
    "        attention_weights = torch.sigmoid(self.attention(combined))  # (batch, 1)\n",
    "        # 加权融合\n",
    "        fused_feat = attention_weights * text_feat + (1 - attention_weights) * image_feat\n",
    "        output = self.classifier(fused_feat)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = AttentionFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28.1.6. <a id='toc28_1_6_'></a>[高阶融合](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0248, -0.0359],\n",
       "        [ 0.0652, -0.0690],\n",
       "        [ 0.0805, -0.0663],\n",
       "        [ 0.0584, -0.0093],\n",
       "        [ 0.0452, -0.0590],\n",
       "        [ 0.0372, -0.0490],\n",
       "        [ 0.0078, -0.0247],\n",
       "        [ 0.0474, -0.0621],\n",
       "        [ 0.0738, -0.0500],\n",
       "        [ 0.0369, -0.0351]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models/high_order_fusion.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class HighOrderFusionModel(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim, output_dim, order=2):\n",
    "        super(HighOrderFusionModel, self).__init__()\n",
    "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_dim, hidden_dim)\n",
    "        self.order = order\n",
    "        # 高阶特征交互\n",
    "        self.high_order_fc = nn.Linear(hidden_dim ** order, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text_feat = torch.relu(self.text_fc(text))  # (batch, hidden_dim)\n",
    "        image_feat = torch.relu(self.image_fc(image))  # (batch, hidden_dim)\n",
    "        # 高阶交互，通过外积实现\n",
    "        if self.order == 2:\n",
    "            fused_feat = torch.bmm(text_feat.unsqueeze(2), image_feat.unsqueeze(1))  # (batch, hidden_dim, hidden_dim)\n",
    "            fused_feat = fused_feat.view(fused_feat.size(0), -1)  # (batch, hidden_dim^2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"当前仅支持二阶融合\")\n",
    "        fused_feat = torch.relu(self.high_order_fc(fused_feat))\n",
    "        output = self.classifier(fused_feat)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 测试\n",
    "batch_size = 10\n",
    "text_dim = 100\n",
    "hidden_dim = 128\n",
    "image_dim = 100\n",
    "num_classes = 2\n",
    "\n",
    "model = HighOrderFusionModel(text_dim, image_dim, hidden_dim, num_classes)\n",
    "model(text, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.2. <a id='toc28_2_'></a>[简单示例](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# 模拟的多模态数据，假设基因组、转录组、代谢组的数据分别有不同维度\n",
    "np.random.seed(42)\n",
    "genomics_data = np.random.rand(1000, 500)  # 基因组数据 (1000 samples, 500 features)\n",
    "transcriptomics_data = np.random.rand(1000, 300)  # 转录组数据 (1000 samples, 300 features)\n",
    "metabolomics_data = np.random.rand(1000, 100)  # 代谢组数据 (1000 samples, 100 features)\n",
    "\n",
    "# 标签 (假设为二分类问题：健康或疾病)\n",
    "labels = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# 归一化数据\n",
    "scaler = StandardScaler()\n",
    "genomics_data = scaler.fit_transform(genomics_data)\n",
    "transcriptomics_data = scaler.fit_transform(transcriptomics_data)\n",
    "metabolomics_data = scaler.fit_transform(metabolomics_data)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train_genomics, X_test_genomics, X_train_transcript, X_test_transcript, X_train_metabol, X_test_metabol, y_train, y_test = train_test_split(\n",
    "    genomics_data, \n",
    "    transcriptomics_data, \n",
    "    metabolomics_data, \n",
    "    labels, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# 转换为Tensor\n",
    "X_train_genomics = torch.tensor(X_train_genomics, dtype=torch.float32)\n",
    "X_test_genomics = torch.tensor(X_test_genomics, dtype=torch.float32)\n",
    "X_train_transcript = torch.tensor(X_train_transcript, dtype=torch.float32)\n",
    "X_test_transcript = torch.tensor(X_test_transcript, dtype=torch.float32)\n",
    "X_train_metabol = torch.tensor(X_train_metabol, dtype=torch.float32)\n",
    "X_test_metabol = torch.tensor(X_test_metabol, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, genomics, transcriptomics, metabolomics, labels):\n",
    "        self.genomics = genomics\n",
    "        self.transcriptomics = transcriptomics\n",
    "        self.metabolomics = metabolomics\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.genomics[idx], self.transcriptomics[idx], self.metabolomics[idx], self.labels[idx])\n",
    "\n",
    "\n",
    "# 创建训练集和测试集的DataLoader\n",
    "train_dataset = MultiModalDataset(X_train_genomics, X_train_transcript, X_train_metabol, y_train)\n",
    "test_dataset = MultiModalDataset(X_test_genomics, X_test_transcript, X_test_metabol, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "        \n",
    "        # 基因组数据的网络分支\n",
    "        self.genomics_fc1 = nn.Linear(500, 256)\n",
    "        self.genomics_fc2 = nn.Linear(256, 128)\n",
    "\n",
    "        # 转录组数据的网络分支\n",
    "        self.transcript_fc1 = nn.Linear(300, 256)\n",
    "        self.transcript_fc2 = nn.Linear(256, 128)\n",
    "\n",
    "        # 代谢组数据的网络分支\n",
    "        self.metabol_fc1 = nn.Linear(100, 128)\n",
    "        self.metabol_fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # 融合层\n",
    "        self.fc1 = nn.Linear(128 + 128 + 64, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)  # 假设二分类问题\n",
    "\n",
    "    def forward(self, genomics, transcriptomics, metabolomics):\n",
    "        # 基因组网络\n",
    "        x1 = F.relu(self.genomics_fc1(genomics))\n",
    "        x1 = F.relu(self.genomics_fc2(x1))\n",
    "        \n",
    "        # 转录组网络\n",
    "        x2 = F.relu(self.transcript_fc1(transcriptomics))\n",
    "        x2 = F.relu(self.transcript_fc2(x2))\n",
    "        \n",
    "        # 代谢组网络\n",
    "        x3 = F.relu(self.metabol_fc1(metabolomics))\n",
    "        x3 = F.relu(self.metabol_fc2(x3))\n",
    "\n",
    "        # 融合三种模态数据\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "\n",
    "        # 融合后的全连接层\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # 输出层 (softmax 在 loss 中计算)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 20.94676155090332\n",
      "Epoch 2/100, Loss: 54.83552772283554\n",
      "Epoch 3/100, Loss: 10.057543358802796\n",
      "Epoch 4/100, Loss: 0.7026899170875549\n",
      "Epoch 5/100, Loss: 0.6973190450668335\n",
      "Epoch 6/100, Loss: 0.6928165507316589\n",
      "Epoch 7/100, Loss: 0.6944693756103516\n",
      "Epoch 8/100, Loss: 0.6974156284332276\n",
      "Epoch 9/100, Loss: 0.6922244310379029\n",
      "Epoch 10/100, Loss: 0.695613100528717\n",
      "Epoch 11/100, Loss: 0.6915403628349304\n",
      "Epoch 12/100, Loss: 0.6957681703567505\n",
      "Epoch 13/100, Loss: 0.6990368723869324\n",
      "Epoch 14/100, Loss: 0.7004756379127502\n",
      "Epoch 15/100, Loss: 0.7100483918190003\n",
      "Epoch 16/100, Loss: 0.7087678623199463\n",
      "Epoch 17/100, Loss: 0.6971363520622254\n",
      "Epoch 18/100, Loss: 0.6948053312301635\n",
      "Epoch 19/100, Loss: 0.6938735198974609\n",
      "Epoch 20/100, Loss: 0.695681836605072\n",
      "Epoch 21/100, Loss: 0.6923375487327575\n",
      "Epoch 22/100, Loss: 0.700504195690155\n",
      "Epoch 23/100, Loss: 0.6940534663200378\n",
      "Epoch 24/100, Loss: 0.6968497586250305\n",
      "Epoch 25/100, Loss: 0.6928295707702636\n",
      "Epoch 26/100, Loss: 0.6989257454872131\n",
      "Epoch 27/100, Loss: 0.6959793329238891\n",
      "Epoch 28/100, Loss: 0.6933456635475159\n",
      "Epoch 29/100, Loss: 0.6992210912704467\n",
      "Epoch 30/100, Loss: 0.6968233442306518\n",
      "Epoch 31/100, Loss: 0.6959753060340881\n",
      "Epoch 32/100, Loss: 0.696601676940918\n",
      "Epoch 33/100, Loss: 0.692946445941925\n",
      "Epoch 34/100, Loss: 0.702085223197937\n",
      "Epoch 35/100, Loss: 0.6969559621810913\n",
      "Epoch 36/100, Loss: 0.6938698530197144\n",
      "Epoch 37/100, Loss: 0.6991508340835572\n",
      "Epoch 38/100, Loss: 0.6942765116691589\n",
      "Epoch 39/100, Loss: 0.6967732954025269\n",
      "Epoch 40/100, Loss: 0.6980652523040771\n",
      "Epoch 41/100, Loss: 0.6964621186256409\n",
      "Epoch 42/100, Loss: 0.7023824501037598\n",
      "Epoch 43/100, Loss: 0.6996036982536316\n",
      "Epoch 44/100, Loss: 0.6996751117706299\n",
      "Epoch 45/100, Loss: 0.6942368912696838\n",
      "Epoch 46/100, Loss: 0.6935684394836426\n",
      "Epoch 47/100, Loss: 0.6933724498748779\n",
      "Epoch 48/100, Loss: 0.6991984438896179\n",
      "Epoch 49/100, Loss: 0.6959720516204834\n",
      "Epoch 50/100, Loss: 0.6930283093452454\n",
      "Epoch 51/100, Loss: 0.6962665939331054\n",
      "Epoch 52/100, Loss: 0.6955298185348511\n",
      "Epoch 53/100, Loss: 0.6974543595314026\n",
      "Epoch 54/100, Loss: 0.6964155435562134\n",
      "Epoch 55/100, Loss: 0.6923963618278504\n",
      "Epoch 56/100, Loss: 0.6940584421157837\n",
      "Epoch 57/100, Loss: 0.6940929794311523\n",
      "Epoch 58/100, Loss: 0.692244930267334\n",
      "Epoch 59/100, Loss: 0.6955211091041565\n",
      "Epoch 60/100, Loss: 0.6940890693664551\n",
      "Epoch 61/100, Loss: 0.6935715794563293\n",
      "Epoch 62/100, Loss: 0.6945450139045716\n",
      "Epoch 63/100, Loss: 0.6960492849349975\n",
      "Epoch 64/100, Loss: 0.6982914662361145\n",
      "Epoch 65/100, Loss: 0.700158109664917\n",
      "Epoch 66/100, Loss: 0.6945721244812012\n",
      "Epoch 67/100, Loss: 0.7055608606338502\n",
      "Epoch 68/100, Loss: 0.7001825904846192\n",
      "Epoch 69/100, Loss: 0.6948332619667054\n",
      "Epoch 70/100, Loss: 0.6945928001403808\n",
      "Epoch 71/100, Loss: 0.6959373688697815\n",
      "Epoch 72/100, Loss: 0.6945960283279419\n",
      "Epoch 73/100, Loss: 0.6948668599128723\n",
      "Epoch 74/100, Loss: 0.6942522001266479\n",
      "Epoch 75/100, Loss: 0.6976808142662049\n",
      "Epoch 76/100, Loss: 0.6988477826118469\n",
      "Epoch 77/100, Loss: 0.6944482898712159\n",
      "Epoch 78/100, Loss: 0.6940049934387207\n",
      "Epoch 79/100, Loss: 0.6933387780189514\n",
      "Epoch 80/100, Loss: 0.7015858173370362\n",
      "Epoch 81/100, Loss: 0.6948337483406067\n",
      "Epoch 82/100, Loss: 0.6959353876113892\n",
      "Epoch 83/100, Loss: 0.6962678265571595\n",
      "Epoch 84/100, Loss: 0.6959707140922546\n",
      "Epoch 85/100, Loss: 0.7037600779533386\n",
      "Epoch 86/100, Loss: 0.7078559255599975\n",
      "Epoch 87/100, Loss: 0.6986056971549988\n",
      "Epoch 88/100, Loss: 0.6954821085929871\n",
      "Epoch 89/100, Loss: 0.6937740540504456\n",
      "Epoch 90/100, Loss: 0.6924272108078003\n",
      "Epoch 91/100, Loss: 0.6956948852539062\n",
      "Epoch 92/100, Loss: 0.7021110033988953\n",
      "Epoch 93/100, Loss: 0.6964201259613038\n",
      "Epoch 94/100, Loss: 0.6948275518417358\n",
      "Epoch 95/100, Loss: 0.6972855257987977\n",
      "Epoch 96/100, Loss: 0.6960871601104737\n",
      "Epoch 97/100, Loss: 0.696740026473999\n",
      "Epoch 98/100, Loss: 0.6918070650100708\n",
      "Epoch 99/100, Loss: 0.694259696006775\n",
      "Epoch 100/100, Loss: 0.6952045392990113\n",
      "Accuracy: 49.5%\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = MultiModalNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for genomics, transcriptomics, metabolomics, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(genomics, transcriptomics, metabolomics)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# 测试函数\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for genomics, transcriptomics, metabolomics, labels in test_loader:\n",
    "            outputs = model(genomics, transcriptomics, metabolomics)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# 测试模型\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29. <a id='toc29_'></a>[argparse](#toc0_)\n",
    "```python\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Multi-Modal Learning')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "\n",
    "\n",
    "# 创建ArgumentParser对象\n",
    "parser = argparse.ArgumentParser(\n",
    "    prog=\"argparse demo in PyTorch\",\n",
    "    description=\"demo for argparse which is used to parse command-line arguments\",\n",
    "    usage=\"python argparse_demo.py [options]\",\n",
    "    epilog=\"End of ArgumentParser demo\",\n",
    "    add_help=True   # 是否显示帮助信息\n",
    ")\n",
    "\n",
    "# 添加参数\n",
    "parser.add_argument(\n",
    "    '-b',                                               # 短选项   \n",
    "    '--batch_size',                                      # 长选项\n",
    "    type=int,                                           # 参数类型\n",
    "    default=32,                                          # 默认值\n",
    "    required=True,                                    # 是否必须\n",
    "    choices=[4, 8, 16, 32, 64, 128, 256, 512],       # 可选值\n",
    "    help='batch size for training or inference',          # 帮助信息\n",
    "    action='store'                                      # 存储方式\n",
    ")\n",
    "\n",
    "# 解析参数\n",
    "args = parser.parse_args()\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30. <a id='toc30_'></a>[ml_collections](#toc0_)\n",
    "[https://github.com/google/ml_collections](https://github.com/google/ml_collections)  \n",
    "[https://ml-collections.readthedocs.io/en/latest/](https://ml-collections.readthedocs.io/en/latest/)\n",
    "\n",
    "```python\n",
    "# Install\n",
    "pip install ml-collections \n",
    "\n",
    "# Module structure\n",
    "mlc.ConfigDict()                # 可读写\n",
    "mlc.FrozenConfigDict()          # 只读，不可改\n",
    "mlc.FieldReference()            # 变量声明 (占位符)\n",
    "mlc.config_dict()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections as mlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 100\n",
       "learning_rate: 0.01\n",
       "save_dir:\n",
       "  dir_base: ./bs/train/checkpoints\n",
       "  prefix: demo\n",
       "  suffix: .ckpt\n",
       "steps_counter: 0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "演示ml_collections的使用\n",
    "'''\n",
    "\n",
    "\n",
    "# 1. ConfigDict: 以字典方式传参\n",
    "config = mlc.ConfigDict(\n",
    "    {\n",
    "        'learning_rate': 0.01,\n",
    "        'epochs': 100,\n",
    "        'save_dir': {\n",
    "            'dir_base': './bs/train/checkpoints',\n",
    "            'prefix': 'demo', \n",
    "            'suffix': '.ckpt'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. 直接添加\n",
    "config.steps_counter = 0\n",
    "\n",
    "# 以x.x的方式取值\n",
    "config.learning_rate\n",
    "config.epochs\n",
    "config.save_dir\n",
    "config.save_dir.dir_base\n",
    "config.save_dir.prefix\n",
    "config.save_dir.suffix\n",
    "config.steps_counter\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.learning_rate = 0    # 可修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning_rate: 0.001\n",
       "optm: optm"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "FrozenConfigDict不可改动\n",
    "'''\n",
    "\n",
    "\n",
    "fc = mlc.FrozenConfigDict({\n",
    "    'learning_rate': 0.001,\n",
    "    'optm': 'optm'\n",
    "})\n",
    "\n",
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.learning_rate = 0    # 不可修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning_rate: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "  _field_type: !!python/name:builtins.float ''\n",
       "  _ops: []\n",
       "  _required: false\n",
       "  _value: 1.0e-05"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "FieldReference的使用演示\n",
    "'''\n",
    "\n",
    "\n",
    "lr = mlc.FieldReference(default=0.001, field_type=float)\n",
    "gama = mlc.FieldReference(default=0.01, field_type=float)\n",
    "\n",
    "c = mlc.ConfigDict({\n",
    "    'learning_rate': lr * gama  # 只保存逻辑\n",
    "})\n",
    "\n",
    "c.learning_rate\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. <a id='toc31_'></a>[functools](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.1. <a id='toc31_1_'></a>[partial](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 6)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def add_fn(a, b):\n",
    "    return a + b\n",
    "\n",
    "a_add = partial(add_fn, a=1)    # 固定了a的值\n",
    "\n",
    "a_add(b=2), a_add(b=3), a_add(b=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32. <a id='toc32_'></a>[copy](#toc0_)\n",
    "直接赋值：其实就是对象的引用（别名）。\n",
    "\n",
    "浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象。\n",
    "\n",
    "深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.1. <a id='toc32_1_'></a>[列表类型的拷贝](#toc0_)\n",
    "列表**浅拷贝**即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [1, 2, 3])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''浅拷贝'''\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = a.copy()\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 100], [1, 2, 3])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.append(100)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.2. <a id='toc32_2_'></a>[字典类型的拷贝](#toc0_)\n",
    "字典必须**深拷贝**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'k1': [1, 2, 3]}, {'k1': [1, 2, 3]}, {'k1': [1, 2, 3]})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''浅拷贝'''\n",
    "\n",
    "a_dict = {\"k1\": [1, 2, 3]}\n",
    "b_dict = a_dict.copy()                      # copy.copy(a_dict)     浅拷贝\n",
    "c_dict = copy.deepcopy(a_dict)              # copy.deepcopy(a_dict) 深拷贝\n",
    "\n",
    "a_dict, b_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'k1': [1, 2, 3, 100]}, {'k1': [1, 2, 3, 100]}, {'k1': [1, 2, 3]})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dict['k1'].append(100)    # 只改a_dict\n",
    "\n",
    "a_dict, b_dict, c_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33. <a id='toc33_'></a>[转格式](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Learn-Pytorch.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 46 image(s).\n",
      "[NbConvertApp] Writing 4788510 bytes to Format/Learn-Pytorch.html\n"
     ]
    }
   ],
   "source": [
    "# ipynb to html\n",
    "!jupyter nbconvert --to html Learn-Pytorch.ipynb --output-dir=./Format\n",
    "\n",
    "# browse translate html to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Learn-Pytorch.ipynb to markdown\n",
      "[NbConvertApp] Support files will be in Learn-Pytorch_files/\n",
      "[NbConvertApp] Writing 1088323 bytes to Format/Learn-Pytorch.md\n"
     ]
    }
   ],
   "source": [
    "# ipynb to markdown\n",
    "!jupyter nbconvert --to markdown Learn-Pytorch.ipynb --output-dir=./Format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
