**Table of contents**<a id='toc0_'></a>    
- 1. [HuggingFace](#toc1_)    
- 2. [Transformer](#toc2_)    
- 3. [BERT](#toc3_)    
  - 3.1. [tokenizer](#toc3_1_)    
    - 3.1.1. [train a new tokenizer](#toc3_1_1_)    
    - 3.1.2. [make tokenizer to be used in transformers with AutoTokenizer](#toc3_1_2_)    
  - 3.2. [model](#toc3_2_)    
  - 3.3. [datas](#toc3_3_)    
  - 3.4. [trainer](#toc3_4_)    
- 4. [GPT](#toc4_)    
- 5. [T5](#toc5_)    
- 6. [BART](#toc6_)    
- 7. [LLaMa](#toc7_)    
  - 7.1. [Tokenizer](#toc7_1_)    
- 8. [DeepSeek](#toc8_)    
  - 8.1. [R1](#toc8_1_)    
- 9. [ä»€ä¹ˆæ˜¯RAGï¼Ÿ](#toc9_)    
  - 9.1. [æ–‡æœ¬çŸ¥è¯†æ£€ç´¢](#toc9_1_)    
    - 9.1.1. [çŸ¥è¯†åº“æ„å»º](#toc9_1_1_)    
    - 9.1.2. [æŸ¥è¯¢æ„å»º](#toc9_1_2_)    
    - 9.1.3. [å¦‚ä½•æ£€ç´¢ï¼Ÿ-æ–‡æœ¬æ£€ç´¢](#toc9_1_3_)    
    - 9.1.4. [å¦‚ä½•å–‚ç»™å¤§æ¨¡å‹ï¼Ÿ-ç”Ÿæˆå¢å¼º](#toc9_1_4_)    
  - 9.2. [å¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢](#toc9_2_)    
  - 9.3. [åº”ç”¨](#toc9_3_)    
- 10. [éƒ¨ç½²å¤§æ¨¡å‹](#toc10_)    
  - 10.1. [ä¸‹è½½æ¨¡å‹](#toc10_1_)    
  - 10.2. [ollama](#toc10_2_)    
    - 10.2.1. [Install and run model](#toc10_2_1_)    
    - 10.2.2. [API on web port](#toc10_2_2_)    
    - 10.2.3. [Python ollama module](#toc10_2_3_)    
      - 10.2.3.1. [demoï¼šç¿»è¯‘ä¸­æ–‡ä¸ºè‹±æ–‡](#toc10_2_3_1_)    
  - 10.3. [ktransformers](#toc10_3_)    
    - 10.3.1. [Dockerå®‰è£…](#toc10_3_1_)    
    - 10.3.2. [ç¼–è¯‘å®‰è£…](#toc10_3_2_)    
      - 10.3.2.1. [prepare](#toc10_3_2_1_)    
      - 10.3.2.2. [æ–¹å¼ä¸€ï¼špip3 install whl](#toc10_3_2_2_)    
      - 10.3.2.3. [æ–¹å¼äºŒï¼šç¼–è¯‘](#toc10_3_2_3_)    
    - 10.3.3. [è™šæ‹Ÿæœºä¸­ç¼–è¯‘å®‰è£…](#toc10_3_3_)    
    - 10.3.4. [ä½¿ç”¨](#toc10_3_4_)    
- 11. [å¯åŠ¨å­é¢„æµ‹](#toc11_)    
- 12. [è½¬æ ¼å¼](#toc12_)    

<!-- vscode-jupyter-toc-config
	numbering=true
	anchor=true
	flat=false
	minLevel=1
	maxLevel=6
	/vscode-jupyter-toc-config -->
<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->

# 1. <a id='toc1_'></a>[HuggingFace](#toc0_)


```python
import os 


os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```


```bash
%%bash
export HF_ENDPOINT="https://hf-mirror.com"
```

# 2. <a id='toc2_'></a>[Transformer](#toc0_)


```python

```

# 3. <a id='toc3_'></a>[BERT](#toc0_)

## 3.1. <a id='toc3_1_'></a>[tokenizer](#toc0_)

### 3.1.1. <a id='toc3_1_1_'></a>[train a new tokenizer](#toc0_)


```python
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace
from transformers import PreTrainedTokenizerFast, AutoModelForMaskedLM


# åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ WordPiece æ¨¡å‹
tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

# è®¾ç½®è®­ç»ƒå‚æ•°
trainer = WordPieceTrainer(
    vocab_size=30000,        # è¯æ±‡è¡¨å¤§å°
    min_frequency=2,         # æœ€å°è¯é¢‘
    show_progress=True,      # æ˜¾ç¤ºè¿›åº¦
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
)
```


```python
# è®­ç»ƒ
tokenizer.train(files=["data/huggingface/dna_1g.txt"], trainer=trainer)

# ä¿å­˜
tokenizer.save("data/huggingface/dna_wordpiece_dict.json")
```


    ---------------------------------------------------------------------------

    Exception                                 Traceback (most recent call last)

    Cell In[7], line 2
          1 # è®­ç»ƒ
    ----> 2 tokenizer.train(files=["data/huggingface/dna_1g.txt"], trainer=trainer)
          4 # ä¿å­˜
          5 tokenizer.save("data/huggingface/dna_wordpiece_dict.json")


    Exception: No such file or directory (os error 2)


### 3.1.2. <a id='toc3_1_2_'></a>[make tokenizer to be used in transformers with AutoTokenizer](#toc0_)


```python
new_tokenizer = Tokenizer.from_file("data/huggingface/dna_wordpiece_dict.json")

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=new_tokenizer,
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)

# ä¿å­˜
wrapped_tokenizer.save_pretrained("data/huggingface/dna_wordpiece_dict")
```




    ('data/huggingface/dna_wordpiece_dict/tokenizer_config.json',
     'data/huggingface/dna_wordpiece_dict/special_tokens_map.json',
     'data/huggingface/dna_wordpiece_dict/tokenizer.json')




```python
from transformers import AutoTokenizer


# åŠ è½½
tokenizer = AutoTokenizer.from_pretrained("data/huggingface/dna_wordpiece_dict")
#tokenizer.pad_token = tokenizer.eos_token
```


```python
# ç¼–ç 
tokenizer("ATCGGATCG")
```




    {'input_ids': [6, 766, 22, 10], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}




```python
tokenizer
```




    PreTrainedTokenizerFast(name_or_path='data/huggingface/dna_wordpiece_dict', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
    	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    	1: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    	2: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    	3: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    	4: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    }



## 3.2. <a id='toc3_2_'></a>[model](#toc0_)


```python
from transformers import BertConfig, BertForMaskedLM 


# é…ç½®
max_len = 1024 

config = BertConfig(
    vocab_size = len(tokenizer),
    max_position_embeddings=max_len, 
    pad_token_id=tokenizer.pad_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id
) 

# æ¨¡å‹
model = BertForMaskedLM(config=config)
```

    BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
      - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
      - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
      - If you are not the owner of the model architecture class, please contact the model code owner to update it.


## 3.3. <a id='toc3_3_'></a>[datas](#toc0_)


```python
from datasets import load_dataset 


raw_dataset = load_dataset('text', data_files='data/huggingface/dna_1g.txt')
raw_dataset
```




    DatasetDict({
        train: Dataset({
            features: ['text'],
            num_rows: 1079595
        })
    })




```python
dataset = raw_dataset["train"].train_test_split(test_size=0.1, shuffle=True)
dataset
```




    DatasetDict({
        train: Dataset({
            features: ['text'],
            num_rows: 971635
        })
        test: Dataset({
            features: ['text'],
            num_rows: 107960
        })
    })




```python
tokenizer._tokenizer.model.max_input_chars_per_word = 10000


def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_len)


# å¯¹æ•°æ®é›†åº”ç”¨åˆ†è¯å‡½æ•°
tokenized_datasets = dataset.map(tokenize_function, batched=False, remove_columns=['text'], num_proc=50)  # è®¾ç½®ä¸ºä½ çš„ CPU æ ¸å¿ƒæ•°æˆ–æ ¹æ®éœ€è¦è°ƒæ•´

```


    Map (num_proc=50):   0%|          | 0/971635 [00:00<?, ? examples/s]



    Map (num_proc=50):   0%|          | 0/107960 [00:00<?, ? examples/s]



```python
from transformers import DataCollatorForLanguageModeling


# åˆ›å»ºä¸€ä¸ªæ•°æ®æ”¶é›†å™¨ï¼Œç”¨äºåŠ¨æ€å¡«å……å’Œé®è”½,æ³¨æ„mlm=true
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)
```


```python
dataset["train"][0]
```




    {'text': 'GAATATTTGTCTATTCTTCTTAACTTTCTCCACTGTAAATTAAATTGCTCCTCAGGGTGCTATATGGCATCCCTTGCTATTTTTGGAGCAAATCTTAAATTCTTCAACAATTTTATCAAGACAAACACAACTTTCAGTAAATTCATTGTTTAAATTTGGTGAAAAGTCAGATTTCTTTACACATAGTAAAGCAAATGTAAAATAATATATCAATGTGATTCTTTTAATAAAATACCATTATTGCCAATGGTTTTTAATAGTTCACTGTTTGAAAGAGACCACAAAATTCATGTGCAAAAATCACAAGCATTCTTATACAACAGTGACAGACAAACAGAGAGCCAAATCAGGAATGAACTTCCATTCACAATTGCTTCAAAGAGAATCAAATACCTAGGAATCCAACTTACAAGGGATGTAAAGGACCTCTTCAAGGAGAACTACAAACCACTGCTCAGTGAAATAAAAGAGGACACAAACAAATGGAAGAACATACCATGCTCATGGATAGGAAGAATCAATATCGTGAAAATGGCCATACTGCCCAAGGTAATTTATAGATTCAATGCCATCCCCATCAAGCTACCAATGAGTTTCTTCACAGAATTGGAAAAAACTGTTTTAAAGTTCATATGGAACCAAAAAAGAACCCACATTGCCAAGACAATCCTAAGTCAAATGAACAAAGCTGGAGGGATCATGCTACCTGACTTCAAACTATACTACAAGGCTACAGTAACCAAAATAGCATGGTACTGGTACCAAAACAGAAATATAGACCAATGGAACAGCATAGAGTCCTCAGAAATAATACCACACATCTACATCTTTGATAAATCTGACAAAAACAAGAAATGGGGAAAGGATTCTCTATATAATAAATGGTGCTGGGAAAATTGGCTAGCCATAAGTAGAAAGCTGAAACTGGATCCTTTCCTTACTCTTTATACGAAAATTAATTCAAGATGGAGTAGAGACTTAAATGTTAGACCTAATACCA'}




```python
tokenizer.tokenize(dataset["train"][0]["text"][:100])
```




    ['GAA',
     '##TATTTG',
     '##TCTATT',
     '##CTTCTTAA',
     '##CTTTCTCC',
     '##A',
     '##CTGTAAATT',
     '##AAATT',
     '##GCTCC',
     '##TCAGG',
     '##GTGCTA',
     '##TATGGCA',
     '##TCCCTT',
     '##GCTATTTT',
     '##TGGAGCAA',
     '##A',
     '##TCTTAAA',
     '##T']



## 3.4. <a id='toc3_4_'></a>[trainer](#toc0_)


```python
from transformers import TrainingArguments, Trainer


run_path = "cache/bert_run"
train_epoches = 5
batch_size = 2


training_args = TrainingArguments(
        output_dir=run_path,
        overwrite_output_dir=True,
        num_train_epochs=train_epoches,
        per_device_train_batch_size=batch_size,
        save_steps=2000,
        save_total_limit=2,
        prediction_loss_only=True,
        fp16=True, #v100æ²¡æ³•ç”¨
    )


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    data_collator=data_collator,
)
```

    Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.



```python
trainer.train()
trainer.save_model("cache/dna_bert_v0")
```

# 4. <a id='toc4_'></a>[GPT](#toc0_)


```python

```

# 5. <a id='toc5_'></a>[T5](#toc0_)


```python

```

# 6. <a id='toc6_'></a>[BART](#toc0_)


```python

```

# 7. <a id='toc7_'></a>[LLaMa](#toc0_)

## 7.1. <a id='toc7_1_'></a>[Tokenizer](#toc0_)


```python
import sentencepiece as spm


spm.SentencePieceTrainer.train(
    input="data/huggingface/dna_1g.txt,data/huggingface/protein_1g.txt", 
    model_prefix="dna_llama", 
    vocab_size=60000, 
    model_type="bpe", 
    # max_sentence_length=1000000,
    num_threads=50, 
)
```


```python
tokenizer = spm.SentencePieceProcessor(model_file="dna_llama.model")

tokenizer.encode("ATCGGATCG")

```

# 8. <a id='toc8_'></a>[DeepSeek](#toc0_)

## 8.1. <a id='toc8_1_'></a>[R1](#toc0_)


```python
# Use a pipeline as a high-level helper
from transformers import pipeline


messages = [
    {"role": "user", "content": "Who are you?"},
]

pipe = pipeline("text-generation", model="deepseek-ai/DeepSeek-R1", trust_remote_code=True)

pipe(messages)
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    Cell In[5], line 9
          2 from transformers import pipeline
          5 messages = [
          6     {"role": "user", "content": "Who are you?"},
          7 ]
    ----> 9 pipe = pipeline("text-generation", model="deepseek-ai/DeepSeek-R1", trust_remote_code=True)
         11 pipe(messages)


    File ~/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/__init__.py:895, in pipeline(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
        893 if isinstance(model, str) or framework is None:
        894     model_classes = {"tf": targeted_task["tf"], "pt": targeted_task["pt"]}
    --> 895     framework, model = infer_framework_load_model(
        896         model,
        897         model_classes=model_classes,
        898         config=config,
        899         framework=framework,
        900         task=task,
        901         **hub_kwargs,
        902         **model_kwargs,
        903     )
        905 model_config = model.config
        906 hub_kwargs["_commit_hash"] = model.config._commit_hash


    File ~/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py:296, in infer_framework_load_model(model, config, model_classes, task, framework, **model_kwargs)
        294         for class_name, trace in all_traceback.items():
        295             error += f"while loading with {class_name}, an error is thrown:\n{trace}\n"
    --> 296         raise ValueError(
        297             f"Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\n\n{error}\n"
        298         )
        300 if framework is None:
        301     framework = infer_framework(model.__class__)


    ValueError: Could not load model deepseek-ai/DeepSeek-R1 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:
    
    while loading with AutoModelForCausalLM, an error is thrown:
    Traceback (most recent call last):
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
        response = self._make_request(
                   ^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 490, in _make_request
        raise new_e
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 466, in _make_request
        self._validate_conn(conn)
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1095, in _validate_conn
        conn.connect()
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py", line 652, in connect
        sock_and_verified = _ssl_wrap_socket_and_match_hostname(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py", line 805, in _ssl_wrap_socket_and_match_hostname
        ssl_sock = ssl_wrap_socket(
                   ^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 465, in ssl_wrap_socket
        ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 509, in _ssl_wrap_socket_impl
        return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py", line 455, in wrap_socket
        return self.sslsocket_class._create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py", line 1042, in _create
        self.do_handshake()
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py", line 1320, in do_handshake
        self._sslobj.do_handshake()
    ConnectionResetError: [Errno 104] Connection reset by peer
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
        resp = conn.urlopen(
               ^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
        retries = retries.increment(
                  ^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
        raise reraise(type(error), error, _stacktrace)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
        raise value.with_traceback(tb)
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
        response = self._make_request(
                   ^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 490, in _make_request
        raise new_e
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 466, in _make_request
        self._validate_conn(conn)
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py", line 1095, in _validate_conn
        conn.connect()
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py", line 652, in connect
        sock_and_verified = _ssl_wrap_socket_and_match_hostname(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py", line 805, in _ssl_wrap_socket_and_match_hostname
        ssl_sock = ssl_wrap_socket(
                   ^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 465, in ssl_wrap_socket
        ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py", line 509, in _ssl_wrap_socket_impl
        return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py", line 455, in wrap_socket
        return self.sslsocket_class._create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py", line 1042, in _create
        self.do_handshake()
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/ssl.py", line 1320, in do_handshake
        self._sslobj.do_handshake()
    urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/pipelines/base.py", line 283, in infer_framework_load_model
        model = model_class.from_pretrained(model, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 559, in from_pretrained
        return model_class.from_pretrained(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3589, in from_pretrained
        if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/utils/hub.py", line 655, in has_file
        response = get_session().head(
                   ^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py", line 624, in head
        return self.request("HEAD", url, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
        resp = self.send(prep, **send_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
        r = adapter.send(request, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 93, in send
        return super().send(request, *args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/bmp/backup/zhaosy/miniconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py", line 682, in send
        raise ConnectionError(err, request=request)
    requests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 787d5459-e5a7-4279-8e49-e516f6e6aa22)')
    
    




```python
# Load model directly
from transformers import AutoModelForCausalLM


model = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-R1", trust_remote_code=True)
```

# 9. <a id='toc9_'></a>[ä»€ä¹ˆæ˜¯RAGï¼Ÿ](#toc0_)

RAGçš„åˆ†ç±»ï¼š

|Model | æ£€ç´¢å™¨å¾®è°ƒ | å¤§é¢„è¨€æ¨¡å‹å¾®è°ƒ| ä¾‹å¦‚ |
|---|---|---| --- |
| é»‘ç›’ | - | - | e.g. In-context ralm |
| é»‘ç›’ | æ˜¯ | - | e.g. Rplug |
| ç™½ç›’ | - | æ˜¯ | e.g. realm, self-rag |
| ç™½ç›’ | æ˜¯ | æ˜¯ | e.g. altas |

## 9.1. <a id='toc9_1_'></a>[æ–‡æœ¬çŸ¥è¯†æ£€ç´¢](#toc0_)
å¦‚ä½•æ£€ç´¢å‡ºç›¸å…³ä¿¡æ¯æ¥è¾…åŠ©æ”¹å–„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè´¨é‡çš„ç³»ç»Ÿã€‚çŸ¥è¯†æ£€ç´¢é€šå¸¸åŒ…æ‹¬çŸ¥è¯†åº“æ„å»ºã€æŸ¥è¯¢æ„å»ºã€æ–‡æœ¬æ£€ç´¢å’Œæ£€ç´¢ç»“æœé‡æ’å››éƒ¨åˆ†ã€‚

### 9.1.1. <a id='toc9_1_1_'></a>[çŸ¥è¯†åº“æ„å»º](#toc0_)
æ–‡æœ¬å—çš„çŸ¥è¯†åº“æ„å»ºï¼Œå¦‚ç»´åŸºç™¾ç§‘ã€æ–°é—»ã€è®ºæ–‡ç­‰ã€‚

æ–‡æœ¬åˆ†å—ï¼šå°†æ–‡æœ¬åˆ†æˆå¤šä¸ªå—ï¼Œæ¯ä¸ªå—åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå¥å­ã€‚
- å›ºå®šå¤§å°å—ï¼šå°†æ–‡æœ¬åˆ†æˆå›ºå®šå¤§å°çš„å—ï¼Œå¦‚æ¯ä¸ªå—åŒ…å«512ä¸ªå­—ç¬¦ã€‚
- åŸºäºå†…å®¹å—ï¼šå°†æ–‡æœ¬åˆ†æˆåŸºäºå†…å®¹çš„å—ï¼Œå¦‚æ¯ä¸ªå—åŒ…å«ä¸€ä¸ªå¥å­ã€‚
  - é€šè¿‡å¥å­åˆ†å‰²ç¬¦åˆ†å‰²å¥å­ã€‚
  - ç”¨LLMè¿›è¡Œåˆ†å‰²

çŸ¥è¯†åº“å¢å¼ºï¼šçŸ¥è¯†åº“å¢å¼ºæ˜¯é€šè¿‡æ”¹è¿›å’Œä¸°å¯ŒçŸ¥è¯†åº“çš„å†…å®¹å’Œç»“æ„ï¼Œä¸ºæŸ¥è¯¢æä¾›"æŠ“æ‰‹â€ï¼ŒåŒ…æ‹¬æŸ¥è¯¢ç”Ÿæˆä¸æ ‡é¢˜ç”Ÿæˆä¸¤ç§æ–¹æ³•ã€‚
- ä¼ªæŸ¥è¯¢ç”Ÿæˆ
- æ ‡é¢˜ç”Ÿæˆ

### 9.1.2. <a id='toc9_1_2_'></a>[æŸ¥è¯¢æ„å»º](#toc0_)
æŸ¥è¯¢æ„å»ºï¼šæ—¨åœ¨é€šè¿‡æŸ¥è¯¢å¢å¼ºçš„æ–¹å¼ï¼Œæ‰©å±•å’Œä¸°å¯Œç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰å’Œå†…å®¹ï¼Œæé«˜æ£€ç´¢ç»“æœçš„å‡†ç¡®æ€§å’Œå…¨é¢æ€§ï¼Œâ€œé’©"å‡ºç›¸åº”å†…å®¹ã€‚å¢å¼ºæ–¹å¼å¯åˆ†ä¸ºè¯­ä¹‰å¢å¼ºä¸å†…å®¹å¢å¼ºã€‚
- è¯­ä¹‰å¢å¼ºï¼šåŒä¸€å¥è¯å¤šç§è¡¨è¾¾æ–¹å¼
- å†…å®¹å¢å¼ºï¼šå¢åŠ èƒŒæ™¯çŸ¥è¯†

### 9.1.3. <a id='toc9_1_3_'></a>[å¦‚ä½•æ£€ç´¢ï¼Ÿ-æ–‡æœ¬æ£€ç´¢](#toc0_)
`æ£€ç´¢å™¨`ï¼šç»™å®šçŸ¥è¯†åº“å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œæ–‡æœ¬æ£€ç´¢æ—¨åœ¨æ‰¾åˆ°çŸ¥è¯†åº“ä¸­ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„çŸ¥è¯†æ–‡æœ¬;æ£€ç´¢æ•ˆç‡å¢å¼ºæ—¨åœ¨è§£å†³æ£€ç´¢æ—¶çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚æ‰€ä»¥æ£€ç´¢è´¨é‡ã€æ£€ç´¢æ•ˆç‡å¾ˆé‡è¦ã€‚å¸¸è§æ£€ç´¢å™¨æœ‰ä¸‰ç±»ï¼š
- åˆ¤åˆ«å¼æ£€ç´¢å™¨ï¼š
  - ç¨€ç–æ£€ç´¢å™¨ï¼Œe.g. TF-IDF
  - åŒå‘ç¼–ç æ£€ç´¢å™¨ï¼Œe.g. ç”¨berté¢„å…ˆå°†æ–‡æœ¬å—è¿›è¡Œç¼–ç æˆå‘é‡
  - äº¤å‰ç¼–ç æ£€ç´¢å™¨ï¼Œe.g. 
- ç”Ÿæˆå¼æ£€ç´¢å™¨ï¼šå™¨ç›´æ¥å°†çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ä¿¡æ¯è®°å¿†åœ¨æ¨¡å‹å‚æ•°ä¸­ã€‚ç„¶åï¼Œåœ¨æ¥æ”¶åˆ°æŸ¥è¯¢è¯·æ±‚æ—¶ï¼Œèƒ½å¤Ÿç›´æ¥ç”Ÿæˆç›¸å…³æ–‡æ¡£çš„æ ‡è¯†ç¬¦å¤ºï¼ˆå³Doc IDï¼‰ï¼Œä»¥å®Œæˆæ£€ç´¢ã€‚
- å›¾æ£€ç´¢å™¨ï¼šå›¾æ£€ç´¢å™¨çš„çŸ¥è¯†åº“ä¸ºå›¾æ•°æ®åº“ï¼ŒåŒ…æ‹¬å¼€æ”¾çŸ¥è¯†å›¾è°±å’Œè‡ªå»ºå›¾ä¸¤ç§ï¼Œå®ƒä»¬ä¸€èˆ¬ç”±<ä¸»ä½“ã€è°“è¯å’Œå®¢ä½“>ä¸‰å…ƒç»„æ„æˆã€‚è¿™æ ·åšä¸ä»…å¯ä»¥æ•æ‰æ¦‚å¿µé—´çš„è¯­ä¹‰å…³ç³»ï¼Œè¿˜å…è®¸äººç±»å’Œæœºå™¨å¯ä»¥å…±åŒå¯¹çŸ¥è¯†è¿›è¡Œç†è§£ä¸æ¨ç†ã€‚

`é‡æ’å™¨`ï¼šæ£€ç´¢é˜¶æ®µä¸ºäº†ä¿è¯æ£€ç´¢é€Ÿåº¦é€šå¸¸ä¼šæŸå¤±ä¸€å®šçš„æ€§èƒ½ï¼Œå¯èƒ½æ£€ç´¢åˆ°è´¨é‡è¾ƒä½çš„æ–‡æ¡£ã€‚é‡æ’çš„ç›®çš„æ˜¯å¯¹æ£€ç´¢åˆ°çš„æ®µè½è¿›è¡Œè¿›ä¸€æ­¥çš„æ’åºç²¾é€‰ã€‚é‡æ’å¯ä»¥åˆ†ä¸ºåŸºäºäº¤å‰ç¼–ç çš„æ–¹æ³•å’ŒåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–¹æ³•ã€‚

### 9.1.4. <a id='toc9_1_4_'></a>[å¦‚ä½•å–‚ç»™å¤§æ¨¡å‹ï¼Ÿ-ç”Ÿæˆå¢å¼º](#toc0_)
RAGå¢å¼ºæ¯”è¾ƒï¼š

|æ¶æ„åˆ†ç±»|ä¼˜ç‚¹|ç¼ºç‚¹|
|-|-|-|
|è¾“å…¥ç«¯prompt|ç®€å•|tokenså¤ªå¤š|
|ä¸­é—´å±‚|é«˜æ•ˆ|è€—GPUèµ„æº|
|è¾“å‡ºç«¯|-|-|

## 9.2. <a id='toc9_2_'></a>[å¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢](#toc0_)
## 9.3. <a id='toc9_3_'></a>[åº”ç”¨](#toc0_)
å¯¹è¯æœºå™¨äººã€çŸ¥è¯†åº“æ–‡ç­”...

# 10. <a id='toc10_'></a>[éƒ¨ç½²å¤§æ¨¡å‹](#toc0_)


## 10.1. <a id='toc10_1_'></a>[ä¸‹è½½æ¨¡å‹](#toc0_)


```python
# https://huggingface.co/deepseek-ai/DeepSeek-R1
hfd.sh deepseek-ai/DeepSeek-R1 -x 10 -j 10 

# https://huggingface.co/unsloth/DeepSeek-R1-GGUF
hfd.sh unsloth/DeepSeek-R1-GGUF -x 10 -j 10 --include DeepSeek-R1-Q8_0

```

## 10.2. <a id='toc10_2_'></a>[ollama](#toc0_)
### 10.2.1. <a id='toc10_2_1_'></a>[Install and run model](#toc0_)


```python
# start the serve
ollama serve

# list all model images
ollama list 

# run model from image
ollama run model_card
```

### 10.2.2. <a id='toc10_2_2_'></a>[API on web port](#toc0_)
communicatation with local model via web port.

`generate` and `chat`.


```bash
%%bash
curl http://localhost:11434/api/generate -d '{
  "model": "deepseek-r1:7b",
  "prompt": "Who are you?",
  "stream": false,
  "options": {
    "temperature": 0.6
  },
  "format": "json"
}'
```

      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   456  100   315  100   141    142     64  0:00:02  0:00:02 --:--:--   206


    {"model":"deepseek-r1:7b","created_at":"2025-02-18T02:49:32.744934023Z","response":"{\"}\u003cthink\u003e{\"\n\n\n\n\n\n\n\n\n\n:\n\n{\n\n}\n\n}\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n","done":false}


```bash
%%bash
curl http://localhost:11434/api/chat -d '{
  "model": "deepseek-r1:7b",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ],
  "stream": false
}'
```

      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100  5034    0  4905  100   129    326      8  0:00:16  0:00:15  0:00:01   719


    {"model":"deepseek-r1:7b","created_at":"2025-02-18T02:49:26.56791501Z","message":{"role":"assistant","content":"\u003cthink\u003e\nOkay, so I just read that \"Why is the sky blue?\" and now I'm trying to figure it out myself. Let me think through this step by step.\n\nFirst off, when you look at the sky on a clear day, it's usually blue, especially during the day when the sun is out. But sometimes I've seen it turn other colors too, like red in the evening or during sunrise. So why is it mostly blue?\n\nI know that light travels through the atmosphere, but how does it get colored? I remember learning about something called Rayleigh scattering from my science class. Let me try to recall what that was about. Rayleigh scattering involves light interacting with particles much smaller than the wavelength of light itself. When sunlight enters the Earth's atmosphere, it reaches tiny molecules in the air, like nitrogen and oxygen.\n\nWait, so these small particles scatter the sunlight in all directions. But why does this result in a blue sky? I think it has something to do with the wavelengths of light. Visible light ranges from violet to red, right? And I remember that violet light has a shorter wavelength than blue. So maybe the shorter wavelengths are scattered more.\n\nBut if blue is scattered more, wouldn't it be easier to see at night when there's less atmosphere overhead? Hmm, but then why does the sky turn red in the evening or during sunrise?\n\nOh right! During sunrise and sunset, the light has to pass through a much thicker layer of atmosphere. That makes sense because we're looking at the light after it's been scattered through more particles. The longer path means that all the shorter wavelengths (like violet) are scattered out, leaving red to dominate because it has a longer wavelength.\n\nSo during the day, when the sun is directly overhead, blue and green wavelengths get scattered away by Rayleigh scattering, making the sky appear blue. But in the early morning or late afternoon, as the sun is near the horizon, the light has to pass through more atmosphere, so red comes through because it's not scattered as much.\n\nWait, but isn't there also something called Mie scattering? I think that happens when particles are larger than the wavelength of light. Does that affect the color of the sky too?\n\nI believe Mie scattering is more significant for larger particles, like dust or droplets in clouds. So it might cause some effects we see during sunrise and sunset, but not as much as Rayleigh does for small molecules.\n\nSo to summarize: The sky appears blue on a clear day because blue light scatters more in the atmosphere due to Rayleigh scattering by tiny gas molecules. During sunrise and sunset, the longer path allows red light to dominate, giving the sky its reddish hues.\n\nBut wait, what about when we see other colors? I mean, sometimes during the day it's not just blue; I've seen green or yellow in some places. Is that because of Rayleigh scattering changing as the atmosphere gets thicker?\n\nOr maybe it's due to other factors like pollution or particles in the air affecting light differently. That might complicate things.\n\nAlso, does humidity play a role? Sometimes when it's humid, does the sky appear clearer instead of blue? I think higher humidity can affect how light scatters because more water vapor means more molecules to scatter off.\n\nSo maybe on days with high humidity, the Rayleigh scattering is less effective, making the sky look different. But that's probably a secondary effect compared to the basic reason for the color being blue.\n\nIn any case, I think the primary reason is Rayleigh scattering by nitrogen and oxygen in the atmosphere causing shorter wavelengths like blue to scatter more, resulting in a blue sky during clear days.\n\u003c/think\u003e\n\nThe sky appears blue primarily due to a phenomenon known as Rayleigh scattering. When sunlight enters Earth's atmosphere, it interacts with tiny molecules of nitrogen and oxygen. These particles scatter shorter wavelengths of light, such as blue and violet, which have shorter wavelengths than red or orange. This scattering is more effective for shorter wavelengths, causing the sky to appear blue during the day.\n\nDuring sunrise and sunset, the light passes through a thicker layer of atmosphere, where all shorter wavelengths are scattered away, leaving longer wavelengths like red to dominate, resulting in the reddish hues observed at these times.\n\nOther factors such as humidity, pollution, or atmospheric particles can influence how light scatters, but the primary reason for the sky's blue color remains Rayleigh scattering by nitrogen and oxygen molecules."},"done_reason":"stop","done":true,"total_duration":15024265818,"load_duration":22057058,"prompt_eval_count":9,"prompt_eval_duration":7000000,"eval_count":901,"eval_duration":14993000000}

### 10.2.3. <a id='toc10_2_3_'></a>[Python ollama module](#toc0_)


```python
import ollama


texts = '''
è¯¦ç»†æ¯”è¾ƒdeepseekæ¯å…¬å¸å’ŒopenAIå…¬å¸çš„åŒºåˆ«
'''

# model_card = "deepseek-r1:7b"
model_card = "modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF"

# æ–¹å¼ä¸€ï¼ˆéæµå¼è¾“å‡ºï¼‰ï¼š
# outputs = ollama.generate(model_card, inputs)
# print(f'{outputs['response']}')

# æ–¹å¼äºŒï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š
outputs = ollama.generate(
    stream=True,
    model=model_card,
    prompt=texts,
)
for chunk in outputs:
    if not chunk['done']:
        print(f'{chunk['response']}', end='', flush=True)
```

#### 10.2.3.1. <a id='toc10_2_3_1_'></a>[demoï¼šç¿»è¯‘ä¸­æ–‡ä¸ºè‹±æ–‡](#toc0_)


```python
import ollama 


class zh2en():
    def __init__(self, model_card):
        self.model_card = model_card
        
    def build_prompt(self, texts):
        # with open(prompt_template_path, 'r') as f:
        #     prompt_template = f.read()
        #     # str with replace function
        #     prompt = prompt_template.replace(var, texts)
        prompt_template = """
        ä¸“ä¸šç¿»è¯‘ï¼š\n
        ---\n
        {Chinese_words} \n
        --- \n
        ä½œä¸ºç¿»è¯‘ä¸“å®¶ï¼Œå°†ä¸Šè¿°ä¸­æ–‡å‡†ç¡®ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚ \n
        """
        prompt = prompt_template.replace("{Chinese_words}", texts)
        return prompt

    def translate(self, texts):
        prompt = self.build_prompt(texts = texts)
        # key step
        outputs = ollama.generate(
            stream=True,
            model=self.model_card,
            prompt=prompt,
        )
        for chunk in outputs:
            if not chunk['done']:
                print(f'{chunk['response']}', end='', flush=True)
            else:
                print('âš¡')


translater = zh2en(model_card='modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF')

translater.translate('åŸºäºæ·±åº¦å­¦ä¹ çš„å¯¹æ¯è‰èŠ½èƒæ†èŒèŠ½èƒå½¢æˆç›¸å…³åŸºå› çš„ç ”ç©¶ã€‚')
translater.translate('é€šè¿‡å®åŸºå› ç»„ç ”ç©¶å¾®ç”Ÿç‰©ä¸æ¤ç‰©ç›¸äº’ä½œç”¨çš„æœºåˆ¶ã€‚')
```

    <think>
    å—¯ï¼Œé¦–å…ˆæˆ‘è¦ç†è§£è¿™ä¸ªé¢˜ç›®çš„æ„æ€ã€‚â€œåŸºäºæ·±åº¦å­¦ä¹ â€æŒ‡çš„æ˜¯ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥è¿›è¡Œç ”ç©¶ã€‚è€Œâ€œå¯¹æ¯è‰èŠ½èƒæ†èŒèŠ½èƒå½¢æˆç›¸å…³åŸºå› çš„ç ”ç©¶â€åˆ™æ˜¯å…·ä½“çš„ç ”ç©¶å†…å®¹ï¼Œæ¶‰åŠåˆ°æ¯è‰èŠ½èƒæ†èŒåœ¨å½¢æˆèŠ½èƒè¿‡ç¨‹ä¸­ç›¸å…³çš„åŸºå› ã€‚
    
    æˆ‘éœ€è¦æŠŠè¿™æ•´ä¸ªå¥å­å‡†ç¡®åœ°ç¿»è¯‘æˆè‹±æ–‡ã€‚é¦–å…ˆï¼Œâ€œåŸºäºæ·±åº¦å­¦ä¹ â€å¯ä»¥ç›´æ¥ç¿»è¯‘ä¸ºâ€œBased on deep learningâ€ã€‚æ¥ä¸‹æ¥æ˜¯â€œç ”ç©¶â€ï¼Œå¯¹åº”çš„è‹±æ–‡æ˜¯â€œstudyâ€ã€‚ç„¶åæ˜¯â€œæ¯è‰èŠ½èƒæ†èŒâ€ï¼Œè¿™ä¸ªåº”è¯¥æ˜¯ä¸€ä¸ªä¸“æœ‰åè¯ï¼Œå¯èƒ½éœ€è¦æŸ¥ä¸€ä¸‹æ­£ç¡®çš„è‹±è¯‘åç§°ï¼Œæ¯”å¦‚â€œBacillus subtilisâ€ã€‚
    
    æ¥ç€æ˜¯â€œèŠ½èƒå½¢æˆç›¸å…³åŸºå› â€ï¼Œè¿™éƒ¨åˆ†å¯ä»¥ç¿»è¯‘ä¸ºâ€œgenes related to spore formationâ€ã€‚æœ€åï¼ŒæŠŠæ•´ä¸ªå¥å­è¿è´¯èµ·æ¥ï¼Œå°±æ˜¯â€œBased on deep learning study of genes related to spore formation in Bacillus subtilis.â€
    
    è¿™æ ·ç»„åˆèµ·æ¥ï¼Œæ—¢å‡†ç¡®ä¼ è¾¾äº†åŸæ„ï¼Œåˆç¬¦åˆè‹±æ–‡çš„è¡¨è¾¾ä¹ æƒ¯ã€‚æˆ‘è§‰å¾—è¿™ä¸ªç¿»è¯‘åº”è¯¥æ˜¯æ¯”è¾ƒä¸“ä¸šå’Œå‡†ç¡®çš„ã€‚
    </think>
    
    Study of Genes Related to Spore Formation in *Bacillus subtilis* Based on Deep Learningâš¡
    <think>
    å¥½çš„ï¼Œé¦–å…ˆæˆ‘è¦ç†è§£ç”¨æˆ·çš„éœ€æ±‚ã€‚ä»–ç»™äº†ä¸€ä¸ªä¸­è‹±å¯¹ç…§çš„å¥å­ï¼Œè¦æ±‚ä¸“ä¸šç¿»è¯‘ï¼Œå¹¶ä¸”éœ€è¦å°†ä¸­æ–‡å¥å­â€œé€šè¿‡å®åŸºå› ç»„ç ”ç©¶å¾®ç”Ÿç‰©ä¸æ¤ç‰©ç›¸äº’ä½œç”¨çš„æœºåˆ¶ã€‚â€å‡†ç¡®åœ°ç¿»è¯‘æˆè‹±æ–‡ã€‚
    
    æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦åˆ†æåŸæ–‡çš„æ„æ€ã€‚å¥å­çš„ä¸»å¹²æ˜¯â€œé€šè¿‡å®åŸºå› ç»„ç ”ç©¶â€¦â€ï¼Œè¿™é‡Œçš„å…³é”®è¯æœ‰â€œå®åŸºå› ç»„â€ã€â€œå¾®ç”Ÿç‰©â€ã€â€œæ¤ç‰©â€ä»¥åŠâ€œç›¸äº’ä½œç”¨çš„æœºåˆ¶â€ã€‚æ‰€ä»¥ï¼Œé¦–å…ˆè¦ç¡®å®šè¿™äº›æœ¯è¯­åœ¨è‹±æ–‡ä¸­çš„å‡†ç¡®å¯¹åº”è¯ã€‚
    
    â€œå®åŸºå› ç»„â€é€šå¸¸ç¿»è¯‘ä¸ºâ€œmetagenomeâ€æˆ–è€…â€œmeta-genomicsâ€ï¼Œä½†æ›´å¸¸è§çš„æ˜¯ä½¿ç”¨â€œmetagenomicsâ€æ¥è¡¨ç¤ºè¿™ä¸€ç ”ç©¶é¢†åŸŸã€‚å› æ­¤ï¼Œè¿™é‡Œé€‰æ‹©â€œmetagenomicsâ€ä½œä¸ºç¿»è¯‘ã€‚
    
    ç„¶åï¼Œâ€œé€šè¿‡â€¦ç ”ç©¶â€¦â€çš„ç»“æ„åœ¨è‹±æ–‡ä¸­å¯ä»¥ç”¨â€œthroughâ€æˆ–è€…â€œby means ofâ€æ¥è¡¨è¾¾ï¼Œä½†ä¸ºäº†ç®€æ´å’Œä¸“ä¸šï¼Œç›´æ¥ä½¿ç”¨â€œThroughâ€æ¯”è¾ƒåˆé€‚ã€‚
    
    æ¥ä¸‹æ¥æ˜¯â€œå¾®ç”Ÿç‰©ä¸æ¤ç‰©ç›¸äº’ä½œç”¨çš„æœºåˆ¶â€ã€‚è¿™é‡Œéœ€è¦æ³¨æ„è¯­åºå’Œç”¨è¯ã€‚æ•´ä½“ç»“æ„åº”è¯¥æ˜¯â€œthe mechanisms underlying the interactions between microorganisms and plants.â€ è¿™æ ·ä¸ä»…æ¸…æ™°ï¼Œè€Œä¸”ç¬¦åˆå­¦æœ¯å†™ä½œçš„è§„èŒƒã€‚
    
    æœ€åï¼ŒæŠŠè¿™äº›éƒ¨åˆ†ç»„åˆèµ·æ¥ï¼Œç¡®ä¿å¥å­é€šé¡ºä¸”å‡†ç¡®ã€‚æ‰€ä»¥ï¼Œæœ€ç»ˆç¿»è¯‘ä¸ºï¼šâ€œThrough metagenomics research on the mechanisms of interaction between microorganisms and plants.â€
    
    åœ¨æ•´ä¸ªæ€è€ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘æ³¨æ„åˆ°ç”¨æˆ·å¯èƒ½æ˜¯åœ¨æ’°å†™å­¦æœ¯è®ºæ–‡æˆ–è€…å‡†å¤‡ç ”ç©¶æŠ¥å‘Šï¼Œå› æ­¤å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§æ˜¯å…³é”®ã€‚æ­¤å¤–ï¼Œä¿æŒå¥å­çš„ç®€æ´ä¹Ÿæ˜¯å¿…è¦çš„ï¼Œä»¥ä¾¿è¯»è€…èƒ½å¤Ÿå¿«é€Ÿç†è§£å†…å®¹ã€‚
    
    æœ€åï¼Œå†æ£€æŸ¥ä¸€éç¿»è¯‘æ˜¯å¦å¿ å®äºåŸæ–‡ï¼Œå¹¶ä¸”ç¬¦åˆè‹±è¯­è¡¨è¾¾ä¹ æƒ¯ã€‚ç¡®è®¤æ— è¯¯åï¼Œå°±å¯ä»¥å°†è¿™ä¸ªç¿»è¯‘ç»“æœæä¾›ç»™ç”¨æˆ·äº†ã€‚
    </think>
    
    Through metagenomics research on the mechanisms of interaction between microorganisms and plants.âš¡


## 10.3. <a id='toc10_3_'></a>[ktransformers](#toc0_)


### 10.3.1. <a id='toc10_3_1_'></a>[Dockerå®‰è£…](#toc0_)

[https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md](https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md)


```python
# pull the image from docker hub 
# about 19 GB
# docker pull approachingai/ktransformers:0.1.1
docker pull approachingai/ktransformers:0.2.1

# docker run \
#     --gpus all \
#     -v /path/to/models:/models \
#     -p 10002:10002 \
#     approachingai/ktransformers:v0.1.1 \
#     --port 10002 \
#     --gguf_path /models/path/to/gguf_path \
#     --model_path /models/path/to/model_path \
#     --web True

# Directly run
docker run  \
    -v /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai:/models \
    -p 10002:10002 \
    approachingai/ktransformers:0.1.1 \
    --port 10002 \
    --model_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1 \
    --gguf_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1-Q4_K_M_GGUF \
    --web True

# or
docker run \
    # -d \
    --gpus all \
    -it \
    -p 10002:10002 \
    approachingai/ktransformers:0.1.1 \
    /bin/bash 

## and then
docker exec -it container_ID /bin/bash
```

QA:

- Q: Dockerè¿è¡Œåå‡ºç° Illegal instruction (core dumped)æŠ¥é”™
  - [https://github.com/kvcache-ai/ktransformers/issues/356](https://github.com/kvcache-ai/ktransformers/issues/356)
  - é‡æ–°ç¼–è¯‘ä»¥ä¸‹:
    ```bash
    USE_NUMA=1
    bash install.sh
    ```

### 10.3.2. <a id='toc10_3_2_'></a>[ç¼–è¯‘å®‰è£…](#toc0_)

[https://kvcache-ai.github.io/ktransformers/en/install.html](https://kvcache-ai.github.io/ktransformers/en/install.html)

#### 10.3.2.1. <a id='toc10_3_2_1_'></a>[prepare](#toc0_)


```python
name="ktransformers"

conda create -n $name python=3.11 -y 
conda activate $name


# Install CudaToolkit and nvcc ...
conda install nvidia/label/cuda-12.4.0::cuda -y --channel nvidia/label/cuda-12.4.0

# Anaconda provides a package called `libstdcxx-ng` that includes a newer version of `libstdc++`, which can be installed via `conda-forge`.
conda install -c conda-forge libstdcxx-ng -y 

strings ~/miniconda3/envs/${name}/lib/libstdc++.so.6 | grep GLIBCXX


# Install PyTorch via pip ...
# pip3 install torch torchvision torchaudio
conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -c pytorch -y

# pip3 install packaging ninja cpufeature numpy
conda install conda-forge::ninja conda-forge::packaging anaconda::numpy -y


```

#### 10.3.2.2. <a id='toc10_3_2_2_'></a>[æ–¹å¼ä¸€ï¼špip3 install whl](#toc0_)


```python
# ktransformers
pip3 install ktransformers-0.2.1.post1+cu124torch24avx2-cp311-cp311-linux_x86_64.whl

# flash_attn
# pip3 install flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp311-cp311-linux_x86_64.whl
pip3 install flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

# flashinfer
pip3 install flashinfer_python-0.2.2+cu124torch2.4-cp38-abi3-linux_x86_64.whl
python -c "import torch; print(torch.cuda.get_device_capability())"
export TORCH_CUDA_ARCH_LIST="8.0"
```

#### 10.3.2.3. <a id='toc10_3_2_3_'></a>[æ–¹å¼äºŒï¼šç¼–è¯‘](#toc0_)


```python
# Make sure your system has dual sockets and double size RAM than the model's size (e.g. 1T RAM for 512G model)
export USE_NUMA=1
bash install.sh # or `make dev_install`
```

### 10.3.3. <a id='toc10_3_3_'></a>[è™šæ‹Ÿæœºä¸­ç¼–è¯‘å®‰è£…](#toc0_)

ç”±äº`GLIBCxxx`æŠ¥é”™ï¼Œæ”¹ç”¨è™šæ‹Ÿæœºä¸­ubuntuå®‰è£…ktransformers


```python
# æ‹‰å–åŒ…å«GLIBC 2.29+çš„é•œåƒï¼ˆå¦‚Ubuntu 20.04ï¼‰
docker pull ubuntu:20.04

# å¯åŠ¨å®¹å™¨å¹¶æŒ‚è½½é¡¹ç›®ç›®å½• 
# docker run -it -v /path/to/your/code:/app ubuntu:20.04 /bin/bash 
docker run \
    --gpus all \
    -p 10002:10002\
    -it \
    ubuntu:20.04 /bin/bash 

# Install wget and then 
apt-get update 
apt-get install wget 

wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh

docker exec --gpus all -it ubuntu:20.04 /bin/bash
```

### 10.3.4. <a id='toc10_3_4_'></a>[ä½¿ç”¨](#toc0_)


```python
# å°†ä¸»æœºå’Œå®¹å™¨çš„10002ç«¯å£åšæ˜ å°„
docker run --gpus all -p 10002:10002 -it \
    ubuntu_2004:ktransformers /bin/bash


# æ£€æŸ¥æ˜ å°„
docker port [å®¹å™¨ID]
```


```python
# As we use dual socket, we set cpu_infer to 65
python -m ktransformers.local_chat \
    --model_path DeepSeek-R1 \
    --gguf_path DeepSeek-R1-Q4_K_M_GGUF \
    --cpu_infer 60 \
    --max_new_tokens 10000 \
    --cache_lens 50000 \
    --total_context 50000 \
    --cache_q4 true \
    --temperature 0.6 \
    --top_p 0.95 \
    --force_think \
    --use_cuda_graph \
    --port 10002
    # --optimize_config_path ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml \
    # --host 127.0.0.1 \
```


```python
# http://localhost:10002/web/index.html#/chat
ktransformers \
    --model_path DeepSeek-R1/ \
    --gguf_path DeepSeek-R1-Q4_K_M_GGUF/ \
    --port 10002 \
    --web True
```


```bash
%%bash
curl -X 'GET' \
  'http://localhost:10002/api/tags' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "messages": [
    {
      "content": "tell a joke",
      "role": "user"
    }
  ],
  "model": "DeepSeek-R1",
  "stream": true
}'
```

      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   213  100    81  100   132  40500  66000 --:--:-- --:--:-- --:--:--  104k


    {"models":[{"name":"DeepSeek-Coder-V2-Instruct","modified_at":"123","size":123}]}


```bash
%%bash
curl -X 'POST' \
  'http://localhost:10002/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "messages": [
    {
      "content": "tell a joke",
      "role": "user"
    }
  ],
  "model": "DeepSeek-R1",
  "stream": true
}'
```


```bash
%%bash
curl -X 'POST' \
  'http://localhost:10002/api/generate' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d "{
  "model": "DeepSeek-R1",
  "prompt": "tell me a joke",
  "stream": true
}"
```

# 11. <a id='toc11_'></a>[å¯åŠ¨å­é¢„æµ‹](#toc0_)


```python

```

# 12. <a id='toc12_'></a>[è½¬æ ¼å¼](#toc0_)


```bash
%%bash
# ipynb to html
jupyter nbconvert \
    --to html LLMs.ipynb \
    --output-dir=./Format/LLMs \
    # --NbConvertApp.log_level=ERROR

cp -rf Pytorch_Pictures ./Format/LLMs/
# browse translate html to pdf
```

    [NbConvertApp] Converting notebook LLMs.ipynb to html
    [NbConvertApp] Writing 405891 bytes to Format/LLMs/LLMs.html



```python
# ipynb to markdown
!jupyter nbconvert --to markdown LLMs.ipynb --output-dir=./Format/LLMs/
```

    [NbConvertApp] Converting notebook LLMs.ipynb to markdown
    [NbConvertApp] Writing 39952 bytes to Format/LLMs/LLMs.md

