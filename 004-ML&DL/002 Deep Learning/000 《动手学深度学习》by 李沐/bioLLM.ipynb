{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Scientific HuggingFace](#toc1_)    \n",
    "- 2. [datas](#toc2_)    \n",
    "- 3. [Tokenizer](#toc3_)    \n",
    "  - 3.1. [Training a new tokenizer](#toc3_1_)    \n",
    "  - 3.2. [Using a pre-trained tokenizer](#toc3_2_)    \n",
    "    - 3.2.1. [Áõ¥Êé•Âä†ËΩΩ](#toc3_2_1_)    \n",
    "    - 3.2.2. [Âú®Transformers‰∏≠‰ΩøÁî®](#toc3_2_2_)    \n",
    "      - 3.2.2.1. [Â∞ÅË£Ö](#toc3_2_2_1_)    \n",
    "      - 3.2.2.2. [Âä†ËΩΩ](#toc3_2_2_2_)    \n",
    "- 4. [BERT](#toc4_)    \n",
    "  - 4.1. [datas](#toc4_1_)    \n",
    "  - 4.2. [tokenizer](#toc4_2_)    \n",
    "    - 4.2.1. [train a new tokenizer](#toc4_2_1_)    \n",
    "    - 4.2.2. [make tokenizer to be used in transformers with AutoTokenizer](#toc4_2_2_)    \n",
    "  - 4.3. [trainer](#toc4_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Scientific HuggingFace](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[datas](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f242f2befe4b1ea5d72d506afcee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/360 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c4e8015e714b59bcb43bb338906a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a63403dd73c4ae8954c39bde3ee7ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets \n",
    "\n",
    "\n",
    "datas = datasets.load_dataset('dnagpt/dna_promoters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Training a new tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# ÂàùÂßãÂåñ‰∏Ä‰∏™BPEÊ®°Âûã \n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# ËÆæÁΩÆÈ¢ÑÂ§ÑÁêÜ\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False) #use_regex=False,Á©∫Ê†ºÂΩìÊàê‰∏ÄËà¨Â≠óÁ¨¶‰∏≤\n",
    "# ËÆæÁΩÆËÆ≠ÁªÉÂô®\n",
    "trainer = trainers.BpeTrainer(vocab_size=30000, special_tokens=[\"<|endoftext|>\"]) #3w words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ËÆ≠ÁªÉ\n",
    "tokenizer.train([\"data/huggingface/dna_1g.txt\"], trainer=trainer) #all file list, take 10-20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TG', 'GCGTGAA', 'CCCGG', 'GATCGG', 'G']\n"
     ]
    }
   ],
   "source": [
    "# ÁºñÁ†Å\n",
    "encoding = tokenizer.encode(\"TGGCGTGAACCCGGGATCGGG\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TG GCGTGAA CCCGG GATCGG G\n"
     ]
    }
   ],
   "source": [
    "# Ëß£Á†Å\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰øùÂ≠ò\n",
    "tokenizer.save(\"data/huggingface/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Using a pre-trained tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. <a id='toc3_2_1_'></a>[Áõ¥Êé•Âä†ËΩΩ](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TG', 'GCGTGAA', 'CCCGG', 'GATCGG', 'G']\n",
      "TG GCGTGAA CCCGG GATCGG G\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "# Âä†ËΩΩËá™ÂÆö‰πâÁöÑtokenizer\n",
    "tokenizer = Tokenizer.from_file(\"data/huggingface/tokenizer.json\")\n",
    "\n",
    "# ÁºñÁ†Å\n",
    "encoding = tokenizer.encode(\"TGGCGTGAACCCGGGATCGGG\")\n",
    "print(encoding.tokens)\n",
    "\n",
    "# Ëß£Á†Å\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. <a id='toc3_2_2_'></a>[Âú®Transformers‰∏≠‰ΩøÁî®](#toc0_)\n",
    "‰∏∫‰∫ÜËÉΩÂ§ü‰ªéAutoTokenizer‰∏≠Ë∞ÉÁî®„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.1. <a id='toc3_2_2_1_'></a>[Â∞ÅË£Ö](#toc0_)\n",
    "Ë¶ÅÂú® ü§ó Transformers ‰∏≠‰ΩøÁî®Ëøô‰∏™Ê†áËÆ∞Âô®ÔºåÊàë‰ª¨ÂøÖÈ°ªÂ∞ÜÂÆÉÂåÖË£πÂú®‰∏Ä‰∏™ PreTrainedTokenizerFast Á±ª‰∏≠„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/huggingface/dna_bpe_dict/tokenizer_config.json',\n",
       " 'data/huggingface/dna_bpe_dict/special_tokens_map.json',\n",
       " 'data/huggingface/dna_bpe_dict/vocab.json',\n",
       " 'data/huggingface/dna_bpe_dict/merges.txt',\n",
       " 'data/huggingface/dna_bpe_dict/added_tokens.json',\n",
       " 'data/huggingface/dna_bpe_dict/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "dna_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "dna_tokenizer.save_pretrained(\"data/huggingface/dna_bpe_dict\")\n",
    "\n",
    "# dna_tokenizer.push_to_hub(\"dna_bpe_dict_1g\", organization=\"dnagpt\", use_auth_token=\"hf_*****\") # push to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.2. <a id='toc3_2_2_2_'></a>[Âä†ËΩΩ](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# ÊàêÂäü\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/huggingface/dna_bpe_dict\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. <a id='toc4_2_'></a>[tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. <a id='toc4_2_1_'></a>[train a new tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "# ÂàùÂßãÂåñ‰∏Ä‰∏™Á©∫ÁöÑ WordPiece Ê®°Âûã\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30000,        # ËØçÊ±áË°®Â§ßÂ∞è\n",
    "    min_frequency=2,         # ÊúÄÂ∞èËØçÈ¢ë\n",
    "    show_progress=True,      # ÊòæÁ§∫ËøõÂ∫¶\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function train:\n",
      "\n",
      "train(self, files, trainer=None) method of tokenizers.Tokenizer instance\n",
      "    Train the Tokenizer using the given files.\n",
      "\n",
      "    Reads the files line by line, while keeping all the whitespace, even new lines.\n",
      "    If you want to train from data store in-memory, you can check\n",
      "    :meth:`~tokenizers.Tokenizer.train_from_iterator`\n",
      "\n",
      "    Args:\n",
      "        files (:obj:`List[str]`):\n",
      "            A list of path to the files that we should use for training\n",
      "\n",
      "        trainer (:obj:`~tokenizers.trainers.Trainer`, `optional`):\n",
      "            An optional trainer that should be used to train our Model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ËÆ≠ÁªÉ\n",
    "tokenizer.train(files=[\"data/huggingface/dna_1g.txt\"], trainer=trainer)\n",
    "\n",
    "# ‰øùÂ≠ò\n",
    "tokenizer.save(\"data/huggingface/dna_wordpiece_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. <a id='toc4_2_2_'></a>[make tokenizer to be used in transformers with AutoTokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/huggingface/dna_wordpiece_dict/tokenizer_config.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/special_tokens_map.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer = Tokenizer.from_file(\"data/huggingface/dna_wordpiece_dict.json\")\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=new_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# ‰øùÂ≠ò\n",
    "wrapped_tokenizer.save_pretrained(\"data/huggingface/dna_wordpiece_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Âä†ËΩΩ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/huggingface/dna_wordpiece_dict\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6, 766, 22, 10], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÁºñÁ†Å\n",
    "tokenizer(\"ATCGGATCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='data/huggingface/dna_wordpiece_dict', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM \n",
    "\n",
    "\n",
    "# ÈÖçÁΩÆ\n",
    "max_len = 1024 \n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size = len(tokenizer),\n",
    "    max_position_embeddings=max_len, \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ") \n",
    "\n",
    "# Ê®°Âûã\n",
    "model = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1079595\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "\n",
    "raw_dataset = load_dataset('text', data_files='data/huggingface/dna_1g.txt')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 971635\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 107960\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6371b499b174ff89c4e7bb54315a9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/971635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065019d3ba1c46c598849b00771e8a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/107960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer._tokenizer.model.max_input_chars_per_word = 10000\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_len)\n",
    "\n",
    "\n",
    "# ÂØπÊï∞ÊçÆÈõÜÂ∫îÁî®ÂàÜËØçÂáΩÊï∞\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=False, remove_columns=['text'], num_proc=50)  # ËÆæÁΩÆ‰∏∫‰Ω†ÁöÑ CPU Ê†∏ÂøÉÊï∞ÊàñÊ†πÊçÆÈúÄË¶ÅË∞ÉÊï¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "# ÂàõÂª∫‰∏Ä‰∏™Êï∞ÊçÆÊî∂ÈõÜÂô®ÔºåÁî®‰∫éÂä®ÊÄÅÂ°´ÂÖÖÂíåÈÅÆËîΩ,Ê≥®ÊÑèmlm=true\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GAATATTTGTCTATTCTTCTTAACTTTCTCCACTGTAAATTAAATTGCTCCTCAGGGTGCTATATGGCATCCCTTGCTATTTTTGGAGCAAATCTTAAATTCTTCAACAATTTTATCAAGACAAACACAACTTTCAGTAAATTCATTGTTTAAATTTGGTGAAAAGTCAGATTTCTTTACACATAGTAAAGCAAATGTAAAATAATATATCAATGTGATTCTTTTAATAAAATACCATTATTGCCAATGGTTTTTAATAGTTCACTGTTTGAAAGAGACCACAAAATTCATGTGCAAAAATCACAAGCATTCTTATACAACAGTGACAGACAAACAGAGAGCCAAATCAGGAATGAACTTCCATTCACAATTGCTTCAAAGAGAATCAAATACCTAGGAATCCAACTTACAAGGGATGTAAAGGACCTCTTCAAGGAGAACTACAAACCACTGCTCAGTGAAATAAAAGAGGACACAAACAAATGGAAGAACATACCATGCTCATGGATAGGAAGAATCAATATCGTGAAAATGGCCATACTGCCCAAGGTAATTTATAGATTCAATGCCATCCCCATCAAGCTACCAATGAGTTTCTTCACAGAATTGGAAAAAACTGTTTTAAAGTTCATATGGAACCAAAAAAGAACCCACATTGCCAAGACAATCCTAAGTCAAATGAACAAAGCTGGAGGGATCATGCTACCTGACTTCAAACTATACTACAAGGCTACAGTAACCAAAATAGCATGGTACTGGTACCAAAACAGAAATATAGACCAATGGAACAGCATAGAGTCCTCAGAAATAATACCACACATCTACATCTTTGATAAATCTGACAAAAACAAGAAATGGGGAAAGGATTCTCTATATAATAAATGGTGCTGGGAAAATTGGCTAGCCATAAGTAGAAAGCTGAAACTGGATCCTTTCCTTACTCTTTATACGAAAATTAATTCAAGATGGAGTAGAGACTTAAATGTTAGACCTAATACCA'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GAA',\n",
       " '##TATTTG',\n",
       " '##TCTATT',\n",
       " '##CTTCTTAA',\n",
       " '##CTTTCTCC',\n",
       " '##A',\n",
       " '##CTGTAAATT',\n",
       " '##AAATT',\n",
       " '##GCTCC',\n",
       " '##TCAGG',\n",
       " '##GTGCTA',\n",
       " '##TATGGCA',\n",
       " '##TCCCTT',\n",
       " '##GCTATTTT',\n",
       " '##TGGAGCAA',\n",
       " '##A',\n",
       " '##TCTTAAA',\n",
       " '##T']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(dataset[\"train\"][0][\"text\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. <a id='toc4_3_'></a>[trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "run_path = \"cache/bert_run\"\n",
    "train_epoches = 5\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=run_path,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=train_epoches,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        save_steps=2000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        fp16=True, #v100Ê≤°Ê≥ïÁî®\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"cache/dna_bert_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"data/huggingface/dna_1g.txt,data/huggingface/protein_1g.txt\", \n",
    "    model_prefix=\"dna_llama\", \n",
    "    vocab_size=60000, \n",
    "    model_type=\"bpe\", \n",
    "    # max_sentence_length=1000000,\n",
    "    num_threads=50, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file=\"dna_llama.model\")\n",
    "\n",
    "tokenizer.encode(\"ATCGGATCG\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
