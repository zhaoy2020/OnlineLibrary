{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Scientific HuggingFace](#toc1_)    \n",
    "- 2. [datas](#toc2_)    \n",
    "- 3. [Tokenizer](#toc3_)    \n",
    "  - 3.1. [Training a new tokenizer](#toc3_1_)    \n",
    "  - 3.2. [Using a pre-trained tokenizer](#toc3_2_)    \n",
    "    - 3.2.1. [直接加载](#toc3_2_1_)    \n",
    "    - 3.2.2. [在Transformers中使用](#toc3_2_2_)    \n",
    "      - 3.2.2.1. [封装](#toc3_2_2_1_)    \n",
    "      - 3.2.2.2. [加载](#toc3_2_2_2_)    \n",
    "- 4. [BERT](#toc4_)    \n",
    "  - 4.1. [tokenizer](#toc4_1_)    \n",
    "    - 4.1.1. [train a new tokenizer](#toc4_1_1_)    \n",
    "    - 4.1.2. [make tokenizer to be used in transformers with AutoTokenizer](#toc4_1_2_)    \n",
    "  - 4.2. [model](#toc4_2_)    \n",
    "  - 4.3. [datas](#toc4_3_)    \n",
    "  - 4.4. [trainer](#toc4_4_)    \n",
    "- 5. [LLaMa](#toc5_)    \n",
    "  - 5.1. [Tokenizer](#toc5_1_)    \n",
    "- 6. [DeepSeek](#toc6_)    \n",
    "  - 6.1. [R1](#toc6_1_)    \n",
    "- 7. [什么是RAG？](#toc7_)    \n",
    "  - 7.1. [文本知识检索](#toc7_1_)    \n",
    "    - 7.1.1. [知识库构建](#toc7_1_1_)    \n",
    "    - 7.1.2. [查询构建](#toc7_1_2_)    \n",
    "    - 7.1.3. [如何检索？-文本检索](#toc7_1_3_)    \n",
    "    - 7.1.4. [如何喂给大模型？-生成增强](#toc7_1_4_)    \n",
    "  - 7.2. [多模态知识检索](#toc7_2_)    \n",
    "  - 7.3. [应用](#toc7_3_)    \n",
    "- 8. [部署大模型](#toc8_)    \n",
    "  - 8.1. [ollama](#toc8_1_)    \n",
    "    - 8.1.1. [Install and run model](#toc8_1_1_)    \n",
    "    - 8.1.2. [API on web port](#toc8_1_2_)    \n",
    "    - 8.1.3. [Python ollama module](#toc8_1_3_)    \n",
    "      - 8.1.3.1. [demo：翻译中文为英文](#toc8_1_3_1_)    \n",
    "  - 8.2. [ktransformers](#toc8_2_)    \n",
    "    - 8.2.1. [DeepSeek-R1_Q4_K_M with ktransformers docker container](#toc8_2_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Scientific HuggingFace](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[datas](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f242f2befe4b1ea5d72d506afcee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/360 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c4e8015e714b59bcb43bb338906a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a63403dd73c4ae8954c39bde3ee7ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets \n",
    "\n",
    "\n",
    "datas = datasets.load_dataset('dnagpt/dna_promoters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Training a new tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# 初始化一个BPE模型 \n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# 设置预处理\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False) #use_regex=False,空格当成一般字符串\n",
    "# 设置训练器\n",
    "trainer = trainers.BpeTrainer(vocab_size=30000, special_tokens=[\"<|endoftext|>\"]) #3w words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "tokenizer.train([\"data/huggingface/dna_1g.txt\"], trainer=trainer) #all file list, take 10-20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TG', 'GCGTGAA', 'CCCGG', 'GATCGG', 'G']\n"
     ]
    }
   ],
   "source": [
    "# 编码\n",
    "encoding = tokenizer.encode(\"TGGCGTGAACCCGGGATCGGG\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TG GCGTGAA CCCGG GATCGG G\n"
     ]
    }
   ],
   "source": [
    "# 解码\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "tokenizer.save(\"data/huggingface/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Using a pre-trained tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. <a id='toc3_2_1_'></a>[直接加载](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TG', 'GCGTGAA', 'CCCGG', 'GATCGG', 'G']\n",
      "TG GCGTGAA CCCGG GATCGG G\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "# 加载自定义的tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"data/huggingface/tokenizer.json\")\n",
    "\n",
    "# 编码\n",
    "encoding = tokenizer.encode(\"TGGCGTGAACCCGGGATCGGG\")\n",
    "print(encoding.tokens)\n",
    "\n",
    "# 解码\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. <a id='toc3_2_2_'></a>[在Transformers中使用](#toc0_)\n",
    "为了能够从AutoTokenizer中调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.1. <a id='toc3_2_2_1_'></a>[封装](#toc0_)\n",
    "要在 🤗 Transformers 中使用这个标记器，我们必须将它包裹在一个 PreTrainedTokenizerFast 类中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/huggingface/dna_bpe_dict/tokenizer_config.json',\n",
       " 'data/huggingface/dna_bpe_dict/special_tokens_map.json',\n",
       " 'data/huggingface/dna_bpe_dict/vocab.json',\n",
       " 'data/huggingface/dna_bpe_dict/merges.txt',\n",
       " 'data/huggingface/dna_bpe_dict/added_tokens.json',\n",
       " 'data/huggingface/dna_bpe_dict/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "dna_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "dna_tokenizer.save_pretrained(\"data/huggingface/dna_bpe_dict\")\n",
    "\n",
    "# dna_tokenizer.push_to_hub(\"dna_bpe_dict_1g\", organization=\"dnagpt\", use_auth_token=\"hf_*****\") # push to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.2. <a id='toc3_2_2_2_'></a>[加载](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# 成功\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/huggingface/dna_bpe_dict\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. <a id='toc4_1_'></a>[tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. <a id='toc4_1_1_'></a>[train a new tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "# 初始化一个空的 WordPiece 模型\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# 设置训练参数\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30000,        # 词汇表大小\n",
    "    min_frequency=2,         # 最小词频\n",
    "    show_progress=True,      # 显示进度\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function train:\n",
      "\n",
      "train(self, files, trainer=None) method of tokenizers.Tokenizer instance\n",
      "    Train the Tokenizer using the given files.\n",
      "\n",
      "    Reads the files line by line, while keeping all the whitespace, even new lines.\n",
      "    If you want to train from data store in-memory, you can check\n",
      "    :meth:`~tokenizers.Tokenizer.train_from_iterator`\n",
      "\n",
      "    Args:\n",
      "        files (:obj:`List[str]`):\n",
      "            A list of path to the files that we should use for training\n",
      "\n",
      "        trainer (:obj:`~tokenizers.trainers.Trainer`, `optional`):\n",
      "            An optional trainer that should be used to train our Model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "tokenizer.train(files=[\"data/huggingface/dna_1g.txt\"], trainer=trainer)\n",
    "\n",
    "# 保存\n",
    "tokenizer.save(\"data/huggingface/dna_wordpiece_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. <a id='toc4_1_2_'></a>[make tokenizer to be used in transformers with AutoTokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/huggingface/dna_wordpiece_dict/tokenizer_config.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/special_tokens_map.json',\n",
       " 'data/huggingface/dna_wordpiece_dict/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer = Tokenizer.from_file(\"data/huggingface/dna_wordpiece_dict.json\")\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=new_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# 保存\n",
    "wrapped_tokenizer.save_pretrained(\"data/huggingface/dna_wordpiece_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# 加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/huggingface/dna_wordpiece_dict\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6, 766, 22, 10], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 编码\n",
    "tokenizer(\"ATCGGATCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='data/huggingface/dna_wordpiece_dict', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. <a id='toc4_2_'></a>[model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM \n",
    "\n",
    "\n",
    "# 配置\n",
    "max_len = 1024 \n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size = len(tokenizer),\n",
    "    max_position_embeddings=max_len, \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ") \n",
    "\n",
    "# 模型\n",
    "model = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. <a id='toc4_3_'></a>[datas](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1079595\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "\n",
    "raw_dataset = load_dataset('text', data_files='data/huggingface/dna_1g.txt')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 971635\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 107960\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6371b499b174ff89c4e7bb54315a9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/971635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065019d3ba1c46c598849b00771e8a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/107960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer._tokenizer.model.max_input_chars_per_word = 10000\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_len)\n",
    "\n",
    "\n",
    "# 对数据集应用分词函数\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=False, remove_columns=['text'], num_proc=50)  # 设置为你的 CPU 核心数或根据需要调整\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "# 创建一个数据收集器，用于动态填充和遮蔽,注意mlm=true\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GAATATTTGTCTATTCTTCTTAACTTTCTCCACTGTAAATTAAATTGCTCCTCAGGGTGCTATATGGCATCCCTTGCTATTTTTGGAGCAAATCTTAAATTCTTCAACAATTTTATCAAGACAAACACAACTTTCAGTAAATTCATTGTTTAAATTTGGTGAAAAGTCAGATTTCTTTACACATAGTAAAGCAAATGTAAAATAATATATCAATGTGATTCTTTTAATAAAATACCATTATTGCCAATGGTTTTTAATAGTTCACTGTTTGAAAGAGACCACAAAATTCATGTGCAAAAATCACAAGCATTCTTATACAACAGTGACAGACAAACAGAGAGCCAAATCAGGAATGAACTTCCATTCACAATTGCTTCAAAGAGAATCAAATACCTAGGAATCCAACTTACAAGGGATGTAAAGGACCTCTTCAAGGAGAACTACAAACCACTGCTCAGTGAAATAAAAGAGGACACAAACAAATGGAAGAACATACCATGCTCATGGATAGGAAGAATCAATATCGTGAAAATGGCCATACTGCCCAAGGTAATTTATAGATTCAATGCCATCCCCATCAAGCTACCAATGAGTTTCTTCACAGAATTGGAAAAAACTGTTTTAAAGTTCATATGGAACCAAAAAAGAACCCACATTGCCAAGACAATCCTAAGTCAAATGAACAAAGCTGGAGGGATCATGCTACCTGACTTCAAACTATACTACAAGGCTACAGTAACCAAAATAGCATGGTACTGGTACCAAAACAGAAATATAGACCAATGGAACAGCATAGAGTCCTCAGAAATAATACCACACATCTACATCTTTGATAAATCTGACAAAAACAAGAAATGGGGAAAGGATTCTCTATATAATAAATGGTGCTGGGAAAATTGGCTAGCCATAAGTAGAAAGCTGAAACTGGATCCTTTCCTTACTCTTTATACGAAAATTAATTCAAGATGGAGTAGAGACTTAAATGTTAGACCTAATACCA'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GAA',\n",
       " '##TATTTG',\n",
       " '##TCTATT',\n",
       " '##CTTCTTAA',\n",
       " '##CTTTCTCC',\n",
       " '##A',\n",
       " '##CTGTAAATT',\n",
       " '##AAATT',\n",
       " '##GCTCC',\n",
       " '##TCAGG',\n",
       " '##GTGCTA',\n",
       " '##TATGGCA',\n",
       " '##TCCCTT',\n",
       " '##GCTATTTT',\n",
       " '##TGGAGCAA',\n",
       " '##A',\n",
       " '##TCTTAAA',\n",
       " '##T']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(dataset[\"train\"][0][\"text\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. <a id='toc4_4_'></a>[trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "run_path = \"cache/bert_run\"\n",
    "train_epoches = 5\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=run_path,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=train_epoches,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        save_steps=2000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        fp16=True, #v100没法用\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"cache/dna_bert_v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[LLaMa](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. <a id='toc5_1_'></a>[Tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"data/huggingface/dna_1g.txt,data/huggingface/protein_1g.txt\", \n",
    "    model_prefix=\"dna_llama\", \n",
    "    vocab_size=60000, \n",
    "    model_type=\"bpe\", \n",
    "    # max_sentence_length=1000000,\n",
    "    num_threads=50, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file=\"dna_llama.model\")\n",
    "\n",
    "tokenizer.encode(\"ATCGGATCG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[DeepSeek](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. <a id='toc6_1_'></a>[R1](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c6f5f30e844fd5a8922e065acd1ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee715dfaf0cf491bbffed492592c3765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  71%|#######1  | 3.07G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496ebe69cfd7465895deab93781effff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  71%|#######1  | 3.07G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cfcbea58cd43879237c4eff71f3134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  71%|#######1  | 3.07G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a458e767a8be4ae5a65c7369f0fbe2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  73%|#######3  | 3.15G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758beb60f6c644e8b8128833645d8757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  73%|#######3  | 3.16G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95084f648f2400c934fce0375602c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  75%|#######5  | 3.23G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bcd2179c974d6f967599b4cae777e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  75%|#######5  | 3.23G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19069b4fe22f43c49ef63b2e5cef8c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  83%|########2 | 3.55G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf-mirror.com/repos/e7/f7/e7f7b8810f2020d7ff50a46aef578773eecb7386ccba95924d21eae90685f990/d6f299f7b410b9a7806927b5d2d413fae1f2c1dfa340bb0037d02d220cd8c080?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00007-of-000163.safetensors%3B+filename%3D%22model-00007-of-000163.safetensors%22%3B&Expires=1739353834&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTM1MzgzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U3L2Y3L2U3ZjdiODgxMGYyMDIwZDdmZjUwYTQ2YWVmNTc4NzczZWVjYjczODZjY2JhOTU5MjRkMjFlYWU5MDY4NWY5OTAvZDZmMjk5ZjdiNDEwYjlhNzgwNjkyN2I1ZDJkNDEzZmFlMWYyYzFkZmEzNDBiYjAwMzdkMDJkMjIwY2Q4YzA4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=uWn2eRLEE%7EkEPseLsNFZx3nYDabBpqrL2gJZdKix4fRMvtXUj-QBn8R4yfwVaxb%7EzgsgIh2jRpAy6BLf1bEfzJv1SByB3-z4bCnf8OhuOM81SM2u5kO-CDNjGdbPADY6HfMFKRioqgbFlgd6PAIC6eGNUtM6B5jHJxa9yzKxEKU9PRM9O0JDJPH4IvYT-6SmKqEyDG2pZKPAojQm9FJNAytHHXordFtbH8gmhhposRmfXk8mGxEeSYGuhlDX6A129aP8hoSEosrX30-dbEZk1uVLnvIMofNS0gML2pJ7wdRcu7vJSqegpAyifcNW4FlJFDMw1Xqz7-LAzKdpX3n64g__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344791e3ca5e41e799fdd04c30b10910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-000163.safetensors:  83%|########2 | 3.55G/4.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)\n",
    "\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[什么是RAG？](#toc0_)\n",
    "\n",
    "RAG的分类：\n",
    "\n",
    "|Model | 检索器微调 | 大预言模型微调| 例如 |\n",
    "|---|---|---| --- |\n",
    "| 黑盒 | - | - | e.g. In-context ralm |\n",
    "| 黑盒 | 是 | - | e.g. Rplug |\n",
    "| 白盒 | - | 是 | e.g. realm, self-rag |\n",
    "| 白盒 | 是 | 是 | e.g. altas |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. <a id='toc7_1_'></a>[文本知识检索](#toc0_)\n",
    "如何检索出相关信息来辅助改善大语言模型生成质量的系统。知识检索通常包括知识库构建、查询构建、文本检索和检索结果重排四部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. <a id='toc7_1_1_'></a>[知识库构建](#toc0_)\n",
    "文本块的知识库构建，如维基百科、新闻、论文等。\n",
    "\n",
    "文本分块：将文本分成多个块，每个块包含一个或多个句子。\n",
    "- 固定大小块：将文本分成固定大小的块，如每个块包含512个字符。\n",
    "- 基于内容块：将文本分成基于内容的块，如每个块包含一个句子。\n",
    "  - 通过句子分割符分割句子。\n",
    "  - 用LLM进行分割\n",
    "\n",
    "知识库增强：知识库增强是通过改进和丰富知识库的内容和结构，为查询提供\"抓手”，包括查询生成与标题生成两种方法。\n",
    "- 伪查询生成\n",
    "- 标题生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. <a id='toc7_1_2_'></a>[查询构建](#toc0_)\n",
    "查询构建：旨在通过查询增强的方式，扩展和丰富用户查询的语义和内容，提高检索结果的准确性和全面性，“钩\"出相应内容。增强方式可分为语义增强与内容增强。\n",
    "- 语义增强：同一句话多种表达方式\n",
    "- 内容增强：增加背景知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3. <a id='toc7_1_3_'></a>[如何检索？-文本检索](#toc0_)\n",
    "`检索器`：给定知识库和用户查询，文本检索旨在找到知识库中与用户查询相关的知识文本;检索效率增强旨在解决检索时的性能瓶颈问题。所以检索质量、检索效率很重要。常见检索器有三类：\n",
    "- 判别式检索器：\n",
    "  - 稀疏检索器，e.g. TF-IDF\n",
    "  - 双向编码检索器，e.g. 用bert预先将文本块进行编码成向量\n",
    "  - 交叉编码检索器，e.g. \n",
    "- 生成式检索器：器直接将知识库中的文档信息记忆在模型参数中。然后，在接收到查询请求时，能够直接生成相关文档的标识符夺（即Doc ID），以完成检索。\n",
    "- 图检索器：图检索器的知识库为图数据库，包括开放知识图谱和自建图两种，它们一般由<主体、谓词和客体>三元组构成。这样做不仅可以捕捉概念间的语义关系，还允许人类和机器可以共同对知识进行理解与推理。\n",
    "\n",
    "`重排器`：检索阶段为了保证检索速度通常会损失一定的性能，可能检索到质量较低的文档。重排的目的是对检索到的段落进行进一步的排序精选。重排可以分为基于交叉编码的方法和基于上下文学习的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4. <a id='toc7_1_4_'></a>[如何喂给大模型？-生成增强](#toc0_)\n",
    "RAG增强比较：\n",
    "\n",
    "|架构分类|优点|缺点|\n",
    "|-|-|-|\n",
    "|输入端prompt|简单|tokens太多|\n",
    "|中间层|高效|耗GPU资源|\n",
    "|输出端|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. <a id='toc7_2_'></a>[多模态知识检索](#toc0_)\n",
    "## 7.3. <a id='toc7_3_'></a>[应用](#toc0_)\n",
    "对话机器人、知识库文答..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[部署大模型](#toc0_)\n",
    "## 8.1. <a id='toc8_1_'></a>[ollama](#toc0_)\n",
    "### 8.1.1. <a id='toc8_1_1_'></a>[Install and run model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the serve\n",
    "ollama serve\n",
    "\n",
    "# list all model images\n",
    "ollama list \n",
    "\n",
    "# run model from image\n",
    "ollama run model_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. <a id='toc8_1_2_'></a>[API on web port](#toc0_)\n",
    "communicatation with local model via web port.\n",
    "\n",
    "`generate` and `chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   456  100   315  100   141    142     64  0:00:02  0:00:02 --:--:--   206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"deepseek-r1:7b\",\"created_at\":\"2025-02-18T02:49:32.744934023Z\",\"response\":\"{\\\"}\\u003cthink\\u003e{\\\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n:\\n\\n{\\n\\n}\\n\\n}\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\",\"done\":false}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"deepseek-r1:7b\",\n",
    "  \"prompt\": \"Who are you?\",\n",
    "  \"stream\": false,\n",
    "  \"options\": {\n",
    "    \"temperature\": 0.6\n",
    "  },\n",
    "  \"format\": \"json\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5034    0  4905  100   129    326      8  0:00:16  0:00:15  0:00:01   719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"deepseek-r1:7b\",\"created_at\":\"2025-02-18T02:49:26.56791501Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\u003cthink\\u003e\\nOkay, so I just read that \\\"Why is the sky blue?\\\" and now I'm trying to figure it out myself. Let me think through this step by step.\\n\\nFirst off, when you look at the sky on a clear day, it's usually blue, especially during the day when the sun is out. But sometimes I've seen it turn other colors too, like red in the evening or during sunrise. So why is it mostly blue?\\n\\nI know that light travels through the atmosphere, but how does it get colored? I remember learning about something called Rayleigh scattering from my science class. Let me try to recall what that was about. Rayleigh scattering involves light interacting with particles much smaller than the wavelength of light itself. When sunlight enters the Earth's atmosphere, it reaches tiny molecules in the air, like nitrogen and oxygen.\\n\\nWait, so these small particles scatter the sunlight in all directions. But why does this result in a blue sky? I think it has something to do with the wavelengths of light. Visible light ranges from violet to red, right? And I remember that violet light has a shorter wavelength than blue. So maybe the shorter wavelengths are scattered more.\\n\\nBut if blue is scattered more, wouldn't it be easier to see at night when there's less atmosphere overhead? Hmm, but then why does the sky turn red in the evening or during sunrise?\\n\\nOh right! During sunrise and sunset, the light has to pass through a much thicker layer of atmosphere. That makes sense because we're looking at the light after it's been scattered through more particles. The longer path means that all the shorter wavelengths (like violet) are scattered out, leaving red to dominate because it has a longer wavelength.\\n\\nSo during the day, when the sun is directly overhead, blue and green wavelengths get scattered away by Rayleigh scattering, making the sky appear blue. But in the early morning or late afternoon, as the sun is near the horizon, the light has to pass through more atmosphere, so red comes through because it's not scattered as much.\\n\\nWait, but isn't there also something called Mie scattering? I think that happens when particles are larger than the wavelength of light. Does that affect the color of the sky too?\\n\\nI believe Mie scattering is more significant for larger particles, like dust or droplets in clouds. So it might cause some effects we see during sunrise and sunset, but not as much as Rayleigh does for small molecules.\\n\\nSo to summarize: The sky appears blue on a clear day because blue light scatters more in the atmosphere due to Rayleigh scattering by tiny gas molecules. During sunrise and sunset, the longer path allows red light to dominate, giving the sky its reddish hues.\\n\\nBut wait, what about when we see other colors? I mean, sometimes during the day it's not just blue; I've seen green or yellow in some places. Is that because of Rayleigh scattering changing as the atmosphere gets thicker?\\n\\nOr maybe it's due to other factors like pollution or particles in the air affecting light differently. That might complicate things.\\n\\nAlso, does humidity play a role? Sometimes when it's humid, does the sky appear clearer instead of blue? I think higher humidity can affect how light scatters because more water vapor means more molecules to scatter off.\\n\\nSo maybe on days with high humidity, the Rayleigh scattering is less effective, making the sky look different. But that's probably a secondary effect compared to the basic reason for the color being blue.\\n\\nIn any case, I think the primary reason is Rayleigh scattering by nitrogen and oxygen in the atmosphere causing shorter wavelengths like blue to scatter more, resulting in a blue sky during clear days.\\n\\u003c/think\\u003e\\n\\nThe sky appears blue primarily due to a phenomenon known as Rayleigh scattering. When sunlight enters Earth's atmosphere, it interacts with tiny molecules of nitrogen and oxygen. These particles scatter shorter wavelengths of light, such as blue and violet, which have shorter wavelengths than red or orange. This scattering is more effective for shorter wavelengths, causing the sky to appear blue during the day.\\n\\nDuring sunrise and sunset, the light passes through a thicker layer of atmosphere, where all shorter wavelengths are scattered away, leaving longer wavelengths like red to dominate, resulting in the reddish hues observed at these times.\\n\\nOther factors such as humidity, pollution, or atmospheric particles can influence how light scatters, but the primary reason for the sky's blue color remains Rayleigh scattering by nitrogen and oxygen molecules.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":15024265818,\"load_duration\":22057058,\"prompt_eval_count\":9,\"prompt_eval_duration\":7000000,\"eval_count\":901,\"eval_duration\":14993000000}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"deepseek-r1:7b\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ],\n",
    "  \"stream\": false\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. <a id='toc8_1_3_'></a>[Python ollama module](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "texts = '''\n",
    "详细比较deepseek母公司和openAI公司的区别\n",
    "'''\n",
    "\n",
    "# model_card = \"deepseek-r1:7b\"\n",
    "model_card = \"modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF\"\n",
    "\n",
    "# 方式一（非流式输出）：\n",
    "# outputs = ollama.generate(model_card, inputs)\n",
    "# print(f'{outputs['response']}')\n",
    "\n",
    "# 方式二（流式输出）：\n",
    "outputs = ollama.generate(\n",
    "    stream=True,\n",
    "    model=model_card,\n",
    "    prompt=texts,\n",
    ")\n",
    "for chunk in outputs:\n",
    "    if not chunk['done']:\n",
    "        print(f'{chunk['response']}', end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3.1. <a id='toc8_1_3_1_'></a>[demo：翻译中文为英文](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，首先我要理解这个题目的意思。“基于深度学习”指的是使用深度学习技术来进行研究。而“对枯草芽胞杆菌芽胞形成相关基因的研究”则是具体的研究内容，涉及到枯草芽胞杆菌在形成芽胞过程中相关的基因。\n",
      "\n",
      "我需要把这整个句子准确地翻译成英文。首先，“基于深度学习”可以直接翻译为“Based on deep learning”。接下来是“研究”，对应的英文是“study”。然后是“枯草芽胞杆菌”，这个应该是一个专有名词，可能需要查一下正确的英译名称，比如“Bacillus subtilis”。\n",
      "\n",
      "接着是“芽胞形成相关基因”，这部分可以翻译为“genes related to spore formation”。最后，把整个句子连贯起来，就是“Based on deep learning study of genes related to spore formation in Bacillus subtilis.”\n",
      "\n",
      "这样组合起来，既准确传达了原意，又符合英文的表达习惯。我觉得这个翻译应该是比较专业和准确的。\n",
      "</think>\n",
      "\n",
      "Study of Genes Related to Spore Formation in *Bacillus subtilis* Based on Deep Learning⚡\n",
      "<think>\n",
      "好的，首先我要理解用户的需求。他给了一个中英对照的句子，要求专业翻译，并且需要将中文句子“通过宏基因组研究微生物与植物相互作用的机制。”准确地翻译成英文。\n",
      "\n",
      "接下来，我需要分析原文的意思。句子的主干是“通过宏基因组研究…”，这里的关键词有“宏基因组”、“微生物”、“植物”以及“相互作用的机制”。所以，首先要确定这些术语在英文中的准确对应词。\n",
      "\n",
      "“宏基因组”通常翻译为“metagenome”或者“meta-genomics”，但更常见的是使用“metagenomics”来表示这一研究领域。因此，这里选择“metagenomics”作为翻译。\n",
      "\n",
      "然后，“通过…研究…”的结构在英文中可以用“through”或者“by means of”来表达，但为了简洁和专业，直接使用“Through”比较合适。\n",
      "\n",
      "接下来是“微生物与植物相互作用的机制”。这里需要注意语序和用词。整体结构应该是“the mechanisms underlying the interactions between microorganisms and plants.” 这样不仅清晰，而且符合学术写作的规范。\n",
      "\n",
      "最后，把这些部分组合起来，确保句子通顺且准确。所以，最终翻译为：“Through metagenomics research on the mechanisms of interaction between microorganisms and plants.”\n",
      "\n",
      "在整个思考过程中，我注意到用户可能是在撰写学术论文或者准备研究报告，因此准确性和专业性是关键。此外，保持句子的简洁也是必要的，以便读者能够快速理解内容。\n",
      "\n",
      "最后，再检查一遍翻译是否忠实于原文，并且符合英语表达习惯。确认无误后，就可以将这个翻译结果提供给用户了。\n",
      "</think>\n",
      "\n",
      "Through metagenomics research on the mechanisms of interaction between microorganisms and plants.⚡\n"
     ]
    }
   ],
   "source": [
    "import ollama \n",
    "\n",
    "\n",
    "class zh2en():\n",
    "    def __init__(self, model_card):\n",
    "        self.model_card = model_card\n",
    "        \n",
    "    def build_prompt(self, texts):\n",
    "        # with open(prompt_template_path, 'r') as f:\n",
    "        #     prompt_template = f.read()\n",
    "        #     # str with replace function\n",
    "        #     prompt = prompt_template.replace(var, texts)\n",
    "        prompt_template = \"\"\"\n",
    "        专业翻译：\\n\n",
    "        ---\\n\n",
    "        {Chinese_words} \\n\n",
    "        --- \\n\n",
    "        作为翻译专家，将上述中文准确翻译为英文。 \\n\n",
    "        \"\"\"\n",
    "        prompt = prompt_template.replace(\"{Chinese_words}\", texts)\n",
    "        return prompt\n",
    "\n",
    "    def translate(self, texts):\n",
    "        prompt = self.build_prompt(texts = texts)\n",
    "        # key step\n",
    "        outputs = ollama.generate(\n",
    "            stream=True,\n",
    "            model=self.model_card,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "        for chunk in outputs:\n",
    "            if not chunk['done']:\n",
    "                print(f'{chunk['response']}', end='', flush=True)\n",
    "            else:\n",
    "                print('⚡')\n",
    "\n",
    "\n",
    "translater = zh2en(model_card='modelscope.cn/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF')\n",
    "\n",
    "translater.translate('基于深度学习的对枯草芽胞杆菌芽胞形成相关基因的研究。')\n",
    "translater.translate('通过宏基因组研究微生物与植物相互作用的机制。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. <a id='toc8_2_'></a>[ktransformers](#toc0_)\n",
    "### 8.2.1. <a id='toc8_2_1_'></a>[DeepSeek-R1_Q4_K_M with ktransformers docker container](#toc0_)\n",
    "\n",
    "[https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md](https://github.com/kvcache-ai/ktransformers-private/blob/main/doc/en/Docker.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the image from docker hub \n",
    "# about 19 GB\n",
    "docker pull approachingai/ktransformers:0.1.1\n",
    "\n",
    "# docker run \\\n",
    "#     --gpus all \\\n",
    "#     -v /path/to/models:/models \\\n",
    "#     -p 10002:10002 \\\n",
    "#     approachingai/ktransformers:v0.1.1 \\\n",
    "#     --port 10002 \\\n",
    "#     --gguf_path /models/path/to/gguf_path \\\n",
    "#     --model_path /models/path/to/model_path \\\n",
    "#     --web True\n",
    "\n",
    "# maybe happen some errors\n",
    "docker run  \\\n",
    "    -v /bmp/backup/zhaosy/ws/ktransformers/models:/models \\\n",
    "    -p 10002:10002 \\\n",
    "    approachingai/ktransformers:0.1.1 \\\n",
    "    --port 10002 \\\n",
    "    --model_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1 \\\n",
    "    --gguf_path /bmp/backup/zhaosy/ProgramFiles/hf/deepseek-ai/DeepSeek-R1-Q4_K_M_GGUF \\\n",
    "    --web True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
