{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [下载和安装](#toc1_)    \n",
    "- 2. [教程](#toc2_)    \n",
    "- 3. [概要](#toc3_)    \n",
    "- 4. [神经网络搭建八股](#toc4_)    \n",
    "- 5. [basic](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[下载和安装](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm-haiku version: 0.0.11\n",
      "Jax version: 0.4.20\n",
      "tensorflow version: 2.10.0\n",
      "tensorflow_datasets version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "print(f\"dm-haiku version: {hk.__version__}\")\n",
    "\n",
    "import jax\n",
    "print(f\"Jax version: {jax.__version__}\")\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "print(f\"tensorflow_datasets version: {tfds.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[教程](#toc0_)\n",
    "```\n",
    "https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html\n",
    "https://zhuanlan.zhihu.com/p/471892075\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[概要](#toc0_)\n",
    "```\n",
    "google:\n",
    "    Tensorflow(Sonnet)\n",
    "    Haiku(JAX)\n",
    "facebook:\n",
    "    Pytorch\n",
    "Microsoftware:\n",
    "    CNTK\n",
    "AWA:\n",
    "    MXnet\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[神经网络搭建八股](#toc0_)\n",
    "```\n",
    "1. 定义网络结构，计算预测值(y_hat);\n",
    "2. 构造loss函数；\n",
    "3. 训练（迭代）：更新权重(w)和偏置(b)。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# 1.定义神经网络结构\n",
    "class Network(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mlp = hk.Sequential([hk.Linear(768, name='hidden1'),\n",
    "                            jax.nn.relu,\n",
    "                            hk.Linear(10, name='hidden2'),\n",
    "                            jax.nn.relu,\n",
    "                            jax.nn.softmax])\n",
    "        logits = mlp(x)\n",
    "        return logits\n",
    "# 2.初始化网络获得params:w,b\n",
    "model = hk.transform(lambda x: Network(name='TestNetwork')(x))\n",
    "params = model.init(rng=rng, x=jnp.ones((256, 28*28), dtype=jnp.float32))\n",
    "# params\n",
    "\n",
    "# 3.定义loss和优化器：自动包括损失函数、反向传播（自动微分、更新权重）\n",
    "def loss(x, params, y):\n",
    "    logits = model.apply(x=x, params=params, rng=None)\n",
    "    cce = jnp.mean(-jnp.sum(jnp.log(logits) * y))\n",
    "    return cce\n",
    "\n",
    "opt = optimizers.sgd(step_size=0.001)\n",
    "opt_state = opt.init_fn(params) # 需要接受网络结构的相关信息\n",
    "# opt_state\n",
    "\n",
    "# 4.准备数据集\n",
    "x_train = jnp.load(\"Minist/mnist_train_x.npy\")\n",
    "y_train = jnp.load(\"Minist/mnist_train_y.npy\")\n",
    "y_train = jax.nn.one_hot(x=y_train, num_classes=len(jnp.unique(y_train))) ## y_trian要做独热编码处理\n",
    "## x y配对并打乱顺序\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_train = tfds.as_numpy(ds_train)\n",
    "\n",
    "# 5.开始训练\n",
    "acc_list = []\n",
    "for epoch in range(100): \n",
    "    # epochs=100，所有数据迭代100次\n",
    "    step = 0\n",
    "    for batch in ds_train:\n",
    "        # 一个整体分为batch_size=256份，迭代256次去更新params\n",
    "        features = batch[0].reshape((-1, 28*28))\n",
    "        features = features/255\n",
    "        labels = batch[1].reshape((-1, 10))\n",
    "        # print(features.shape, labels.shape)\n",
    "\n",
    "        grads = jax.grad(loss, argnums=(1))(features, params, labels)       # argnums=(0,1,2,3)指定需要求导的自变量（这里是params)\n",
    "        opt_state = opt.update_fn(step, grads, opt_state)                   # 更新优化器的opt_state\n",
    "        params = opt.params_fn(opt_state)                                   # 用新的opt_state去生成新的params\n",
    "        step += 1\n",
    "\n",
    "    ## 计算acc\n",
    "    prediction = model.apply(x=x_train.reshape(-1,28*28)/255, params=params, rng=None)\n",
    "    # print(prediction.shape) # (60000, 10)\n",
    "    # print(y_trains) # (60000, 10)\n",
    "    pred_targets = jnp.argmax(prediction, axis=1)           # 返回最大数字的下标(预测)\n",
    "    y_targets = jnp.argmax(y_train, axis=1)                 # 返回最大数字的下标(真实)\n",
    "    ok = jnp.sum(pred_targets == y_targets)                 # 比较下表是否一致，是则ok否则不ok\n",
    "    acc = jnp.divide(ok, y_train.shape[0])                  # ok的占比\n",
    "    acc_list.append(acc)\n",
    "    print(acc)\n",
    "\n",
    "    ## 动态绘图\n",
    "    if epoch %10 == 0:\n",
    "        plt.clf()\n",
    "        plt.plot(acc_list)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('acc')\n",
    "        plt.pause(0.000001)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# 定义神经网络结构\n",
    "class Network(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mlp = hk.Sequential([hk.Linear(768, name='hidden1'),\n",
    "                       jax.nn.relu,\n",
    "                       hk.Linear(10, name='hidden2'),\n",
    "                       jax.nn.relu,\n",
    "                       jax.nn.softmax])\n",
    "        logits = mlp(x)\n",
    "        return logits\n",
    "# 初始化网络获得params:w,b\n",
    "model = hk.transform(lambda x: Network(name='TestNetwork')(x))\n",
    "params = model.init(rng=rng, x=jnp.ones((256, 28*28), dtype=jnp.float32))\n",
    "# params\n",
    "\n",
    "# 定义loss和优化器：自动包括损失函数、反向传播（自动微分、更新权重）\n",
    "def loss(x, params, y):\n",
    "    logits = model.apply(x=x, params=params, rng=None)\n",
    "    cce = jnp.mean(-jnp.sum(jnp.log(logits) * y))\n",
    "    return cce\n",
    "\n",
    "opt = optimizers.sgd(step_size=0.001)\n",
    "opt_state = opt.init_fn(params) # 需要接受网络结构的相关信息\n",
    "# opt_state\n",
    "\n",
    "@jax.jit\n",
    "def update(step, params, features, labels, opt_state):\n",
    "    grads = jax.grad(loss, argnums=(1))(features, params, labels)       # argnums=(0,1,2,3)指定需要求导的自变量（这里是params)\n",
    "    opt_state = opt.update_fn(step, grads, opt_state)                   # 更新优化器的opt_state\n",
    "    params = opt.params_fn(opt_state)    \n",
    "    return opt_state, params                               # 用新的opt_state去生成新的params\n",
    "\n",
    "# 准备数据集\n",
    "x_train = jnp.load(\"../Minist/mnist_train_x.npy\")\n",
    "y_train = jnp.load(\"../Minist/mnist_train_y.npy\")\n",
    "## y_trian要做独热编码处理\n",
    "y_train = jax.nn.one_hot(x=y_train, num_classes=len(jnp.unique(y_train)))\n",
    "## x y配对并打乱顺序\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_train = tfds.as_numpy(ds_train)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(100):\n",
    "    step = 0\n",
    "    for batch in ds_train:\n",
    "        features = batch[0].reshape((-1, 28*28))\n",
    "        features = features/255\n",
    "        labels = batch[1].reshape((-1, 10))\n",
    "        # print(features[0])\n",
    "        # print(features.shape, labels.shape)\n",
    "\n",
    "        opt_state, params = update(step, params, features, labels, opt_state)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # 计算acc\n",
    "    prediction = model.apply(x=x_train.reshape(-1,28*28)/255, params=params, rng=None)\n",
    "    # print(prediction.shape) # (60000, 10)\n",
    "    # print(y_trains) # (60000, 10)\n",
    "    pred_targets = jnp.argmax(prediction, axis=1) # 返回最大数字的下标(预测)\n",
    "    y_targets = jnp.argmax(y_train, axis=1) # 返回最大数字的下标(真实)\n",
    "\n",
    "    ok = jnp.sum(pred_targets == y_targets) # 比较下表是否一致，是则ok否则不ok\n",
    "    acc = jnp.divide(ok, y_train.shape[0]) # ok的占比\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    # 动态绘图\n",
    "    if epoch % 10 == 0:\n",
    "        print(acc)\n",
    "        plt.clf()\n",
    "        plt.plot(acc_list)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('acc')\n",
    "        plt.pause(0.000001)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[basic](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619]],      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "def forward(x):\n",
    "    mlp = hk.nets.MLP([300, 100, 10])\n",
    "    return mlp(x)\n",
    "forward = hk.transform(forward)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "x = jnp.ones([8, 28*28])\n",
    "\n",
    "params = forward.init(rng, x)\n",
    "logits = forward.apply(params, rng, x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619]],      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@hk.transform\n",
    "def forward2(x):\n",
    "    mlp = hk.nets.MLP([300, 100, 10])\n",
    "    return mlp(x)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "x = jnp.ones([8, 28*28])\n",
    "\n",
    "params2 = forward2.init(rng, x)\n",
    "logits = forward2.apply(params2, rng, x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619],\n",
       "       [ 0.45413703, -0.3273952 , -0.36023346,  0.58912265,  0.24682917,\n",
       "        -0.08996333, -0.01644108,  0.06207579, -0.49413946, -0.07068619]],      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Forward3(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mlp = hk.nets.MLP([300, 100, 10])\n",
    "        return mlp(x)\n",
    "    \n",
    "rng = jax.random.PRNGKey(42)\n",
    "x = jnp.ones([8, 28*28])\n",
    "\n",
    "forward3 = hk.transform(lambda x: Forward3()(x))\n",
    "params3 = forward3.init(rng, x)\n",
    "logits = forward3.apply(params3, rng, x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkStation\\PyhtonWorkStation\\SmallTools\\Library\\004-ML&DL\\LearnAlphaFlod\\MLP-Frame\\LearnHaiKu.ipynb 单元格 14\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnHaiKu.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m train_data \u001b[39m=\u001b[39m load_mnist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnHaiKu.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_data:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnHaiKu.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     image \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mreshape(batch[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mastype(jnp\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m, (batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnHaiKu.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     label \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mone_hot(batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m10\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkStation/PyhtonWorkStation/SmallTools/Library/004-ML%26DL/LearnAlphaFlod/MLP-Frame/LearnHaiKu.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# 执行训练步骤\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 定义简单的全连接神经网络\n",
    "class SimpleNN(hk.Module):\n",
    "    def __init__(self, hidden_size=64, output_size=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = hk.Linear(self.hidden_size)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = hk.Linear(self.output_size)(x)\n",
    "        return x\n",
    "\n",
    "# 定义训练步骤\n",
    "def train_step(params, opt_state, batch):\n",
    "    def loss(params, batch):\n",
    "        logits = model.apply(params, batch[\"image\"])\n",
    "        return jnp.mean(jax.nn.softmax_cross_entropy_with_logits(logits=logits, labels=batch[\"label\"]))\n",
    "\n",
    "    grads = jax.grad(loss)(params, batch)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state\n",
    "\n",
    "# 加载 MNIST 数据\n",
    "def load_mnist():\n",
    "    # 使用 TensorFlow Datasets 加载 MNIST 数据集\n",
    "    ds = tfds.load(\"mnist\", split=\"train\", shuffle_files=True, as_supervised=True)\n",
    "    ds = ds.batch(64)\n",
    "    return tfds.as_numpy(ds)\n",
    "\n",
    "# 初始化 Haiku 模型、参数和优化器\n",
    "model = hk.transform(SimpleNN)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "input_shape = (64, 28 * 28)  # Batch size of 64, flattened image shape (28 * 28)\n",
    "params = model.init(rng, jnp.ones(input_shape))\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(5):\n",
    "    # 加载和预处理 MNIST 数据\n",
    "    train_data = load_mnist()\n",
    "\n",
    "    for batch in train_data:\n",
    "        image = jnp.reshape(batch[\"image\"].astype(jnp.float32) / 255.0, (batch[\"image\"].shape[0], -1))\n",
    "        label = jax.nn.one_hot(batch[\"label\"], 10)\n",
    "\n",
    "        # 执行训练步骤\n",
    "        params, opt_state = train_step(params, opt_state, {\"image\": image, \"label\": label})\n",
    "\n",
    "    # 打印训练损失\n",
    "    logits = model.apply(params, image)\n",
    "    loss_value = jnp.mean(jax.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label))\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss_value}\")\n",
    "\n",
    "# 推断\n",
    "test_data = tfds.load(\"mnist\", split=\"test\", shuffle_files=True, as_supervised=True).batch(10)\n",
    "test_batch = next(tfds.as_numpy(test_data))\n",
    "test_image = jnp.reshape(test_batch[0].astype(jnp.float32) / 255.0, (test_batch[0].shape[0], -1))\n",
    "\n",
    "# 使用训练好的模型进行推断\n",
    "predictions = jax.nn.softmax(model.apply(params, test_image))\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
